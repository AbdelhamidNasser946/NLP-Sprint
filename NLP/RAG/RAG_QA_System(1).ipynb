{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "KxrVHsB6qqEx"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain chromadb faiss-cpu pypdf transformers sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community faiss-cpu chromadb pypdf transformers sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xRiILugeq0Np",
        "outputId": "c4a54c55-0689-4b24-c21c-07634fc086b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U :class:`~langchain-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRMsclr31S9n",
        "outputId": "627b848b-eb68-43cd-817d-68d653be8875"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: unexpected EOF while looking for matching ``'\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "qu3Ds_W9rvuE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '/content/AI2.pdf'\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6mlUOXorPvO",
        "outputId": "095c1241-557c-46a5-ce52-9e5556a0499e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Fdyh0_yisS-y",
        "outputId": "f013e086-8282-4d3d-ded8-181b00319165"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 0, 'page_label': '1'}, page_content=''),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 1, 'page_label': '2'}, page_content='Artificial Intelligence \\nIlluminated\\nBen Coppin\\nJONES AND BARTLETT PUBLISHERS'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 2, 'page_label': '3'}, page_content='Copyright © 2004 by Jones and Bartlett Publishers, Inc.\\nCover image © Photodisc\\nLibrary of Congress Cataloging-in-Publication Data\\nCoppin, Ben.\\nArtificial intelligence illuminated / by Ben Coppin.--1\\nst ed.\\np. cm.\\nIncludes bibliographical references and index.\\nISBN 0-7637-3230-3\\n1. Artificial intelligence. I. Title.\\nQ335.C586  2004\\n006.3--dc22\\n2003020604\\n3382\\nAll rights reserved. No part of the material protected by this copyright notice may be\\nreproduced or utilized in any form, electronic or mechanical, including photocopying,\\nrecording, or any information storage or retrieval system, without written permission\\nfrom the copyright owner.\\nAcquisitions Editor: Stephen Solomon\\nProduction Manager: Amy Rose\\nMarketing Manager: Matthew Bennett\\nEditorial Assistant: Caroline Senay\\nManufacturing Buyer: Therese Bräuer\\nCover Design: Kristin E. Ohlin\\nT ext Design: Kristin E. Ohlin\\nComposition: Northeast Compositors\\nT echnical Artist: George Nichols\\nPrinting and Binding: Malloy, Inc.\\nCover Printing: Malloy, Inc.\\nPrinted in the United States of America\\n08  07  06  05  04        10  9  8  7  6  5  4  3  2  1\\nWorld Headquarters\\nJones and Bartlett Publishers \\n40 Tall Pine Drive\\nSudbury, MA 01776\\n978-443-5000\\ninfo@jbpub.com\\nwww.jbpub.com\\nJones and Bartlett Publishers \\nCanada\\n2406 Nikanna Road\\nMississauga, ON L5C 2W6\\nCANADA\\nJones and Bartlett Publishers \\nInternational \\nBarb House, Barb Mews\\nLondon W6 7PA\\nUK'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 3, 'page_label': '4'}, page_content='For Erin'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 4, 'page_label': '5'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 5, 'page_label': '6'}, page_content='Preface\\nWho Should Read This Book\\nThis book is intended for students of computer science at the college level,\\nor students of other subjects that cover Artificial Intelligence. It also is\\nintended to be an interesting and relevant introduction to the subject for\\nother students or individuals who simply have an interest in the subject.\\nThe book assumes very little knowledge of computer science, but does\\nassume some familiarity with basic concepts of algorithms and computer\\nsystems. Data structures such as trees, graphs, and stacks are explained\\nbriefly in this book, but if you do not already have some familiarity with\\nthese concepts, you should probably seek out a suitable book on algorithms\\nor data structures.\\nIt would be an advantage to have some experience in a programming lan-\\nguage such as C++ or Java, or one of the languages commonly used in Arti-\\nficial Intelligence research, such as PROLOG and LISP , but this experience\\nis neither necessary nor assumed.\\nMany of the chapters include practical exercises that require the reader to\\ndevelop an algorithm or program in a programming language of his or her\\nchoice. Most readers should have no difficulty with these exercises. How-\\never, if any reader does not have the necessary skills he or she simply should\\ndescribe in words (or in pseudocode) how his or her programs work, giving\\nas much detail as possible.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 6, 'page_label': '7'}, page_content='How to Read This Book\\nThis book can be read in several ways. Some readers will choose to read the\\nchapters through in order from Chapter 1 through Chapter 21. Any chapter\\nthat uses material which is presented in another chapter gives a clear refer-\\nence to that chapter, and readers following the book from start to finish\\nshould not need to jump forward at any point, as the chapter dependencies\\ntend to work in a forward direction.\\nAnother perfectly reasonable way to use this book is as a reference. When a\\nreader needs to know more about a particular subject, he or she can pick up\\nthis book and select the appropriate chapter or chapters, and can be illumi-\\nnated on the subject (at least, that is the author’s intent!)\\nChapter 12 contains a diagram that shows how the dependencies between\\nchapters work (Section 12.6.2). This diagram shows, for example, that if a\\nreader wants to read Chapter 8, it would be a good idea to already have read\\nChapter 7.\\nThis book is divided into six parts, each of which is further divided into a\\nnumber of chapters. The chapters are laid out as follows:\\nPart 1: Introduction to Artificial Intelligence\\nChapter 1: A Brief History of Artificial Intelligence\\nChapter 2: Uses and Limitations\\nChapter 3: Knowledge Representation \\nPart 2: Search\\nChapter 4: Search Methodologies \\nChapter 5: Advanced Search\\nChapter 6: Game Playing\\nPart 3: Logic\\nChapter 7: Propositional and Predicate Logic\\nChapter 8: Inference and Resolution for Problem Solving\\nChapter 9: Rules and Expert Systems\\nvi Preface'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 7, 'page_label': '8'}, page_content='Part 4: Machine Learning \\nChapter 10: Introduction to Machine Learning\\nChapter 11: Neural Networks\\nChapter 12: Probabilistic Reasoning and Bayesian Belief Networks \\nChapter 13: Artificial Life: Learning through Emergent Behavior \\nChapter 14: Genetic Algorithms\\nPart 5: Planning \\nChapter 15: Introduction to Planning\\nChapter 16: Planning Methods\\nPart 6: Advanced Topics \\nChapter 17: Advanced Knowledge Representation \\nChapter 18: Fuzzy Reasoning\\nChapter 19: Intelligent Agents\\nChapter 20: Understanding Language\\nChapter 21: Machine Vision\\nEach chapter includes an introduction that explains what the chapter cov-\\ners, a summary of the chapter, some exercises and review questions, and\\nsome suggestions for further reading. There is a complete bibliography at\\nthe back of the book.\\nThis book also has a glossary, which includes a brief definition of most of\\nthe important terms used in this book. When a new term is introduced in\\nthe text it is highlighted in bold, and most of these words are included in\\nthe glossary. The only such terms that are not included in the glossary are\\nthe ones that are defined in the text, but that are not used elsewhere in the\\nbook.\\nThe use of third person pronouns is always a contentious issue for authors\\nof text books, and this author has chosen to use he and she interchangeably.\\nIn some cases the word “he” is used, and in other cases “she. ” This is not\\nintended to follow any particular pattern, or to make any representations\\nabout the genders, but simply is in the interests of balance.\\nPreface vii'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 8, 'page_label': '9'}, page_content='The first few chapters of this book provide introductory material, explain-\\ning the nature of Artificial Intelligence and providing a historical back-\\nground, as well as describing some of the connections with other\\ndisciplines. Some readers will prefer to skip these chapters, but it is advis-\\nable to at least glance through Chapter 3 to ensure that you are familiar\\nwith the concepts of that chapter, as they are vital to the understanding of\\nmost of the rest of the book.\\nAcknowledgments\\nAlthough I wrote this book single-handedly, it was not without help. I\\nwould like to thank, in chronological order, Frank Abelson; Neil Salkind\\nand everyone at Studio B; Michael Stranz, Caroline Senay, Stephen\\nSolomon, and Tracey Chapman at Jones & Bartlett; also a number of peo-\\nple who read chapters of the book: Martin Charlesworth, Patrick Coyle,\\nPeter and Petra Farrell, Robert Kealey, Geoffrey Price, Nick Pycraft, Chris\\nSwannack, Edwin Y oung, my parents, T ony and Frances, and of course\\nErin—better late than never.\\nThanks also to:\\nThe MIT Press for the excerpt from ‘Learning in Multiagent Systems’ by\\nSandip Sen and Gerhard Weiss, © 2001, The MIT Press.\\nThe MIT Press for the excerpt from ‘Adaptation in Natural and Artificial\\nSystems’ by John H. Holland, © 1992, The MIT Press.\\nThe MIT Press for the excerpt from ‘The Artificial Life Roots of Artificial\\nIntelligence’ by Luc Steels, © 1994, the Massachusetts Institute of T echnol-\\nogy.\\nThe IEEE for the excerpt from ‘Steps T owards Artificial Intelligence’ by\\nMarvin Minsky, © 2001, IEEE.\\nI have attempted to contact the copyright holders of all copyrighted quotes\\nused in this book. If I have used any quotes without permission, then this\\nwas inadvertent, and I apologize. I will take all measures possible to rectify\\nthe situation in future printings of the book.\\nviii Preface'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 9, 'page_label': '10'}, page_content='Contents\\nPreface v\\nP A R T  1 Introduction to Artificial Intelligence 1\\nChapter 1 A Brief History of Artificial Intelligence 3\\n1.1 Introduction 3\\n1.2 What Is Artificial Intelligence? 4\\n1.3 Strong Methods and Weak Methods 5\\n1.4 From Aristotle to Babbage 6\\n1.5 Alan Turing and the 1950s 7\\n1.6 The 1960s to the 1990s 9\\n1.7 Philosophy 10\\n1.8 Linguistics 11\\n1.9 Human Psychology and Biology 12\\n1.10 All Programming Languages 12\\n1.10.1 PROLOG 13\\n1.10.2 LISP 14\\n1.11 Chapter Summary 15\\n1.12 Review Questions 16\\n1.13 Further Reading 17'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 10, 'page_label': '11'}, page_content='Chapter 2 Uses and Limitations 19\\n2.1 Introduction 19\\n2.2 The Chinese Room 20\\n2.3 HAL—Fantasy or Reality? 21\\n2.4 AI in the 21\\nst Century 23\\n2.5 Chapter Summary 24\\n2.6 Review Questions 24\\n2.7 Further Reading 25\\nChapter 3 Knowledge Representation 27\\n3.1 Introduction 27\\n3.2 The Need for a Good Representation 28\\n3.3 Semantic Nets 29\\n3.4 Inheritance 31\\n3.5 Frames 32\\n3.5.1 Why Are Frames Useful? 34\\n3.5.2 Inheritance 34\\n3.5.3 Slots as Frames 35\\n3.5.4 Multiple Inheritance 36\\n3.5.5 Procedures 37\\n3.5.6 Demons 38\\n3.5.7 Implementation 38\\n3.5.8 Combining Frames with Rules 40\\n3.5.9 Representational Adequacy 40\\n3.6 Object-Oriented Programming 41\\n3.7 Search Spaces 42\\n3.8 Semantic Trees 44\\n3.9 Search Trees 46\\n3.9.1 Example 1: Missionaries and Cannibals 47\\n3.9.2 Improving the Representation 49\\n3.9.3 Example 2: The Traveling Salesman 50\\n3.9.4 Example 3: The T owers of Hanoi 54\\n3.9.5 Example 4: Describe and Match 56\\n3.10 Combinatorial Explosion 57\\n3.11 Problem Reduction 57\\nx Contents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 11, 'page_label': '12'}, page_content='3.12 Goal Trees 58\\n3.12.1 T op Down or Bottom Up? 60\\n3.12.2 Uses of Goal Trees 61\\nExample 1: Map Coloring\\nExample 2: Proving Theorems \\nExample 3: Parsing Sentences 63\\nExample 4: Games\\n3.13 Chapter Summary 64\\n3.14 Review Questions 65\\n3.15 Exercises 65\\n3.16 Further Reading 66\\nP A R T  2 Search 69\\nChapter 4 Search Methodologies 71\\n4.1 Introduction 71\\n4.2 Problem Solving as Search 72\\n4.3 Data-Driven or Goal-Driven Search 73\\n4.4 Generate and T est 74\\n4.5 Depth-First Search 75\\n4.6 Breadth-First Search 76\\n4.7 Properties of Search Methods 78\\n4.7.1 Complexity 78\\n4.7.2 Completeness 79\\n4.7.3 Optimality 79\\n4.7.4 Irrevocability 80\\n4.8 Why Humans Use Depth-First Search? 80\\n4.8.1 Example 1: Traversing a Maze 81\\n4.8.2 Example 2: Searching for a Gift 81\\n4.9 Implementing Depth-First and Breadth-First Search 83\\n4.10 Example: Web Spidering 88\\n4.11 Depth-First Iterative Deepening 88\\n4.12 Using Heuristics for Search 90\\n4.12.1 Informed and Uninformed Methods 91\\n4.12.2 Choosing a Good Heuristic 92\\n4.12.3 The 8-Puzzle 92\\nContents xi'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 12, 'page_label': '13'}, page_content='4.12.4 Monotonicity 95\\n4.12.5 Example: The Modified Traveling Salesman \\nProblem 96\\n4.13 Hill Climbing 98\\n4.13.1 Steepest Ascent Hill Climbing 98\\n4.13.2 Foothills, Plateaus, and Ridges 101\\n4.14 Best-First Search 104\\n4.15 Beam Search 106\\n4.16 Identifying Optimal Paths 107\\n4.16.1 A* Algorithms 108\\n4.16.2 Uniform Cost Search 110\\n4.16.3 Greedy Search 111\\n4.16.4 Example: The Knapsack Problem 111\\n4.17 Chapter Summary 113\\n4.18 Review Questions 114\\n4.19 Exercises 115\\n4.20 Further Reading 116\\nChapter 5 Advanced Search 117\\n5.1 Introduction 117\\n5.2 Constraint Satisfaction Search 118\\n5.3 Forward Checking 121\\n5.4 Most-Constrained Variables 121\\n5.5 Example: Cryptographic Problems 122\\n5.6 Heuristic Repair 123\\n5.7 Combinatorial Optimization Problems 125\\n5.8 Local Search and Metaheuristics 126\\n5.8.1 Exchanging Heuristics 126\\n5.8.2 Iterated Local Search 127\\n5.8.3 Tabu Search 127\\n5.8.4 Ant Colony Optimization 128\\n5.9 Simulated Annealing 128\\n5.9.1 Uses of Simulated Annealing 130\\n5.10 Genetic Algorithms for Search 131\\n5.11 Real-Time A* 131\\nxii Contents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 13, 'page_label': '14'}, page_content='5.12 Iterative-Deepening A* (IDA*) 132\\n5.13 Parallel Search 132\\n5.13.1 Task Distribution 134\\n5.13.2 Tree Ordering 135\\n5.13.3 Search Engines 135\\n5.14 Bidirectional Search 136\\n5.15 Nondeterministic Search 136\\n5.16 Island-Driven Search 137\\n5.17 Nonchronological Backtracking 137\\n5.18 Chapter Summary 138\\n5.19 Review Questions 139\\n5.20 Exercises 140\\n5.21 Further Reading 141\\nChapter 6 Game Playing 143\\n6.1 Introduction 143\\n6.2 Game Trees 144\\n6.2.1 Rationality, Zero Sum, and Other \\nAssumptions 145\\n6.2.2 Evaluation Functions 146\\n6.2.3 Searching Game Trees 148\\n6.3 Minimax 149\\n6.3.1 Bounded Lookahead 151\\n6.4 Alpha-Beta Pruning 153\\n6.4.1 The Effectiveness of Alpha-Beta Pruning 154\\n6.4.2 Implementation 155\\n6.5 Checkers 159\\n6.5.1 Chinook 160\\n6.5.2 Chinook’s Databases 161\\n6.5.3 Chinook’s Evaluation Function 162\\n6.5.4 Forward Pruning 163\\n6.5.5 Limitations of Minimax 163\\n6.5.6 Blondie 24 164\\n6.6 Chess 164\\nContents xiii'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 14, 'page_label': '15'}, page_content='6.7 Go 165\\n6.7.1 Go-Moku 166\\n6.8 Othello (Reversi) 166\\n6.9 Games of Chance 166\\n6.9.1 Expectiminimax 167\\n6.10 Chapter Summary 167\\n6.11 Review Questions 168\\n6.12 Exercises 169\\n6.13 Further Reading 170\\nP A R T  3 Knowledge Representation and Automated \\nReasoning 173\\nChapter 7 Propositional and Predicate Logic 175\\n7.1 Introduction 175\\n7.2 What Is Logic? 176\\n7.3 Why Logic Is Used in Artificial Intelligence 176\\n7.4 Logical Operators 177\\n7.5 Translating between English and Logic Notation 178\\n7.6 Truth Tables 181\\n7.6.1 Not 181\\n7.6.2 And 182\\n7.6.3 Or 182\\n7.6.4 Implies 183\\n7.6.5 iff 184\\n7.7 Complex Truth Tables 184\\n7.8 Tautology 186\\n7.9 Equivalence 187\\n7.10 Propositional Logic 189\\n7.10.1 Syntax 189\\n7.10.2 Semantics 190\\n7.11 Deduction 191\\n7.11.1 ^-Introduction 191\\n7.11.2 ^-Eliminations 191\\n7.11.3 Or-Introduction 192\\n7.11.4 ?Elimination 192\\nxiv Contents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 15, 'page_label': '16'}, page_content='7.11.5 Reductio Ad Absurdum 192\\n7.11.6 ?Introduction 193\\n7.11.7 ¬¬Elimination 193\\n7.11.8 Example 1 193\\n7.11.9 Example 2 194\\n7.11.10 Example 3 194\\n7.11.11 Example 4 195\\n7.12 The Deduction Theorem 195\\n7.13 Predicate Calculus 196\\n7.13.1 Syntax 196\\n7.13.2 Relationships between \" and $ 197\\n7.13.3 Functions 199\\n7.14 First-Order Predicate Logic 199\\n7.15 Soundness 200\\n7.16 Completeness 200\\n7.17 Decidability 200\\n7.18 Monotonicity 201\\n7.19 Abduction and Inductive Reasoning 201\\n7.20 Modal Logics and Possible Worlds 203\\n7.20.1 Reasoning in Modal Logic 204\\n7.21 Dealing with Change 205\\n7.22 Chapter Summary 205\\n7.23 Review Questions 205\\n7.24 Exercises 206\\n7.25 Further Reading 208\\nChapter 8 Inference and Resolution for Problem Solving 209\\n8.1 Introduction 209\\n8.2 Resolution in Propositional Logic 210\\n8.2.1 Normal Forms 210\\n8.2.2 The Resolution Rule 212\\n8.2.3 Resolution Refutation 213\\n8.2.4 Proof by Refutation 214\\n8.3 Applications of Resolution 216\\n8.4 Resolution in Predicate Logic 218\\nContents xv'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 16, 'page_label': '17'}, page_content='8.5 Normal Forms for Predicate Logic 219\\n8.6 Skolemization 220\\n8.6.1 Example of Skolemization 221\\n8.6.2 Second Example of Skolemization 222\\n8.6.3 Unification 222\\n8.6.4 Most General Unifiers 224\\n8.6.5 Unification Algorithm 224\\n8.6.6 Unification Example 225\\n8.7 Resolution Algorithm 226\\n8.8 Horn Clauses and PROLOG 227\\n8.9 Herbrand Universes 229\\n8.9.1 The Herbrand Base 230\\n8.9.2 Herbrand Interpretations 231\\n8.9.3 Example 232\\n8.10 Resolution for Problem Solving 233\\n8.11 Chapter Summary 237\\n8.12 Review Questions 238\\n8.13 Exercises 238\\n8.14 Further Reading 239\\nChapter 9 Rules and Expert Systems 241\\n9.1 Introduction 241\\n9.2 Rules for Knowledge Representation 242\\n9.3 Rule-Based Systems 243\\n9.3.1 Forward Chaining 244\\n9.3.2 Conflict Resolution 246\\n9.3.3 Meta Rules 247\\n9.3.4 Backward Chaining 248\\n9.3.5 Comparing Forward and Backward Chaining 249\\n9.4 Rule-Based Expert Systems 251\\n9.4.1 The People Involved in an Expert System 251\\n9.4.2 Architecture of an Expert System 252\\n9.4.3 The Expert Shell System 253\\n9.4.4 The Rete Algorithm 253\\n9.4.5 Knowledge Engineering 254\\n9.5 CLIPS (C Language Integrated Production System) 255\\nxvi Contents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 17, 'page_label': '18'}, page_content='9.6 Backward Chaining in Rule-Based Expert Systems 257\\n9.7 CYC 259\\n9.8 Chapter Summary 260\\n9.9 Review Questions 261\\n9.10 Exercises 261\\n9.11 Further Reading 261\\nP A R T  4 Machine Learning 265\\nChapter 10 Introduction to Machine Learning 267\\n10.1 Introduction 267\\n10.2 Training 268\\n10.3 Rote Learning 270\\n10.4 Learning Concepts 270\\n10.5 General-to-Specific Ordering 272\\n10.5.1 A Simple Learning Algorithm 273\\n10.6 V ersion Spaces 274\\n10.7 Candidate Elimination 275\\n10.8 Inductive Bias 276\\n10.9 Decision-Tree Induction 276\\n10.9.1 Information Gain 278\\n10.9.2 Example 279\\n10.9.3 Inductive Bias of ID3 281\\n10.10 The Problem of Overfitting 282\\n10.11 The Nearest Neighbor Algorithm 283\\n10.12 Learning Neural Networks 284\\n10.13 Supervised Learning 285\\n10.14 Unsupervised Learning 285\\n10.15 Reinforcement Learning 286\\n10.16 Chapter Summary 286\\n10.17 Review Questions 287\\n10.18 Exercises 288\\n10.19 Further Reading 288\\nChapter 11 Neural Networks 291\\n11.1 Introduction 291\\n11.2 Neurons 292\\nContents xvii'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 18, 'page_label': '19'}, page_content='11.2.1 Biological Neurons 292\\n11.2.2 Artificial Neurons 293\\n11.3 Perceptrons 295\\n11.4 Multilayer Neural Networks 300\\n11.4.1 Backpropagation 302\\n11.4.2 Improving the Performance of\\nBackpropagation 305\\n11.5 Recurrent Networks 306\\n11.5.1 Hopfield Networks 307\\n11.5.2 Bidirectional Associative Memories (BAMs) 314\\n11.6 Unsupervised Learning Networks 317\\n11.6.1 Kohonen Maps 317\\n11.6.2 Kohonen Map Example 319\\n11.6.3 Hebbian Learning 321\\n11.7 Evolving Neural Networks 322\\n11.8 Chapter Summary 323\\n11.9 Review Questions 324\\n11.10 Exercises 325\\n11.11 Further Reading 326\\nChapter 12 Probabilistic Reasoning and Bayesian Belief \\nNetworks 327\\n12.1 Introduction 327\\n12.2 Probabilistic Reasoning 328\\n12.3 Joint Probability Distributions 330\\n12.4 Bayes’ Theorem 330\\n12.4.1 Example: Medical Diagnosis 331\\n12.4.2 Example: Witness Reliability 332\\n12.4.3 Comparing Conditional Probabilities 334\\n12.4.4 Normalization 335\\n12.5 Simple Bayesian Concept Learning 337\\n12.6 Bayesian Belief Networks 339\\n12.6.1 Example: Life at College 342\\n12.6.2 Example: Chapter Dependencies 346\\n12.7 The Noisy-V Function 346\\nxviii Contents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 19, 'page_label': '20'}, page_content='12.8 Bayes’ Optimal Classifier 349\\n12.9 The Naïve Bayes Classifier 351\\n12.10 Collaborative Filtering 356\\n12.11 Chapter Summary 357\\n12.12 Review Questions 358\\n12.13 Exercises 359\\n12.14 Further Reading 359\\nChapter 13 Artificial Life: Learning through Emergent \\nBehavior 363\\n13.1 Introduction 363\\n13.2 What Is Life? 364\\n13.3 Emergent Behavior 365\\n13.4 Finite State Automata 366\\n13.5 Cellular Automata 368\\n13.5.1 Conway’s Life 368\\n13.5.2 One-Dimensional Cellular Automata 370\\n13.6 Self-Reproducing Systems 371\\n13.7 Evolution 372\\n13.7.1 Ramps 373\\n13.8 Evolution Strategies 373\\n13.9 Genetic Programming 374\\n13.10 Evolutionary Programming 375\\n13.11 L-Systems 376\\n13.12 Classifier Systems 377\\n13.13 Artificial Immune Systems 381\\n13.14 Chapter Summary 382\\n13.15 Review Questions 382\\n13.16 Further Reading 383\\nChapter 14 Genetic Algorithms 387\\n14.1 Introduction 387\\n14.2 Representations 388\\n14.3 The Algorithm 389\\n14.4 Fitness 390\\nContents xix'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 20, 'page_label': '21'}, page_content='14.5 Crossover 390\\n14.6 Mutation 392\\n14.7 T ermination Criteria 392\\n14.8 Optimization of a Mathematic Function 393\\n14.9 Why Genetic Algorithms Work 396\\n14.9.1 Schemata 397\\n14.9.2 How Reproduction Affects Schemata 399\\n14.9.3 How Mutation and Crossover Affect \\nSchemata 401\\n14.9.4 The Building-Block Hypothesis 403\\n14.9.5 Deception 404\\n14.10 Messy Genetic Algorithms 405\\n14.11 Prisoner’s Dilemma 406\\n14.11.1 Strategy Representation 407\\n14.11.2 Possible Strategies 408\\n14.11.3 Evolution of Strategies 410\\n14.11.4 Choice of Opponents 410\\n14.12 Diversity 411\\n14.13 Evolving Pictures 412\\n14.14 Predators and Coevolution 413\\n14.15 Other Problems 414\\n14.16 Chapter Summary 414\\n14.17 Review Questions 415\\n14.18 Exercises 416\\n14.19 Further Reading 417\\nP A R T  5 Planning 419\\nChapter 15 Introduction to Planning 421\\n15.1 Introduction 421\\n15.2 Planning as Search 423\\n15.3 Situation Calculus 426\\n15.4 The Frame Problem 427\\n15.5 Means-Ends Analysis 428\\n15.6 Chapter Summary 430\\nxx Contents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 21, 'page_label': '22'}, page_content='15.7 Review Questions 431\\n15.8 Exercises 431\\n15.9 Further Reading 432\\nChapter 16 Planning Methods 433\\n16.1 Introduction 433\\n16.2 STRIPS 434\\n16.2.1 Planning and Executing 435\\n16.2.2 Operators 436\\n16.2.3 Implementation of STRIPS 437\\n16.2.4 Example: STRIPS 438\\n16.2.5 Example: STRIPS and Resolution 441\\n16.3 The Sussman Anomaly 443\\n16.4 Partial Order Planning 444\\n16.5 The Principle of Least Commitment 447\\n16.6 Propositional Planning 448\\n16.7 SAT Planning 450\\n16.8 Planning Graphs 451\\n16.8.1 GraphPlan 454\\n16.8.2 Mutex Conditions 455\\n16.9 ADL and PDDL 455\\n16.10 Probabilistic Planning 456\\n16.11 Dynamic World Planning 456\\n16.12 Case-Based Planning Systems 457\\n16.13 Planning and Scheduling 458\\n16.14 Chapter Summary 459\\n16.15 Review Questions 460\\n16.16 Exercises 461\\n16.17 Further Reading 461\\nP A R T  6 Advanced Topics 463\\nChapter 17 Advanced Knowledge Representation 465\\n17.1 Introduction 465\\n17.2 Representations and Semantics 468\\nContents xxi'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 22, 'page_label': '23'}, page_content='17.3 The Blackboard Architecture 469\\n17.3.1 Implementation 471\\n17.3.2 HEARSAY 472\\n17.4 Scripts 472\\n17.5 Copycat Architecture 474\\n17.6 Nonmonotonic Reasoning 476\\n17.6.1 Nonmonotonic Logic with the Modal \\nOperator 477\\n17.6.2 Default Reasoning 477\\n17.6.3 Truth Maintenance Systems 478\\n17.6.4 Closed-World Assumption 480\\n17.6.5 The Ramification Problem 480\\n17.6.6 Circumscription 480\\n17.6.7 Abductive Reasoning 482\\n17.6.8 The Dempster-Shafer Theory 483\\n17.6.9 MYCIN and Certainty Factors 485\\n17.7 Reasoning about Change 487\\n17.7.1 T emporal Logic 487\\n17.7.2 Using T emporal Logic 488\\n17.7.3 Event Calculus 490\\n17.7.4 Mental Situation Calculus 492\\n17.8 Knowledge Engineering 494\\n17.9 Case-Based Reasoning 495\\n17.10 Chapter Summary 496\\n17.11 Review Questions 497\\n17.12 Exercises 498\\n17.13 Further Reading 500\\nChapter 18 Fuzzy Reasoning 503\\n18.1 Introduction 503\\n18.2 Bivalent and Multivalent Logics 504\\n18.3 Linguistic Variables 504\\n18.4 Fuzzy Sets 505\\n18.4.1 Fuzzy Set Membership Functions 507\\n18.4.2 Fuzzy Set Operators 508\\nxxii Contents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 23, 'page_label': '24'}, page_content='18.4.3 Hedges 510\\n18.5 Fuzzy Logic 511\\n18.6 Fuzzy Logic as Applied to Traditional Logical \\nParadoxes 515\\n18.7 Fuzzy Rules 516\\n18.8 Fuzzy Inference 516\\n18.9 Fuzzy Expert Systems 522\\n18.9.1 Defining the Fuzzy Sets 523\\n18.9.2 Defining Fuzzy Rules 527\\n18.9.3 Relating Observations to Fuzzy Sets 528\\n18.9.4 Evaluating Each Case for the Fuzzy Rules 530\\n18.9.5 Defuzzification 531\\n18.10 Fuzzy Systems that Learn 534\\n18.10.1 Neuro-fuzzy Systems 534\\n18.10.2 Layer 1: The Input Layer 536\\n18.10.3 Layer 2: The Fuzzification Layer 536\\n18.10.4 Layer 3: The Fuzzy Rule Layer 537\\n18.10.5 Layer 4: The Output Membership Function \\nLayer 537\\n18.10.6 Layer 5: The Defuzzification Layer 538\\n18.10.7 How the System Learns 538\\n18.11 Chapter Summary 539\\n18.12 Review Questions 539\\n18.13 Exercises 540\\n18.14 Further Reading 540\\nChapter 19 Intelligent Agents 543\\n19.1 Introduction 543\\n19.2 Properties of Agents 544\\n19.2.1 Intelligence 544\\n19.2.2 Autonomy 545\\n19.2.3 Ability to Learn 545\\n19.2.4 Cooperation 545\\n19.2.5 Other Agent Properties 546\\n19.3 Agent Classification 546\\nContents xxiii'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 24, 'page_label': '25'}, page_content='19.4 Reactive Agents 547\\n19.4.1 Goal-based Agents 548\\n19.4.2 Utility-based Agents 549\\n19.4.3 Utility Functions 549\\n19.5 Interface Agents 551\\n19.6 Mobile Agents 552\\n19.7 Information Agents 553\\n19.8 Multiagent Systems 554\\n19.9 Collaborative Agents 556\\n19.10 Agent Architectures 556\\n19.10.1 Subsumption Architecture 556\\n19.10.2 BDI Architectures 558\\n19.10.3 Other Architectures 558\\n19.11 Accessibility 560\\n19.12 Learning Agents 561\\n19.12.1 Multiagent Learning 562\\n19.13 Robotic Agents 562\\n19.14 Braitenberg V ehicles 563\\n19.15 Chapter Summary 565\\n19.16 Review Questions 566\\n19.17 Exercises 567\\n19.18 Further Reading 567\\nChapter 20 Understanding Language 571\\n20.1 Introduction 571\\n20.2 Natural Language Processing 573\\n20.2.1 Morphologic Analysis 574\\n20.2.2 BNF 575\\n20.2.3 Grammers 579\\n20.2.4 Parsing: Syntactic Analysis 581\\n20.2.5 Transition Networks 582\\n20.2.6 Augmented Transition Networks 585\\n20.2.7 Chart Parsing 585\\n20.2.8 Semantic Analysis 588\\n20.2.9 Ambiguity and Pragmatic Analysis 589\\nxxiv Contents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 25, 'page_label': '26'}, page_content='20.3 Machine Translation 592\\n20.3.1 Language Identification 593\\n20.4 Information Retrieval 594\\n20.4.1 Stemming 596\\n20.4.2 Precision and Recall 598\\n20.5 Chapter Summary 599\\n20.6 Review Questions 600\\n20.7 Exercises 600\\n20.8 Further Reading 601\\nChapter 21 Machine Vision 605\\n21.1 Introduction 605\\n21.2 Human Vision 606\\n21.3 Image Processing 608\\n21.3.1 Edge Detection 609\\n21.3.2 Convolution and the Canny Edge Detector 611\\n21.3.3 Segmentation 612\\n21.3.4 Classifying Edges in Line Drawings 613\\n21.4 Using T exture 616\\n21.4.1 Identifying T extures 616\\n21.4.2 Structural T exture Analysis 620\\n21.4.3 Determining Shape and Orientation from \\nT exture 620\\n21.5 Interpreting Motion 623\\n21.6 Making Use of Vision 625\\n21.7 Face Recognition 627\\n21.8 Chapter Summary 628\\n21.9 Review Questions 629\\n21.10 Exercises 630\\n21.11 Further Reading 630\\nGlossary 633\\nBibliography 697\\nIndex 719\\nContents xxv'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 26, 'page_label': '27'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 27, 'page_label': '28'}, page_content='Introduction to Artificial\\nIntelligence\\n1\\nIntroduction to Part 1\\nPart 1 is divided into three chapters.\\nA Brief History of Artificial Intelligence\\nThis chapter provides a brief overview of the history of the\\nstudy of Artificial Intelligence. It also provides background\\nfrom philosophy, psychology, biology, and linguistics and\\nexplains how these subjects have contributed to the subject.\\nUses and Limitations\\nThe second chapter discusses the prevalence of Artificial\\nIntelligence in our world today, at the beginning of the 21st\\ncentury. It also looks at the limitations of Artificial Intelli-\\ngence and discusses some of the arguments against the\\nprinciple of strong AI, which claims that a machine that\\ncan behave in an intelligent way is actually capable of hav-\\ning mental states, much like a human being.\\nKnowledge Representation\\nThis chapter introduces an idea that is used throughout\\nthis book: knowledge representation. It explains why repre-\\nsentation is so important and why it is vital to choose the\\nright representation to solve a problem.\\nIt also explains some common representational methods\\nused in Artificial Intelligence, such as frames, semantic\\nnets, and search trees, which are used more extensively in\\nChapters 4 and 5.\\nThis chapter also provides a number of example problems\\nand explains how to use the representational methods\\nintroduced to solve the problems.\\nPART\\n1\\nCHAPTER\\n2\\nCHAPTER\\n3\\nCHAPTER'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 28, 'page_label': '29'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 29, 'page_label': '30'}, page_content='1CHAPTER\\nA Brief History of Artificial\\nIntelligence\\nWhat is all knowledge too but recorded experience, and a product of history; of\\nwhich, therefore, reasoning and belief, no less than action and passion, are\\nessential materials?\\n—Thomas Carlyle, Critical and Miscellaneous Essays\\nHistory is Philosophy from Examples.\\n—Dionysius, Ars Rhetorica\\nScience is built upon facts, as a house is built of stones; but an accumulation of\\nfacts is no more a science than a heap of stones is a house.\\n—Henri Poincaré, Science and Hypothesis\\nYou seek for knowledge and wisdom as I once did; and I ardently hope that the\\ngratification of your wishes may not be a serpent to sting you, as mine has been.\\n—Mary Shelley, Frankenstein\\n1.1 Introduction\\nAlthough Artificial Intelligence is one of the newest fields of intellectual\\nresearch, its foundations began thousands of years ago. In studying Artifi-\\ncial Intelligence, it is useful to have an understanding of the background of\\na number of other subjects, primarily philosophy, linguistics, psychology,\\nand biology.\\nThis chapter will present a selected history of the thinking and research\\nthat led up to the present state of what we now call Artificial Intelligence.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 30, 'page_label': '31'}, page_content='4 CHAPTER 1 A Brief History of Artificial Intelligence\\nIn this chapter, we will look at the contributions made by philosophy, lin-\\nguistics, psychology, and biology to Artificial Intelligence. We will also look\\nat the difference between the claims made by proponents of weak AI (AI is\\na commonly used abbreviation for Artificial Intelligence) compared with\\nthose who support strong AI, as well as look at the difference between\\nstrong methods and weak methods in Artificial Intelligence.\\nWe will begin by looking at Artificial Intelligence itself and trying to find a\\ndefinition for the subject.\\n1.2 What Is Artificial Intelligence?\\nPerhaps a better starting point would be to ask, “What is intelligence?” This\\nis a complex question with no well-defined answer that has puzzled biolo-\\ngists, psychologists, and philosophers for centuries. In Chapter 13 we pose\\na similar question when we ask, “What is life?” in order to help us under-\\nstand what Artificial Life, a branch of Artificial Intelligence, is.\\nOne could certainly define intelligence by the properties it exhibits: an abil-\\nity to deal with new situations; the ability to solve problems, to answer\\nquestions, to devise plans, and so on. It is perhaps harder to define the dif-\\nference between the intelligence exhibited by humans and that exhibited by\\ndolphins or apes.\\nFor now we will confine ourselves, then, to the somewhat simpler question\\nthat is posed by the title of this section: What Is Artificial Intelligence?\\nA simple definition might be as follows:\\nArtificial intelligence is the study of systems that act in a way that to any\\nobserver would appear to be intelligent.\\nThis definition is fine, but in fact it does not cover the whole of Artificial\\nIntelligence. In many cases, Artificial Intelligence techniques are used to\\nsolve relatively simple problems or complex problems that are internal to\\nmore complex systems. For example, the search techniques described in\\nChapter 4 are rarely used to provide a robot with the ability to find its way\\nout of a maze, but are frequently used for much more prosaic problems.\\nThis may lead us to another definition of Artificial Intelligence, as follows:\\nArtificial Intelligence involves using methods based on the intelligent behavior\\nof humans and other animals to solve complex problems.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 31, 'page_label': '32'}, page_content='1.3 Strong Methods and Weak Methods 5\\nHence, in Chapter 20, we look at systems that are able to “understand”\\nhuman speech, or at least are able to extract some meaning from human\\nutterances, and carry out actions based on those utterances. Such systems\\nmay not be designed to behave in an intelligent way, but simply to provide\\nsome useful function. The methods they use, however, are based on the\\nintelligent behavior of humans.\\nThis distinction is brought into sharper contrast when we look at the dif-\\nference between so-called strong AI and weak AI.\\nThe followers of strong AI believe that by giving a computer program suffi-\\ncient processing power, and by providing it with enough intelligence, one\\ncan create a computer that can literally think and is conscious in the same\\nway that a human is conscious.\\nMany philosophers and Artificial Intelligence researchers consider this view\\nto be false, and even ludicrous. The possibility of creating a robot with emo-\\ntions and real consciousness is one that is often explored in the realms of\\nscience fiction but is rarely considered to be a goal of Artificial Intelligence.\\nWeak AI, in contrast, is simply the view that intelligent behavior can be\\nmodeled and used by computers to solve complex problems. This point of\\nview argues that just because a computer behaves intelligently does not\\nprove that it is actually intelligent in the way that a human is. We will exam-\\nine this argument in more detail in Chapter 2, when we look at the Chinese\\nRoom thought experiment and the arguments around it.\\n1.3 Strong Methods and Weak Methods\\nWe have discussed the difference between the claims of weak AI and strong\\nAI. This difference is not to be confused with the difference between strong\\nmethods and weak methods.\\nWeak methods in Artificial Intelligence use systems such as logic, auto-\\nmated reasoning, and other general structures that can be applied to a wide\\nrange of problems but that do not necessarily incorporate any real knowl-\\nedge about the world of the problem that is being solved.\\nIn contrast, strong method problem solving depends on a system being\\ngiven a great deal of knowledge about its world and the problems that it\\nmight encounter. Strong method problem solving depends on the weak'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 32, 'page_label': '33'}, page_content='6 CHAPTER 1 A Brief History of Artificial Intelligence\\nmethods because a system with knowledge is useless without some\\nmethodology for handling that knowledge.\\nHence, the production systems we will examine in Chapter 9 are based on\\nthe weak method expert system shells but use strong method rules to\\nencode their knowledge.\\nThe earliest research in Artificial Intelligence focused on weak methods.\\nNewell and Simon’s General Problem Solver (GPS), which is discussed in\\nChapter 15, was an attempt to use weak methods to build a system that\\ncould solve a wide range of general problems. That this approach ulti-\\nmately failed led to a realization that more was needed than simple repre-\\nsentations and algorithms to make Artificial Intelligence work: knowledge\\nwas the key ingredient.\\nA great number of the subjects covered in this book are weak methods.\\nThis does not mean that they are not worth studying, or even that they are\\nnot useful. In many situations, weak methods are ideal for solving prob-\\nlems. However, the addition of knowledge is almost always essential to\\nbuild systems that are able to deal intelligently with new problems; if our\\naim is to build systems that appear to behave intelligently, then strong\\nmethods are certainly essential.\\n1.4 From Aristotle to Babbage\\nIn Chapter 7 of this book, we present the propositional and predicate log-\\nics. These systems for logical reasoning are based on the logic invented by\\nAristotle, a philosopher from ancient Greece, who lived from 384 to 322\\nB.C. and who studied under Plato during that time. The writings of Aristo-\\ntle (on this and many other subjects) have formed the basis for a great deal\\nof our modern scientific thinking.\\nFrom the point of view of Artificial Intelligence, the most interesting aspect\\nof Aristotle’s work is his study of logic. He invented the idea of the syllo-\\ngism, which he defined as follows:\\n“A discourse in which certain things having been stated, something else\\nfollows of necessity from their being so. ”\\nAristotle’s logic was developed and expanded on by later philosophers,\\nmathematicians, and logicians. The first real steps in the study of logic after\\nAristotle took place in the 12th century, when Peter Abelard (who lived'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 33, 'page_label': '34'}, page_content='1.5 Alan Turing and the 1950s 7\\nfrom 1079 to 1142 A.D.) wrote Dialectica, a treatise on logic. In the follow-\\ning centuries, more work was carried out, but the greatest developments\\nwere made in the last few centuries.\\nIn the late 17th to early 18th centuries, Gottfried Leibniz, the German\\nmathematician and philosopher who along with Isaac Newton had a part\\nin the invention of the calculus used by mathematicians today, invented the\\nidea of developing a formal mathematical language for reasoning. His uni-\\nversal language would allow us to express with great precision problems of\\nall kinds, and then go about solving them. Leibniz did not succeed in creat-\\ning this universal language, but his work provided the basis for the propo-\\nsitional and predicate logics that are so important to Artificial Intelligence\\nresearch today.\\nIn the 19th century, George Boole, an English mathematician, who lived\\nfrom 1815 to 1864, developed Boolean algebra, the logical system we still\\nuse as part of propositional and predicate logics. Boolean algebra is widely\\nused by electronics engineers in developing logical gates for silicon chips\\nand is also used by computer scientists. Boolean algebra provides a language\\nfor expressing concepts such as “A is true” and “A is true but B is false. ”\\nAround the same time that Boole was inventing his algebra, Charles Babbage\\ninvented the world’s first computer—the Analytic Engine. He didn’t ever\\nmanage to build the computer, but his designs were later used to build a work-\\ning model. The designs of computers in the 20th century didn’t bear much\\nresemblance to Babbage’s computer, but they certainly owed a great deal to it.\\nBabbage’s idea of a digital computer remained a dream until around the\\nmiddle of the 20th century. By the 1950s, a number of working computers\\nhad been built. Unlike Babbage’s mechanical engines, these computers were\\nelectronic. The very first electromechanical computers were soon replaced\\nby computers based on vacuum tubes.\\n1.5 Alan Turing and the 1950s\\nOne of the great figures in the history of Artificial Intelligence is Alan Tur-\\ning. During World War II, Turing famously worked in Bletchley Park, help-\\ning to solve the Germans’ codes. After the war, he began to work on the idea\\nof the possibility of building a computer that could think. His paper pub-\\nlished in 1950, Computing Machinery & Intelligence , was one of the first\\npapers to be written on this subject.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 34, 'page_label': '35'}, page_content='8 CHAPTER 1 A Brief History of Artificial Intelligence\\nThe Turing test was designed by Turing as a way to judge the success or\\notherwise of an attempt to produce a thinking computer. More specifically,\\nit was based on the idea that if a person who interrogated the computer\\ncould not tell if it was a human or a computer, then to all intents and pur-\\nposes, Turing said, it is intelligent.\\nThe test is designed as follows:\\nThe interrogator is given access to two individuals, one of whom is a\\nhuman and the other of whom is a computer. The interrogator can ask the\\ntwo individuals questions, but cannot directly interact with them. Probably\\nthe questions are entered into a computer via a keyboard, and the responses\\nappear on the computer screen.\\nThe human is intended to attempt to help the interrogator, but if the com-\\nputer is really intelligent enough, it should be able to fool the interrogator\\ninto being uncertain about which is the computer and which is the human.\\nThe human can give answers such as “I’m the human—the other one is the\\ncomputer, ” but of course, so can the computer. The real way in which the\\nhuman proves his or her humanity is by giving complex answers that a\\ncomputer could not be expected to comprehend. Of course, the inventors\\nof the truly intelligent computer program would have given their program\\nthe ability to anticipate all such complexities.\\nTuring’s test has resulted in a number of computer programs (such as\\nWeizenbaum’s ELIZA, designed in 1965) that were designed to mimic\\nhuman conversation. Of course, this in itself is not a particularly useful\\nfunction, but the attempt has led to improvements in understanding of\\nareas such as natural language processing. T o date, no program has passed\\nthe Turing test, although cash prizes are regularly offered to the inventor of\\nthe first computer program to do so.\\nLater in the 1950s computer programs began to be developed that could\\nplay games such as checkers and chess (see Chapter 6), and also the first\\nwork was carried out into developing computer programs that could\\nunderstand human language (Chapter 20).\\nA great deal of work at this stage was done in computer translation. It was,\\nindeed, widely believed that computers could eventually be programmed to\\ntranslate accurately from one human language to another. It has since been\\nfound that the task of machine translation is actually an extremely difficult'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 35, 'page_label': '36'}, page_content='1.6 The 1960s to the 1990s 9\\none, and not one that has yet been completely solved. This subject is dis-\\ncussed in more detail in Chapter 20.\\nIn 1956, the term Artificial Intelligence was first used by John McCarthy at\\na conference in Dartmouth College, in Hanover, New Hampshire.\\nIn 1957, Newell and Simon invented the idea of the GPS, whose purpose\\nwas, as the name suggests, to solve almost any logical problem. The program\\nused a methodology known as means ends analysis, which is based on the\\nidea of determining what needs to be done and then working out a way to\\ndo it. This works well enough for simple problems, but AI researchers soon\\nrealized that this kind of method could not be applied in such a general\\nway—the GPS could solve some fairly specific problems for which it was\\nideally suited, but its name was really a misnomer.\\nAt this time there was a great deal of optimism about Artificial Intelligence.\\nPredictions that with hindsight appear rash were widespread. Many com-\\nmentators were predicting that it would be only a few years before comput-\\ners could be designed that would be at least as intelligent as real human\\nbeings and able to perform such tasks as beating the world champion at\\nchess, translating from Russian into English, and navigating a car through a\\nbusy street. Some success has been made in the past 50 years with these\\nproblems and other similar ones, but no one has yet designed a computer\\nthat anyone would describe reasonably as being intelligent.\\nIn 1958, McCarthy invented the LISP programming language, which is still\\nwidely used today in Artificial Intelligence research.\\n1.6 The 1960s to the 1990s\\nSince the 1950s, a great deal of the original optimism has gone out of Arti-\\nficial Intelligence and has been replaced with a degree of realism.\\nThe aim of the study of Artificial Intelligence is no longer to create a robot\\nas intelligent as a human, but rather to use algorithms, heuristics, and\\nmethodologies based on the ways in which the human brain solves prob-\\nlems. Hence, systems have been designed such as Thomas Evans’ Analogy\\nand Melanie Mitchell’s Copycat Architecture, which were designed to be\\nable to solve problems that involve analogies. Mitchell’s Copycat, for exam-\\nple, can solve problems such as “ABC is to CBA as DEF is to ???. ”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 36, 'page_label': '37'}, page_content='10 CHAPTER 1 A Brief History of Artificial Intelligence\\nThe ability to solve problems of this kind does not represent intelligence, but\\nthe development of systems that can solve such problems is the mainstay of\\nArtificial Intelligence research and arguably an extremely useful step along\\nthe way to producing more and more useful computer software systems.\\nIn Chapter 2, we will discuss the subject of whether a computer program\\ncan really be “intelligent. ”\\nIn the most recent decades, the study of Artificial Intelligence has flour-\\nished. Areas of particular importance include the following:\\n■ machine learning\\n■ multi-agent systems\\n■ artificial life\\n■ computer vision\\n■ planning\\n■ playing games (chess in particular)\\nIn Chapter 2, we will look at the prevalence of Artificial Intelligence in\\nthe world today. This prevalence has more than justified the work of the\\npast 50 years.\\n1.7 Philosophy\\nThe philosophy of great thinkers, from Plato to Descartes and to Daniel\\nDennett, has had a great deal of influence on the modern study of Artificial\\nIntelligence.\\nThe influence of Aristotle has already been mentioned, but it has been\\nargued (Dreyfus, 1972) that the history of Artificial Intelligence begins\\nwhen Plato wrote that his teacher Socrates said, “I want to know what is\\ncharacteristic of piety which makes all actions pious. . . that I may have it to\\nturn to, and to use as a standard whereby to judge your actions and those of\\nother men.”\\nSocrates was claiming that an algorithm could be defined that described\\nthe behavior of humans and determined whether a person’s behavior was\\ngood or bad.\\nThis leads us to a fundamental question that has been asked by philoso-\\nphers and students of Artificial Intelligence for many years: Is there more to'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 37, 'page_label': '38'}, page_content='1.8 Linguistics 11\\nthe mind than simply a collection of neurons? Or, to put it another way, if\\neach neuron in the human brain was replaced by an equivalent computa-\\ntional device, would the resultant be the same person? Would it indeed be\\ncapable of intelligent thought?\\nThis kind of question is regularly debated by modern philosophers such as\\nDaniel Dennett, and while the answer is far from clear, it is an instructive\\ndebate to follow, and its implications for Artificial Intelligence are enormous.\\nIn the 17th century, the great philosopher René Descartes was a strong believer\\nin dualism, the idea that the universe consists of two entirely separate things:\\nmind and matter. Descartes’s view was that the mind (or soul) was entirely\\nseparate from the physical body and not constrained by it in any way.\\nImportantly, Descartes did not believe that this dualism extended to ani-\\nmals. In other words, in his view a cat or a dog is simply a machine: a highly\\ncomplex machine, but a machine nonetheless. This view gives hope to the\\nproponents of Artificial Intelligence who believe that by simply putting\\nenough computing power together and programming it in the correct way,\\na machine could be made to behave in the same way as an animal, or even a\\nhuman being.\\n1.8 Linguistics\\nThe study of human language has a vital role to play in Artificial Intelli-\\ngence. As is discussed in some detail in Chapter 20, compared with com-\\nputer languages such as Java and LISP , human languages are extraordinarily\\ncomplex and are full of pitfalls that almost seem designed to trap anyone\\n(human or computer) inexperienced in the use of the language.\\nThis complexity, combined with a sense of optimism, may well have been\\npart of the reason that natural language processing was such a popular\\nresearch area in the early days of Artificial Intelligence.\\nSome of the optimism surrounding Natural Language Processing came\\nfrom the writings of Noam Chomsky, who in the 1950s proposed his the-\\nory of Syntactic Structures, which was a formal theory of the structure of\\nhuman language. His theory also attempted to provide a structure for\\nhuman knowledge, based on the knowledge of language.\\nThis idea of knowledge representation is at the very core of Artificial Intel-\\nligence and is a recurring theme throughout this book.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 38, 'page_label': '39'}, page_content='12 CHAPTER 1 A Brief History of Artificial Intelligence\\nAlmost all of the techniques described in this book depend on a formal\\nmethod of representation for knowledge that enables a computer to use\\ninformation from the world, or concerning the problems it is to solve,\\nwithout necessarily needing to understand that knowledge.\\nThere is a close relationship between linguistics and Artificial Intelligence,\\nand the two fields join together in the study of natural language processing,\\nwhich is discussed in some detail in Chapter 20.\\n1.9 Human Psychology and Biology\\nSome of the techniques, such as search algorithms, described in this book\\ndo not clearly map onto any specific biological or psychological function of\\nhuman beings. On the other hand, many of them do. For example, McCul-\\nloch and Pitts’s electronic neurons, which are used today to build neural\\nnetworks, are directly based on the way in which neurons in the human\\nbrain function.\\nIn a similar way, much research in Artificial Intelligence has been related to\\ncognitive psychology , which is based on the idea that the human brain\\nuses knowledge or information that it is capable of processing in order to\\nsolve problems, make decisions, draw conclusions, and carry out other\\nintelligent acts.\\nThis form of psychology was in contrast to behaviorism, which prevailed\\nfor much of the first half of the 20th century. Behaviorism relates behavior\\ndirectly to stimuli, without taking into account knowledge or information\\nthat might be contained in the brain. This is the kind of psychology that\\nPavlov was demonstrating in his famous experiment with dogs.\\nPsychology is certainly useful to the study of Artificial Intelligence in one\\nrespect: it helps to answer the important question, “What is intelligence?”\\nAs we have seen already, this is a difficult question to answer, but in study-\\ning it, psychologists give us a great deal of information that is useful in\\nforming the ideas behind Artificial Intelligence.\\n1.10 AI Programming Languages\\nA number of programming languages exist that are used to build Artificial\\nIntelligence systems. General programming languages such as C++ and\\nJava are often used because these are the languages with which most com-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 39, 'page_label': '40'}, page_content='1.10 AI Programming Languages 13\\nputer scientists have experience. There also exist two programming lan-\\nguages that have features that make them particularly useful for program-\\nming Artificial Intelligence projects—PROLOG and LISP .\\nWe will now provide a brief overview of these two languages and explain\\nhow they are used in Artificial Intelligence research. Of course, a number of\\nother programming languages exist that are also widely used for Artificial\\nIntelligence, but we will focus on PROLOG and LISP because these are cer-\\ntainly the most widely used and the ones on which there is the widest range\\nof relevant literature.\\n1.10.1 PROLOG\\nPROLOG (PROgramming in LOGic) is a language designed to enable pro-\\ngrammers to build a database of facts and rules, and then to have the sys-\\ntem answer questions by a process of logical deduction using the facts and\\nrules in the database.\\nFacts entered into a PROLOG database might look as follows:\\ntasty (cheese).\\nmade_from (cheese, milk).\\ncontains (milk, calcium).\\nThese facts can be expressed as the following English statements:\\nCheese is tasty.\\nCheese is made from milk.\\nMilk contains calcium.\\nWe can also specify rules in a similar way, which express relationships between\\nobjects and also provide the instructions that the PROLOG theorem prover\\nwill use to answer queries. The following is an example of a rule in PROLOG:\\ncontains (X, Y) :- made_from (X, Z), contains (Z, Y).\\nThis rule is made up of two main parts, separated by the symbol “:-” .\\nThe rule thus takes the form:\\nB :- A\\nwhich means “if A is true, then B is true, ” or “A implies B. ”\\nHence, the rule given above can be translated as “If X is made from Z and Z\\ncontains Y then X contains Y. ”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 40, 'page_label': '41'}, page_content='14 CHAPTER 1 A Brief History of Artificial Intelligence\\nIn Chapters 7, 8, and 9, we make a great deal of use of rules of this kind.\\nHaving entered the three facts and one rule given above, the user might\\nwant to ask the system a question:\\n?- contains (cheese, calcium).\\nUsing a process known as resolution (which is described in detail in Chap-\\nter 8), the PROLOG system is able to use the rule and the facts to determine\\nthat because cheese is made from milk, and because milk contains calcium,\\ntherefore cheese does contain calcium. It thus responds:\\nyes\\nIt would also be possible to ask the system to name everything that con-\\ntains calcium:\\n?- contains (X, calcium)\\nThe system will use the same rules and facts to deduce that milk and cheese\\nboth contain calcium, and so will respond:\\nX=milk.\\nX=cheese.\\nThis has been a very simple example, but it should serve to illustrate how\\nPROLOG works. Far more complex databases of facts and rules are rou-\\ntinely built using PROLOG, and in some cases simple databases are built\\nthat are able to solve complex mathematical problems.\\nPROLOG is not an efficient programming language, and so for many prob-\\nlems a language such as C++ would be more appropriate. In cases where\\nlogical deduction is all that is required, and the interactive nature of the\\nPROLOG interface is suitable, then PROLOG is the clear choice. PROLOG\\nprovides a way for programmers to manipulate data in the form of rules\\nand facts without needing to select algorithms or methodologies for han-\\ndling those data.\\n1.10.2 LISP\\nLISP (LISt Programming) is a language that more closely resembles the\\nimperative programming languages such as C++ and Pascal than does\\nPROLOG. As its name suggests, LISP is based around handling of lists of\\ndata. A list in LISP is contained within brackets, such as:\\n[A B C]'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 41, 'page_label': '42'}, page_content='Chapter Summary 15\\nThis is a list of three items. LISP uses lists to represent data, but also to rep-\\nresent programs. Hence, a program in LISP can be treated as data. This\\nintroduces the possibility of writing self-modifying programs in LISP , and\\nas we see in Chapter 13, it also allows us to use evolutionary techniques to\\n“evolve” better LISP programs.\\nLISP is a far more complex language syntactically than PROLOG, and so we\\nwill not present any detail on its syntax here. It provides the usual kinds of\\nmechanisms that other programming languages provide, such as assign-\\nment, looping, evaluating functions, and conditional control\\n( i f ...t h e n ...) .I t  also provides a great deal of list manipulation functions,\\nsuch as car and cdr, which are used to return the first entry in a list and all\\nthe entries except for the first entry, respectively.\\n1.11 Chapter Summary\\n■ Intelligence is difficult to define, and as a result Artificial Intelli-\\ngence is also hard to define.\\n■ One definition of Artificial Intelligence is:\\nArtificial intelligence is the study of systems that act in a way that to\\nany observer would appear to be intelligent.\\n■ Proponents of strong AI believe that a computer that behaves in an\\nintelligent way is capable of possessing mental states and, there-\\nfore, of being truly conscious and intelligent in the same way that\\nhumans are.\\n■ Weak AI is a less controversial idea—that computers can be pro-\\ngrammed to behave in intelligent ways in order to solve specific\\nproblems. This book is concerned with the methods of weak AI.\\n■ Weak and strong AI are not to be confused with weak and\\nstrong methods.\\n■ Weak methods are those that do not rely on any knowledge or\\nunderstanding of the world and the problems being solved. Most\\nof the techniques described in this book are weak methods.\\n■ Strong methods are those that use knowledge about the world and\\nabout the problem being solved. The strong method approach is\\nessential for solving many complex real world problems using Arti-\\nficial Intelligence.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 42, 'page_label': '43'}, page_content='16 CHAPTER 1 A Brief History of Artificial Intelligence\\n■ In studying Artificial Intelligence, it is extremely useful to understand\\nthe background of philosophy, linguistics, biology, and psychology.\\n■ Philosophers, from Plato and Aristotle to Searle and Dennett, have\\nasked questions and provided opinions concerning the nature of\\nintelligence and the ability to define it in a way that would enable\\nus to program a computer with real intelligence.\\n■ The 1950s were a time of great optimism in Artificial Intelligence\\nand also a time of great progress in the field.\\n■ Turing’s test is a way to determine if a computer is truly intelligent,\\nby seeing if it could fool a human in conversation into thinking\\nthat it too was human. It is widely believed today that even if a\\ncomputer could pass the Turing test, it would still not truly be con-\\nscious or intelligent in the way that humans are.\\n■ In 1956 the term Artificial Intelligence was coined by John McCarthy.\\n■ Since the 1950s, the study of Artificial Intelligence has been fla-\\nvored with a great deal more realism. The progress in recent years\\nhas been phenomenal.\\n1.12 Review Questions\\n1.1 What is intelligence?\\n1.2 What is Artificial Intelligence? What do you hope to learn by read-\\ning this book?\\n1.3 Is Artificial Intelligence a branch of computer science or an alter-\\nnative to computer science?\\n1.4 Why is Artificial Intelligence a worthwhile subject to study?\\n1.5 Explain the difference between strong and weak methods in Artifi-\\ncial Intelligence. Explain how this dichotomy differs from the dif-\\nference between strong and weak AI.\\n1.6 Why are PROLOG and LISP so well suited to Artificial Intelligence\\nresearch? Do you think languages such as C++ and Java could also\\nbe used for such research?\\n1.7 What do you think led mankind to embark upon the study of Arti-\\nficial Intelligence? Which fields of study particularly fed into it?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 43, 'page_label': '44'}, page_content='Further Reading 17\\nWhat human desires did the study of Artificial Intelligence seek to\\nsatisfy?\\n1.8 When did Artificial Intelligence first begin to be studied? Y our\\nanswer should be more detailed than a simple date.\\n1.13 Further Reading\\nCrevier (1999) gives a fascinating history of the subject of Artificial\\nIntelligence.\\nThroughout this book, details are given of other books that can be refer-\\nenced to learn more about the material covered herein. The following\\nbooks are general Artificial Intelligence texts that cover almost all of the\\ntopics covered by this book and also provide excellent introductions to the\\nsubject as a whole.\\nEach of these books takes a different approach to the material, and it is\\nworth selecting the text that best fits your personal preferences in studying\\nthis subject.\\nFor example, Russell and Norvig present the material in terms of intelligent\\nagents. Winston explains his material with a great deal of examples but\\ntends not to go into a great deal of detail, while Luger goes into greater\\ndepth, but with fewer examples. Schalkoff gives a good coverage of Artifi-\\ncial Intelligence using examples in PROLOG and LISP; it also therefore\\nserves as a useful text in those languages.\\nComputation & Intelligence, edited by George Luger, contains a number of\\nextremely important papers collected from the whole history of Artificial\\nIntelligence. It includes papers by such pioneers of the subject as Alan Tur-\\ning, Marvin Minsky, John McCarthy, Allen Newell, and Herbert Simon.\\nThe Handbook of Artificial Intelligence, edited by A. Barr and E. Feigenbaum\\n(1989 – William Kaufman)\\nThe Essence of Artificial Intelligence, by Alison Cawsey (1998 – Prentice Hall)\\nIntroduction to Artificial Intelligence , by Eugene Charniak and Drew\\nMcDermott (1985 – Addison Wesley; out of print)\\nThe Computational Brain , by Patricia S. Churchland and T errence J.\\nSejnowski (1992 – The MIT Press)\\nAI: The Tumultuous History of the Search for Artificial Intelligence, by Daniel\\nCrevier (1999 – Basic Books)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 44, 'page_label': '45'}, page_content='18 CHAPTER 1 A Brief History of Artificial Intelligence\\nUnderstanding Artificial Intelligence (Science Made Accessible), compiled by\\nSandy Fritz (2002 – Warner Books)\\nThe Anatomy of Programming Languages, by Alice E. Fischer and Frances S.\\nGrodzinsky (1993 – Prentice Hall)\\nIntroduction to Artificial Intelligence , by Philip C. Jackson (1985 – Dover\\nPublications)\\nAI Application Programming, by M. Tim Jones (2003 – Charles River Media)\\nArtificial Intelligence: Structures and Strategies for Complex Problem-Solving,\\nby George F. Luger (2002 – Addison Wesley)\\nComputation & Intelligence: Collected Readings , edited by George F. Luger\\n(1995 – The AAAI Press / The MIT Press)\\nArtificial Intelligence: A Guide to Intelligent Systems, by Michael Negnevitsky\\n(2002 – Addison Wesley)\\nArtificial Intelligence: A New Synthesis , by N.J. Nilsson (1998 – Morgan\\nKauffman)\\nArtificial Intelligence: A Modern Approach , by Stuart Russell and Peter\\nNorvig (1995 – Prentice Hall)\\nThe Emperor’s New Mind: Concerning Computers, Minds, and the Laws of\\nPhysics, by Roger Penrose (1989 – Oxford University Press)\\nUnderstanding Intelligence, by Rolf Pfeiffer and Christian Scheier (2000 –\\nThe MIT Press)\\nArtificial Intelligence: An Engineering Approach, by Robert J. Schalkoff (1990\\n– McGraw Hill)\\nThe Encyclopedia of Artificial Intelligence, edited by S.C. Shapiro (1992 - Wiley)\\nArtificial Intelligence, by Patrick Henry Winston (1992 – Addison Wesley)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 45, 'page_label': '46'}, page_content='2CHAPTER\\nUses and Limitations\\nThe limits of my language mean the limits of my world.\\n—Ludwig Wittgenstein,Tractatus Logico-Philosophicus\\nWhy, sometimes I’ve believed as many as six impossible things before breakfast.\\n—Lewis Carroll, Through the Looking Glass\\nWho hath put wisdom in the inward parts? Or who hath given understanding\\nto the heart?\\n—The Book of Job, Chapter 38, V erse 36\\n2.1 Introduction\\nAs was explained in Chapter 1, the early history of Artificial Intelligence\\nwas filled with a great deal of optimism—optimism that today seems at\\nbest to have been unfounded. In this chapter, we look at some of the argu-\\nments against strong AI (the belief that a computer is capable of having\\nmental states) and also look at the prevalence of Artificial Intelligence\\ntoday and explain why it has become such a vital area of study.\\nWe will also look at the extent to which the Artificial Intelligence commu-\\nnity has been successful so far in achieving the goals that were believed to\\nbe possible decades ago. In particular, we will look at whether the computer\\nHAL in the science fiction film 2001: A Space Odyssey is a possibility with\\ntoday’s technologies.\\nWe will also look at the prevalence of Artificial Intelligence, and how it is\\nused in the world today, the 21st century.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 46, 'page_label': '47'}, page_content='20 CHAPTER 2 Uses and Limitations\\n2.2 The Chinese Room\\nWe will start by examining philosophical objections to strong AI, in partic-\\nular the Chinese Room argument of John Searle.\\nThe American philosopher John Searle has argued strongly against the pro-\\nponents of strong AI who believe that a computer that behaves sufficiently\\nintelligently could in fact be intelligent and have consciousness, or mental\\nstates, in much the same way that a human does.\\nOne example of this is that it is possible using data structures called scripts\\n(see Chapter 17) to produce a system that can be given a story (for example,\\na story about a man having dinner in a restaurant) and then answer ques-\\ntions (some of which involve a degree of subtlety) about the story. Propo-\\nnents of strong AI would claim that systems that can extend this ability to\\ndeal with arbitrary stories and other problems would be intelligent.\\nSearle’s Chinese Room experiment was based on this idea and is described\\nas follows:\\nAn English-speaking human is placed inside a room. This human does not\\nspeak any language other than English and in particular has no ability to\\nread, speak, or understand Chinese.\\nInside the room with the human are a set of cards, upon which are printed\\nChinese symbols, and a set of instructions that are written in English.\\nA story, in Chinese, is fed into the room through a slot, along with a set of\\nquestions about the story. By following the instructions that he has, the\\nhuman is able to construct answers to the questions from the cards with\\nChinese symbols and pass them back out through the slot to the questioner.\\nIf the system were set up properly, the answers to the questions would be suf-\\nficient that the questioner would believe that the room (or the person inside\\nthe room) truly understood the story, the questions, and the answers it gave.\\nSearle’s argument is now a simple one. The man in the room does not\\nunderstand Chinese. The pieces of card do not understand Chinese. The\\nroom itself does not understand Chinese, and yet the system as a whole is\\nable to exhibit properties that lead an observer to believe that the system\\n(or some part of it) does understand Chinese.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 47, 'page_label': '48'}, page_content='2.3 HAL—Fantasy or Reality? 21\\nIn other words, running a computer program that behaves in an intelligent\\nway does not necessarily produce understanding, consciousness, or real\\nintelligence.\\nThis argument clearly contrasts with Turing’s view that a computer system\\nthat could fool a human into thinking it was human too would actually be\\nintelligent.\\nOne response to Searle’s Chinese Room argument, the Systems Reply,\\nclaims that although the human in the room does not understand Chinese,\\nthe room itself does. In other words, the combination of the room, the\\nhuman, the cards with Chinese characters, and the instructions form a sys-\\ntem that in some sense is capable of understanding Chinese stories. There\\nhave been a great number of other objections to Searle’s argument, and the\\ndebate continues.\\nThere are other objections to the ideas of strong AI. TheHalting Problemand\\nGödel’s incompleteness theorem tell us that there are some functions that a\\ncomputer cannot be programmed to compute, and as a result, it would seem to\\nbe impossible to program a computer to perform all the computations needed\\nfor real consciousness. This is a difficult argument, and one potential response\\nto it is to claim that the human brain is in fact a computer, and that although it\\nmust also be limited by the Halting Problem, it is still capable of intelligence.\\nThis claim that the human brain is a computer is an interesting one. Upon it\\nis based the idea of neural networks. By combining the processing power of\\nindividual neurons, we are able to produce artificial neural networks that are\\ncapable of solving extremely complex problems, such as recognizing faces.\\nProponents of strong AI might argue that such successes are steps along the\\nway to producing an electronic human being, whereas objectors would point\\nout that this is simply a way to solve one small set of problems—not only\\ndoes it not solve the whole range of problems that humans are capable of,\\nbut it also does not in any way exhibit anything approaching consciousness.\\n2.3 HAL—Fantasy or Reality?\\nOne of the most famous fictional accounts of Artificial Intelligence comes\\nin the film 2001: A Space Odyssey, based on the story by Arthur C. Clarke.\\nOne of the main characters in the film is HAL, a Heuristically programmed\\nALgorithmic computer. In the film, HAL behaves, speaks, and interacts'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 48, 'page_label': '49'}, page_content='22 CHAPTER 2 Uses and Limitations\\nwith humans in much the same way that a human would (albeit in a dis-\\nembodied form). In fact, this humanity is taken to extremes by the fact that\\nHAL eventually goes mad.\\nIn the film, HAL played chess, worked out what people were saying by read-\\ning their lips, and engaged in conversation with other humans. How many\\nof these tasks are computers capable of today?\\nWe shall see in Chapter 6 that there has been a great deal of success with\\ndeveloping computers that can play chess. In 1997, a computer, Deep Blue,\\nbeat the chess world champion Garry Kasparov. As we discuss in Chapter 6,\\nthis was not the end of supremacy at chess for mankind, however. The vic-\\ntory was not a particularly convincing one and has not been repeated.\\nChess-playing computers are certainly capable of beating most human\\nchess players, but those who predicted that chess computers would be\\nvastly superior to even the best human players by now were clearly wrong.\\nIn some games, such as Go, the best computers in the world are able to play\\nonly at the level of a reasonably accomplished amateur human player. The\\ngame is so complex that even the best heuristics and Artificial Intelligence\\ntechniques are not able to empower a computer with the ability to come\\nclose to matching the capabilities of the best human players.\\nIn Chapter 20, we look at techniques that are used to enable computers to\\nunderstand human language and in theory to enable them to engage in\\nconversation. Clearly no computer program has yet been designed that is\\nable to pass the Turing test and engage fully in conversation in such a way\\nthat would be indistinguishable from a human, and there is no sign that\\nany such program will be designed in the near future.\\nThe ability to interpret spoken words by examining the movement of lips is\\none that only a few humans have. It combines a number of complex prob-\\nlems: first, the visual problem of identifying sounds from the shape of lips.\\nIn Chapter 21, we will see how computers can be programmed to interpret\\nvisual information in the same kinds of ways that humans do. Interpreting\\nthe shape of human lips would probably not be impossible, and it is likely\\nthat a neural network could be trained to solve such a problem. The next\\nproblem is to combine the sounds together into words—again, not a diffi-\\ncult problem given a suitably large lexicon of words. Finally, HAL would\\nhave needed to be able to interpret and understand the words in the same\\nway that he would have done when listening to spoken words.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 49, 'page_label': '50'}, page_content='2.4 AI in the 21st Century 23\\nHAL, as portrayed in the film, did have some capabilities that Artificial\\nIntelligence has given to computers today, but it is certainly not the case\\nthat computers exist with the breadth of capabilities and in particular the\\nability to communicate in so human a manner. Finally, the likelihood of a\\ncomputer becoming insane is a rather remote one, although it is of course\\npossible that a malfunction of some kind could cause a computer to exhibit\\nproperties not unlike insanity!\\nArtificial Intelligence has been widely represented in other films. The Stephen\\nSpielberg film AI: Artificial Intelligence is a good example. In this film, a cou-\\nple buy a robotic boy to replace their lost son. The audience’s sympathies are\\nfor the boy who feels emotions and is clearly as intelligent (if not more so) as\\na human being. This is strong AI, and while it may be the ultimate goal of\\nsome Artificial Intelligence research, even the most optimistic proponents of\\nstrong AI would agree that it is not likely to be achieved in the next century.\\n2.4 AI in the 21st Century\\nArtificial Intelligence is all around us. The techniques described in this\\nbook are used in a staggering array of machines and systems that we use\\nevery day. Fuzzy logic, for example, is widely used in washing machines,\\ncars, and elevator control mechanisms. (Note that no one would claim that\\nas a result those machines were intelligent, or anything like it! They are\\nsimply using techniques that enable them to behave in a more intelligent\\nway than a simpler control mechanism would allow.)\\nIntelligent agents, which are described in Chapter 19, are widely used. For\\nexample, there are agents that help us to solve problems while using our\\ncomputers and agents that traverse the Internet, helping us to find docu-\\nments that might be of interest. The physical embodiment of agents,\\nrobots, are also becoming more widely used. Robots are used to explore the\\noceans and other worlds, being able to travel in environments inhospitable\\nto humans. It is still not the case, as was once predicted, that robots are\\nwidely used by households, for example, to carry shopping items or to play\\nwith children, although the AIBO robotic dog produced by Sony and other\\nsimilar toys are a step in this direction.\\nExpert systems are used by doctors to help with symptoms that are hard to\\ndiagnose or to prescribe treatments in cases where even human experts\\nhave difficulty.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 50, 'page_label': '51'}, page_content='24 CHAPTER 2 Uses and Limitations\\nArtificial Intelligence systems are used in a wide range of industries, from\\nhelping travel agents select suitable holidays to enabling factories to sched-\\nule machines.\\nArtificial Intelligence is particularly useful in situations where traditional\\nmethods would be too slow. Combinatorial problems, such as scheduling\\nteachers and pupils to classrooms, are not well solved by traditional com-\\nputer science techniques. In such cases, the heuristics and techniques pro-\\nvided by Artificial Intelligence can provide excellent solutions.\\nMany computer games have been designed based on Artificial Intelligence.\\nIn order to provide more realistic play, the computer game Republic: The\\nRevolution, launched in 2003, contained a million individual Artificial\\nIntelligences, each capable of interacting with the world and with the player\\nof the game, as well as capable of being manipulated by the player.\\nIt is likely that Artificial Intelligence will become more prevalent in our\\nsociety. And whether or not we eventually create an Artificial Intelligence\\nthat is truly intelligent, we are likely to find computers, machines, and other\\nobjects appearing to become more intelligent—at least in terms of the way\\nthey behave.\\n2.5 Chapter Summary\\n■ The Chinese Room argument is a thought experiment designed by\\nJohn Searle, which is designed to refute strong AI.\\n■ The computer HAL, as described in the film 2001: A Space Odyssey,\\nis not strictly possible using today’s technology, but many of its\\ncapabilities are not entirely unrealistic today.\\n■ The computer program, Deep Blue, beat world chess champion\\nGarry Kasparov in a six-game chess match in 1997. This feat has\\nnot been repeated, and it does not yet represent the end of human\\nsupremacy at this game.\\n■ Artificial Intelligence is all around us and is widely used in indus-\\ntry, computer games, cars, and other devices, as well as being a\\nvaluable tool used in many computer software programs.\\n2.6 Review Questions\\n2.1 Explain the difference between strong AI and weak AI. Which of\\nthe two do you think this book will be about? Why?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 51, 'page_label': '52'}, page_content='Further Reading 25\\n2.2 Are there any tasks that a human can do that you think a computer\\ncould never be programmed to do? Why?\\n2.3 What kinds of problems that humans find difficult do you think\\ncomputers are particularly well suited to solve? Are there any such\\nproblems that you know of that computers cannot currently solve\\nbut which you believe computers will one day be able to solve?\\nWhat advances in technology or understanding are necessary\\nbefore those problems can be solved?\\n2.4 Explain the Chinese Room argument, and present some of the\\narguments against it, and the counter-arguments. Which do you\\nfind most convincing? How does this affect your view on the over-\\nall worth of the study of Artificial Intelligence?\\n2.5 If a computer passed the Turing T est, what would that prove? What\\nconditions would you want to be sure had been observed in setting\\nup the test?\\n2.6 If you replaced each of the neurons in your brain one by one with\\nelectronic neurons (take on trust for now that electronic neurons\\nare possible), what do you think would be the effect? How would\\nyour perceptions of the world change during the process? At the\\nend of the process, would you still be you? Would you still be con-\\nscious? Would you still be capable of having mental states and\\nemotions? (Note: there are no right answers to these questions. The\\npurpose in asking them is to make you think about them and\\nhopefully to inspire you to read more about the subject.)\\n2.7 Further Reading\\nThe works of Dreyfus and Dennett provide a great introduction to the\\nphilosophical arguments surrounding strong AI. The opposing view can be\\nfound thoroughly explored in Kurzweil’s works, among others. The origi-\\nnal Chinese Room argument can be found in Searle (1980).\\nA number of other books give good coverage of the popularity of Artificial\\nIntelligence in the modern world. Challoner (2002) is probably too basic\\nfor most readers but does provide an entertaining introduction to the sub-\\nject that would make a good introduction for a younger relative who was\\ninterested in learning more about the subject.\\nCambrian Intelligence: The Early History of the New AI , by Rodney A.\\nBrooks (1999 – MIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 52, 'page_label': '53'}, page_content='26 CHAPTER 2 Uses and Limitations\\nArtificial Intelligence, by Jack Challoner (2002 – Dorling Kindersley, Essen-\\ntial Science)\\nThe Turing Test and the Frame Problem: AI’s Mistaken Understanding of\\nIntelligence, by Larry J. Crockett (1994 – Intellect)\\nBrainstorms: Philosophical Essays on Mind and Psychology , by Daniel Den-\\nnett (1978 – Bradford)\\nConsciousness Explained, by Daniel Dennett (1992 – Little, Brown & Co.)\\nWhat Computers Still Can’t Do, by Hubert L. Dreyfus (1999 – The MIT Press)\\nArtificial Intelligence: The Very Idea, by J. Haugeland (1985 – The MIT Press)\\nThe Age of Spiritual Machines, by Ray Kurzweil (1999 – Viking Penguin)\\nThe Society of Mind, by Marvin Minsky (1988 – Simon & Schuster)\\nRobot: Mere Machine to Transcendent Mind , by Hans P . Moravec (2000 –\\nOxford University Press)\\nViews into the Chinese Room: New Essays on Searle and Artificial Intelligence,\\nedited by John Preston and Mark Bishop (2002 – Oxford University Press)\\nAre We Spiritual Machines?: Ray Kurzweil vs. the Critics of Strong A.I., edited\\nby Jay W. Richards (2002 – Discovery Institute)\\nThe Turing Test: The Elusive Standard of Artificial Intelligence , edited by\\nJames H. Moor (2003 – Kluwer Academic Publishers)\\nMinds, Brains, and Programs , by John R. Searle (1980 – in The Behavioral\\nand Brain Sciences, vol. 3, Cambridge University Press)\\nMinds, Brains and Science, by John R. Searle (1986 – Harvard University Press)\\nIn the Mind of the Machine: The Breakthrough in Artificial Intelligence ,b y\\nKevin Warwick (1998 – Random House)\\nArguing A. I.: The Battle for Twenty-First Century Science, by Sam Williams\\n(2002 – Random House)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 53, 'page_label': '54'}, page_content='3CHAPTER\\nKnowledge Representation\\nIf, for a given problem, we have a means of checking a proposed solution, then\\nwe can solve the problem by testing all possible answers. But this always takes\\nmuch too long to be of practical interest. Any device that can reduce this search\\nmay be of value.\\n—Marvin Minsky, Steps Toward Artificial Intelligence\\nStudy is like the heaven’s glorious sun,\\nThat will not be deep-search’d with saucy looks;\\nSmall have continual plodders ever won,\\nSave base authority from others’ books.\\nThese earthly godfathers of Heaven’s lights\\nThat give a name to every fixed star,\\nHave no more profit of their shining nights\\nThan those that walk and wot not what they are.\\n—William Shakespeare, Love’s Labours Lost\\nBetter the rudest work that tells a story or records a fact, than the richest with-\\nout meaning.\\n—John Ruskin, Seven Lamps of Architecture\\n3.1 Introduction\\nThroughout this book we will be discussing representations. The reason for\\nthis is that in order for a computer to solve a problem that relates to the real\\nworld, it first needs some way to represent the real world internally. In dealing\\nwith that internal representation, the computer is then able to solve problems.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 54, 'page_label': '55'}, page_content='28 CHAPTER 3 Knowledge Representation\\nThis chapter introduces a number of representations that are used else-\\nwhere in this book, such as semantic nets, goal trees, and search trees, and\\nexplains why these representations provide such a powerful way to solve a\\nwide range of problems.\\nThis chapter also introduces frames and the way in which inheritance can\\nbe used to provide a powerful representational system.\\nThis chapter is illustrated with a number of problems and suitable repre-\\nsentations that can be used to solve those problems.\\n3.2 The Need for a Good Representation\\nAs we will see elsewhere in this book, the representation that is used to repre-\\nsent a problem is very important. In other words, the way in which the com-\\nputer represents a problem, the variables it uses, and the operators it applies\\nto those variables can make the difference between an efficient algorithm\\nand an algorithm that doesn’t work at all. This is true of all Artificial Intelli-\\ngence problems, and as we see in the following chapters, it is vital for search.\\nImagine that you are looking for a contact lens that you dropped on a foot-\\nball field. Y ou will probably use some knowledge about where you were on\\nthe field to help you look for it. If you spent time in only half of the field,\\nyou do not need to waste time looking in the other half.\\nNow let us suppose that you are having a computer search the field for the\\ncontact lens, and let us further suppose that the computer has access to an\\nomniscient oracle that will answer questions about the field and can accu-\\nrately identify whether the contact lens is in a particular spot.\\nNow we must choose a representation for the computer to use so that it can\\nformulate the correct questions to ask.\\nOne representation might be to have the computer divide the field into\\nfour equal squares and ask the oracle for each square, “Is the lens in this\\nsquare?” This will identify the location on the field of the lens but will not\\nreally be very helpful to you because you will still have a large area to search\\nonce you find which quarter of the field the lens is in.\\nAnother representation might be for the computer to have a grid con-\\ntaining a representation of every atom contained in the field. For each'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 55, 'page_label': '56'}, page_content='3.3 Semantic Nets 29\\natom, the computer could ask its oracle, “Is the lens in contact with this\\natom?”\\nThis would give a very accurate answer indeed, but would be an extremely\\ninefficient way of finding the lens. Even an extremely powerful computer\\nwould take a very long time indeed to locate the lens.\\nPerhaps a better representation would be to divide the field up into a grid\\nwhere each square is one foot by one foot and to eliminate all the squares\\nfrom the grid that you know are nowhere near where you were when you\\nlost the lens. This representation would be much more helpful.\\nIn fact, the representations we have described for the contact lens problem\\nare all really the same representation, but at different levels of granularity.\\nThe more difficult problem is to determine the data structure that will be\\nused to represent the problem we are exploring. As we will see throughout\\nthis book, there are a wide range of representations used in Artificial\\nIntelligence.\\nWhen applying Artificial Intelligence to search problems, a useful, efficient,\\nand meaningful representation is essential. In other words, the representa-\\ntion should be such that the computer does not waste too much time on\\npointless computations, it should be such that the representation really\\ndoes relate to the problem that is being solved, and it should provide a\\nmeans by which the computer can actually solve the problem.\\nIn this chapter, we look at a number of representations that are used in\\nsearch, and in particular we will look at search trees, which are used\\nthroughout this part of the book.\\n3.3 Semantic Nets\\nThe semantic net is a commonly used representation in Artificial Intelli-\\ngence. A semantic net is a graph consisting of nodes that are connected by\\nedges. The nodes represent objects, and the links between nodes represent\\nrelationships between those objects. The links are usually labeled to indi-\\ncate the nature of the relationship.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 56, 'page_label': '57'}, page_content='30 CHAPTER 3 Knowledge Representation\\nchases\\nchases\\nDog\\nCat\\nFido\\nBob\\nCheese\\nBuilder\\nFang\\nMice\\nowns\\nis a\\nis a\\nis a\\neats\\neat\\nFigure 3.1\\nA simple semantic net\\nA simple example of a semantic net is shown in Figure 3.1.\\nNote that in this semantic net, the links are arrows, meaning that they have\\na direction. In this way, we can tell from the diagram that Fido chases Fang,\\nnot that Fang chases Fido. It may be that Fang does chase Fido as well, but\\nthis information is not presented in this diagram.\\nSemantic nets provide a very intuitive way to represent knowledge about\\nobjects and the relationships that exist between those objects. The data in\\nsemantic nets can be reasoned about in order to produce systems that have\\nknowledge about a particular domain. Semantic nets do have limitations,\\nsuch as the inability to represent negations: “Fido is not a cat. ” As we see in\\nChapter 7, this kind of fact can be expressed easily in first-order predicate\\nlogic and can also be managed by rule-based systems.\\nNote that in our semantic net we have represented some specific individu-\\nals, such as Fang, Bob, and Fido, and have also represented some general\\nclasses of things, such as cats and dogs. The specific objects are generally\\nreferred to as instances of a particular class. Fido is an instance of the class\\ndog. Bob is an instance of the class Builder.\\nIt is a little unclear from Figure 3.1 whether cheese is a class or an instance of\\na class. This information would need to be derived by the system that is\\nmanipulating the semantic net in some way. For example, the system might\\nhave a rule that says “any object that does not have an ‘is-a’ relationship to a\\nclass is considered to represent a class of objects. ” Rules such as this must be\\napplied with caution and must be remembered when building a semantic net.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 57, 'page_label': '58'}, page_content='3.4 Inheritance 31\\nAn important feature of semantic nets is that they convey meaning. That is\\nto say, the relationship between nodes and edges in the net conveys infor-\\nmation about some real-world situation. A good example of a semantic net\\nis a family tree diagram. Usually, nodes in these diagrams represent people,\\nand there are edges that represent parental relationships, as well as relation-\\nships by marriage.\\nEach node in a semantic net has a label that identifies what the node repre-\\nsents. Edges are also labeled. Edges represent connections or relationships\\nbetween nodes. In the case of searching a dictionary for a page that con-\\ntains a particular word, each node might represent a single page, and each\\nedge would represent a way of getting from one page to another.\\nThe particular choice of semantic net representation for a problem will\\nhave great bearing on how the problem is solved. A simple representation\\nfor searching for a word in a dictionary would be to have the nodes\\narranged in a chain with one connection from the first node to the second,\\nand then from the second to the third, and so on. Clearly, any method that\\nattempts to search this graph will be fairly inefficient because it means vis-\\niting each node in turn until the desired node is found. This is equivalent\\nto flicking through the pages of the dictionary in order until the desired\\npage is found.\\nAs we see in Section 3.7, representing the dictionary by a different data\\nstructure can give much more efficient ways of searching.\\n3.4 Inheritance\\nInheritance is a relationship that can be particularly useful in AI and in\\nprogramming. The idea of inheritance is one that is easily understood\\nintuitively. For example, if we say that all mammals give birth to live\\nbabies, and we also say that all dogs are mammals, and that Fido is a dog,\\nthen we can conclude that Fido gives birth to live mammals. Of course,\\nthis particular piece of reasoning does not take into account the fact that\\nFido might be male, or if Fido is female, might be too young or too old to\\ngive birth.\\nSo, inheritance allows us to specify properties of a superclass and then to\\ndefine a subclass, which inherits the properties of the superclass. In our'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 58, 'page_label': '59'}, page_content='32 CHAPTER 3 Knowledge Representation\\nexample, mammals are the superclass of dogs and Fido. Dogs are the sub-\\nclass of mammals and the superclass of Fido.\\nIf you have programmed with an object-oriented programming language\\nsuch as C++ or Java, then you will be familiar with the concept of inheri-\\ntance and will appreciate its power. Object-oriented programming is dis-\\ncussed further in Section 3.6.\\nAs has been shown, although inheritance is a useful way to express general-\\nities about a class of objects, in some cases we need to express exceptions to\\nthose generalities (such as, “Male animals do not give birth” or “Female\\ndogs below the age of six months do not give birth”). In such cases, we say\\nthat thedefault valuehas beenoverriddenin the subclass.\\nAs we will see, it is usually useful to be able to express in our chosen repre-\\nsentation which values can be overridden and which cannot.\\n3.5 Frames\\nFrame-based representation is a development of semantic nets and allows\\nus to express the idea of inheritance.\\nAs with semantic nets, a frame system consists of a set of frames (or\\nnodes), which are connected together by relations. Each frame describes\\neither an instance (an instance frame) or a class (a class frame).\\nThus far, we have said that instances are “objects” without really saying\\nwhat an object is. In this context, an object can be a physical object, but it\\ndoes not have to be. An object can be a property (such as a color or a\\nshape), or it can be a place, or a situation, or a feeling. This idea of objects\\nis the same that is used in object-oriented programming languages, such as\\nC++ and Java. Frames are thus an object-oriented representation that can\\nbe used to build expert systems. Object-oriented programming is further\\ndiscussed in Section 3.6.\\nEach frame has one or more slots, which are assigned slot values. This is\\nthe way in which the frame system network is built up. Rather than simply\\nhaving links between frames, each relationship is expressed by a value being\\nplaced in a slot. For example, the semantic net in Figure 3.1 might be repre-\\nsented by the following frames:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 59, 'page_label': '60'}, page_content='3.5 Frames 33\\nIs a\\nOwns\\nEats\\nBuilder\\nFido\\nCheese\\nBob\\nIs a\\nChases\\nDog\\nFang\\nFido\\nFigure 3.2\\nPartial representation for\\na frame system for the\\nsemantic net shown in Fig-\\nure 3.1\\nFrame Name Slot Slot Value\\nBob is a Builder\\nowns Fido\\neats Cheese\\nFido is a Dog\\nchases Fang\\nFang is a Cat\\nchases Mice\\nMice eat Cheese\\nCheese\\nBuilder\\nDog\\nCat\\nWe can also represent this frame system in a diagrammatic form using rep-\\nresentations such as those shown in Figure 3.2.\\nWhen we say, “Fido is a dog, ” we really mean, “Fido is an instance of the\\nclass dog,” or “Fido is a member of the class of dogs.” Hence, the “is-a” rela-\\ntionship is very important in frame-based representations because it\\nenables us to express membership of classes. This relationship is also\\nknown as generalization because referring to the class of mammals is more\\ngeneral than referring to the class of dogs, and referring to the class of dogs\\nis more general than referring to Fido.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 60, 'page_label': '61'}, page_content='34 CHAPTER 3 Knowledge Representation\\nIt is also useful to be able to talk about one object being a part of another\\nobject. For example, Fido has a tail, and so the tail is part of Fido. This rela-\\ntionship is known as aggregation because Fido can be considered an aggre-\\ngate of dog parts.\\nOther relationships are known as association. An example of such a relation-\\nship is the “chases” relationship. This explains how Fido and Fang are related\\nor associated with each other. Note that association relationships have mean-\\ning in two directions. The fact that Fido chases Fang means that Fang is chased\\nby Fido, so we are really expressing two relationships in one association.\\n3.5.1 Why Are Frames Useful?\\nFrames can be used as a data structure by Expert Systems, which are dis-\\ncussed in more detail in Chapter 9.\\nThe main advantage of using frame-based systems for expert systems over\\nthe rule-based approach is that all the information about a particular\\nobject is stored in one place. In a rule-based system, information about\\nFido might be stored in a number of otherwise unrelated rules, and so if\\nFido changes, or a deduction needs to be made about Fido, time may be\\nwasted examining irrelevant rules and facts in the system, whereas with the\\nframe system, the Fido frame could be quickly examined.\\nThis difference becomes particularly clear when we consider frames that\\nhave a very large number of slots and where a large number of relationships\\nexist between frames (i.e., a situation in which objects have a lot of proper-\\nties, and a lot of objects are related to each other). Clearly, many real-world\\nsituations have these properties.\\n3.5.2 Inheritance\\nWe might extend our frame system with the following additional information:\\nDogs chase cats\\nCats chase mice\\nIn expressing these pieces of information, we now do not need to state\\nexplicitly that Fido chases Fang or that Fang chases mice. In this case, we\\ncan inherit this information because Fang is an instance of the class Cats,\\nand Fido is an instance of the class Dogs.\\nWe might also add the following additional information:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 61, 'page_label': '62'}, page_content='3.5 Frames 35\\nMammals breathe\\nDogs are mammals\\nCats are mammals\\nHence, we have now created a new superclass, mammals, of which dogs and\\ncats are subclasses. In this way, we do not need to express explicitly that cats\\nand dogs breathe because we can inherit this information. Similarly, we do\\nnot need to express explicitly that Fido and Fang breathe—they are\\ninstances of the classes Dogs and Cats, and therefore they inherit from\\nthose classes’ superclasses.\\nNow let us add the following fact:\\nMammals have four legs\\nOf course, this is not true, because humans do not have four legs, for exam-\\nple. In a frame-based system, we can express that this fact is the default\\nvalue and that it may be overridden. Let us imagine that in fact Fido has\\nhad an unfortunate accident and now has only three legs. This information\\nmight be expressed as follows:\\nFrame Name Slot Slot Value\\nMammal *number of legs four\\nDog subclass Mammal\\nCat subclass Mammal\\nFido is a Dog\\nnumber of legs three\\nFang is a Cat\\nHere we have used an asterisk (*) to indicate that the value for the “number\\nof legs” slot for the Mammal class is a default value and can be overridden,\\nas has been done for Fido.\\n3.5.3 Slots as Frames\\nIt is also possible to express a range of values that a slot can take—for\\nexample, the number of legs slot might be allowed a number between 1 and\\n4 (although, for the insects class, it might be allowed 6).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 62, 'page_label': '63'}, page_content='36 CHAPTER 3 Knowledge Representation\\nOne way to express this kind of restriction is by allowing slots to be frames.\\nIn other words, the number of legs slot can be represented as a frame,\\nwhich includes information about what range of values it can take:\\nFrame Name Slot Slot Value\\nNumber of legs minimum value 1\\nmaximum value 4\\nIn this way, we can also express more complex ideas about slots, such as the\\ninverse of a slot (e.g., the “chases” slot has an inverse, which is the “chased\\nby” slot). We can also place further limitations on a slot, such as to specify\\nwhether or not it can take multiple values (e.g., the “number of legs” slot\\nshould probably only take one value, whereas the “eats” slot should be\\nallowed to take many values).\\n3.5.4 Multiple Inheritance\\nIt is possible for a frame to inherit properties from more than one other\\nframe. In other words, a class can be a subclass of two superclasses, and an\\nobject can be an instance of more than one class. This is known as multiple\\ninheritance.\\nFor example, we might add the following frames to our system:\\nFrame Name Slot Slot Value\\nHuman Subclass Mammal\\nNumber of legs two\\nBuilder Builds houses\\nBob is a Human\\nFrom this, we can see that Bob is a human, as well as being a builder. Hence,\\nwe can inherit the following information about Bob:\\nHe has two legs\\nHe builds houses'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 63, 'page_label': '64'}, page_content='3.5 Frames 37\\nIn some cases, we will encounter conflicts, where multiple inheritance\\nleads us to conclude contradictory information about a frame. For exam-\\nple, let us consider the following simple frame system:\\nFrame Name Slot Slot Value\\nCheese is smelly\\nThing wrapped in foil is not smelly\\nCheddar is a Cheese\\nis a Thing wrapped in foil\\n(Note: the slot “is” might be more accurately named “has property. ” We\\nhave named it “is” to make the example clearer.)\\nHere we can see that cheddar is a type of cheese and that it comes wrapped\\nin foil. Cheddar should inherit its smelliness from the Cheese class, but it\\nalso inherits nonsmelliness from the Thing wrapped in foil class. In this\\ncase, we need a mechanism to decide which features to inherit from which\\nsuperclasses. One simple method is to simply say that conflicts are resolved\\nby the order in which they appear. So if a fact is established by inheritance,\\nand then that fact is contradicted by inheritance, the first fact is kept\\nbecause it appeared first, and the contradiction is discarded.\\nThis is clearly rather arbitrary, and it would almost certainly be better to\\nbuild the frame system such that conflicts of this kind cannot occur.\\nMultiple inheritance is a key feature of most object-oriented programming\\nlanguages. This is discussed in more detail in Section 3.6.\\n3.5.5 Procedures\\nIn object-oriented programming languages such as C++ or Java, classes\\n(and hence objects) have methods associated with them. This is also true\\nwith frames. Frames have methods associated with them, which are called\\nprocedures. Procedures associated with frames are also called procedural\\nattachments.\\nA procedure is a set of instructions associated with a frame that can be exe-\\ncuted on request. For example, a slot reader procedure might return the\\nvalue of a particular slot within the frame. Another procedure might insert'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 64, 'page_label': '65'}, page_content='38 CHAPTER 3 Knowledge Representation\\na value into a slot (a slot writer ). Another important procedure is the\\ninstance constructor, which creates an instance of a class.\\nSuch procedures are called when needed and so are called WHEN-\\nNEEDED procedures. Other procedures can be set up that are called auto-\\nmatically when something changes.\\n3.5.6 Demons\\nA demon is a particular type of procedure that is run automatically when-\\never a particular value changes or when a particular event occurs.\\nSome demons act when a particular value is read. In other words, they are\\ncalled automatically when the user of the system, or the system itself, wants\\nto know what value is placed in a particular slot. Such demons are called\\nWHEN-READ procedures. In this way, complex calculations can be made\\nthat calculate a value to return to the user, rather than simply giving back\\nstatic data that are contained within the slot. This could be useful, for\\nexample, in a large financial system with a large number of slots because it\\nwould mean that the system would not necessarily need to calculate every\\nvalue for every slot. It would need to calculate some values only when they\\nwere requested.\\nWHEN-CHANGED procedures (also known as WHEN-WRITTEN pro-\\ncedures) are run automatically when the value of a slot is changed. This\\ntype of function can be particularly useful, for example, for ensuring that\\nthe values assigned to a slot fit within a set of constraints. For example, in\\nour example above, a WHEN-WRITTEN procedure might run to ensure\\nthat the “number of legs” slot never has a value greater than 4 or less than 1.\\nIf a value of 7 is entered, a system message might be produced, telling the\\nuser that he or she has entered an incorrect value and that he or she should\\nenter a different value.\\n3.5.7 Implementation\\nWith the addition of procedures and demons, a frame system becomes a\\nvery powerful tool for reasoning about objects and relationships. The sys-\\ntem has procedural semantics as opposed to declarative semantics, which'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 65, 'page_label': '66'}, page_content='3.5 Frames 39\\nmeans that the order in which things occur affects the results that the sys-\\ntem produces. In some cases, this can cause problems and can make it\\nharder to understand how the system will behave in a given situation.\\nThis lack of clarity is usually compensated for by the level of flexibility\\nallowed by demons and the other features that frame systems possess.\\nFrame systems can be implemented by a very simple algorithm if we do not\\nallow multiple inheritance. The following algorithm allows us to find the\\nvalue of a slot S, for a frame F. In this algorithm definition, we will use the\\nnotation\\nF[S]to indicate the value of slot S in frame F. We also use the nota-\\ntion instance (F1, F2) to indicate that frame F1 is an instance of frame F2\\nand subclass (F1, F2)to indicate that frame F1 is a subclass of frame F2.\\nFunction find_slot_value (S, F)\\n{\\nif F[S] == V             // if the slot contains\\nthen return V       // a value, return it.\\nelse if instance (F, F’)\\nthen return find_slot_value (S, F’)\\nelse if subclass (F, Fs)\\nthen return find_slot_value (S, Fs)\\nelse return FAILURE;\\n}\\nIn other words, the slot value of a frame F will either be contained within\\nthat frame, or a superclass of F, or another frame of which F is an instance.\\nIf none of these provides a value, then the algorithm fails.\\nClearly, frames could also be represented in an object-oriented program-\\nming language such as C++ or Java.\\nA frame-based expert system can be implemented in a similar way to the\\nrule-based systems, which we examine in Chapter 9. T o answer questions\\nabout an object, the system can simply examine that object’s slots or the\\nslots of classes of which the object is an instance or a subclass.\\nIf the system needs additional information to proceed, it can ask the user\\nquestions in order to fill in additional information. In the same way as with\\nrule-based systems, WHEN-CHANGED procedures can be set up that'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 66, 'page_label': '67'}, page_content='40 CHAPTER 3 Knowledge Representation\\nmonitor the values of slots, and when a particular set of values is identified,\\nthis can be used by the system to derive a conclusion and thus recommend\\nan action or deliver an explanation for something.\\n3.5.8 Combining Frames with Rules\\nIt is possible to combine frames with rules, and, in fact, many frame-based\\nexpert systems use rules in much the same way that rule-based systems do,\\nwith the addition of pattern matching clauses, which are used to identify\\nvalues that match a set of conditions from all the frames in the system.\\nTypically, a frame-based system with rules will use rules to try to derive\\nconclusions, and in some cases where it cannot find a value for a particular\\nslot, a WHEN-NEEDED procedure will run to determine the value for that\\nslot. If no value is found from that procedure, then the user will be asked to\\nsupply a value.\\n3.5.9 Representational Adequacy\\nWe can represent the kinds of relationships that we can describe with\\nframes in first-order predicate logic. For example:\\n/H7001x Dog(x) → Mammal(x)\\nFirst-order predicate logic is discussed in detail in Chapter 7. For now, you\\nsimply need to know how to read that expression. It is read as follows:\\n“For all x’s, if x is a dog, then x is a mammal. ”\\nThis can be rendered in more natural English as:\\n“All dogs are mammals. ”\\nIn fact, we can also express this relationship by the introduction of a new\\nsymbol, which more closely mirrors the meaning encompassed by the idea\\nof inheritance:\\nAlmost anything that can be expressed using frames can be expressed using\\nfirst-order predicate logic (FPOL). The same is not true in reverse. For\\nexample, it is not easy to represent negativity (“Fido is not a cat”) or quan-\\ntification (“there is a cat that has only one leg”). We say that FOPL has\\ngreater representational adequacy than frame-based representations.\\nDog Mammalsubset\\uf8e7→\\uf8e7\\uf8e7'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 67, 'page_label': '68'}, page_content='3.6 Object-Oriented Programming 41\\nIn fact, frame-based representations do have some aspects that cannot be\\neasily represented in FOPL. The most significant of these is the idea of\\nexceptions, or overriding default values.\\nAllowing exceptions to override default values for slots means that the\\nframe-based system is not monotonic (monotonicity is discussed in Chap-\\nter 7). In other words, conclusions can be changed by adding new facts to\\nthe system.\\nIn this section, we have discussed three main representational methods:\\nlogic, rules, and frames (or semantic nets). Each of these has advantages\\nand disadvantages, and each is preferable over the others in different situa-\\ntions. The important thing is that in solving a particular problem, the cor-\\nrect representation must be chosen.\\n3.6 Object-Oriented Programming\\nWe now briefly explore some of the ideas used in object-oriented program-\\nming, and, in particular, we see how they relate to some of the ideas we have\\nseen in Sections 3.4 and 3.5 on inheritance and frames.\\nTwo of the best-known object-oriented programming languages are Java\\nand C++. These two languages use a similar syntax to define classes and\\nobjects that are instantiations of those classes.\\nA typical class in these languages might be defined as:\\nclass animal\\n{\\nanimal ();\\nEye *eyes;\\nLeg *legs;\\nHead head;\\nTail tail;\\n}\\nThis defines a class called animal that has a number of fields, which are the\\nvarious body parts. It also has a constructor, which is a function that is\\ncalled when an instantiation of the class is called. Classes can have other\\nfunctions too, and these functions are equivalent to the procedures we saw\\nin Section 3.5.5.\\nWe can create an instance of the class animal as follows:\\nanimal an_animal = new animal ();'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 68, 'page_label': '69'}, page_content='42 CHAPTER 3 Knowledge Representation\\nThis creates an instance of the class animal. The instance, which is an object,\\nis called “an_animal” . In creating it, the constructor animal () is called.\\nWe can also create a subclass of animal:\\nClass dog : animal\\n{\\nbark ();\\n}\\nHere we have created a subclass of animal called dog. Dog has inherited all of\\nthe properties of animal and also has a new function of its own called bark ().\\nIn some object-oriented programming languages, it is possible to use mul-\\ntiple inheritance. This means that one class inherits properties from more\\nthan one parent class. While C++ does allow multiple inheritance, Java,\\nwhich itself inherited many features from C++, does not allow multiple\\ninheritance. This is because multiple inheritance was seen by the develop-\\ners of Java as an “unclean” idea—one that creates unnecessarily compli-\\ncated object-oriented structures. Additionally, it is always possible to\\nachieve the same results using single inheritance as it is with multiple\\ninheritance.\\nObject-oriented programming languages such as Java and C++ use the\\nprinciples that were invented for the frames structure. There are also\\nobject-oriented programming languages such as IBM’s APL2 that use a\\nframe-based structure.\\nThe ideas explored in Sections 3.4 and 3.5 of this book are thus very rele-\\nvant to object-oriented programming, as well as being an important part of\\nArtificial Intelligence research.\\n3.7 Search Spaces\\nMany problems in Artificial Intelligence can be represented as search\\nspaces. In simple terms, a search space is a representation of the set of pos-\\nsible choices in a given problem, one or more of which are the solution to\\nthe problem.\\nFor example, attempting to find a particular word in a dictionary with 100\\npages, a search space will consist of each of the 100 pages. The page that is\\nbeing searched for is called a goal, and it can be identified by seeing'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 69, 'page_label': '70'}, page_content='3.7 Search Spaces 43\\nState 1\\nRobot in room A.\\nBlock in room A.\\nState 2\\nRobot in room B.\\nBlock in room A.\\nState 3\\nRobot in room C.\\nBlock in room A.\\nState 4\\nRobot in room A.\\nBlock in room B.\\nState 5\\nRobot in room B.\\nBlock in room B.\\nState 6\\nRobot in room C.\\nBlock in room B.\\nState 7\\nRobot in room A.\\nBlock in room C.\\nState 8\\nRobot in room B.\\nBlock in room C.\\nState 9\\nRobot in room C.\\nBlock in room C.\\nFigure 3.3\\nA simple state-space \\ndiagram\\nwhether the word we are looking for is on the page or not. (In fact, this\\nidentification might be a search problem in itself, but for this example we\\nwill assume that this is a simple, atomic action.)\\nThe aim of most search procedures is to identify one or more goals and,\\nusually, to identify one or more paths to those goals (often the shortest\\npath, or path with least cost).\\nBecause a search space consists of a set of states, connected by paths that\\nrepresent actions, they are also known as state spaces. Many search prob-\\nlems can be represented by a state space, where the aim is to start with the\\nworld in one state and to end with the world in another, more desirable\\nstate. In the missionaries and cannibals problem that is discussed later in\\nthis chapter, the start state has all missionaries and cannibals on one side of\\nthe river, and the goal state has them on the other side. The state space for\\nthe problem consists of all possible states in between.\\nFigure 3.3 shows a very simple state-space diagram for a robot that lives in\\nan environment with three rooms (room A, room B, and room C) and with'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 70, 'page_label': '71'}, page_content='44 CHAPTER 3 Knowledge Representation\\nA A\\nB\\nB\\nC\\nC\\nGFE\\nE\\nD\\nD\\nFigure 3.4\\nA semantic net and a\\nsemantic tree\\na block that he can move from room to room. Each state consists of a pos-\\nsible arrangement of the robot and the block. Hence, for example, in state\\n1, both the robot and the block are in room A. Note that this diagram does\\nnot explain how the robot gets from one room to another or how the block\\nis moved. This kind of representation assumes that the robot has a repre-\\nsentation of a number of actions that it can take. T o determine how to get\\nfrom one state to another state, the robot needs to use a process called\\nplanning, which is covered in detail in Part 5 of this book.\\nIn Figure 3.3, the arrows between states represent state transitions.N o t e\\nthat there are not transitions between every pair of states. For example, it is\\nnot possible to go from state 1 to state 4 without going through state 5. This\\nis because the block cannot move on its own and can only be moved to a\\nroom if the robot moves there. Hence, a state-space diagram is a valuable\\nway to represent the possible actions that can be taken in a given state and\\nthus to represent the possible solutions to a problem.\\n3.8 Semantic Trees\\nA semantic tree is a kind of semantic net that has the following properties:\\n■ Each node (except for the root node, described below) has exactly\\none predecessor (parent) and one or more successors (children).\\nIn the semantic tree in Figure 3.4, node A is the predecessor of\\nnode B: node A connects by one edge to node B and comes before\\nit in the tree. The successors of node B, nodes D and E, connect\\ndirectly (by one edge each) to node B and come after it in the tree.\\nWe can write these relationships as: succ (B) = D and pred (B) = A.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 71, 'page_label': '72'}, page_content='3.8 Semantic Trees 45\\nThe nonsymmetric nature of this relationship means that a seman-\\ntic tree is a directed graph. By contrast, nondirected graphs are\\nones where there is no difference between an arc from A to B and\\nan arc from B to A.\\n■ One node has no predecessors. This node is called the root\\nnode. In general, when searching a semantic tree, we start at the\\nroot node. This is because the root node typically represents a\\nstarting point of the problem. For example, when we look at\\ngame trees in Chapter 6, we will see that the game tree for a\\ngame of chess represents all the possible moves of the game,\\nstarting from the initial position in which neither player has\\nmade a move. This initial position corresponds to the root node\\nin the game tree.\\n■ Some nodes have no successors. These nodes are called leaf nodes.\\nOne or more leaf nodes are called goal nodes. These are the nodes\\nthat represent a state where the search has succeeded.\\n■ Apart from leaf nodes, all nodes have one or more successors.\\nApart from the root node, all nodes have exactly one predecessor.\\n■ An ancestor of a node is a node further up the tree in some path. A\\ndescendent comes after a node in a path in the tree.\\nA path is a route through the semantic tree, which may consist of just one\\nnode (a path of length 0). A path of length 1 consists of a node, a branch\\nthat leads from that node, and the successor node to which that branch\\nleads. A path that leads from the root node to a goal node is called a com-\\nplete path. A path that leads from the root node to a leaf node that is not a\\ngoal node is called a partial path.\\nWhen comparing semantic nets and semantic trees visually, one of the\\nmost obvious differences is that semantic nets can contain cycles, but\\nsemantic trees cannot. A cycle is a path through the net that visits the same\\nnode more than once. Figure 3.4 shows a semantic net and a semantic tree.\\nIn the semantic net, the path A, B, C, D, A. . . is a cycle.\\nIn semantic trees, an edge that connects two nodes is called a branch.I fa\\nnode has n successors, that node is said to have a branching factor of n.A\\ntree is often said to have a branching factor of n if the average branching\\nfactor of all the nodes in the tree is n.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 72, 'page_label': '73'}, page_content='46 CHAPTER 3 Knowledge Representation\\nA\\nBE E CC\\nA\\nECD\\nDB\\nA\\nD\\nBE E\\nA\\nD\\nA\\nC\\nThe root node of a tree is said to be at level 0, and the successors of the root\\nnode are at level 1. Successors of nodes at level n are at level n + 1.\\n3.9 Search Trees\\nSearching a semantic net involves traversing the net systematically (or in\\nsome cases, not so systematically), examining nodes, looking for a goal\\nnode. Clearly following a cyclic path through the net is pointless because\\nfollowing A,B,C,D,A will not lead to any solution that could not be reached\\njust by starting from A. We can represent the possible paths through a\\nsemantic net as a search tree, which is a type of semantic tree.\\nThe search tree shown in Figure 3.5 represents the possible paths through\\nthe semantic net shown in Figure 3.4. Each node in the tree represents a\\npath, with successive layers in the tree representing longer and longer paths.\\nNote that we do not include cyclical paths, which means that some\\nbranches in the search tree end on leaf nodes that are not goal nodes. Also\\nnote that we label each node in the search tree with a single letter, which\\nFigure 3.5\\nA search tree representa-\\ntion for the semantic net\\nin Figure 3.4.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 73, 'page_label': '74'}, page_content='3.9 Search Trees 47\\nrepresents the path from the root node to that node in the semantic net in\\nFigure 3.4.\\nHence, searching for a node in a search tree corresponds to searching for a\\ncomplete path in a semantic net.\\n3.9.1 Example 1: Missionaries and Cannibals\\nThe Missionaries and Cannibals problem is a well-known problem that is\\noften used to illustrate AI techniques. The problem is as follows:\\nThree missionaries and three cannibals are on one side of a river, with a canoe.\\nThey all want to get to the other side of the river. The canoe can only hold one\\nor two people at a time. At no time should there be more cannibals than mis-\\nsionaries on either side of the river, as this would probably result in the mis-\\nsionaries being eaten.\\nT o solve this problem, we need to use a suitable representation.\\nFirst of all, we can consider a state in the solving of the problem to consist\\nof a certain number of cannibals and a certain number of missionaries on\\neach side of the river, with the boat on one side or the other. We could rep-\\nresent this, for example, as\\n3, 3, 1 0, 0, 0\\nThe left-hand set of numbers represents the number of cannibals, mission-\\naries, and canoes on one side of the river, and the right-hand side repre-\\nsents what is on the other side.\\nBecause the number that is on one side is entirely dependent on the num-\\nber that is on the other side, we can in fact just show how many of each are\\non the finishing side, meaning that the starting state is represented as\\n0, 0, 0\\nand the goal state is\\n3, 3, 1\\nAn example of a state that must be avoided is\\n2, 1, 1\\nHere, there are two cannibals, one canoe, and just one missionary on the\\nother side of the river. This missionary will probably not last very long.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 74, 'page_label': '75'}, page_content='48 CHAPTER 3 Knowledge Representation\\n0,0,0\\n0,0,0 0,0,0 1,0,0 1,0,0\\n1,1,11,0,1 2,0,1\\n11 23\\n215\\nFigure 3.6\\nA partial search tree for\\nthe missionaries and can-\\nnibals problem\\nT o get from one state to another, we need to apply an operator. The opera-\\ntors that we have available are the following:\\n1. Move one cannibal to the other side\\n2. Move two cannibals to the other side\\n3. Move one missionary to the other side\\n4. Move two missionaries to the other side\\n5. Move one cannibal and one missionary to the other side\\nSo if we apply operator 5 to the state represented by 1, 1, 0, then we would\\nresult in state 2, 2, 1. One cannibal, one missionary, and the canoe have now\\nmoved over to the other side. Applying operator 3 to this state would lead\\nto an illegal state: 2, 1, 0.\\nWe consider rules such as this to be constraints, which limit the possible\\noperators that can be applied in each state. If we design our representation\\ncorrectly, the constraints are built in, meaning we do not ever need to\\nexamine illegal states.\\nWe need to have a test that can identify if we have reached the goal\\nstate—3, 3, 1.\\nWe will consider the cost of the path that is chosen to be the number of steps\\nthat are taken, or the number of times an operator is applied. In some cases,\\nas we will see later, it is desirable to find a solution that minimizes cost.\\nThe first three levels of the search tree for the missionaries and cannibals\\nproblem is shown in Figure 3.6 (arcs are marked with which operator has\\nbeen applied).\\nNow, by extending this tree to include all possible paths, and the states\\nthose paths lead to, a solution can be found. A solution to the problem\\nwould be represented as a path from the root node to a goal node.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 75, 'page_label': '76'}, page_content='3.9 Search Trees 49\\nThis tree represents the presence of a cycle in the search space. Note that the\\nuse of search trees to represent the search space means that our representa-\\ntion never contains any cycles, even when a cyclical path is being followed\\nthrough the search space.\\nBy applying operator 1 (moving one cannibal to the other side) as the first\\naction, and then applying the same operator again, we return to the start\\nstate. This is a perfectly valid way to try to solve the problem, but not a very\\nefficient one.\\n3.9.2 Improving the Representation\\nA more effective representation for the problem would be one that did not\\ninclude any cycles. Figure 3.7 is an extended version of the search tree for\\nthe problem that omits cycles and includes goal nodes.\\nNote that in this tree, we have omitted most repeated states. For example,\\nfrom the state 1,0,0, operator 2 is the only one shown. In fact, operators 1\\nand 3 can also be applied, leading to states 2,0,1 and 1,1,1 respectively. Nei-\\nther of these transitions is shown because those states have already\\nappeared in the tree.\\nAs well as avoiding cycles, we have thus removed suboptimal paths from\\nthe tree. If a path of length 2 reaches a particular state, s, and another path\\nof length 3 also reaches that state, it is not worth pursuing the longer path\\nbecause it cannot possibly lead to a shorter path to the goal node than the\\nfirst path.\\nHence, the two paths that can be followed in the tree in Figure 3.7 to the\\ngoal node are the shortest routes (the paths with the least cost) to the goal,\\nbut they are by no means the only paths. Many longer paths also exist.\\nBy choosing a suitable representation, we are thus able to improve the effi-\\nciency of our search method. Of course, in actual implementations, things\\nmay not be so simple. T o produce the search tree without repeated states, a\\nmemory is required that can store states in order to avoid revisiting them. It\\nis likely that for most problems this memory requirement is a worthwhile\\ntradeoff for the saving in time, particularly if the search space being\\nexplored has many repeated states and cycles.\\nSolving the Missionaries and Cannibals problem involves searching the\\nsearch tree. As we will see, search is an extremely useful method for solving\\nproblems and is widely used in Artificial Intelligence.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 76, 'page_label': '77'}, page_content='50 CHAPTER 3 Knowledge Representation\\n0,0,0\\n1,1,11,0,1 2,0,1\\n2,2,1\\n1,3,1\\n3,3,13,3,1\\n0,3,0\\n2,3,1\\n2,2,01,3,0\\n2,0,0\\n3,0,1\\n1,0,0 3\\n1,0,0\\n1,1,0\\n2\\n1\\n2\\n2\\n25\\n1\\n1\\n4\\n4\\n5\\n1 5\\n13\\nFigure 3.7\\nSearch tree without cycles\\n3.9.3 Example 2: The Traveling Salesman\\nThe Traveling Salesman problem is another classic problem in Artificial\\nIntelligence and is NP-Complete, meaning that for large instances of the\\nproblem, it can be very difficult for a computer program to solve in a rea-\\nsonable period of time. A problem is defined as being in the class P if it can\\nbe solved in polynomial time. This means that as the size of the problem\\nincreases, the time it will take a deterministic computer to solve the prob-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 77, 'page_label': '78'}, page_content='3.9 Search Trees 51\\nlem will increase by some polynomial function of the size. Problems that\\nare NP can be solved nondeterministically in polynomial time. This means\\nthat if a possible solution to the problem is presented to the computer, it\\nwill be able to determine whether it is a solution or not in polynomial time.\\nThe hardest NP problems are termed NP-Complete. It was shown by\\nStephen Cook that a particular group of problems could be transformed\\ninto the satisfiability problem (see Chapter 16). These problems are defined\\nas being NP-Complete. This means that if one can solve the satisfiability\\nproblem (for which solutions certainly do exist), then one can solve any\\nNP-Complete problem. It also means that NP-Complete problems take a\\ngreat deal of computation to solve.\\nThe Traveling Salesman problem is defined as follows:\\nA salesman must visit each of a set of cities and then return home. The aim of\\nthe problem is to find the shortest path that lets the salesman visit each city.\\nLet us imagine that our salesman is touring the following American cities:\\nA Atlanta\\nB Boston\\nC Chicago\\nD Dallas\\nE El Paso\\nOur salesman lives in Atlanta and must visit all of the other four cities\\nbefore returning home. Let us imagine that our salesman is traveling by\\nplane and that the cost of each flight is directly proportional to distance\\nbeing traveled and that direct flights are possible between any pair of cities.\\nHence, the distances can be shown on a graph as in Figure 3.8.\\n(Note: The distances shown are not intended to accurately represent the\\ntrue locations of these cities but have been approximated for the purposes\\nof this illustration.)\\nThe graph in Figure 3.8 shows the relationships between the cities. We\\ncould use this graph to attempt to solve the problem. Certainly, we can use\\nit to find possible paths: One possible path is A,B,C,E,D,A, which has a\\nlength of 4500 miles.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 78, 'page_label': '79'}, page_content='52 CHAPTER 3 Knowledge Representation\\n800\\n1500\\n700\\n700\\n1700\\n1100\\n1000\\n600\\n600\\n900\\nA\\nBC\\nE\\nD\\nFigure 3.8\\nSimplified map showing\\nTraveling Salesman prob-\\nlem with five cities\\nT o solve the problem using search, a different representation would be\\nneeded, based on this graph. Figure 3.9 shows a part of the search tree that\\nrepresents the possible paths through the search space in this problem.\\nEach node is marked with a letter that represents the city that has been\\nreached by the path up to that point. Hence, in fact, each node represents\\nthe path from city A to the city named at that node. The root node of the\\ngraph thus represents the path of length 0, which consists simply of the city\\nA. As with the previous example, cyclical paths have been excluded from\\nthe tree, but unlike the tree for the missionaries and cannibals problem, the\\ntree does allow repeated states. This is because in this problem each state\\nmust be visited once, and so a complete path must include all states. In the\\nMissionaries and Cannibals problem, the aim was to reach a particular\\nstate by the shortest path that could be found. Hence, including a path such\\nas A,B,C,D where a path A,D had already been found would be wasteful\\nbecause it could not possibly lead to a shorter path than A,D. With the\\nTraveling Salesman problem, this does not apply, and we need to examine\\nevery possible path that includes each node once, with the start node at the\\nbeginning and the end.\\nFigure 3.9 is only a part of the search tree, but it shows two complete paths:\\nA,B,C,D,E,A and A,B,C,E,D,A. The total path costs of these two paths are\\n4000 miles and 4500 miles, respectively.\\nIn total there will be (n /H110021)! possible paths for a Traveling Salesman prob-\\nlem with n cities. This is because we are constrained in our starting city'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 79, 'page_label': '80'}, page_content='3.9 Search Trees 53\\n1,000\\nA\\nB E\\nEC\\nE\\nE\\nAA\\nCCD\\nD\\nD\\nDBB\\nCD\\n900\\n600\\n800800 700\\n700\\n700 600\\n600600\\n7001000\\n6001500\\n1500\\n1700\\n700\\nFigure 3.9\\nPartial search tree for Traveling Salesman problem with five cities\\nand, thereafter, have a choice of any combination of (n /H110021) cities. In prob-\\nlems with small numbers of cities, such as 5 or even 10, this means that the\\ncomplete search tree can be evaluated by a computer program without\\nmuch difficulty; but if the problem consists of 40 cities, there would be 40!\\npaths, which is roughly 10\\n48, a ludicrously large number. As we see in the\\nnext chapter, methods that try to examine all of these paths are called\\nbrute-force search methods. T o solve search problems with large trees,\\nknowledge about the problem needs to be applied in the form of heuris-\\ntics, which enable us to find more efficient ways to solve the problem. A\\nheuristic is a rule or piece of information that is used to make search or\\nanother problem-solving method more effective or more efficient. The use\\nof heuristics for search is explained in more detail in Chapters 4 and 5.\\nFor example, a heuristic search approach to solving the Traveling Salesman\\nproblem might be: rather than examining every possible path, we simply\\nextend the path by moving to the city closest to our current position that\\nhas not yet been examined. This is called the nearest neighbor heuristic.I n\\nour example above, this would lead to the path A,C,D,E,B,A, which has a\\ntotal cost of 4500 miles. This is certainly not the best possible path, as we'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 80, 'page_label': '81'}, page_content='54 CHAPTER 3 Knowledge Representation\\n12 3 12 3\\nFigure 3.10\\nTwo states in the Towers of\\nHanoi problem\\nhave already seen one path (A,B,C,D,E,A) that has a cost of 4000 miles. This\\nillustrates the point that although heuristics may well make search more\\nefficient, they will not necessarily give the best results. We will see methods\\nin the next chapters that illustrate this and will also discuss ways of choos-\\ning heuristics that usually do give the best result.\\n3.9.4 Example 3: The Towers of Hanoi\\nThe T owers of Hanoi problem is defined as follows:\\nWe have three pegs and a number of disks of different sizes. The aim is to\\nmove from the starting state where all the disks are on the first peg, in size\\norder (smallest at the top) to the goal state where all the pegs are on the\\nthird peg, also in size order. We are allowed to move one disk at a time, as\\nlong as there are no disks on top of it, and as long as we do not move it on\\ntop of a peg that is smaller than it.\\nFigure 3.10 shows the start state and a state after one disk has been moved\\nfrom peg 1 to peg 2 for a T owers of Hanoi problem with three disks.\\nNow that we know what our start state and goal state look like, we need to\\ncome up with a set of operators:\\nOp1 Move disk from peg 1 to peg 2\\nOp2 Move disk from peg 1 to peg 3\\nOp3 Move disk from peg 2 to peg 1\\nOp4 Move disk from peg 2 to peg 3\\nOp5 Move disk from peg 3 to peg 1\\nOp6 Move disk from peg 3 to peg 2\\nWe also need a way to represent each state. For this example, we will use\\nvectors of numbers where 1 represents the smallest peg and 3 the largest'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 81, 'page_label': '82'}, page_content='3.9 Search Trees 55\\n(2,3)(1)(   )\\n(1,3)(  )(2) (1,3)(2)(  )\\n(1,3)(2)(  )\\n(2,3)(  )(1)\\n(3)(  )(1,2) (3)(1,2)(  )\\n(3)(1,2)(  )\\n(1,3)(  )(2)\\n(3)(  )(1,2)\\n(3)(1)(2) (3)(2)(1)\\n(1,2,3)(  )(   )\\nFigure 3.11\\nThe first five levels of the\\nsearch tree for the Towers\\nof Hanoi problem with\\nthree disks\\npeg. The first vector represents the first peg, and so on. Hence, the starting\\nstate is represented as\\n(1,2,3) () ()\\nThe second state shown in figure 3.10 is represented as\\n(2,3) (1) ()\\nand the goal state is\\n() () (1,2,3)\\nThe first few levels of the search tree for the T owers of Hanoi problem with\\nthree disks is shown in Figure 3.11. Again, we have ignored cyclical paths.\\nIn fact, with the T owers of Hanoi problem, at each step, we can always\\nchoose to reverse the previous action. For example, having applied opera-\\ntor Op1 to get from the start state to (2,3) (1) (), we can now apply opera-\\ntor Op3, which reverses this move and brings us back to the start state.\\nClearly, this behavior will always lead to a cycle, and so we ignore such\\nchoices in our representation.\\nAs we see later in this book, search is not the only way to identify solutions\\nto problems like the T owers of Hanoi. A search method would find a solu-\\ntion by examining every possible set of actions until a path was found that\\nled from the start state to the goal state. A more intelligent system might be'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 82, 'page_label': '83'}, page_content='56 CHAPTER 3 Knowledge Representation\\nPENGUIN KIWI\\nIS IT BLACK AND WHITE?\\nCAN IT FL Y?\\nNOYES\\nYES\\nYES NO\\nNO\\nDODO\\nIS IT EXTINCT?\\nFigure 3.12\\nSearch tree representation\\nused with Describe and\\nMatch to identify a \\npenguin\\ndeveloped that understood more about the problem and, in fact, under-\\nstood how to go about solving the problem without necessarily having to\\nexamine any alternative paths at all.\\n3.9.5 Example 4: Describe and Match\\nA method used in Artificial Intelligence to identify objects is to describe it\\nand then search for the same description in a database, which will identify\\nthe object.\\nAn example of Describe and Match is as follows:\\nAlice is looking out of her window and can see a bird in the garden. She\\ndoes not know much about birds but has a friend, Bob, who does. She calls\\nBob and describes the bird to him. From her description, he is able to tell\\nher that the bird is a penguin.\\nWe could represent Bob’s knowledge of birds in a search tree, where each\\nnode represents a question, and an arc represents an answer to the ques-\\ntion. A path through the tree describes various features of a bird, and a leaf\\nnode identifies the bird that is being described.\\nHence, Describe and Match enables us to use search in combination with\\nknowledge to answer questions about the world.\\nA portion of the search tree Bob used to identify the penguin outside Alice’s\\nwindow is shown in Figure 3.12.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 83, 'page_label': '84'}, page_content='3.11 Problem Reduction 57\\nFirst, the question at the top of the tree, in the root node, is asked. The\\nanswer determines which branch to follow from the root node. In this case,\\nif the answer is “yes, ” the left-hand branch is taken (this branch is not\\nshown in the diagram). If the answer is “no, ” then the right-hand branch is\\ntaken, which leads to the next question—“Is it extinct?”\\nIf the answer to this question is “yes, ” then a leaf node is reached, which\\ngives us the answer: the bird is a dodo. If the answer is “no, ” then we move\\non to the next question. The process continues until the algorithm reaches\\na leaf node, which it must eventually do because each step moves one level\\ndown the tree, and the tree does not have an infinite number of levels.\\nThis kind of tree is called a decision tree, and we learn more about them in\\nChapter 10, where we see how they are used in machine learning.\\n3.10 Combinatorial Explosion\\nThe search tree for a Traveling Salesman problem becomes unmanageably\\nlarge as the number of cities increases. Many problems have the property\\nthat as the number of individual items being considered increases, the\\nnumber of possible paths in the search tree increases exponentially, mean-\\ning that as the problem gets larger, it becomes more and more unreason-\\nable to expect a computer program to be able to solve it. This problem is\\nknown as combinatorial explosion because the amount of work that a\\nprogram needs to do to solve the problem seems to grow at an explosive\\nrate, due to the possible combinations it must consider.\\n3.11 Problem Reduction\\nIn many cases we find that a complex problem can be most effectively\\nsolved by breaking it down into several smaller problems. If we solve all of\\nthose smaller subproblems, then we have solved the main problem. This\\napproach to problem solving is often referred to as goal reduction because\\nit involves considering the ultimate goal of solving the problem in a way\\nthat involves generating subgoals for that goal.\\nFor example, to solve the T owers of Hanoi problem with n disks, it turns\\nout that the first step is to solve the smaller problem with n /H110021 disks.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 84, 'page_label': '85'}, page_content='58 CHAPTER 3 Knowledge Representation\\n123\\nFigure 3.13\\nThe starting state of the\\nTowers of Hanoi problem\\nwith four disks 123\\nFigure 3.14\\nTowers of Hanoi problem\\nof size 4 reduced to a prob-\\nlem of size 3 by first mov-\\ning the largest disk from\\npeg 1 to peg 3\\nFor example, let us examine the T owers of Hanoi with four disks, whose\\nstarting state is shown in Figure 3.13.\\nT o solve this problem, the first step is to move the largest block from peg 1 to\\npeg 3. This will then leave a T owers of Hanoi problem of size 3, as shown in\\nFigure 3.14, where the aim is to move the disks from peg 2 to peg 3. Because\\nthe disk that is on peg 3 is the largest disk, any other disk can be placed on\\ntop of it, and because it is in its final position, it can effectively be ignored.\\nIn this way, a T owers of Hanoi problem of any size n can be solved by first\\nmoving the largest disk to peg 3, and then applying the T owers of Hanoi\\nsolution to the remaining disks, but swapping peg 1 and peg 2.\\nThe method for moving the largest disk is not difficult and is left as an exercise.\\n3.12 Goal Trees\\nA goal tree (also called an and-or tree) is a form of semantic tree used to\\nrepresent problems that can be broken down in this way. We say that the\\nsolution to the problem is the goal, and each individual step along the way\\nis a subgoal. In the case of the T owers of Hanoi, moving the largest disk to\\npeg 3 is a subgoal.\\nEach node in a goal tree represents a subgoal, and that node’s children are\\nthe subgoals of that goal. Some goals can be achieved only by solving all of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 85, 'page_label': '86'}, page_content='3.12 Goal Trees 59\\nMOVE A, B, C, D FROM 1 TO 3\\nMOVE A, B, C, \\nFROM 2 TO 3\\nMOVE A, B, \\nFROM 1 TO 3\\nMOVE C \\nFROM 2 TO 3\\nMOVE B \\nFROM 1 TO 3\\nMOVE D \\nFROM 1 TO 3\\nMOVE A \\nFROM 2 TO 3\\nFigure 3.15\\nGoal tree for Towers of Hanoi problem with four disks\\nits subgoals. Such nodes on the goal tree are and-nodes, which represent\\nand-goals.\\nIn other cases, a goal can be achieved by achieving any one of its subgoals.\\nSuch goals are or-goals and are represented on the goal tree by or-nodes.\\nGoal trees are drawn in the same way as search trees and other semantic\\ntrees. An and-node is shown by drawing an arc across the arcs that join it to\\nits subgoals (children). Or-nodes are not marked in this way. The main dif-\\nference between goal trees and normal search trees is that in order to solve\\na problem using a goal tree, a number of subproblems (in some cases, all\\nsubproblems) must be solved for the main problem to be solved. Hence,\\nleaf nodes are called success nodes rather than goal nodes because each leaf\\nnode represents success at a small part of the problem.\\nSuccess nodes are always and-nodes. Leaf nodes that are or-nodes are\\nimpossible to solve and are called failure nodes.\\nA goal tree for the T owers of Hanoi problem with four disks is shown in\\nFigure 3.15. The root node represents the main goal, or root goal, of the\\nproblem, which is to move all four disks from peg 1 to peg 3. In this tree, we\\nhave represented the four disks as A,B,C, and D, where A is the smallest\\ndisk, and D is the largest. The pegs are numbered from 1 to 3. All of the\\nnodes in this tree are and-nodes. This is true of most problems where there\\nis only one reasonable solution.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 86, 'page_label': '87'}, page_content='60 CHAPTER 3 Knowledge Representation\\nFigure 3.15 is somewhat of an oversimplification because it does not\\nexplain how to solve each of the subgoals that is presented. T o produce a\\nsystem that could solve the problem, a larger goal tree that included addi-\\ntional subgoals would be needed. This is left as an exercise.\\nBreaking down the problem in this way is extremely advantageous because it\\ncan be easily extended to solving T owers of Hanoi problems of all sizes. Once\\nwe know how to solve the T owers of Hanoi with three disks, we then know\\nhow to solve it for four disks. Hence, we also know how to solve it for five\\ndisks, six disks, and so on. Computer programs can be developed easily that\\ncan solve the T owers of Hanoi problem with enormous numbers of disks.\\nAnother reason that reducing problems to subgoals in this way is of such\\ngreat interest in Artificial Intelligence research is that this is the way in\\nwhich humans often go about solving problems. If you want to cook a\\nfancy dinner for your friends, you probably have a number of subgoals to\\nsolve first:\\n■ find a recipe\\n■ go to the supermarket\\n■ buy ingredients\\n■ cook dinner\\n■ set the table\\nAnd so on. Solving the problem in this way is very logical for humans\\nbecause it treats a potentially complex problem as a set of smaller, simpler\\nproblems. Humans work very well in this way, and in many cases comput-\\ners do too.\\nOne area in which goal trees are often used is computer security. A threat\\ntree represents the possible threats to a computer system, such as a com-\\nputerized banking system. If the goal is “steal Edwin’s money from the\\nbank, ” you can (guess or convince me to divulge my PIN) and (steal or\\ncopy my card) and so on. The threat tree thus represents the possible paths\\nan attacker of the system might take and enables security experts to deter-\\nmine the weaknesses in the system.\\n3.12.1 Top Down or Bottom Up?\\nThere are two main approaches to breaking down a problem into sub-\\ngoals—top down and bottom up.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 87, 'page_label': '88'}, page_content='3.12 Goal Trees 61\\nA top-down approach involves first breaking down the main problem into\\nsmaller goals and then recursively breaking down those goals into smaller\\ngoals, and so on, until leaf nodes, or success nodes, are reached, which can\\nbe solved.\\nA bottom-up approach involves first determining all of the subgoals that\\nare necessary to solve the entire problem, and then starting by solving the\\nsuccess nodes, and working up until the complete solution is found. As we\\nsee elsewhere in this book, both of these approaches are valid, and the cor-\\nrect approach should be taken for each problem.\\nAgain, humans often think in these terms.\\nBusinesses often look at solving problems either from the top down or\\nfrom the bottom up. Solving a business problem from the top down means\\nlooking at the global picture and working out what subgoals are needed to\\nchange that big picture in a satisfactory way. This often means passing\\nthose subgoals onto middle managers, who are given the task of solving\\nthem. Each middle manager will then break the problem down into smaller\\nsubproblems, each of which will be passed down the chain to subordinates.\\nIn this way, the overall problem is solved without the senior management\\never needing to know how it was actually solved. Individual staff members\\nsolve their small problems without ever knowing how that impacts on the\\noverall business.\\nA bottom-up approach to solving business problems would mean looking\\nat individual problems within the organization and fixing those. Computer\\nsystems might need upgrading, and certain departments might need to\\nwork longer hours. The theory behind this approach is that if all the indi-\\nvidual units within the business are functioning well, then the business as a\\nwhole will be functioning well.\\n3.12.2 Uses of Goal Trees\\nWe can use goal-driven search to search through a goal tree. As we describe\\nelsewhere in this book, this can be used to solve a number of problems in\\nArtificial Intelligence.\\n3.12.3 Example 1: Map Coloring\\nMap-coloring problems can be represented by goal trees. For example, Fig-\\nure 3.16 shows a goal tree that can be used to represent the map-coloring'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 88, 'page_label': '89'}, page_content='62 CHAPTER 3 Knowledge Representation\\nrgby rgby rgby rgby rgbyrgby\\n123 4 56\\nFigure 3.16\\nGoal tree representing a map-coloring problem with six countries and four colors\\nproblem for six countries with four colors. The tree has just two levels. The\\ntop level consists of a single and-node, which represents the fact that all\\ncountries must be colored. The next level has an or-node for each country,\\nrepresenting the choice of colors that can be applied.\\nOf course, this tree alone does not represent the entire problem. Con-\\nstraints must be applied that specify that no two adjacent countries may\\nhave the same color. Solving the tree while applying these constraints solves\\nthe map-coloring problem. In fact, to apply a search method to this prob-\\nlem, the goal tree must be redrawn as a search tree because search methods\\ngenerally are not able to deal with and-nodes.\\nThis can be done by redrawing the tree as a search tree, where paths\\nthrough the tree represent plans rather than goals. Plans are discussed in\\nmore detail in Part 5 of this book. A plan consists of steps that can be taken\\nto solve the overall problem. A search tree can thus be devised where nodes\\nrepresent partial plans. The root node has no plan at all, and leaf nodes rep-\\nresent complete plans.\\nA part of the search tree for the map-coloring problem with six countries\\nand four colors is shown in Figure 3.17.\\nOne of the search methods described in Chapter 4 or 5 can be applied to\\nthis search tree to find a solution. This may not be the most efficient way to\\nsolve the map-coloring problem, though.\\n3.12.4 Example 2: Proving Theorems\\nAs will be explained in Part 3 of this book, goal trees can be used to repre-\\nsent theorems that are to be proved. The root goal is the theorem that is to'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 89, 'page_label': '90'}, page_content='3.12 Goal Trees 63\\nNO PLAN\\nSELECT A COLOR \\nFOR COUNTRY 1\\nCHOOSE\\nRED\\nCHOOSE\\nGREEN\\nCHOOSE\\nGREEN\\nCHOOSE\\nBLUE\\nSELECT A COLOR \\nFOR COUNTRY 2\\nSELECT A COLOR \\nFOR COUNTRY 2\\nFigure 3.17\\nPartial search tree for\\nmap-coloring problem\\nwith six countries and four\\ncolors\\nbe proved. It is an or-node because there may be several ways to prove the\\ntheorem. The next level down consists of and-nodes, which are lemmas\\nthat are to be proven. Each of these lemmas again may have several ways to\\nbe proved so, therefore, is an or-node. The leaf-nodes of the tree represent\\naxioms that do not need to be proved.\\n3.12.5 Example 3: Parsing Sentences\\nAs is described in Chapter 20, a parser is a tool that can be used to analyze\\nthe structure of a sentence in the English language (or any other human\\nlanguage). Sentences can be broken down into phrases, and phrases can be\\nbroken down into nouns, verbs, adjectives, and so on. Clearly, this is ideally\\nsuited to being represented by goal trees.\\n3.12.6 Example 4: Games\\nGame trees , which are described in more detail in Chapter 6, are goal\\ntrees that are used to represent the choices made by players when play-\\ning two-player games, such as chess, checkers, and Go. The root node of\\na game tree represents the current position, and this is an or-node\\nbecause I must choose one move to make. The next level down in the\\ngame tree represents the possible choices my opponent might make,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 90, 'page_label': '91'}, page_content='64 CHAPTER 3 Knowledge Representation\\nand because I need to consider all possible responses that I might make\\nto that move, this level consists of and-nodes. Eventually, the leaf nodes\\nrepresent final positions in the game, and a path through the tree repre-\\nsents a sequence of moves from start to finish, resulting in a win, loss,\\nor a draw.\\nThis kind of tree is a pure and-or tree because it has an or-node at the top,\\neach or-node has and-nodes as its direct successors, and each and-node has\\nor-nodes as its direct successors. Another condition of a pure and-or tree is\\nthat it does not have any constraints that affect which choices can be made.\\n3.13 Chapter Summary\\n■ Artificial Intelligence can be used to solve a wide range of prob-\\nlems, but for the methods to work effectively, the correct represen-\\ntation must be used.\\n■ Semantic nets use graphs to show relationships between objects.\\nFrame-based systems show the same information in frames.\\n■ Frame-based systems allow for inheritance, whereby one frame can\\ninherit features from another.\\n■ Frames often have procedures associated with them that enable a\\nsystem to carry out actions on the basis of data within the frames.\\n■ Search trees are a type of semantic tree. Search methods (several of\\nwhich are described in Chapters 4 and 5) are applied to search\\ntrees, with the aim of finding a goal.\\n■ Describe and Match is a method that can be used to identify an\\nobject by searching a tree that represents knowledge about the uni-\\nverse of objects that are being considered.\\n■ Problems such as the T owers of Hanoi problem can be solved effec-\\ntively by breaking them down into smaller subproblems, thus\\nreducing an overall goal to a set of subgoals.\\n■ Goal trees (or and-or trees) are an effective representation for\\nproblems that can be broken down in this way.\\n■ Data-driven search (forward chaining) works from a start state\\ntoward a goal. Goal-driven search (backward chaining) works in\\nthe other direction, starting from the goal.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 91, 'page_label': '92'}, page_content='Exercises 65\\n3.14 Review Questions\\n3.1 Why are representations so important in Artificial Intelligence?\\nWhat risks are inherent in using the wrong representation?\\n3.2 Explain the connection between frames and object-oriented struc-\\ntures in programming languages, such as Java and C++.\\n3.3 Explain the relationship between graphs, semantic nets, semantic\\ntrees, search spaces, and search trees.\\n3.4 Explain why goal trees are so useful to artificial intelligence\\nresearch. Give illustrations of how they are used.\\n3.5 Explain the connection between decision trees and the Describe\\nand Match algorithm. How efficient do you think this algorithm is?\\nCan you think of any ways to improve it?\\n3.6 Explain the problem of combinatorial explosion. What impact\\ndoes this have on the methods we use for solving large problems\\nusing search?\\n3.7 Explain why removing cycles from a search tree is a good idea.\\n3.8 Explain how and-or trees can be used to represent games. What\\nlimitations do you think a system that uses game trees to play chess\\nmight face? Would it face different limitations if it played tic-tac-\\ntoe? Or poker?\\n3.9 What is the difference between a top-down approach to solving a\\nproblem and a bottom-up approach? In what kinds of situations\\nmight each be more appropriate?\\n3.15 Exercises\\n3.10 Convert the following information into:\\na) a semantic net\\nb) a frame-based representation\\nA Ford is a type of car. Bob owns two cars. Bob parks his car at\\nhome. His house is in California, which is a state. Sacramento is the\\nstate capital of California. Cars drive on the freeway, such as Route\\n101 and Highway 81.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 92, 'page_label': '93'}, page_content='66 CHAPTER 3 Knowledge Representation\\n3.11 Design a decision tree that enables you to identify an item from a\\ncategory in which you are interested (e.g., cars, animals, pop\\nsingers, films, etc.).\\n3.12 Devise your own representation for the Missionaries and Canni-\\nbals problem and implement it either with pen and paper or in the\\nprogramming language of your choice. Use it to solve the problem.\\nHow efficient is your representation compared with that used in\\nSection 3.9.1 of this book? Does it come up with the same answer?\\nWhich approach is easier for an observer to quickly grasp? Which\\nwould you say is the better representation overall, and why?\\n3.13 Design a suitable representation and draw the complete search tree\\nfor the following problem:\\nA farmer is on one side of a river and wishes to cross the river with\\na wolf, a chicken, and a bag of grain. He can take only one item at a\\ntime in his boat with him. He can’t leave the chicken alone with the\\ngrain, or it will eat the grain, and he can’t leave the wolf alone with\\nthe chicken, or the wolf will eat the chicken. How does he get all\\nthree safely across to the other side?\\n3.14 Write a program using the programming language of your choice\\nto implement the representation you designed for Review Ques-\\ntion 3.3. Have your program solve the problem, and have it show\\non the screen how it reaches the solution. Does it find the best pos-\\nsible solution? Does it find it as quickly as it might?\\n3.15 Write a program that solves either\\na) the T owers of Hanoi problem with up to 1000 disks, or,\\nb) the Traveling Salesman problem with up to 10 cities.\\nY ou may need to wait until you have read about some of the search\\ntechniques described in Chapter 4 before you can write this pro-\\ngram. For now, you can design a suitable representation and\\nimplement a suitable data structure for the problem in the lan-\\nguage of your choice.\\n3.16 Further Reading\\nAll Artificial Intelligence textbooks deal with the subject of representation.\\nA particularly good description in terms of search for problem solving is\\nfound in Russell and Norvig (1995).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 93, 'page_label': '94'}, page_content='Further Reading 67\\nWinston (1993) provides a good description in terms of semantics.\\nDromey (1982) provides an excellent description of the development of an\\nalgorithm for the T owers of Hanoi problem by problem reduction.\\nAnd-or trees and their uses are particularly well described by Luger (2002)\\nand Charniak and McDermott (1985).\\nFrames were introduced by Marvin Minsky in his 1975 paper,A framework\\nfor Representing Knowledge.\\nKnowledge Representation, Reasoning and Declarative Problem Solving ,b y\\nChitta Baral (2003 – Cambridge University Press)\\nHow to Solve it by Computer, by R.G. Dromey (1982 – out of print)\\nKnowledge Representation and Defeasible Reasoning (Studies in Cognitive\\nSystems, Vol 5) , edited by Ronald P . Loui and Greg N. Carlson (1990 –\\nKluwer Academic Publishers)\\nA Framework for Representing Knowledge , by Marvin Minsky (1975 – in\\nComputation & Intelligence – Collected Readings, edited by George F. Luger,\\nThe MIT Press)\\nKnowledge Representation: Logical, Philosophical, and Computational Foun-\\ndations, by John F. Sowa and David Dietz (1999 – Brooks Cole)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 94, 'page_label': '95'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 95, 'page_label': '96'}, page_content='Search\\n2\\nIntroduction to Part 2\\nPart 2 is divided into three chapters.\\nSearch Methodologies\\nChapter 4 introduces a number of search methods, includ-\\ning depth-first search and breadth-first search. Metrics are\\npresented that enable analysis of search methods and pro-\\nvide a way to determine which search methods are most\\nsuitable for particular problems.\\nThis chapter also introduces the idea of heuristics for\\nsearch and presents a number of methods, such as best-first\\nsearch, that use heuristics to improve the performance of\\nsearch methods.\\nAdvanced Search\\nChapter 5 introduces a number of more complex search\\nmethods. In particular, it explains the way that search can\\nbe used to solve combinatorial optimization problems\\nusing local search and presents a number of local search\\nmethods, such as simulated annealing and tabu search. The\\nchapter also explains how search can be run in parallel and\\ndiscusses some of the complications that this introduces.\\nGame Playing\\nThis chapter explains the relationship between search and\\ngames, such as chess, checkers, and tic-tac-toe. It explains\\nthe Minimax algorithm and how alpha–beta pruning can\\nbe used to make it more efficient. It explains some of the\\nmore advanced techniques used in modern game-playing\\ncomputers and discusses why computers are currently\\nunable to beat humans at games such as Go.\\nPART\\n4\\nCHAPTER\\n5\\nCHAPTER\\n6\\nCHAPTER'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 96, 'page_label': '97'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 97, 'page_label': '98'}, page_content='4CHAPTER\\nSearch Methodologies\\nResearch is the process of going up alleys to see if they are blind.\\n—Marston Bates\\nWhen a thing is funny, search it carefully for a hidden truth.\\n—George Bernard Shaw\\nIf we do not find anything pleasant, at least we shall find something new.\\n—Voltaire,Candide\\nEveryone that asketh receiveth; and he that seeketh findeth.\\n—The Gospel according to St Matthew, Chapter 7, V erse 8\\n4.1 Introduction\\nIn Chapter 3, we introduced search trees and other methods and represen-\\ntations that are used for solving problems using Artificial Intelligence tech-\\nniques such as search. In Chapter 4, we introduce a number of methods\\nthat can be used to search, and we discuss how effective they are in different\\nsituations. Depth-first search and breadth-first search are the best-known\\nand widest-used search methods, and in this chapter we examine why this\\nis and how they are implemented. We also look at a number of properties\\nof search methods, including optimality and completeness, that can be\\nused to determine how useful a search method will be for solving a partic-\\nular problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 98, 'page_label': '99'}, page_content='72 CHAPTER 4 Search Methodologies\\nThe methods that are described in this chapter and Chapter 5 impact on\\nalmost every aspect of Artificial Intelligence. Because of the serial nature in\\nwhich computers tend to operate, search is a necessity to determine solu-\\ntions to an enormous range of problems.\\nThis chapter starts by discussing blind search methods and moves on to\\nexamine search methods that are more informed—these search methods\\nuse heuristics to examine a search space more efficiently.\\n4.2 Problem Solving as Search\\nProblem solving is an important aspect of Artificial Intelligence. A problem\\ncan be considered to consist of a goal and a set of actions that can be taken\\nto lead to the goal. At any given time, we consider the state of the search\\nspace to represent where we have reached as a result of the actions we have\\napplied so far.\\nFor example, consider the problem of looking for a contact lens on a foot-\\nball field. The initial state is how we start out, which is to say we know that\\nthe lens is somewhere on the field, but we don’t know where. If we use the\\nrepresentation where we examine the field in units of one square foot, then\\nour first action might be to examine the square in the top-left corner of the\\nfield. If we do not find the lens there, we could consider the state now to be\\nthat we have examined the top-left square and have not found the lens.\\nAfter a number of actions, the state might be that we have examined 500\\nsquares, and we have now just found the lens in the last square we exam-\\nined. This is a goal state because it satisfies the goal that we had of finding\\na contact lens.\\nSearch is a method that can be used by computers to examine a problem\\nspace like this in order to find a goal. Often, we want to find the goal as\\nquickly as possible or without using too many resources. A problem space\\ncan also be considered to be a search space because in order to solve the\\nproblem, we will search the space for a goal state. We will continue to use\\nthe term search space to describe this concept.\\nIn this chapter, we will look at a number of methods for examining a search\\nspace. These methods are called search methods.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 99, 'page_label': '100'}, page_content='4.3 Data-Driven or Goal-Driven Search 73\\n4.3 Data-Driven or Goal-Driven Search\\nThere are two main approaches to searching a search tree, which roughly\\ncorrespond to the top-down and bottom-up approaches discussed in Sec-\\ntion 3.12.1. Data-driven search starts from an initial state and uses actions\\nthat are allowed to move forward until a goal is reached. This approach is\\nalso known as forward chaining.\\nAlternatively, search can start at the goal and work back toward a start state,\\nby seeing what moves could have led to the goal state. This is goal-driven\\nsearch, also known as backward chaining.\\nMost of the search methods we will examine in this chapter and Chapter 5\\nare data-driven search: they start from an initial state (the root node in the\\nsearch tree) and work toward the goal node.\\nIn many circumstances, goal-driven search is preferable to data driven-\\nsearch, but for most of this part of the book, when we refer to “search, ” we\\nare talking about data-driven search.\\nGoal-driven search and data-driven search will end up producing the same\\nresults, but depending on the nature of the problem being solved, in some\\ncases one can run more efficiently than the other—in particular, in some\\nsituations one method will involve examining more states than the other.\\nGoal-driven search is particularly useful in situations in which the goal can be\\nclearly specified (for example, a theorem that is to be proved or finding an exit\\nfrom a maze). It is also clearly the best choice in situations such as medical\\ndiagnosis where the goal (the condition to be diagnosed) is known, but the\\nrest of the data (in this case, the causes of the condition) need to be found.\\nData-driven search is most useful when the initial data are provided, and it is\\nnot clear what the goal is. For example, a system that analyzes astronomical\\ndata and thus makes deductions about the nature of stars and planets would\\nreceive a great deal of data, but it would not necessarily be given any direct\\ngoals. Rather, it would be expected to analyze the data and determine conclu-\\nsions of its own. This kind of system has a huge number of possible goals that\\nit might locate. In this case, data-driven search is most appropriate.\\nIt is interesting to consider a maze that has been designed to be traversed\\nfrom a start point in order to reach a particular end point. It is nearly\\nalways far easier to start from the end point and work back toward the start'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 100, 'page_label': '101'}, page_content='74 CHAPTER 4 Search Methodologies\\npoint. This is because a number of dead end paths have been set up from\\nthe start (data) point, and only one path has been set up to the end (goal)\\npoint. As a result, working back from the goal to the start has only one pos-\\nsible path.\\n4.4 Generate and Test\\nThe simplest approach to search is called Generate and T est. This simply\\ninvolves generating each node in the search space and testing it to see if it is\\na goal node. If it is, the search has succeeded and need not carry on. Other-\\nwise, the procedure moves on to the next node.\\nThis is the simplest form of brute-force search (also called exhaustive\\nsearch), so called because it assumes no additional knowledge other than\\nhow to traverse the search tree and how to identify leaf nodes and goal nodes,\\nand it will ultimately examine every node in the tree until it finds a goal.\\nT o successfully operate, Generate and T est needs to have a suitable Genera-\\ntor, which should satisfy three properties:\\n1. It must be complete: In other words, it must generate every possi-\\nble solution; otherwise it might miss a suitable solution.\\n2. It must be nonredundant: This means that it should not generate\\nthe same solution twice.\\n3. It must be well informed: This means that it should only propose\\nsuitable solutions and should not examine possible solutions that\\ndo not match the search space.\\nThe Generate and T est method can be successfully applied to a number of\\nproblems and indeed is the manner in which people often solve problems\\nwhere there is no additional information about how to reach a solution.\\nFor example, if you know that a friend lives on a particular road, but you do\\nnot know which house, a Generate and T est approach might be necessary;\\nthis would involve ringing the doorbell of each house in turn until you\\nfound your friend. Similarly, Generate and T est can be used to find solu-\\ntions to combinatorial problems such as the eight queens problem that is\\nintroduced in Chapter 5.\\nGenerate and T est is also sometimes referred to as a blind search technique\\nbecause of the way in which the search tree is searched without using any\\ninformation about the search space.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 101, 'page_label': '102'}, page_content='4.5 Depth-First Search 75\\n27\\n8\\n1\\nA\\nCB\\nFED\\nGH IJ K L\\n9\\n10\\n113\\n4\\n5\\n6\\n13\\n12 Figure 4.1\\nIllustrating depth-first\\nsearch\\nMore systematic examples of brute-force search are presented in this chap-\\nter, in particular, depth-first search and breadth-first search.\\nMore “intelligent” (or informed) search techniques are explored later in\\nthis chapter.\\n4.5 Depth-First Search\\nA commonly used search algorithm is depth-first search . Depth-first\\nsearch is so called because it follows each path to its greatest depth before\\nmoving on to the next path. The principle behind the depth-first approach\\nis illustrated in Figure 4.1. Assuming that we start from the left side and\\nwork toward the right, depth-first search involves working all the way down\\nthe left-most path in the tree until a leaf node is reached. If this is a goal\\nstate, the search is complete, and success is reported.\\nIf the leaf node does not represent a goal state, search backtracks up to the\\nnext highest node that has an unexplored path. In Figure 4.1, after examining\\nnode G and discovering that it is not a leaf node, search will backtrack to\\nnode D and explore its other children. In this case, it only has one other child,\\nwhich is H. Once this node has been examined, search backtracks to the next\\nunexpanded node, which is A, because B has no unexplored children.\\nThis process continues until either all the nodes have been examined, in\\nwhich case the search has failed, or until a goal state has been reached, in\\nwhich case the search has succeeded. In Figure 4.1, search stops at node J,\\nwhich is the goal node. As a result, nodes F, K, and L are never examined.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 102, 'page_label': '103'}, page_content='76 CHAPTER 4 Search Methodologies\\nA\\nCB\\nFED\\nGH I J K L\\n1\\n23\\n45 6\\n78 9 1 0\\nFigure 4.2\\nIllustrating breadth-first\\nsearch. The numbers indi-\\ncate the order in which the\\nnodes are examined.\\nDepth-first search uses a method called chronological backtracking to\\nmove back up the search tree once a dead end has been found. Chronolog-\\nical backtracking is so called because it undoes choices in reverse order of\\nthe time the decisions were originally made. We will see later in this chapter\\nthat nonchronological backtracking, where choices are undone in a more\\nstructured order, can be helpful in solving certain problems.\\nDepth-first search is an example ofbrute-force search,o rexhaustive search.\\nDepth-first search is often used by computers for search problems such as\\nlocating files on a disk, or by search engines for spidering the Internet.\\nAs anyone who has used the find operation on their computer will know,\\ndepth-first search can run into problems. In particular, if a branch of the\\nsearch tree is extremely large, or even infinite, then the search algorithm\\nwill spend an inordinate amount of time examining that branch, which\\nmight never lead to a goal state.\\n4.6 Breadth-First Search\\nAn alternative to depth-first search is breadth-first search. As its name sug-\\ngests, this approach involves traversing a tree by breadth rather than by\\ndepth. As can be seen from Figure 4.2, the breadth-first algorithm starts by\\nexamining all nodes one level (sometimes called one ply) down from the\\nroot node.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 103, 'page_label': '104'}, page_content='4.6 Breadth-First Search 77\\nTable 4.1 Comparison of depth-first and breadth-first search\\nScenario Depth first Breadth first\\nSome paths are extremely long, or\\neven infinite\\nAll paths are of similar length\\nAll paths are of similar length, and all\\npaths lead to a goal state\\nHigh branching factor\\nPerforms badly\\nPerforms well\\nPerforms well\\nPerformance depends on other factors\\nPerforms well\\nPerforms well\\nWasteful of time and memory\\nPerforms poorly\\nIf a goal state is reached here, success is reported. Otherwise, search contin-\\nues by expanding paths from all the nodes in the current level down to the\\nnext level. In this way, search continues examining nodes in a particular\\nlevel, reporting success when a goal node is found, and reporting failure if\\nall nodes have been examined and no goal node has been found.\\nBreadth-first search is a far better method to use in situations where the\\ntree may have very deep paths, and particularly where the goal node is in a\\nshallower part of the tree. Unfortunately, it does not perform so well where\\nthe branching factor of the tree is extremely high, such as when examining\\ngame trees for games like Go or Chess (see Chapter 6 for more details on\\ngame trees).\\nBreadth-first search is a poor idea in trees where all paths lead to a goal\\nnode with similar length paths. In situations such as this, depth-first search\\nwould perform far better because it would identify a goal node when it\\nreached the bottom of the first path it examined.\\nThe comparative advantages of depth-first and breadth-first search are tab-\\nulated in Table 4.1.\\nAs will be seen in the next section, depth-first search is usually simpler to\\nimplement than breadth-first search, and it usually requires less memory\\nusage because it only needs to store information about the path it is currently\\nexploring, whereas breadth-first search needs to store information about all\\npaths that reach the current depth. This is one of the main reasons that\\ndepth-first search is used so widely to solve everyday computer problems.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 104, 'page_label': '105'}, page_content='78 CHAPTER 4 Search Methodologies\\nThe problem of infinite paths can be avoided in depth-first search by\\napplying a depth threshold. This means that paths will be considered to\\nhave terminated when they reach a specified depth. This has the disadvan-\\ntage that some goal states (or, in some cases, the only goal state) might be\\nmissed but ensures that all branches of the search tree will be explored in\\nreasonable time. As is seen in Chapter 6, this technique is often used when\\nexamining game trees.\\n4.7 Properties of Search Methods\\nAs we see in this chapter, different search methods perform in different\\nways. There are several important properties that search methods should\\nhave in order to be most useful.\\nIn particular, we will look at the following properties:\\n■ complexity\\n■ completeness\\n■ optimality\\n■ admissibility\\n■ irrevocability\\nIn the following sections, we will explain what each of these properties\\nmeans and why they are useful. We will continue to refer to many of these\\nproperties (in particular, completeness and complexity) as we examine a\\nnumber of search methods in this chapter and in Chapter 5.\\n4.7.1 Complexity\\nIn discussing a search method, it is useful to describe how efficient that\\nmethod is, over time and space. The time complexity of a method is related\\nto the length of time that the method would take to find a goal state. The\\nspace complexity is related to the amount of memory that the method\\nneeds to use.\\nIt is normal to use Big-O notation to describe the complexity of a method. For\\nexample, breadth-first search has a time complexity ofO(b\\nd), where b is the\\nbranching factor of the tree, andd is the depth of the goal node in the tree.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 105, 'page_label': '106'}, page_content='4.7 Properties of Search Methods 79\\nDepth-first search is very efficient in space because it only needs to store\\ninformation about the path it is currently examining, but it is not efficient\\nin time because it can end up examining very deep branches of the tree.\\nClearly, complexity is an important property to understand about a search\\nmethod. A search method that is very inefficient may perform reasonably\\nwell for a small test problem, but when faced with a large real-world prob-\\nlem, it might take an unacceptably long period of time. As we will see, there\\ncan be a great deal of difference between the performance of two search\\nmethods, and selecting the one that performs the most efficiently in a par-\\nticular situation can be very important.\\nThis complexity must often be weighed against the adequacy of the solu-\\ntion generated by the method. A very fast search method might not always\\nfind the best solution, whereas, for example, a search method that examines\\nevery possible solution will guarantee to find the best solution, but it will be\\nvery inefficient.\\n4.7.2 Completeness\\nA search method is described as being complete if it is guaranteed to find a\\ngoal state if one exists. Breadth-first search is complete, but depth-first\\nsearch is not because it may explore a path of infinite length and never find\\na goal node that exists on another path.\\nCompleteness is usually a desirable property because running a search\\nmethod that never finds a solution is not often helpful. On the other hand,\\nit can be the case (as when searching a game tree, when playing a game, for\\nexample) that searching the entire search tree is not necessary, or simply\\nnot possible, in which case a method that searches enough of the tree might\\nbe good enough.\\nA method that is not complete has the disadvantage that it cannot neces-\\nsarily be believed if it reports that no solution exists.\\n4.7.3 Optimality\\nA search method is optimal if it is guaranteed to find the best solution that\\nexists. In other words, it will find the path to a goal state that involves tak-\\ning the least number of steps.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 106, 'page_label': '107'}, page_content='80 CHAPTER 4 Search Methodologies\\nThis does not mean that the search method itself is efficient—it might take\\na great deal of time for an optimal search method to identify the optimal\\nsolution—but once it has found the solution, it is guaranteed to be the best\\none. This is fine if the process of searching for a solution is less time con-\\nsuming than actually implementing the solution. On the other hand, in\\nsome cases implementing the solution once it has been found is very sim-\\nple, in which case it would be more beneficial to run a faster search method,\\nand not worry about whether it found the optimal solution or not.\\nBreadth-first search is an optimal search method, but depth-first search is\\nnot. Depth-first search returns the first solution it happens to find, which\\nmay be the worst solution that exists. Because breadth-first search examines\\nall nodes at a given depth before moving on to the next depth, if it finds a\\nsolution, there cannot be another solution before it in the search tree.\\nIn some cases, the word optimal is used to describe an algorithm that finds\\na solution in the quickest possible time, in which case the concept of\\nadmissibility is used in place of optimality. An algorithm is then defined as\\nadmissible if it is guaranteed to find the best solution.\\n4.7.4 Irrevocability\\nMethods that use backtracking are described as tentative. Methods that do\\nnot use backtracking, and which therefore examine just one path, are\\ndescribed as irrevocable. Depth-first search is an example of tentative\\nsearch. In Section 4.13 we look at hill climbing, a search method that is\\nirrevocable.\\nIrrevocable search methods will often find suboptimal solutions to prob-\\nlems because they tend to be fooled by local optima—solutions that look\\ngood locally but are less favorable when compared with other solutions\\nelsewhere in the search space.\\n4.8 Why Humans Use Depth-First Search\\nBoth depth-first and breadth-first search are easy to implement, although\\ndepth-first search is somewhat easier. It is also somewhat easier for humans\\nto understand because it much more closely relates to the natural way in\\nwhich humans search for things, as we see in the following two examples.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 107, 'page_label': '108'}, page_content='4.8 Why Humans Use Depth-First Search 81\\n4.8.1 Example 1: Traversing a Maze\\nWhen traversing a maze, most people will wander randomly, hoping they\\nwill eventually find the exit (Figure 4.3). This approach will usually be suc-\\ncessful eventually but is not the most rational and often leads to what we\\ncall “going round in circles. ” This problem, of course, relates to search\\nspaces that contain loops, and it can be avoided by converting the search\\nspace into a search tree.\\nAn alternative method that many people know for traversing a maze is to\\nstart with your hand on the left side of the maze (or the right side, if you\\nprefer) and to follow the maze around, always keeping your left hand on\\nthe left edge of the maze wall. In this way, you are guaranteed to find the\\nexit. As can be seen in Figure 4.3, this is because this technique corresponds\\nexactly to depth-first search.\\nIn Figure 4.3, certain special points in the maze have been labeled:\\n■ A is the entrance to the maze.\\n■ M is the exit from the maze.\\n■ C, E, F, G, H, J, L, and N are dead ends.\\n■ B, D, I, and K are points in the maze where a choice can be made as\\nto which direction to go next.\\nIn following the maze by running one’s hand along the left edge, the fol-\\nlowing path would be taken:\\nA, B, E, F, C, D, G, H, I, J, K, L, M\\nY ou should be able to see that following the search tree using depth-first\\nsearch takes the same path. This is only the case because the nodes of the\\nsearch tree have been ordered correctly. The ordering has been chosen so\\nthat each node has its left-most child first and its right-most child last.\\nUsing a different ordering would cause depth-first search to follow a differ-\\nent path through the maze.\\n4.8.2 Example 2: Searching for a Gift\\nWhen looking for a Christmas present for a relative in a number of shops,\\neach of which has several floors, and where each floor has several depart-\\nments, depth-first search might be a natural, if rather simplistic, approach.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 108, 'page_label': '109'}, page_content='82 CHAPTER 4 Search Methodologies\\nEF G H I\\nDC\\nA\\nB\\nJK\\nLM N\\nIN OUTAN M\\nE\\nD I\\nFL\\nB\\nH\\nC\\nJ\\nK\\nG\\nFigure 4.3\\nA maze and a search tree\\nrepresentation of the\\nmaze.\\nThis would involve visiting each floor in the first building before moving\\non to the next building. A breadth-first approach would mean examining\\nthe first department in each shop, and then going back to examine the sec-\\nond department in each shop, and so on. This way does not make sense due\\nto the spatial relationship between the departments, floors, and shops. For\\na computer, either approach would work equally well as long as a represen-\\ntation was used where moving from one building to another did not take\\nany computation time.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 109, 'page_label': '110'}, page_content='4.9 Implementing Depth-First and Breadth-First Search 83\\nA\\nBC\\nDE FG\\nONMLKJIH\\nFigure 4.4\\nA simple search tree with\\nfifteen nodes. The tree has\\na branching factor of two\\nand a depth of three.\\nIn both of the examples above, it can be seen that using breadth-first\\nsearch, although a perfectly reasonable approach for a computer system,\\nwould be rather strange for a human. This is probably because with depth-\\nfirst search, the approach is to explore each path fully before moving onto\\nanother path, whereas with breadth-first search, the approach involves\\nrevisiting and extending particular paths many times.\\nDespite this, implementations in software of both algorithms are nearly\\nidentical, at least when expressed in pseudocode.\\n4.9 Implementing Depth-First and Breadth-First Search\\nA pseudocode implementation of depth-first search is given below.\\nThe variable state represents the current state at any given point in the\\nalgorithm, and queue is a data structure that stores a number of states, in a\\nform that allows insertion and removal from either end. In this algorithm,\\nwe always insert at the front and remove from the front, which as we will\\nsee later on means that depth-first search can be easily implemented using\\na stack.\\nIn this implementation, we have used the function \\nsuccessors (state) ,\\nwhich simply returns all successors of a given state.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 110, 'page_label': '111'}, page_content='84 CHAPTER 4 Search Methodologies\\nFunction depth ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse add_to_front_of_queue (successors (state));\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nTable 4.2 shows the states that the variables queue and state take on when\\nrunning the depth-first search algorithm over a simple search tree, as\\nshown in Figure 4.3.\\nIn fact, depth-first search can be readily implemented on most computer\\nsystems using a stack, which is simply a “last in first out” queue (sometimes\\ncalled a LIFO). In this way, a recursive version of the algorithm given above\\ncan be used, as follows. Because this function is recursive, it needs to be\\ncalled with an argument:\\nrecursive_depth (root_node);\\nThe function is defined as follows:\\nFunction recursive_depth (state)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse\\n{\\nremove_from_stack (state);\\nadd_to_stack (successors (state))\\n}\\nwhile (stack != [])\\n{\\nif recursive_depth (stack [0]) == SUCCESS\\nthen return SUCCESS;\\nremove_first_item_from (stack);\\n}\\nreturn FAILURE;\\n}\\nIf you run through this algorithm on paper (or in a programming language\\nsuch as C++ or LISP), you will find that it follows the tree in the same way\\nas the previous algorithm,\\ndepth.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 111, 'page_label': '112'}, page_content='4.9 Implementing Depth-First and Breadth-First Search 85\\nTable 4.2 Analysis of depth-first search of tree shown in Figure 4.5\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\nA\\nA\\nB\\nB\\nD\\nD\\nH\\nI\\nE\\nE\\nJ\\nK\\nC\\nC\\nF\\nF\\nL\\n(empty)\\nB,C\\nC\\nD,E,C\\nE,C\\nH,I,E,C\\nI,E,C\\nE,C\\nC\\nJ,K,C\\nK,C\\nC\\n(empty)\\nF, G\\nG\\nL,M,G\\nM,G\\nThe queue starts out empty, and the initial state\\nis the root node, which is A.\\nThe successors of A are added to the queue.\\nThe successors of the current state, B, are added\\nto the front of the queue.\\nH has no successors, so no new nodes are added\\nto the queue.\\nSimilarly, I has no successors.\\nAgain, J has no successors.\\nK has no successors. Now we have explored the\\nentire branch below B, which means we back-\\ntrack up to C.\\nThe queue is empty, but we are not at the point\\nin the algorithm where this would mean failing\\nbecause we are about to add successors of C to\\nthe queue.\\nSUCCESS: the algorithm ends because a goal node\\nhas been located. In this case, it is the only goal\\nnode, but the algorithm does not know that and\\ndoes not know how many nodes were left to\\nexplore.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 112, 'page_label': '113'}, page_content='86 CHAPTER 4 Search Methodologies\\nAs was mentioned previously, depth-first search and breadth-first search\\ncan be implemented very similarly. The following is a pseudocode of a non-\\nrecursive implementation of breadth-first search, which should be com-\\npared with the implementation above of depth-first search:\\nFunction breadth ()\\n{\\nqueue = [];      // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse add_to_back_of_queue (successors (state));\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nNotice that the only difference between depth and breadth is that where depth\\nadds successor states to the front of the queue, breadth adds them to the back\\nof the queue. So when applied to the search tree in Figure 4.4, breadth will fol-\\nlow a rather different path from depth, as is shown in Table 4.3.\\nY ou will notice that in this particular case, depth-first search found the goal\\nin two fewer steps than breadth-first search. As has been suggested, depth-\\nfirst search will often find the goal quicker than breadth-first search if all\\nleaf nodes are the same depth below the root node. However, in search trees\\nwhere there is a very large subtree that does not contain a goal, breadth-\\nfirst search will nearly always perform better than depth-first search.\\nAnother important factor to note is that the queue gets much longer when\\nusing breadth-first search. For large trees, and in particular for trees with\\nhigh branching factors, this can make a significant difference because the\\ndepth-first search algorithm will never require a queue longer than the\\nmaximum depth of the tree, whereas breadth-first search in the worst case\\nwill need a queue equal to the number of nodes at the level of the tree with\\nthe most nodes (eight in a tree of depth three with branching factor of two,\\nas in Figure 4.3). Hence, we say that depth-first search is usually more\\nmemory efficient than breadth-first search.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 113, 'page_label': '114'}, page_content='4.9 Implementing Depth-First and Breadth-First Search 87\\nTable 4.3 Analysis of breadth-first search of tree shown in Figure 4.4\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\nA\\nA\\nB\\nB\\nC\\nC\\nD\\nD\\nE\\nE\\nF\\nF\\nG\\nG\\nH\\nI\\nJ\\nK\\nL\\n(empty)\\nB,C\\nC\\nC,D,E\\nD,E\\nD,E,F,G\\nE,F ,G\\nE,F ,G,H,I\\nF ,G,H,I\\nF ,G,H,I,J,K\\nG,H,I,J,K\\nG,H,I,J,K,L,M\\nH,I,J,K,L,M\\nH,I,J,K,L,M,N,O\\nI,J,K,L,M,N,O\\nJ,K,L,M,N,O\\nK,L,M,N,O\\nL,M,N,O\\nM,N,O\\nThe queue starts out empty, and the initial\\nstate is the root node, which is A.\\nThe two descendents of A are added to the\\nqueue.\\nThe two descendents of the current state, B, are\\nadded to the back of the queue.\\nH has no successors, so we have nothing to add\\nto the queue in this state, or in fact for any sub-\\nsequent states.\\nSUCCESS: A goal state has been reached.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 114, 'page_label': '115'}, page_content='88 CHAPTER 4 Search Methodologies\\nAs we have seen, however, depth-first search is neither optimal nor com-\\nplete, whereas breadth-first search is both. This means that depth-first\\nsearch may not find the best solution and, in fact, may not ever find a solu-\\ntion at all. In contrast, breadth-first search will always find the best solution.\\n4.10 Example: Web Spidering\\nAn example of the importance of choosing the right search strategy can be\\nseen in spidering the world wide web. The assumption is made that the\\nmajority of the web is connected, meaning that it is possible to get from\\none page to another by following a finite number of links, where a link con-\\nnects two pages together.\\nSome parts of the Internet have a very high branching factor, with many\\npages containing hundreds of links to other pages. On average though, the\\nbranching factor is reasonably low, and so it seems that breadth-first search\\nmight be a sensible approach to spidering. In practice, however, the search\\ntree that represents the connected part of the Internet is huge, and search-\\ning it by pure breadth-first search would involve a prohibitive storage\\nrequirement. Depth-first search would also not be practical because some\\npaths might have almost infinite depth, particularly given that some pages\\non the Internet are generated automatically at the time they are accessed.\\nHence, Internet spiders must use a combination of approaches, with a par-\\nticular emphasis placed on web pages that change frequently and pages that\\nare considered by some metric to be “important. ” Another important\\naspect of search engines is their ability to search in parallel. We discuss this\\nconcept in more detail in Chapter 5.\\n4.11 Depth-First Iterative Deepening\\nDepth-First Iterative Deepening, or DFID (also called Iterative Deepening\\nSearch or IDS), is an exhaustive search technique that combines depth-first\\nwith breadth-first search. The DFID algorithm involves repeatedly carrying\\nout depth-first searches on the tree, starting with a depth-first search lim-\\nited to a depth of one, then a depth-first search of depth two, and so on,\\nuntil a goal node is found.\\nThis is an algorithm that appears to be somewhat wasteful in terms of the\\nnumber of steps that are required to find a solution. However, it has the\\nadvantage of combining the efficiency of memory use of depth-first search'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 115, 'page_label': '116'}, page_content='4.11 Depth-First Iterative Deepening 89\\nwith the advantage that branches of the search tree that are infinite or\\nextremely large will not sidetrack the search.\\nIt also shares the advantage of breadth-first search that it will always find\\nthe path that involves the fewest steps through the tree (although, as we will\\nsee, not necessarily the best path).\\nAlthough it appears that DFID would be an extremely inefficient way to\\nsearch a tree, it turns out to be almost as efficient as depth-first or breadth-\\nfirst search. This can be seen from the fact that for most trees, the majority\\nof nodes are in the deepest level, meaning that all three approaches spend\\nmost of their time examining these nodes.\\nFor a tree of depth d and with a branching factor of b, the total number\\nof nodes is\\n1 root node\\nb nodes in the first layer\\nb\\n2 nodes in the second layer\\n...\\nb\\nn nodes in the nth layer\\nHence, the total number of nodes is\\n1 + b + b2 + b3 + . . . + bd\\nwhich is a geometric progression equal to\\nFor example, for a tree of depth 2 with a branching factor of 2, there are\\n= 7 nodes\\nUsing depth-first or breadth-first search, this means that the total number\\nof nodes to be examined is seven.\\nUsing DFID, nodes must be examined more than once, resulting in the fol-\\nlowing progression:\\n(d + 1) + b(d) + b2 (d /H110021) + b3(d /H110022) + . . . + bd\\n1 – 8/H50071 – 2\\n1 – bd+1\\n/H50071 – b'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 116, 'page_label': '117'}, page_content='90 CHAPTER 4 Search Methodologies\\nHence, DFID has a time complexity of O(bd). It has the memory efficiency\\nof depth-first search because it only ever needs to store information about\\nthe current path. Hence, its space complexity is O(bd).\\nIn the case of the tree with depth of 2 and branching factor of 2, this means\\nexamining the following number of nodes:\\n(3 + 1) + 3 /H110032 + 4 /H110032 = 18\\nHence, for a small tree, DFID is far more inefficient in time than depth-first\\nor breadth-first search.\\nHowever, if we compare the time needed for a larger tree with depth of 4\\nand branching factor of 10, the tree has the following number of nodes:\\n= 11,111 nodes\\nDFID will examine the following number of nodes:\\n(4 + 1) + 10 /H110034 + 100 /H110033 + 1,000 /H110032 + 10,000 = 12,345 nodes\\nHence, as the tree gets larger, we see that the majority of the nodes to be\\nexamined (in this case, 10,000 out of 12,345) are in the last row, which\\nneeds to be examined only once in either case.\\nLike breadth-first search, DFID is optimal and complete. Because it also has\\ngood space efficiency, it is an extremely good search method to use where\\nthe search space may be very large and where the depth of the goal node is\\nnot known.\\n4.12 Using Heuristics for Search\\nDepth-first and breadth-first search were described as brute-force search\\nmethods. This is because they do not employ any special knowledge of the\\nsearch trees they are examining but simply examine every node in order\\nuntil they happen upon the goal. This can be likened to the human being\\nwho is traversing a maze by running a hand along the left side of the maze\\nwall.\\nIn some cases, this is the best that can be done because there is no addi-\\ntional information available that can be used to direct the search any better.\\nOften, however, such information does exist and can be used. Take the\\nexample of looking for a suitable Christmas gift. V ery few people would\\n1 – 10\\n5\\n/H50071 – 10'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 117, 'page_label': '118'}, page_content='4.12 Using Heuristics for Search 91\\nsimply walk into each shop as they came across it, looking in each\\ndepartment in turn until they happened upon a present. Most people\\nwould go straight to the shop that they considered to be most likely to\\nhave a suitable gift. If no gift was found in that shop, they would then\\nproceed to the shop they considered to be the next most likely to have a\\nsuitable gift.\\nThis kind of information is called a heuristic, and humans use them all the\\ntime to solve all kinds of problems. Computers can also use heuristics, and\\nin many problems heuristics can reduce an otherwise impossible problem\\nto a relatively simple one.\\nA heuristic evaluation function is a function that when applied to a node\\ngives a value that represents a good estimate of the distance of the node\\nfrom the goal. For two nodes m and n, and a heuristic function f,i f f(m) <\\nf(n), then it should be the case that m is more likely to be on an optimal\\npath to the goal node than n. In other words, the lower the heuristic value\\nof a node, the more likely it is that it is on an optimal path to a goal and the\\nmore sensible it is for a search method to examine that node.\\nThe following sections provide details of a number of search methods that\\nuse heuristics and are thus thought of as heuristic search methods ,o r\\nheuristically informed search methods.\\nTypically, the heuristic used in search is one that provides an estimate of the\\ndistance from any given node to a goal node. This estimate may or may not be\\naccurate, but it should at least provide better results than pure guesswork.\\n4.12.1 Informed and Uninformed Methods\\nA search method or heuristic is informed if it uses additional information\\nabout nodes that have not yet been explored to decide which nodes to\\nexamine next. If a method is not informed, it is uninformed,o r  blind.I n\\nother words, search methods that use heuristics are informed, and those\\nthat do not are blind.\\nBest-first search is an example of informed search, whereas breadth-first\\nand depth-first search are uninformed or blind.\\nA heuristic h is said to be more informed than another heuristic, j,i f\\nh(node) ≤ j(node) for all nodes in the search space. (In fact, in order for h'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 118, 'page_label': '119'}, page_content='92 CHAPTER 4 Search Methodologies\\n7\\n4\\n2\\n6\\n3\\n5\\n1\\n8\\n1\\n8\\n7\\n23\\n6\\n4\\n5\\nFigure 4.5\\nThe 8-puzzle, start state\\nand goal state\\nto be more informed than j, there must be some node where h(node) <\\nj(node). Otherwise they are as informed as each other.)\\nThe more informed a search method is, the more efficiently it will search.\\n4.12.2 Choosing a Good Heuristic\\nSome heuristics are better than others, and the better (more informed) the\\nheuristic is, the fewer nodes it needs to examine in the search tree to find a\\nsolution. Hence, like choosing the right representation, choosing the right\\nheuristic can make a significant difference in our ability to solve a problem.\\nIn choosing heuristics, we usually consider that a heuristic that reduces the\\nnumber of nodes that need to be examined in the search tree is a good\\nheuristic. It is also important to consider the efficiency of running the\\nheuristic itself. In other words, if it takes an hour to compute a heuristic\\nvalue for a given state, the fact that doing so saves a few minutes of total\\nsearch time is irrelevant. For most of this section, we will assume that\\nheuristic functions we choose are extremely simple to calculate and so do\\nnot impact on the overall efficiency of the search algorithm.\\n4.12.3 The 8-puzzle\\nT o illustrate the way in which heuristics are developed, we will use the 8-\\npuzzle, as illustrated in Figure 4.5.\\nThe puzzle consists of a 3 /H110033 grid, with the numbers 1 through 8 on tiles\\nwithin the grid and one blank square. Tiles can be slid about within the\\ngrid, but a tile can only be moved into the empty square if it is adjacent to\\nthe empty square. The start state of the puzzle is a random configuration,\\nand the goal state is as shown in the second picture in Figure 4.5, where the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 119, 'page_label': '120'}, page_content='4.12 Using Heuristics for Search 93\\nnumbers go from 1 to 8 clockwise around the empty middle square, with 1\\nin the top left.\\nTypically, it takes about 20 moves to get from a random start state to the\\ngoal state, so the search tree has a depth of around 20. The branching factor\\ndepends on where the blank square is. If it is in the middle of the grid, the\\nbranching factor is 4; if it is on an edge, the branching factor is 3, and if it is\\nin a corner, the branching factor is 2. Hence, the average branching factor\\nof the search tree is 3.\\nSo, an exhaustive search of the search tree would need to examine around\\n3\\n20 states, which is around 3.5 billion. Because there are only 9! or 362,880\\npossible states, the search tree could clearly be cut down significantly by\\navoiding repeated states.\\nIt is useful to find ways to reduce the search tree further, in order to devise\\na way to solve the problem efficiently. A heuristic would help us to do this,\\nby telling us approximately how many moves a given state is from the goal\\nstate. We will examine a number of possible heuristics that could be used\\nwith the 8-puzzle.\\nT o be useful, our heuristic must never overestimate the cost of changing\\nfrom a given state to the goal state. Such a heuristic is defined as being\\nadmissible. As we will see, in many search methods it is essential that the\\nheuristics we use are admissible.\\nThe first heuristic we consider is to count how many tiles are in the wrong\\nplace. We will call this heuristic, h\\n1(node). In the case of the first state\\nshown in Figure 4.5, h1 (node) = 8 because all the tiles are in the wrong\\nplace. However, this is misleading because we could imagine a state with a\\nheuristic value of 8 but where each tile could be moved to its correct place\\nin one move. This heuristic is clearly admissible because if a tile is in the\\nwrong place, it must be moved at least once.\\nAn improved heuristic, h\\n2, takes into account how far each tile had to\\nmove to get to its correct state. This is achieved by summing the Manhat-\\ntan distances of each tile from its correct position. (Manhattan distance is\\nthe sum of the horizontal and vertical moves that need to be made to get\\nfrom one position to another, named after the grid system of roads used in\\nManhattan.)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 120, 'page_label': '121'}, page_content='94 CHAPTER 4 Search Methodologies\\nFor the first state in Figure 4.5, this heuristic would provide a value of\\nh2 (node) = 2 + 2 + 2 + 2 + 3 + 3 + 1 + 3 = 18\\nClearly, this is still an admissible heuristic because in order to solve the puz-\\nzle, each tile must be moved one square at a time from where it starts to\\nwhere it is in the goal state.\\nIt is worth noting that h\\n2 (node) ≥ h1 (node) for any node. This means that\\nh2 dominates h1, which means that a search method using heuristic h2 will\\nalways perform more efficiently than the same search method using h1.\\nThis is because h2 is more informed than h1. Although a heuristic must\\nnever overestimate the cost, it is always better to choose the heuristic that\\ngives the highest possible underestimate of cost. The ideal heuristic would\\nthus be one that gave exactly accurate costs every time.\\nThis efficiency is best understood in terms of the effective branching fac-\\ntor, b*, of a search.\\nIf a search method expands n nodes in solving a particular problem, and\\nthe goal node is at depth d, then b* is the branching factor of a uniform\\ntree that contains n nodes. Heuristics that give a lower effective branching\\nfactor perform better. A search method running with h\\n2 has a lower effec-\\ntive branching factor than the same search method running with h1 in solv-\\ning the 8-puzzle.\\nA third heuristic function, h3, takes into account the fact that there is extra\\ndifficulty involved if two tiles have to move past each other because tiles\\ncannot jump over each other. This heuristic uses a function k(node), which\\nis equal to the number of direct swaps that need to be made between adja-\\ncent tiles to move them into the correct sequence.\\nh\\n3 (node) = h2 (node) + (2 /H11003k(node))\\nBecause k(node) must be at least 0, h3 (node) must be greater than h2\\n(node), meaning that h3 is a more informed heuristic than h2.\\nThe heuristic functions h1, h2, and h3 are all admissible, meaning that using\\nthe A* algorithm (see Section 4.16.1) with any of these heuristics would\\nguarantee to find the quickest solution to the puzzle.\\nThere are a number of possible ways to generate useful heuristic functions.\\nFunctions like h1 and h2 can be generated by relaxing the 8-puzzle prob-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 121, 'page_label': '122'}, page_content='4.12 Using Heuristics for Search 95\\nlem. A relaxed problem is a version of a problem that has fewer con-\\nstraints. For example, a relaxed version of the 8-puzzle might be that a tile\\ncan be moved to an adjacent square regardless of whether that square is\\nempty or not. In that case, h\\n2 (node) would be exactly equal to the number\\nof moves needed to get from a node to the goal node.\\nIf the problem were relaxed further, we might say that a tile could move to\\nany square, even if that square is not adjacent to the square it is starting\\nfrom. In this case, h\\n1 (node) exactly equals the number of moves needed to\\nget from a node to the goal node.\\nHence, using an exact cost function for a relaxed version of a problem is\\noften a good way to generate a heuristic cost function for the main problem.\\nIt is clear that h3 is the best heuristic function to use of the three we gen-\\nerated because it dominates both h1 and h2. In some cases, a number of\\nheuristic functions may exist, none of which dominates the others. In\\nthat case, a new heuristic can be generated from the heuristics h\\n1... hn,a s\\nfollows:\\nh(node) = max (h1 [node], h2 [ n o d e ] ,...,hn [node])\\nBecause all of h1 to hn is admissible, h(node) must also be admissible. The\\nheuristic function h dominates all of the heuristics h1 ... hn and so is clearly\\nthe best one to use.\\nAs we see in Chapter 6, another way to find a heuristic is to take advantage\\nof features of the problem that is being modeled by the search tree. For\\nexample, in the case of playing checkers, computers are able to use heuris-\\ntics such as the fact that a player with more kings on the board is likely to\\nwin against a player with fewer kings.\\n4.12.4 Monotonicity\\nA search method is described as monotone if it always reaches a given node\\nby the shortest possible path.\\nSo, a search method that reaches a given node at different depths in the\\nsearch tree is not monotone. A monotone search method must be admissi-\\nble, provided there is only one goal state.\\nA monotonic heuristic is a heuristic that has this property.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 122, 'page_label': '123'}, page_content='96 CHAPTER 4 Search Methodologies\\n10\\n3 3\\n6\\n43\\n2\\n12\\nA\\nB\\nC\\nE\\nF\\nD\\nFigure 4.6\\nMap of five cities\\nAn admissible heuristic is a heuristic that never overestimates the true dis-\\ntance of a node from the goal. A monotonic heuristic is also admissible,\\nassuming there is only one goal state.\\n4.12.5 Example: The Modified Traveling Salesman Problem\\nIt is usual when examining heuristic search methods to relate the search\\nproblem to a real-world situation in order to derive suitable heuristics. For\\nthis explanation, we will use the example of finding the best route between\\ntwo cities, a variation of the Traveling Salesman problem, as shown in Fig-\\nure 4.6.\\nIn this diagram, each node represents a town, and the vertices between\\nnodes represent roads that join towns together. A is the starting node, and F\\nis the goal node. Each vertex is labeled with a distance, which shows how\\nlong that road is. Clearly the diagram is not drawn to scale.\\nThe aim of this problem is to find the shortest possible path from city A to\\ncity F. This is different from the traditional Traveling Salesman problem, in\\nwhich the problem is to find a way to travel around a group of cities and\\nfinally arrive back at the starting city.\\nWe can represent the search space of the map in Figure 4.6 as a search tree\\nby showing each possible path as a leaf node in the tree. In doing so, we\\nneed to be careful to remove repetitions of paths, or loops, because those'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 123, 'page_label': '124'}, page_content='4.12 Using Heuristics for Search 97\\nE B\\nFF\\nFF\\nFF\\nDCB F E\\nEE DD\\nA\\nB C\\nD C\\nFigure 4.7\\nSearch tree for map in Figure 4.6\\nwould add redundancy to the graph and make searching it inefficient. The\\ntree for this search space is shown in Figure 4.7.\\nY ou will notice that this tree has nine leaf nodes, seven of which are goal\\nnodes. Two of the paths lead to cyclical paths and so are abandoned. There\\nare seven distinct paths that successfully lead from A to F. These seven paths\\ncan be traced from the tree as follows:\\n1 A,B,D,E,F\\n2 A,B,D,F\\n3 A,B,C,E,D,F\\n4 A,B,C,E,F\\n5 A,C,E,F\\n6 A,C,B,D,E,F\\n7 A,C,B,D,F'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 124, 'page_label': '125'}, page_content='98 CHAPTER 4 Search Methodologies\\nThe two cyclical paths are as follows:\\n1 A,B,D,E,C (which would then lead on to A or B)\\n2 A,C,E,D,B (which would then lead on to A or C)\\nA depth-first approach to this problem would provide path number 1,\\nA,B,D,E,F, which has a total distance of 29.\\nBreadth-first search always produces the path that has the least steps, but\\nnot necessarily the shortest path. In this case, it would yield path 2, which is\\nA,B,D,F and which has a length of 19. This is much shorter than the path\\nproduced by depth-first search but is not the shortest path (the shortest\\npath is path 5, A,C,E,F, which has a length of 17).\\nNow we introduce two new search methods that use heuristics to more effi-\\nciently identify search solutions.\\n4.13 Hill Climbing\\nHill climbing is an example of an informed search method because it uses\\ninformation about the search space to search in a reasonably efficient man-\\nner. If you try to climb a mountain in fog with an altimeter but no map,\\nyou might use the hill climbing Generate and T est approach:\\nCheck the height 1 foot away from your current location in each direction:\\nnorth, south, east, and west.\\nAs soon as you find a position where the height is higher than your current\\nposition, move to that location and restart the algorithm.\\nIf all directions lead lower than your current position, then you stop and\\nassume you have reached the summit. As we see later, this might not neces-\\nsarily always be true.\\nIn examining a search tree, hill climbing will move to the first successor\\nnode that is “better” than the current node—in other words, the first\\nnode that it comes across with a heuristic value lower than that of the\\ncurrent node.\\n4.13.1 Steepest Ascent Hill Climbing\\nSteepest ascent hill climbing is similar to hill climbing, except that rather\\nthan moving to the first position you find that is higher than the current'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 125, 'page_label': '126'}, page_content='4.13 Hill Climbing 99\\n8\\nA\\nB\\nC\\nE\\nF\\nD\\n25\\n12\\n620\\nFigure 4.8\\nThe map of five cities\\nwhere the straight-line\\ndistance from each city to\\nthe goal city (F) is shown\\nposition, you always check around you in all four directions and choose the\\nposition that is highest.\\nSteepest ascent hill climbing can also be thought of as a variation on depth-\\nfirst search, which uses information about how far each node is from the\\ngoal node to choose which path to follow next at any given point.\\nFor this method, we apply a heuristic to the search tree shown in Figure 4.7,\\nwhich is the straight-line distance from each town to the goal town. We are\\nusing this heuristic to approximate the actual distance from each town to\\nthe goal, which will of course be longer than the straight-line distance.\\nIn Figure 4.8, we can see the same search problem as presented in Figure\\n4.6, but instead of noting the lengths of vertices, we note how far each city\\nis (using a straight-line measurement) from the goal, city F.\\nNow hill climbing proceeds as with depth-first search, but at each step, the\\nnew nodes to be added to the queue are sorted into order of distance from\\nthe goal. Note that the only difference between this implementation and\\nthat given for depth-first search is that in hill climbing the successors of\\nstate are sorted according to their distance from the goal before being\\nadded to the queue:\\nFunction hill ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 126, 'page_label': '127'}, page_content='100 CHAPTER 4 Search Methodologies\\nTable 4.4 Analysis of hill climbing\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nA\\nA\\nB\\nB\\nD\\nD\\nF\\n(empty)\\nB,C\\nC\\nD,C,C\\nC,C\\nF ,E,C,C\\nE,C,C\\nThe queue starts out empty, and the initial\\nstate is the root node, which is A.\\nThe successors of A are sorted and placed on\\nthe queue. B is placed before C on the queue\\nbecause it is closer to the goal state, F .\\nF is placed first on the queue because it is clos-\\nest to the goal. In fact, it is the goal, as will be\\ndiscovered in the next step.\\nSUCCESS: Path is reported as A,B,D,F .\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse\\n{\\nsort (successors (state));\\nadd_to_front_of_queue (successors (state));\\n}\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\nThis algorithm thus searches the tree in a depth-first manner, at each step\\nchoosing paths that appear to be most likely to lead to the goal.\\nThe steps taken by a hill-climbing algorithm in solving the preceding prob-\\nlem are shown in Table 4.4:\\nIn this case, hill climbing has produced the same path as breadth-first\\nsearch, which is the path with the least steps, but not the shortest path. In'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 127, 'page_label': '128'}, page_content='4.13 Hill Climbing 101\\nmany cases though, using this heuristic enables hill climbing to identify\\nshorter paths than would be identified by depth-first or breadth-first\\nsearch. Hill climbing uses heuristics to identify paths efficiently but does\\nnot necessarily identify the best path.\\nIf we ran the searches from right to left, instead of from left to right (or\\nordered the search tree the other way around), then we would find that\\nbreadth-first search would produce a different path: A,C,E,F (which is in\\nfact the shortest path), but hill climbing would still produce the same\\npath, A,B,D,F. In other words, the particular ordering of nodes used\\naffects which result is produced by breadth-first and depth-first search\\nbut does not affect hill climbing in the same way. This can clearly be a\\nuseful property.\\n4.13.2 Foothills, Plateaus, and Ridges\\nAlthough we have been talking about using search techniques to tra-\\nverse search trees, they can also be used to solve search problems that\\nare represented in different ways. In particular, we often represent a\\nsearch problem as a three-dimensional space, where the x- and y-axes\\nare used to represent variables and the z-axis (or height) is used to rep-\\nresent the outcome.\\nThe goal is usually to maximize the outcome, and so search methods in\\nthese cases are aiming to find the highest point in the space.\\nMany such search spaces can be successfully traversed using hill climbing\\nand other heuristically informed search methods. Some search spaces,\\nhowever, will present particular difficulties for these techniques.\\nIn particular, hill climbing can be fooled by foothills, plateaus, and ridges.\\nFigure 4.9 has three illustrations, showing foothills, a plateau, and a ridge.\\nThis figure shows the search space represented as a three-dimensional ter-\\nrain. In this kind of terrain, the aim of search is to find the x and y values\\nthat give the highest possible value of z—in other words, the highest point\\nin the terrain. This is another way of looking at traditional search: search is\\nnormally aiming to maximize some function, which in this case is shown as\\nthe height of the terrain, but is traditionally a function that details the dis-\\ntance of a node from the goal node.\\nFoothills are often called local maxima by mathematicians. A local maxi-\\nmum is a part of the search space that appears to be preferable to the parts\\naround it, but which is in fact just a foothill of a larger hill. Hill-climbing'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 128, 'page_label': '129'}, page_content='102 CHAPTER 4 Search Methodologies\\nz y\\nGLOBAL MAXIMUM\\nLOCAL MAXIMUM\\nMAXIMUM\\nPLATEAU\\nx\\nz y\\nx\\nz y\\nx\\nA\\nC\\nB\\n(a)\\n(b)\\n(c)\\nFigure 4.9\\n(a)  FOOTHILLS\\n(b)  PLATEAU\\n(c)  RIDGE\\ntechniques will reach this peak, where a more sophisticated technique\\nmight move on from there to look for the global maximum. Figure 4.9 (a)\\nshows a search space that has a single global maximum surrounded by a\\nnumber of foothills, or local maxima. Many search methods would reach\\nthe top of one of these foothills and, because there was nowhere higher\\nnearby, would conclude that this was the best solution to the problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 129, 'page_label': '130'}, page_content='4.13 Hill Climbing 103\\nLater in this chapter and in Chapter 5, we see methods such as simulated\\nannealing that are good at avoiding being trapped by local maxima.\\nA plateau is a region in a search space where all the values are the same. In\\nthis case, although there may well be a suitable maximum value somewhere\\nnearby, there is no indication from the local terrain of which direction to\\ngo to find it. Hill climbing does not perform well in this situation. Figure\\n4.9 (b) shows a search space that consists of just one peak surrounded by a\\nplateau. A hill-climbing search method could well find itself stuck in the\\nplateau with no clear indication of where to go to find a good solution.\\nThe final problem for hill climbing is presented by ridges. A ridge is a long,\\nthin region of high land with low land on either side. When looking in one\\nof the four directions, north, south, east, and west from the ridge, a hill-\\nclimbing algorithm would determine that any point on the top of the ridge\\nwas a maximum because the hill falls away in those four directions. The\\ncorrect direction is a very narrow one that leads up the top of the ridge, but\\nidentifying this direction using hill climbing could be very tricky.\\nFigure 4.9 (c) shows a ridge. The point marked A is lower than the point\\nmarked B, which is the global maximum. When a hill-climbing method\\nfinds itself at point C, it might find it hard to get from there to B. The arrows\\non point C show that in moving north, south, east, or west, the method\\nwould find itself at a lower point. The correct direction is up the ridge.\\n4.14 Best-First Search\\nBest-first search employs a heuristic in a similar manner to hill climbing.\\nThe difference is that with best-first search, the entire queue is sorted after\\nnew paths have been added to it, rather than adding a set of sorted paths.\\nIn practical terms, this means that best-first search follows the best path\\navailable from the current (partially developed) tree, rather than always fol-\\nlowing a depth-first style approach.\\nFunction best ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 130, 'page_label': '131'}, page_content='104 CHAPTER 4 Search Methodologies\\nTable 4.5 Analysis of best-first search of tree shown in Figure 4.4\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nA\\nA\\nA\\nB\\nB\\nB\\nD\\nD\\nD\\nF\\n(empty)\\nB,C\\nB,C\\nC\\nD,C,C\\nD,C,C\\nC,C\\nE,F ,C,C\\nF ,E,C,C\\nE,C,C\\nThe queue starts out empty, and the initial\\nstate is the root node, which is A.\\nThe successors of the current state, B and C, are\\nplaced in the queue.\\nThe queue is sorted, leaving B in front of C\\nbecause it is closer to the goal state, F .\\nThe children of node B are added to the front of\\nthe queue.\\nThe queue is sorted, leaving D at the front\\nbecause it is closer to the goal node than C.\\nNote that although the queue appears to con-\\ntain the same node twice, this is just an artifact\\nof the way the search tree was constructed. In\\nfact, those two nodes are distinct and represent\\ndifferent paths on our search tree.\\nThe children of D are added to the front of the\\nqueue.\\nThe queue is sorted, moving F to the front.\\nSUCCESS: Path is reported as A,B,D,F .\\n{\\nadd_to_front_of_queue (successors (state));\\nsort (queue);\\n}\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nThe path taken through the search tree shown in Figure 4.7 is shown in\\nTable 4.5.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 131, 'page_label': '132'}, page_content='4.14 Best-First Search 105\\nIt can be seen that, in this case, best-first search happens to produce the\\nsame path as hill climbing and breadth-first search, although the queue is\\nordered differently during the process. As with hill climbing, best-first\\nsearch will tend to provide a shorter path than depth first or breadth first,\\nbut not necessarily the shortest path.\\n4.15 Beam Search\\nBeam search is a form of breadth-first search that employs a heuristic, as\\nseen with hill climbing and best-first search. Beam search works using a\\nthreshold so that only the best few paths are followed downward at each\\nlevel. This method is very efficient in memory usage and would be particu-\\nlarly useful for exploring a search space that had a very high branching fac-\\ntor (such as in game trees for games, such as Go or Chess). It has the\\ndisadvantage of not exhaustively searching the entire tree and so may fail to\\never find a goal node.\\nIn this implementation, the function call \\nselect_best_paths (queue, n)\\nremoves all but the best n paths from the queue.\\nFunction beam ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse\\n{\\nadd_to_back_of_queue (successors (state));\\nselect_best_paths (queue, n);\\n}\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nIn this pseudocode, n is used to represent the width threshold, which is set\\nat the beginning of the procedure.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 132, 'page_label': '133'}, page_content='106 CHAPTER 4 Search Methodologies\\nTable 4.6 Analysis of beam search of tree shown in Figure 4.7\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\nA\\nA\\nB\\nB\\nB\\nD\\nD\\nD\\nE\\nE\\nE\\nF\\n(empty)\\nB,C\\nC\\nC,D,C\\nD,C\\nC\\nC,E,F\\nE,F\\nF\\nF ,C,F\\nF, F\\nF\\nThe queue starts out empty, and the initial state\\nis the root node, which is A.\\nThe two children of the current node are added to\\nthe back of the queue.\\nThe two children of B are added to the back of\\nthe queue.\\nAll but the two best paths are discarded from the\\nqueue.\\nThe two children of the current node are added to\\nthe back of the queue.\\nAt this step, C is removed from the queue because\\nwe only require the two best paths.\\nThe two children of E are added to the back of the\\nqueue.\\nThe path that leads to C is discarded, in favor of\\nthe two better paths, both of which lead to F .\\nSUCCESS: Path is reported as A,B,D,E,F .\\nThe interesting aspect of this method is the choice of how to define the\\n“best” paths to include in the queue. Often, the path that involves the fewest\\nsteps is used or the path that has reached the point with the highest heuris-\\ntic value (in other words, the path that got closest to the goal).\\nIn Table 4.6, the value of state and queue are shown for the problem tree\\nshown in Figure 4.7, using beam search with a threshold of 2 (in other\\nwords, only two paths are extended down from each level). For this imple-\\nmentation, we have used the heuristic value of each node to determine'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 133, 'page_label': '134'}, page_content='4.16 Identifying Optimal Paths 107\\nwhich path is the “best” path. So the “best” path will be the one that has\\nreached the closest to a goal node so far.\\n4.16 Identifying Optimal Paths\\nSeveral methods exist that do identify the optimal path through a search\\ntree. The optimal path is the one that has the lowest cost or involves travel-\\ning the shortest distance from start to goal node. The techniques described\\npreviously may find the optimal path by accident, but none of them are\\nguaranteed to find it.\\nThe simplest method for identifying the optimal path is called the British\\nMuseum procedure . This process involves examining every single path\\nthrough the search tree and returning via the best path that was found.\\nBecause every path is examined, the optimal path must be found. This\\nprocess is implemented as an extension of one of the exhaustive search\\ntechniques, such as depth-first or breadth-first search, but rather than stop-\\nping when a solution is found, the solution is stored and the process con-\\ntinues until all paths have been explored. If an alternative solution is found,\\nits path is compared with the stored path, and if it has a lower cost, it\\nreplaces the stored path.\\nThe following more sophisticated techniques for identifying optimal paths\\nare outlined in this section:\\n■ A*\\n■ uniform cost search (Branch and Bound)\\n■ greedy search\\nThe British Museum procedure also has the property that it generates all\\nsolutions. Most of the search methods we look at in this book stop when\\nthey find a solution. In some cases, this will be the best solution, and in\\nother cases it may even be the worst available solution (depth-first search\\nwill do this if the worst solution happens to be the left-most solution).\\nIn some cases, it may be necessary to identify all possible solutions, in\\nwhich case something like the British Museum procedure would be useful.\\nAssuming that none of the branches of the tree is infinitely deep, and that\\nno level has an infinite branching factor, then it does not matter which\\napproach is used (depth first or breadth first, for example) when running'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 134, 'page_label': '135'}, page_content='108 CHAPTER 4 Search Methodologies\\nthe British Museum procedure: because the goal is to visit every node, the\\norder the nodes are visited probably does not matter.\\n4.16.1 A* Algorithms\\nA* algorithms are similar to best-first search but use a somewhat more\\ncomplex heuristic to select a path through the tree. The best-first algorithm\\nalways extends paths that involve moving to the node that appears to be\\nclosest to the goal, but it does not take into account the cost of the path to\\nthat node so far.\\nThe A* algorithm operates in the same manner as best-first search but uses\\nthe following function to evaluate nodes:\\nf(node) = g(node) + h(node)\\ng(node) is the cost of the path so far leading up to the node, and h(node) is\\nan underestimate of the distance of the node from a goal state; f is called a\\npath-based evaluation function. When operating A*, f(node) is evaluated\\nfor successor nodes and paths extended using the nodes that have the low-\\nest values of f.\\nIf h(node) is always an underestimate of the distance of a node to a goal\\nnode, then the A* algorithm is optimal: it is guaranteed to find the shortest\\npath to a goal state. A* is described as being optimally efficient, in that in\\nfinding the path to the goal node, it will expand the fewest possible paths.\\nAgain, this property depends on h(node) always being an underestimate.\\nNote that running the A* algorithm on the search tree shown in Figure 4.4\\nwould not be guaranteed to find the shortest solution because the esti-\\nmated values for h(node) are not all underestimates. In other words, the\\nheuristic that is being used is not admissible. If a nonadmissible heuristic\\nfor h(node) is used, then the algorithm is called A.\\nA* is the name given to the algorithm where the h(node) function is admis-\\nsible. In other words, it is guaranteed to provide an underestimate of the\\ntrue cost to the goal.\\nA* is optimal and complete. In other words, it is guaranteed to find a solu-\\ntion, and that solution is guaranteed to be the best solution.\\nA* is in fact only complete if the tree it is searching has a finite branching\\nfactor and does not contain a path of finite cost, which has an infinite num-\\nber of nodes along it. Both of these conditions are likely to be met in all'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 135, 'page_label': '136'}, page_content='4.16 Identifying Optimal Paths 109\\nreal-world situations, and so for simplicity we can state that A* is complete;\\nalthough, to be more accurate:\\nA* is complete if the graph it is searching is locally finite (that is, it has a\\nfinite branching factor) and if every arc between two nodes in the graph\\nhas a non-zero cost.\\nThat A* is optimal can be proved by considering a counter-example:\\nImagine we are applying the A* algorithm to a graph with two goals, G1\\nand G2. The path cost of G1 is f1 and the path cost of G2 is f2, where f2 >\\nf1. G1 is the goal with the lower cost, but let us imagine a scenario where\\nthe A* algorithm has reached G2 without having explored G1. In other\\nwords, we are imagining a scenario where the algorithm has not chosen the\\ngoal with the lesser cost.\\nIf we consider a node, n, that is on an optimal path from the root node to\\nG1, then because h is an admissible heuristic:\\nf1 ≥ f (n)\\nThe only reason the algorithm would not choose to expand n before it\\nreaches G2 would be if\\nf (n) > f (G2)\\nHence, by combining these two expressions together, we arrive at\\nf1 ≥ f(G2)\\nBecause G2 is a goal state, it must be the case that h(G2) = 0, and thus f(G2)\\n= g(G2). Thus we have\\nf1 ≥ g(G2)\\nThis, therefore, contradicts our original assumption that G2 had a higher\\npath cost than G1, which proves that A* can only ever choose the least cost\\npath to a goal.\\nIt was mentioned that A* is similar to breadth-first search. In fact, breadth-\\nfirst search can be considered to be a special case of A*, where h(node) is\\nalways 0, so f(node) = g(node), and where every direct path between a node\\nand its immediate successor has a cost of 1.\\n4.16.2 Uniform Cost Search\\nUniform cost search (or Branch and Bound ) is a variation on best-first\\nsearch that uses the evaluation function g(node), which for a given node'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 136, 'page_label': '137'}, page_content='110 CHAPTER 4 Search Methodologies\\nevaluates to the cost of the path leading to that node. In other words, this is\\nan A* algorithm but where h(node) is set to zero. At each stage, the path\\nthat has the lowest cost so far is extended. In this way, the path that is gen-\\nerated is likely to be the path with the lowest overall cost, but this is not\\nguaranteed. T o find the best path, the algorithm needs to continue running\\nafter a solution is found, and if a preferable solution is found, it should be\\naccepted in place of the earlier solution.\\nUniform cost search is complete and is optimal, providing the cost of a\\npath increases monotonically. In other words, if for every node m that has a\\nsuccessor n, it is true that g(m) < g(n), then uniform cost is optimal. If it is\\npossible for the cost of a node to be less than the cost of its parent, then\\nuniform cost search may not find the best path.\\nUniform cost search was invented by Dijkstra in 1959 and is also known as\\nDijkstra’s algorithm.\\n4.16.3 Greedy Search\\nGreedy search is a variation of the A* algorithm, where g(node) is set to\\nzero, so that only h(node) is used to evaluate suitable paths. In this way, the\\nalgorithm always selects the path that has the lowest heuristic value or esti-\\nmated distance (or cost) to the goal.\\nGreedy search is an example of a best-first strategy.\\nGreedy-search methods tend to be reasonably efficient, although in the worst\\ncase, like depth-first search, it may never find a solution at all. Additionally,\\ngreedy search is not optimal and can be fooled into following extremely costly\\npaths. This can happen if the first step on the shortest path toward the goal is\\nlonger than the first step along another path, as is shown in Figure 4.10.\\n4.16.4 Example: The Knapsack Problem\\nThe knapsack problem is an interesting illustration of the use of greedy-\\nsearch algorithms and their pitfalls. The fractional knapsack problem can\\nbe expressed as follows:\\nA man is packing items into his knapsack. He wants to take the most valu-\\nable items he can, but there is a limit on how much weight he can fit in his\\nknapsack. Each item has a weight w\\ni and is worth vi. He can only fit a total\\nweight of W in his knapsack. The items that he wants to take are things that\\ncan be broken up and still retain their value (like flour or milk), and he is'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 137, 'page_label': '138'}, page_content='4.16 Identifying Optimal Paths 111\\n11 0\\n1000 10\\nFigure 4.10\\nA search tree where a\\ngreedy-search method will\\nnot find the best solution\\nable to take fractions of items. Hence, the problem is called the fractional\\nknapsack problem.\\nIn solving this problem, a greedy-search algorithm provides the best solution.\\nThe problem is solved by calculating the value per unit weight of each item:\\nv\\ni/wi, and then taking as much as he can carry of the item with the greatest\\nvalue per unit weight. If he still has room, he moves on to the item with the\\nnext highest value per unit weight, and so on.\\nThe 0-1 knapsack problem is the same as the fractional knapsack problem,\\nexcept that he cannot take parts of items. Each item is thus something like\\na television set or a laptop computer, which must be taken whole. In solving\\nthis problem, a greedy-search approach does not work, as can be seen from\\nthe following example:\\nOur man has a knapsack that lets him carry a total of 100 pounds. His\\nitems are:\\n1 gold brick worth $1800 and weighing 50 pounds\\n1 platinum brick worth $1500 and weighing 30 pounds\\n1 laptop computer worth $2000 and weighing 50 pounds\\nHence, we have four items, whose values ofv and w are as follows:\\nv\\n1 = 1800 w1 = 50 v1/w1 = 36\\nv2 = 1500 w2 = 30 v2/w2 = 50\\nv3 = 2000 w3 = 50 v3/w3 = 40'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 138, 'page_label': '139'}, page_content='112 CHAPTER 4 Search Methodologies\\nIn this case, a greedy-search strategy would pick item 2 first, and then\\nwould take item 3, giving a total weight of 80 pounds, and a total value of\\n$3500. In fact, the best solution is to take items 1 and 3 and to leave item 2\\nbehind giving a total weight of 100 pounds and a total value of $3800.\\n4.17 Chapter Summary\\n■ Generate and T est is an extremely simple example of a brute-force\\nor exhaustive search technique.\\n■ Depth-first search and breadth-first search are extremely com-\\nmonly used and well understood exhaustive search methods.\\n■ In analyzing search methods, it is important to examine the com-\\nplexity (in time and space) of the method.\\n■ A search method is complete if it will always find a solution if one\\nexists. A search method is optimal (or admissible) if it always finds\\nthe best solution that exists.\\n■ Depth-First Iterative Deepening (DFID) is a search method that\\nhas the low memory requirements of depth-first search and is opti-\\nmal and complete, like breadth-first search.\\n■ Heuristics can be used to make search methods more informed\\nabout the problem they are solving. A heuristic is a method that\\nprovides a better guess about the correct choice to make at any\\njunction that would be achieved by random guessing.\\n■ One heuristic is more informed than another heuristic if a search\\nmethod that uses it needs to examine fewer nodes to reach a goal.\\n■ Relaxing problems is one way to identify potentially useful heuristics.\\n■ Hill climbing is a heuristic search method that involves continually\\nmoving from one potential solution to a better potential solution\\nuntil no better solution can be found.\\n■ Hill climbing has problems in search spaces that have foothills,\\nplateaus, and ridges.\\n■ A* is a heuristic search method that in most situations is optimal\\nand complete. It uses the path evaluation function to choose suit-\\nable paths through the search space.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 139, 'page_label': '140'}, page_content='Review Questions 113\\n■ Uniform cost search is similar to A* but uses a simpler evaluation\\nfunction, which is based just on the cost of reaching the node so far.\\n■ Greedy search involves always moving to the most immediately\\nattractive position on the next step. It can be used to solve the frac-\\ntional knapsack problem, but not the 1-0 knapsack problem.\\n4.18 Review Questions\\n4.1 Explain the idea behind Generate and T est. Why is this method\\ndescribed as being exhaustive ?\\n4.2 Explain the differences and similarities between depth-first search\\nand breadth-first search. Give examples of the kinds of problems\\nwhere each would be appropriate.\\n4.3 Explain what is meant by the following terms in relation to search\\nmethods:\\n■ complexity\\n■ completeness\\n■ optimality\\n4.4 What is the complexity (in space and in time) of the following\\nsearch methods:\\n■ depth-first search\\n■ breadth-first search\\n■ best-first search\\n■ greedy search\\n4.5 What does it mean to say that a search method is monotonic? How\\ndesirable is this property? Which of the search methods described\\nin this chapter is monotonic?\\n4.6 Explain why Depth-First Search Iterative Deepening is reasonably\\nefficient. Why might it be preferable to use DFID rather than\\ndepth-first search?\\n4.7 Provide a definition of the word “heuristic. ” In what ways can\\nheuristics be useful in search? Name three ways in which you use\\nheuristics in your everyday life.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 140, 'page_label': '141'}, page_content='114 CHAPTER 4 Search Methodologies\\n4.8 Explain the components of the path evaluation function f(node)\\nused by A*. Do you think it is the best evaluation function that\\ncould be used? T o what kinds of problems might it be best suited?\\nAnd to what kinds of problems would it be worst suited?\\n4.9 Show that A* is optimal and complete in most circumstances.\\n4.10 Explain why a greedy method provides suboptimal solutions to the\\n0-1 knapsack problem but provides optimal solutions to the frac-\\ntional knapsack problem. Could there be a search tree for which\\ngreedy search found optimal solutions?\\n4.11 What effect does the ordering of a search tree have on the efficiency\\nof search? What effect does it have on the quality of the results?\\nHow would ordering affect the way that depth-first search or\\ngreedy search would perform when searching the search tree\\nshown in Figure 4.10?\\n4.19 Exercises\\n4.12 Implement a data structure that represents search trees in a pro-\\ngramming language of your choice. Have the program display the\\ntree on the screen, and provide functions that can select nodes and\\ndisplay paths.\\n4.13 Implement depth-first search in your program. Implement\\nbreadth-first search. Build a search tree of depth 10 and with a\\nbranching factor of 2. Which of your search methods finds a goal\\nthe most quickly? Can you change the tree so that the other\\nmethod finds the goal more quickly?\\n4.14 Add the concept of path cost to your implementation. Implement A*.\\nDoes it perform much better than depth-first or breadth-first search?\\nHow well does it do with the large tree you built in Exercise 4.8?\\n4.15 Implement a greedy-search algorithm. How well does it perform\\ncompared with the other methods you have implemented? Invent a\\n0-1 knapsack problem, and use your search tree implementation to\\nmodel this problem. Can you model the fractional knapsack prob-\\nlem using a search tree?\\n4.16 Investigate the file search facility on your computer. Which type of\\nsearch method do you think it uses? Why do you think this partic-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 141, 'page_label': '142'}, page_content='Further Reading 115\\nular search method was chosen? What problems could this\\napproach cause it? How well does it work when it is searching\\ndirectories with large numbers of files in them?\\n4.20 Further Reading\\nSearch is covered well by almost all artificial intelligence text books,\\nalthough the approaches taken vary.\\nA detailed description and analysis of Dijkstra’s algorithm (uniform cost\\nsearch) can be found in Cormen et al. (2001). Books such as this that cover\\nalgorithms in more detail provide an interesting non-Artificial Intelligence\\nperspective on the subject.\\nMarvin Minsky’s 1961 paper,Steps Toward Artificial Intelligence, introduced\\nthe idea of hill climbing and discussed some of the difficulties faced by hill-\\nclimbing methods.\\nAllen Newell and Herbert A. Simon’s 1976 paper, Computer Science as\\nEmpirical Inquiry, contains an excellent discussion of heuristic search for\\nproblem solving.\\nA good description of the way that Prolog uses depth-first search for unifi-\\ncation is contained in Russell and Norvig (1995).\\nIntroduction to Algorithms , by Thomas H. Cormen, Charles E. Leiserson,\\nRonald L. Rivest, and Clifford Stein (2001 – MIT Press)\\nArtificial Intelligence: Strategies, Applications, and Models Through Search,b y\\nBenedict Du Boulay and Christopher James Thornton (1999 – AMACOM)\\nAlgorithmics: The Spirit of Computing, by David Harel (1987 – Addison Wesley)\\nArt of Computer Programming: Sorting and Searching , by Donald Knuth\\n(1973 – Pearson Addison Wesley)\\nSteps Towards Artificial Intelligence, by Marvin Minsky (1961 – in Computa-\\ntion & Intelligence—Collected Readings, edited by George F. Luger, MIT Press)\\nComputer Science as Empirical Enquiry: Symbols and Search , by Allen\\nNewell and Herbert A. Simon (1976 – in Computation & Intelligence—Col-\\nlected Readings, edited by George F. Luger, MIT Press)\\nAlgorithms, by Robert Sedgewick (1988 – Addison Wesley)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 142, 'page_label': '143'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 143, 'page_label': '144'}, page_content='5CHAPTER\\nAdvanced Search\\nThe difficult we do immediately. The impossible takes a little longer.\\n—US Armed Forces slogan\\nHad I been present at the Creation, I would have given some useful hints for\\nthe better ordering of the universe.\\n—Alfonso ‘the wise’ , on studying the Ptolemaic system (13th centuryA.D.)\\nIf we value the pursuit of knowledge, we must be free to follow wherever that\\nsearch may lead us. The free mind is not a barking dog, to be tethered on a ten-\\nfoot chain.\\n—Adlai E. Stevenson Jr., speech at the University of Wisconsin, Madison,\\nOctober 8, 1952\\n5.1 Introduction\\nIn Chapter 4, we examined a range of methods that can be used to search a\\nproblem space. In Chapter 5, we introduce some more sophisticated methods.\\nFirst, we examine constraint satisfaction problems, such as the eight-\\nqueens problem, and search methods and heuristics that can be used to\\nsolve them.\\nWe also discuss local search methods , such as simulated annealing, that\\nattempt to find a solution to large combinatorial problems by moving\\nfrom one possible solution to another that is a little better. We also intro-\\nduce the idea of parallel search—using multiple processors (or multiple'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 144, 'page_label': '145'}, page_content='118 CHAPTER 5 Advanced Search\\ncomputers) to deal with a single search problem to solve it more quickly.\\nMuch of the material in this chapter is introductory in nature, and refer-\\nences are given to books and papers where more information can be\\nlearned on the methods.\\n5.2 Constraint Satisfaction Search\\nSearch can be used to solve problems that are limited by constraints, such\\nas the eight-queens problem. Such problems are often known as Con-\\nstraint Satisfaction Problems,o r  CSPs.\\nIn this problem, eight queens must be placed on a chess board in such a\\nway that no two queens are on the same diagonal, row, or column. If we use\\ntraditional chess board notation, we mark the columns with letters from a\\nto g and the rows with numbers from 1 to 8. So, a square can be referred to\\nby a letter and a number, such as a4 or g7.\\nThis kind of problem is known as a constraint satisfaction problem (CSP)\\nbecause a solution must be found that satisfies the constraints.\\nIn the case of the eight-queens problem, a search tree can be built that rep-\\nresents the possible positions of queens on the board.\\nOne way to represent this is to have a tree that is 8-ply deep, with a branch-\\ning factor of 64 for the first level, 63 for the next level, and so on, down to\\n57 for the eighth level.\\nA goal node in this tree is one that satisfies the constraints that no two\\nqueens can be on the same diagonal, row, or column.\\nAn extremely simplistic approach to solving this problem would be to ana-\\nlyze every possible configuration until one was found that matched the\\nconstraints.\\nA more suitable approach to solving the eight-queens problem would be to\\nuse depth-first search on a search tree that represents the problem in the\\nfollowing manner:\\nThe first branch from the root node would represent the first choice of a\\nsquare for a queen. The next branch from these nodes would represent\\nchoices of where to place the second queen.\\nThe first level would have a branching factor of 64 because there are 64 pos-\\nsible squares on which to place the first queen. The next level would have a'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 145, 'page_label': '146'}, page_content='5.2 Constraint Satisfaction Search 119\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nabcdef gh\\nFigure 5.1\\nThe eight-queens prob-\\nlem. Three queens have\\nbeen placed so far.\\nsomewhat lower branching factor because once a queen has been placed,\\nthe constraints can be used to determine possible squares upon which the\\nnext queen can be placed. The branching factor will decrease as the algo-\\nrithm searches down the tree. At some point, the tree will terminate\\nbecause the path being followed will lead to a position where no more\\nqueens can be placed on legal squares on the board, and there are still some\\nqueens remaining.\\nIn fact, because each row and each column must contain exactly one queen,\\nthe branching factor can be significantly reduced by assuming that the first\\nqueen must be placed in row 1, the second in row 2, and so on. In this way,\\nthe first level will have a branching factor of 8 (a choice of eight squares on\\nwhich the first queen can be placed), the next 7, the next 6, and so on.\\nIn fact, the search tree can be further simplified as each queen placed on the\\nboard “uses up” a diagonal, meaning that the branching factor is only 5 or 6\\nafter the first choice has been made, depending on whether the first queen\\nis placed on an edge of the board (columns a or h) or not. The next level\\nwill have a branching factor of about 4, and the next may have a branching\\nfactor of just 2, as shown in Figure 5.1.\\nThe arrows in Figure 5.1 show the squares to which each queen can move.\\nNote that no queen can move to a square that is already occupied by\\nanother queen.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 146, 'page_label': '147'}, page_content='120 CHAPTER 5 Advanced Search\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nabcdef gh\\nFigure 5.2\\nA solution to the eight-\\nqueens problem\\nIn Figure 5.1, the first queen was placed in column a of row 8, leaving six\\nchoices for the next row. The second queen was placed in column d of row\\n7, leaving four choices for row 6. The third queen was placed in column f in\\nrow 6, leaving just two choices (column c or column h) for row 5.\\nUsing knowledge like this about the problem that is being solved can help\\nto significantly reduce the size of the search tree and thus improve the effi-\\nciency of the search solution.\\nA solution will be found when the algorithm reaches depth 8 and success-\\nfully places the final queen on a legal square on the board. A goal node\\nwould be a path containing eight squares such that no two squares shared a\\ndiagonal, row, or column.\\nOne solution to the eight-queens problem is shown in Figure 5.2.\\nNote that in this solution, if we start by placing queens on squares e8, c7,\\nh6, and then d5, once the fourth queen has been placed, there are only two\\nchoices for placing the fifth queen (b4 or g4). If b4 is chosen, then this\\nleaves no squares that could be chosen for the final three queens to satisfy\\nthe constraints. If g4 is chosen for the fifth queen, as has been done in Fig-\\nure 5.2, only one square is available for the sixth queen (a3), and the final\\ntwo choices are similarly constrained. So, it can be seen that by applying the\\nconstraints appropriately, the search tree can be significantly reduced for\\nthis problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 147, 'page_label': '148'}, page_content='5.4 Most-Constrained Variables 121\\nUsing chronological backtracking in solving the eight-queens problem\\nmight not be the most efficient way to identify a solution because it will\\nbacktrack over moves that did not necessarily directly lead to an error, as\\nwell as ones that did. In this case, nonchronological backtracking, or\\ndependency-directed backtracking (see Section 5.17) could be more use-\\nful because it could identify the steps earlier in the search tree that caused\\nthe problem further down the tree.\\n5.3 Forward Checking\\nIn fact, backtracking can be augmented in solving problems like the eight-\\nqueens problem by using a method called forward checking . As each\\nqueen is placed on the board, a forward-checking mechanism is used to\\ndelete from the set of possible future choices any that have been rendered\\nimpossible by placing the queen on that square. For example, if a queen is\\nplaced on square a1, forward checking will remove all squares in row 1, all\\nsquares in column a, and also squares b2, c3, d4, e5, f6, g7, and h8. In this\\nway, if placing a queen on the board results in removing all remaining\\nsquares, the system can immediately backtrack, without having to attempt\\nto place any more queens. This can often significantly improve the per-\\nformance of solutions for CSPs such as the eight-queens problem.\\n5.4 Most-Constrained Variables\\nA further improvement in performance can be achieved by using the most-\\nconstrained variable heuristic. At each stage of the search, this heuristic\\ninvolves working with the variable that has the least possible number of\\nvalid choices. In the case of the eight-queens problem, this might be\\nachieved by considering the problem to be one of assigning a value to eight\\nvariables, a through h. Assigning value 1 to variable a means placing a\\nqueen in square a1. T o use the most constrained variable heuristic with this\\nrepresentation means that at each move we assign a value to the variable\\nthat has the least choices available to it. Hence, after assigning a = 1, b = 3,\\nand c = 5, this leaves three choices for d, three choices for e, one choice for\\nf, three choices for g, and three choices for h. Hence, our next move is to\\nplace a queen in column f.\\nThis heuristic is perhaps more clearly understood in relation to the map-\\ncoloring problem. It makes sense that, in a situation where a particular'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 148, 'page_label': '149'}, page_content='122 CHAPTER 5 Advanced Search\\ncountry can be given only one color due to the colors that have been\\nassigned to its neighbors, that country be colored next.\\nThe most-constraining variable heuristic is similar in that it involves\\nassigning a value next to the variable that places the greatest number of\\nconstraints on future variables.\\nThe least-constraining value heuristic is perhaps more intuitive than the\\ntwo already presented in this section. This heuristic involves assigning a\\nvalue to a variable that leaves the greatest number of choices for other vari-\\nables. This heuristic can be used to make n-queens problems with\\nextremely large values of n quite solvable.\\n5.5 Example: Cryptographic Problems\\nThe constraint satisfaction procedure is also a useful way to solve problems\\nsuch as cryptographic problems. For example:\\nFORTY\\n+ TEN\\n+ TEN\\nSIXTY\\nSolution:\\n29786\\n+ 850\\n+ 850\\n31486\\nThis cryptographic problem can be solved by using a Generate and T est\\nmethod, applying the following constraints:\\n■ Each letter represents exactly one number.\\n■ No two letters represent the same number.\\nAs explained in Chapter 4, Generate and T est is a brute-force method,\\nwhich in this case involves cycling through all possible assignments of\\nnumbers to letters until a set is found that meets the constraints and solves\\nthe problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 149, 'page_label': '150'}, page_content='5.6 Heuristic Repair 123\\nWithout using constraints, the method would first start by attempting to\\nassign 0 to all letters, resulting in the following sum:\\n00000\\n+ 000\\n+ 000\\n00000\\nAlthough this may appear to be a valid solution to the problem, it does not\\nmeet the constraints laid down that specify that each letter can be assigned\\nonly one number, and each number can be assigned only to one letter.\\nHence, constraints are necessary simply to find the correct solution to the\\nproblem. They also enable us to reduce the size of the search tree. In this\\ncase, for example, it is not necessary to examine possible solutions where\\ntwo letters have been assigned the same number, which dramatically\\nreduces the possible solutions to be examined.\\nAs we see in the next section, there are more efficient methods than Gener-\\nate and T est to solve problems of this nature.\\n5.6 Heuristic Repair\\nHeuristics can be used to improve performance of solutions to con-\\nstraint satisfaction problems. One way to do this is to use a heuristic\\nrepair method , which involves generating a possible solution (ran-\\ndomly, or using a heuristic to generate a position that is close to a solu-\\ntion) and then making changes that reduce the distance of the state\\nfrom the goal.\\nIn the case of the eight-queens problem, this could be done using the min-\\nconflicts heuristic. T o move from one state to another state that is likely to be\\ncloser to a solution using the min-conflicts heuristic, select one queen that\\nconflicts with another queen (in other words, it is on the same row, column,\\nor diagonal as another queen). Now move that queen to a square where it\\nconflicts with as few queens as possible. Continue with another queen.\\nT o see how this method would work, consider the starting position shown\\nin Figure 5.3.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 150, 'page_label': '151'}, page_content='124 CHAPTER 5 Advanced Search\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nabcdef gh\\nFigure 5.3\\nAlmost a solution to the\\neight-queens problem\\nThis starting position has been generated by placing the queens such that there\\nare no conflicts on rows or columns. The only conflict here is that the queen in\\ncolumn 3 (on c7) is on a diagonal with the queen in column h (on h2).\\nT o move toward a solution, we choose to move the queen that is on column\\nh. We will only ever apply a move that keeps a queen on the same column\\nbecause we already know that we need to have one queen on each column.\\nEach square in column h has been marked with a number to show how\\nmany other queens that square conflicts with. Our first move will be to\\nmove the queen on column h up to row 6, where it will conflict only with\\none queen. Then we arrive at the position shown in Figure 5.4.\\nBecause we have created a new conflict with the queen on row 6 (on f6),\\nour next move must be to move this queen. In fact, we can move it to a\\nsquare where it has zero conflicts. This means the problem has been solved,\\nand there are no remaining conflicts.\\nThis method can be used not only to solve the eight-queens problem but\\nalso has been successfully applied to the n-queens problem for extremely\\nlarge values of n. It has been shown that, using this method, the 1,000,000-\\nqueens problem can be solved in an average of around 50 steps.\\nSolving the 1,000,000-queens problem using traditional search techniques\\nwould be impossible because it would involve searching a tree with a\\nbranching factor of 10\\n12.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 151, 'page_label': '152'}, page_content='5.7 Combinatorial Optimization Problems 125\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nabcdef gh\\nFigure 5.4\\nAlmost a solution to the\\neight-queens problem;\\nposition after applying\\nmin-conflicts heuristic\\nonce to the position shown\\nin Figure 5.3\\n5.7 Combinatorial Optimization Problems\\nLocal search uses a range of techniques to solve large combinatorial opti-\\nmization problems . A combinatorial optimization problem is simply a\\nproblem that can be expressed in terms of finding the best possible set of\\nvalues for a group of variables.\\nAn example of a combinatorial optimization problem is the eight-queens\\nproblem presented in Chapter 4. The variables in this case can be consid-\\nered to be the eight queens, which can take on values that represent the\\nsquares on the board. The constraints of the problem make it harder than\\nsimply picking any eight values for the variables, and hence, as we have\\nseen, it is useful to find ways to restrict the number of choices that are avail-\\nable for each queen to avoid the problem of combinatorial explosion.\\nReal-world combinatorial optimization problems include allocating teach-\\ners to classrooms, scheduling machines and workers in factories, and select-\\ning the best routes for buses, taxis, and other vehicles. The traveling\\nsalesman problem is another such problem.\\nA relaxed optimization problem is a version of a problem where there are\\nmore possible solutions (the feasible region is larger), or where there are\\nfewer constraints applied to the possible values that the variables can take.\\nFor example, a relaxed (and trivial) version of the eight-queens problem'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 152, 'page_label': '153'}, page_content='126 CHAPTER 5 Advanced Search\\nmight be that the eight queens must be placed on the board so that no two\\nqueens are on the same row or column. As we see in Section 5.2, finding\\nsolutions to relaxed problems can help to develop heuristics for more com-\\nplex problems.\\n5.8 Local Search and Metaheuristics\\nLocal search methods work by starting from some initial configuration\\n(usually random) and making small changes to the configuration until a\\nstate is reached from which no better state can be achieved. Hill climbing is\\na good example of a local search technique. Local search techniques, used in\\nthis way, suffer from the same problems as hill climbing and, in particular,\\nare prone to finding local maxima that are not the best solution possible.\\nThe methods used by local search techniques are known as metaheuristics.\\nExamples of metaheuristics include simulated annealing (see Section 5.9),\\ntabu search (see Section 5.8.3), genetic algorithms (see Chapter 14), ant colony\\noptimization (see Section 5.8.4), and neural networks (see Chapter 11).\\nThis kind of search method is also known as local optimization because it\\nis attempting to optimize a set of values but will often find local maxima\\nrather than a global maximum.\\nA local search technique applied to the problem of allocating teachers to class-\\nrooms would start from a random position and make small changes until a\\nconfiguration was reached where no inappropriate allocations were made.\\n5.8.1 Exchanging Heuristics\\nThe simplest form of local search is to use an exchanging heuristic.A n\\nexchanging heuristic moves from one state to another by exchanging one or\\nmore variables by giving them different values. We saw this in solving the\\neight-queens problem as heuristic repair. Ak-exchange is considered to be a\\nmethod wherek variables have their values changed at each step. The heuris-\\ntic repair method we applied to the eight-queens problem was 2-exchange.\\nA k-exchange can be used to solve the traveling salesman problem. A tour\\n(a route through the cities that visits each city once, and returns to the\\nstart) is generated at random. Then, if we use 2-exchange, we remove two\\nedges from the tour and substitute them for two other edges. If this pro-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 153, 'page_label': '154'}, page_content='5.8 Local Search and Metaheuristics 127\\nduces a valid tour that is shorter than the previous one, we move on from\\nhere. Otherwise, we go back to the previous tour and try a different set of\\nsubstitutions.\\nIn fact, using k = 2 does not work well for the traveling salesman problem,\\nwhereas using k = 3 produces good results. Using larger numbers of k will\\ngive better and better results but will also require more and more iterations.\\nUsing k = 3 gives reasonable results and can be implemented efficiently. It\\ndoes, of course, risk finding local maxima, as is often the case with local\\nsearch methods.\\n5.8.2 Iterated Local Search\\nIterated local search techniques attempt to overcome the problem of local\\nmaxima by running the optimization procedure repeatedly, from different\\ninitial states. If used with sufficient iterations, this kind of method will\\nalmost always find a global maximum.\\nThe aim, of course, in running methods like this is to provide a very good\\nsolution without needing to exhaustively search the entire problem space.\\nIn problems such as the traveling salesman problem, where the search\\nspace grows extremely quickly as the number of cities increases, results\\ncan be generated that are good enough (i.e., a local maximum) without\\nusing many iterations, where a perfect solution would be impossible to\\nfind (or at least it would be impossible to guarantee a perfect solution—\\neven one iteration of local search may happen upon the global maximum,\\nof course!).\\n5.8.3 Tabu Search\\nT abu search is a metaheuristic that uses a list of states that have already\\nbeen visited to attempt to avoid repeating paths. The tabu search meta-\\nheuristic is used in combination with another heuristic and operates on the\\nprinciple that it is worth going down a path that appears to be poor if it\\navoids following a path that has already been visited. In this way, tabu\\nsearch is able to avoid local maxima.\\nT o quote from the www.tabusearch.net website: “a bad strategic choice can\\nyield more information than a good random choice. ”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 154, 'page_label': '155'}, page_content='128 CHAPTER 5 Advanced Search\\n5.8.4 Ant Colony Optimization\\nForaging ants leave a trail of pheromones so that they can lead other ants to\\nfind the food that they have found. The trail of pheromones is renewed reg-\\nularly, so that if another ant finds a better route, the pheromones along the\\nold route will gradually fade, and the new, superior route will become the\\nmost popular choice.\\nThe ant colony optimization (ACO) metaheuristic is based on this behav-\\nior. For example, when attempting to solve the traveling salesman problem,\\na set of “artificial ants” is sent out along the routes, leaving trails of\\n“pheromones” that indicate how short the route they have taken is.\\nPheromones gradually fade, meaning that ants that follow later will take\\nthe route whose pheromones have been most recently updated, while\\nattempting to follow the pheromones that indicate the shortest path. ACO\\nhas been successfully used to enable engineers to find the best way to route\\ncables through a communications network. Because the “ants” are continu-\\nally foraging through the network, this method is able to cope extremely\\nwell with changes in the environment, such as blockages and new routes.\\nWe will learn more about Artificial Intelligence methods based on biologi-\\ncal systems (artificial life) in Chapter 13.\\n5.9 Simulated Annealing\\nAnnealing is a process of producing very strong glass or metal, which\\ninvolves heating the material to a very high temperature and then allowing\\nit to cool very slowly. In this way, the atoms are able to form the most stable\\nstructures, giving the material great strength.\\nSimulated annealing is a local search metaheuristic based on this method\\nand is an extension of a process calledmetropolis Monte Carlo simulation.\\nSimulated annealing is applied to a multi-value combinatorial problem\\nwhere values need to be chosen for many variables to produce a particular\\nvalue for some global function, dependent on all the variables in the sys-\\ntem. This value is thought of as the energy of the system, and in general the\\naim of simulated annealing is to find a minimum energy for a system.\\nSimple Monte Carlo simulationis a method of learning information (such\\nas shape) about the shape of a search space. The process involves randomly\\nselecting points within the search space. An example of its use is as follows:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 155, 'page_label': '156'}, page_content='5.9 Simulated Annealing 129\\nA square is partially contained within a circle. Simple Monte Carlo simula-\\ntion can be used to identify what proportion of the square is within the cir-\\ncle and what proportion is outside the circle. This is done by randomly\\nsampling points within the square and checking which ones are within the\\ncircle and which are not.\\nMetropolis Monte Carlo simulation extends this simple method as follows:\\nRather than selecting new states from the search space at random, a new\\nstate is chosen by making a small change to the current state. If the new\\nstate means that the system as a whole has a lower energy than it did in the\\nprevious state, then it is accepted. If the energy is higher than for the previ-\\nous state, then a probability is applied to determine whether the new state\\nis accepted or not. This probability is called a Boltzmann acceptance crite-\\nrion and is calculated as follows:\\ne\\n(/H11002dE/T)\\nwhere T is the current temperature of the system, and dE is the increase in\\nenergy that has been produced by moving from the previous state to the\\nnew state. The temperature in this context refers to the percentage of steps\\nthat can be taken that lead to a rise in energy: At a higher temperature, more\\nsteps will be accepted that lead to a rise in energy than at low temperature.\\nT o determine whether to move to a higher energy state or not, the proba-\\nbility e\\n(/H11002dE/T) is calculated, and a random number is generated between 0\\nand 1. If this random number is lower than the probability function, the\\nnew state is accepted. In cases where the increase in energy is very high, or\\nthe temperature is very low, this means that very few states will be accepted\\nthat involve an increase in energy, as e\\n(/H11002dE/T) approaches zero.\\nThe fact that some steps are allowed that increase the energy of the system\\nenables the process to escape from local minima, which means that simu-\\nlated annealing often can be an extremely powerful method for solving\\ncomplex problems with many local maxima.\\nNote: Some systems use e\\n(/H11002dE/kT) as the probability that the search will\\nprogress to a state with a higher energy, where k is Boltzmann’s constant\\n(Boltzmann’s constant is approximately 1.3807 /H1100310/H1100223 Joules per Kelvin).\\nSimulated annealing uses Monte Carlo simulation to identify the most stable\\nstate (the state with the lowest energy) for a system. This is done by running'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 156, 'page_label': '157'}, page_content='130 CHAPTER 5 Advanced Search\\n*VLSI — V ery Large-Scale Integration—a method used to get very large numbers of gates\\nonto silicon chips.\\nsuccessive iterations of metropolis Monte Carlo simulation, using progres-\\nsively lower temperatures. Hence, in successive iterations, fewer and fewer\\nsteps are allowed that lead to an overall increase in energy for the system.\\nA cooling schedule (or annealing schedule) is applied, which determines\\nthe manner in which the temperature will be lowered for successive itera-\\ntions. Two popular cooling schedules are as follows:\\nT\\nnew = Told /H11002dT\\nTnew = C /H11003Told (where C < 1.0)\\nThe cooling schedule is extremely important, as is the choice of the number\\nof steps of metropolis Monte Carlo simulation that are applied in each iter-\\nation. These help to determine whether the system will be trapped by local\\nminima (known as quenching). The number of times the metropolis\\nMonte Carlo simulation is applied per iteration is for later iterations.\\nAlso important in determining the success of simulated annealing are the\\nchoice of the initial temperature of the system and the amount by which\\nthe temperature is decreased for each iteration. These values need to be\\nchosen carefully according to the nature of the problem being solved.\\nWhen the temperature, T, has reached zero, the system is frozen, and if the\\nsimulated annealing process has been successful, it will have identified a\\nminimum for the total energy of the system.\\nSimulated annealing has a number of practical applications in solving prob-\\nlems with large numbers of interdependent variables, such as circuit design.\\nIt has also been successfully applied to the traveling salesman problem.\\n5.9.1 Uses of Simulated Annealing\\nSimulated annealing was invented in 1983 by Kirkpatrick, Gelatt, and V ec-\\nchi. It was first used for placing VLSI\\n* components on a circuit board.\\nSimulated annealing has also been used to solve the traveling salesman\\nproblem, although this approach has proved to be less efficient than using\\nheuristic methods that know more about the problem. It has been used\\nmuch more successfully in scheduling problems and other large combina-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 157, 'page_label': '158'}, page_content='5.11 Real-Time A* 131\\ntorial problems where values need to be assigned to a large number of vari-\\nables to maximize (or minimize) some function of those variables.\\n5.10 Genetic Algorithms for Search\\nGenetic algorithms are discussed in much more detail in Chapter 14. This\\nsection provides a brief overview of the ways in which genetic algorithms\\ncan be used to solve search problems but does not assume any detailed\\nunderstanding of the mechanics of genetic algorithms.\\nGenetic algorithms involve finding solutions to complex problems using a\\nmethod based on the process of evolution that we see in nature. In much\\nthe same way as nature evolves creatures that are best designed to suit their\\nenvironments by selecting features that work (survival of the fittest),\\ngenetic algorithms work by combining potential solutions to a problem\\ntogether in a way that tends to produce better solutions over successive\\ngenerations. This is a form of local optimization, but where mutation and\\ncrossover are used to try to avoid local maxima.\\nAs is explained in Chapter 14, genetic algorithms are usually used to iden-\\ntify optimal solutions to complex problems. This can clearly be easily\\nmapped to search methods, which are aiming toward a similar goal.\\nGenetic algorithms can thus be used to search for solutions to multi-value\\nproblems where the closeness of any attempted solution to the actual solu-\\ntion (fitness) can be readily evaluated.\\nIn short, a population of possible solutions ( chromosomes) is generated,\\nand a fitness value for each chromosome is determined. This fitness is used\\nto determine the likelihood that a given chromosome will survive to the\\nnext generation, or reproduce. Reproduction is done by applying cross-\\nover to two (or more) chromosomes, whereby features ( genes) of each\\nchromosome are combined together. Mutation is also applied, which\\ninvolves making random changes to particular genes.\\n5.11 Real-Time A*\\nReal-time A* is a variation of A*, as presented in Chapter 4. Search contin-\\nues on the basis of choosing paths that have minimum values of f(node) =\\ng(node) + h(node). However,g(node) is the distance of the node from the\\ncurrent node, rather than from the root node. Hence, the algorithm will'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 158, 'page_label': '159'}, page_content='132 CHAPTER 5 Advanced Search\\nbacktrack if the cost of doing so plus the estimated cost of solving the prob-\\nlem from the new node is less than the estimated cost of solving the prob-\\nlem from the current node.\\nImplementing real-time A* means maintaining a hash table of previously\\nvisited states with their h(node) values.\\n5.12 Iterative-Deepening A* (IDA*)\\nBy combining iterative-deepening with A*, we produce an algorithm that is\\noptimal and complete (like A*) and that has the low memory requirements\\nof depth-first search.\\nIDA* is a form of iterative-deepening search where successive iterations\\nimpose a greater limit on f(node) rather than on the depth of a node.\\nIDA* performs well in problems where the heuristic value f (node) has rel-\\natively few possible values. For example, using the Manhattan distance as a\\nheuristic in solving the eight-queens problem, the value of f (node) can\\nonly have values 1, 2, 3, or 4. In this case, the IDA* algorithm only needs to\\nrun through a maximum of four iterations, and it has a time complexity\\nnot dissimilar from that of A*, but with a significantly improved space\\ncomplexity because it is effectively running depth-first search.\\nIn cases such as the traveling salesman problem where the value off (node)\\nis different for every state, the IDA* method has to expand 1 + 2 + 3 + . . .\\n+ n nodes = O(n\\n2) where A* would expand n nodes.\\n5.13 Parallel Search\\nMany of the search methods that have been described in this book were\\ndeveloped in the 1960s, 1970s, and 1980s, when computers lacked the\\npower, memory, and storage space that they have today. Many of the issues\\nthat were thus of concern when those algorithms were developed are no\\nlonger important.\\nNowadays, computers have far more processing power and storage space and\\nso are able to run algorithms, such as search, a great deal faster. As we see in\\nChapter 6, this has helped to lead to a great improvement in the ability of\\nchess-playing computer programs. Another aspect of chess-playing com-\\nputer programs is that they tend to run parallel search. The names of many of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 159, 'page_label': '160'}, page_content='5.13 Parallel Search 133\\nthe best chess computers include the worddeep: Deep Thought, Deep Blue,\\nDeep Junior, and Deep Fritz, for example. The worddeep means parallel.\\nThe idea of parallel processing is that if a task can be broken down into a\\nnumber of sub-tasks, where those sub-tasks do not need to be run sequen-\\ntially, then they can be run in parallel, simultaneously on separate processors.\\nAs with much of Artificial Intelligence, there is a good basis for this idea:\\nthe human brain. The human brain is massively parallel , which means\\nthat it is able to do millions of things simultaneously. Computers are much\\nfaster at raw processing than a human brain, but because the brain is able to\\ndo so many things simultaneously, it is able to operate at a much faster rate\\nthan a computer.\\nApplying this idea to search is clearly desirable because many search prob-\\nlems (such as playing chess) can be heavily time dependent.\\nOne search method that can be simply parallelized is depth-first search. If\\nwe assume that we have two processors, we could simply divide the descen-\\ndants of the root node in half and assign half of them to one processor and\\nhalf to the other. The two processors would then run a series of depth-first\\nsearches on each of its nodes. The first processor to find a goal node would\\nreport success, and the whole computation would stop.\\nMore complex search methods such as alpha–beta pruning , which is\\ndescribed in Chapter 6, are not so easy to implement in parallel. Alpha–beta\\npruning is a method that is used to eliminate portions of the search tree for\\nplaying games such as chess that can provide a great increase in perform-\\nance. It has been shown that running alpha–beta pruning searches in paral-\\nlel by simply dividing the search tree up between processors actually\\nprovides worse results than running it in serial (Fox et al. 1994).\\nT o develop a parallel version of an algorithm such as alpha–beta pruning,\\nmore care needs to be taken in how the tasks are split up so that perform-\\nance is not degraded.\\nOne area where parallel search can be readily applied is in solving con-\\nstraint satisfaction problems. In general, CSPs are not well solved by using\\nbrute-force search because this involves a combinatorial optimization\\nproblem. In situations where the search tree can be reduced somewhat, and\\nno better method can be found than blind search, the performance of the\\nsearch can be significantly improved by running it in a parallel fashion, by'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 160, 'page_label': '161'}, page_content='134 CHAPTER 5 Advanced Search\\nsimply dividing the search tree between processors. In some cases, search\\nproblems can be divided between individual computers.\\nOf course, problems that can be solved using goal reduction are also often\\nsolved more efficiently using parallel search because the goal tree can be\\nbroken down into sub-goal trees, which can be worked on in parallel by\\nseparate processors.\\nWhen distributing work in this way, important concepts to consider are\\ntask distribution (deciding which task to give to which processor), load\\nbalancing (ensuring that all processors have enough work to do and that\\nno single processor is overworked), and tree ordering (determining the\\ncorrect order to process the search tree).\\n5.13.1 Task Distribution\\nCook (1998) explores the process of implementing a parallel version of\\nIDA* search.\\nOne approach to distributing tasks for parallel implementations of IDA*\\nwas to use parallel window search (PWS). This involves searching the dif-\\nferent depth-limited searches concurrently, rather than in series. For exam-\\nple, using three processors, the first processor might search with a depth\\nlimit of 1, the second with a depth limit of 2, and the third with a depth\\nlimit of 3. As soon as any processor completes a search, it is assigned a new\\nsearch with a depth that is deeper than any currently running. Unfortu-\\nnately, if there are too many processors (more than the number of itera-\\ntions needed to find an optimal solution) the PWS method can be very\\ninefficient because many processors will be idle for the entire search.\\nAnother approach used by Cook was distributed tree search (DTS). First,\\na breadth-first search is carried out until there are as many leaf nodes avail-\\nable as there are processors. Then, each of these nodes is assigned to a\\nprocessor for search. T o ensure that this method is optimal, when a proces-\\nsor finishes an iteration, it must wait for all other processors to finish their\\niterations before starting another. This means that there will often be idle\\nprocessors.\\nCook’s paper provides a very detailed analysis of both of these methods and\\ntheir respective advantages and disadvantages.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 161, 'page_label': '162'}, page_content='5.13 Parallel Search 135\\nIn their paper Randomized Parallel Algorithms for Backtrack Search and\\nBranch-and-Bound Computation, Richard Karp and Y anjun Zhang showed\\nthat distributing tasks between processors at random gives better results,\\nparticularly when running a variation of depth-first search called back-\\ntracking search . In backtracking search, when a node is discovered, the\\nprocessor passes one of the children of that node to an idle processor, if one\\nis available. The normal method of determining to which processor to pass\\nthis child node is fairly complex and can create a significant overhead. By\\npassing the child nodes randomly, this overhead can be eliminated, and the\\nsearch becomes much more efficient.\\n5.13.2 Tree Ordering\\nWhen running IDA* in parallel, the order of the tree can be very impor-\\ntant. Since the search tree is expanded in depth-first order from left to\\nright, if the optimal solution is on the left side of the tree, it will be found\\nmuch more quickly than if it is on the right side. Clearly, if a way could be\\nfound to ensure that the optimal solution was always on the left side of the\\ntree, then a search method would not be needed to find it. However, heuris-\\ntics can be used to attempt to examine the tree in a way that will increase\\nthe likelihood of finding an optimal solution quickly. These heuristics\\noperate in much the same way that the heuristics for best-first search and\\nother serial informed search methods use.\\n5.13.3 Search Engines\\nSearch engines are an excellent example of parallel search systems. One\\nproblem faced by search engines is the enormous size of the Internet (esti-\\nmated to be many billions of pages and growing continually). T o index a\\nreasonable percentage of these pages, search engines need to run in parallel.\\nTypically, search engines run their indexing on a number of indexing\\nservers. Pages or websites are distributed among the servers by a scheduling\\nprocess. Clearly, as well as getting the schedule right, it is important that the\\nsearch engines are able to communicate with each other. For example, if\\ntwo search engines both come across the same page, they need to be able to\\ndecide which one will search that page. Instead of crawling independently\\nlike this, some search engine spiders simply have a list of links that they\\nneed to crawl. When a spider finds a page, it indexes the page and extracts'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 162, 'page_label': '163'}, page_content='136 CHAPTER 5 Advanced Search\\nall the links from it. The spider places these links into a central database\\nand carries on with its own list of links. A central scheduling system then\\ndecides how to distribute the links in the central database to the servers. In\\nthis way, no two servers will ever duplicate work.\\n5.14 Bidirectional Search\\nBidirectional search (also known as wave search , due to the wave-like\\nnature in which paths are followed through the search space) is applied\\nwhen searching for the best path between a start node and a known goal\\nnode. This is somewhat different from most of the other search algorithms\\ndiscussed in this part, where the goal node is not known, and the purpose\\nof the algorithm is to find a path to a goal node without knowing where the\\ngoal node will be located in the tree.\\nBidirectional search involves simultaneously spreading out paths in a\\nbreadth-first fashion from both the start and goal nodes.\\nThis requires that a predecessor function be available for each node, as well\\nas the successor function, so that paths can be extended backward from the\\ngoal node.\\nAs soon as the two paths meet, a complete path has been generated that\\nbegins at the start node, goes through the point where the two paths met,\\nand ends at the goal node. This path is guaranteed to be the shortest path\\n(or rather, the path involving the fewest steps).\\n5.15 Nondeterministic Search\\nNondeterministic search is a combination of depth-first and breadth-first\\nsearch, which avoids the problems of both but does not necessarily have the\\nadvantages of either.\\nWhen running a nondeterministic search, new paths are added to the\\nqueue at random positions. In the following pseudo-code implementation,\\nthe function call \\nadd_randomly_to_queue (successors (state)) adds the suc-\\ncessors of state to random positions in the queue:\\nFunction random ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 163, 'page_label': '164'}, page_content='5.17 Nonchronological Backtracking 137\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse add_randomly_to_queue (successors (state));\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nThis method is useful in cases where very little information is available about\\nthe search space—for example, in a situation where there may be extremely\\nlong, or even infinite, paths and may also be an extremely large branching\\nfactor. In situations like that, depth-first search might end up stuck down an\\ninfinitely long path, and breadth-first search could be extremely inefficient\\nin dealing with the large branching factor. A nondeterministic search will\\navoid these problems but will not necessarily find the best path.\\nNondeterministic search can also be used in combination with other search\\ntechniques. For example, by applying a nondeterministic search when a\\nmaximum is found in hill climbing, the problems of local maxima (the\\nfoothill problem) can be avoided.\\n5.16 Island-Driven Search\\nIsland-driven search assumes that an island exists roughly half way\\nbetween the root node and a goal node. The method involves finding a path\\nbetween the root node and the island, and a path between the island and a\\ngoal node. If no path exists that goes through the island, the method reverts\\nto another search method that ignores the island.\\nThis method is useful in situations where it is extremely likely that the\\nisland actually does lie on a path to the goal, for example, if we are trying to\\nidentify a route between Milan and Naples, given the knowledge that all\\nroads lead to Rome.\\n5.17 Nonchronological Backtracking\\nNonchronological backtracking, or dependency-directed backtracking, is\\nan alternative to chronological backtracking, which we saw being used in\\nsearch methods such as depth-first search.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 164, 'page_label': '165'}, page_content='138 CHAPTER 5 Advanced Search\\nChronological backtracking operates as follows:\\nWhen a dead end in a tree is found (in other words, a leaf node that is not a\\ngoal node), move back up the search tree to the last point in the tree where\\na decision had to be made. Undo this decision, and all its consequences,\\nand choose the next option at this junction instead.\\nIn some cases, additional information is available about the search space\\nthat can help to backtrack in a more efficient manner, undoing decisions\\nthat are more likely to lead to success, rather than just undoing each deci-\\nsion in chronological order. In these cases, we use nonchronological back-\\ntracking, which is also known as dependency-directed backtracking.\\nIt is particularly useful in solving constraint satisfaction problems, where\\nbacktracking can be applied by going back to the previous choice that\\ncaused a constraint to fail.\\n5.18 Chapter Summary\\n■ Constraint satisfaction problems (CSPs) such as the eight-queens\\nproblem, can be solved using search.\\n■ Methods such as forward checking and heuristics such as the most-\\nconstrained variable heuristic and min-conflicts make it possible to\\nsolve extremely large CSPs (such as the 1,000,000-queens problem).\\n■ Large combinatorial optimization problems are best solved using\\nlocal search methods.\\n■ Local search methods (or metaheuristics) move from one potential\\nsolution to another by making small changes. When a local maxi-\\nmum is found, the search stops.\\n■ Iterating the local search from different random starting configu-\\nrations can avoid the problem of identifying local maxima and\\nignoring a global maximum.\\n■ Local search methods include tabu search, ant colony optimiza-\\ntion, and simulated annealing.\\n■ Simulated annealing is based on the way in which metals are hard-\\nened by being heated up and then slowly cooled, so that the crys-\\ntalline structure forms the strongest possible arrangement.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 165, 'page_label': '166'}, page_content='5.19 Review Questions 139\\n■ Variations on A* such as real-time A* and iterative-deepening A*\\ncan provide enhanced performance.\\n■ Parallel search methods can take advantage of modern parallel\\ncomputers. Issues such as task distribution, load balancing, and\\ntree ordering need to be considered.\\n5.19 Review Questions\\n5.1 Explain how search can be used to solve constraint satisfaction\\nproblems, such as the eight-queens problem. What difficulties arise\\nwhen such problems become extremely large (e.g., the 1,000,000-\\nqueens problem)? What kinds of methods can be applied to solve\\nsuch large problems efficiently?\\n5.2 Explain the idea behind the following heuristics:\\n■ most-constrained variable\\n■ most-constraining variable\\n■ least-constraining variable\\n■ min-conflicts\\n5.3 Why is local search more practical than depth-first search for solv-\\ning large combinatorial optimization problems? Explain what a\\nmetaheuristic is and why it is useful.\\n5.4 How does iterated local search avoid the problem of local maxima?\\nWhy is this important?\\n5.5 Explain how ant colony optimization works. Why might it be use-\\nful for communications routing?\\n5.6 Describe in layman’s terms the idea behind simulated annealing and\\nwhy it works. What kinds of problems might it be useful for solving?\\n5.7 Explain the purpose of the temperature variable in simulated\\nannealing. How effective would the method be without it?\\n5.8 Explain why IDA* might be used instead of A*. In what kinds of\\nsituations might it be less useful?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 166, 'page_label': '167'}, page_content='140 CHAPTER 5 Advanced Search\\n5.9 Explain the importance of the following principles when running\\nparallel search methods:\\n■ task distribution\\n■ load balancing\\n■ tree ordering\\n5.10 How do search engines make use of search? Research a few of the\\nbest known search engines, and try to find out what kind of search\\nalgorithms they use. How efficient do you think they are at search-\\ning? Could you implement them better?\\n5.20 Exercises\\n5.1 Write a program in a programming language of your choice for\\nsolving the n-queens problem. Run it with 8 queens, and then try it\\nwith 100 queens. How well does it perform? Could your program\\nfind a solution for 1,000,000 queens? If not, why not? If so, what\\noptimizations have you used to make that possible?\\n5.2 Write a program that can solve arbitrary cryptographic problems.\\nAdd heuristics to your implementation to make it more efficient.\\nWhat limitations does your program have?\\n5.3 Investigate tabu search. Write 1000 words explaining how it works\\nand what sorts of problems it is best suited to solving.\\n5.4 Write a program that uses simulated annealing to solve the traveling\\nsalesman problem of arbitrary size. Do you think that simulated\\nannealing is a good way to solve this problem? Explain your answer.\\n5.5 Implement a nondeterministic search algorithm. Build search trees\\nfor which it performs the following:\\na. better than depth-first search\\nb. worse than depth-first search\\nc. better than breadth-first search\\nd. worse than breadth-first search'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 167, 'page_label': '168'}, page_content='5.21 Further Reading 141\\n5.21 Further Reading\\nMost of the material covered in this chapter is covered well by the majority\\nof Artificial Intelligence textbooks. Material on local search is relatively\\nnew, and not so well covered by the older textbooks.\\nT abu Search by Glover and Laguna, the inventors of tabu search, provides a\\ngood insight into the tabu search metaheuristic.\\nThe min-conflicts heuristic was invented by Gu in 1989. Further informa-\\ntion on the method can be found in Minton (1992).\\nPearl (1984) gives a good overview of search methods with a particular\\nfocus on heuristics.\\nRayward-Smith et al. (1996) gives excellent coverage of heuristics and\\nmetaheuristics in particular.\\nJansen (1997) reports on research that has been done using simulated\\nannealing in information retrieval to select a suitable ordering of results to\\nreturn to a user in response to a keyword text query.\\nMultiobjective Heuristic Search: An Introduction to Intelligent Search Meth-\\nods for Multicriteria Optimization by Pallab Dasgupta, P . P . Chakrabarti,\\nS. C. Desarkar (1999 - Friedrich Vieweg & Sohn)\\nAdaptive Parallel Iterative Deepening Searchby Diane J. Cook and R. Craig Var-\\nnell (1998 – inJournal of Artificial Intelligence Research, Vol. 9, pp. 139–166)\\nParallel Computing Works by G. C. Fox, R. D. Williams, and P . C. Messina\\n(1994 – Morgan Kaufmann)\\nT abu Searchby Fred W. Glover, Manuel Laguna (1998 – Kluwer Academic\\nPublishers)\\nSimulated Annealing for Query Results Ranking by B. J. Jansen (1997 – in\\nACM Computer Science Education Conference)\\nLearning to Solve Problems by Searching for Macro-Operators (Research\\nNotes in Artificial Intelligence, Vol. 5) by Richard E. Korf (1985 – Longman\\nGroup United Kingdom)\\nSearch by Richard E. Korf (1987 – in Encyclopedia of Artificial Intelligence\\nedited by E. Shapiro – Wiley)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 168, 'page_label': '169'}, page_content='142 CHAPTER 5 Advanced Search\\nLearning Search Control Knowledge: An Explanation Based Approach by\\nStephen Minton (1988 – Kluwer Academic Publishers)\\nMinimizing Conflicts: A Heuristic Repair Method for Constraint Satisfaction\\nand Scheduling Problems by S. Minton, M. D. Johnson, A. B. Philips, and P .\\nLaird (1992 – Artificial Intelligence, Vol. 58)\\nHow to Solve It: Modern Heuristics by Zbigniew Michalewicz and David B.\\nFogel (1999 – Springer V erlag)\\nLocal Search for Planning and Scheduling: Ecai 2000 Workshop, Berlin, Ger-\\nmany, August 21, 2000: Revised Papers (Lecture Notes in Computer Science,\\n2148) edited by Alexander Nareyek (2001 – Springer V erlag)\\nCombinatorial Optimization: Algorithms and Complexity by Christos H.\\nPapadimitriou and Kenneth Steiglitz (1998 – Dover Publications)\\nHeuristics: Intelligent Search Strategies for Computer Problem Solving by\\nJudea Pearl (1984 – Addison Wesley)\\nModern Heuristic Search Methods edited by V . J. Rayward-Smith, I. H.\\nOsman, Colin R. Reeves, and G. D. Smith (1996 – John Wiley & Sons)\\nThe Algorithm Design Manual by Steven S. Skiena (1997 – T elos)\\nSimulated Annealing: Theory and Applications by P . J. M. Van Laarhoven\\nand E. H. L. Aarts (1987 - D. Reidel Publishing Company – Out of Print)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 169, 'page_label': '170'}, page_content='6CHAPTER\\nGame Playing\\nAfter the other matches I felt hooked to be part of this competition because I\\nbelieve it is very important for the game of chess and the human race as a\\nwhole. Now I hope to use my experience to help set new standards and also\\nprove that human players are not hopeless.\\n—Garry Kasparov before his six-game chess match against Deep Junior\\nOne hundred years from now, the idea that humans could still beat computers\\nwill seem quaint. It will be like men trying to race cars at the turn of the cen-\\ntury. Who’s better? Who cares? The technology is what matters. It’s improving,\\nand that’s what counts.\\n—Professor Jonathan Schaeffer discussing Kasparov’s chess match with\\nDeep Junior\\n‘The Game’ , said he, ‘is never lost till won. ’\\n—George Crabbe, Gretna Green\\nThe Game’s Afoot.\\n—William Shakespeare, Henry V\\n6.1 Introduction\\nOne of the most interesting and well publicized areas of Artificial Intelli-\\ngence research has been in the playing of games. With the success of Deep\\nBlue in 1997, a landmark was reached: a computer program that could\\ndefeat the best chess player in the world.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 170, 'page_label': '171'}, page_content='144 CHAPTER 6 Game Playing\\nGame-playing systems tend to rely heavily on the search techniques\\ndescribed in Chapters 4 and 5, in combination with a number of heuristics\\nand often a detailed database of knowledge about the game.\\nThis chapter explains the relationship between search and games such as\\nchess, checkers, and backgammon. It explains the concepts of alpha–beta\\npruning and Minimax. It uses Chinook, a checkers-playing computer sys-\\ntem, to explain some of the more advanced techniques used in modern\\ngame-playing computers and discusses why computers are currently\\nunable to beat humans at games such as Go.\\n6.2 Game Trees\\nMany two-player games can be efficiently represented using trees, called\\ngame trees. A game tree is an instance of a tree in which the root node rep-\\nresents the state before any moves have been made, the nodes in the tree\\nrepresent possible states of the game (or positions), and arcs in the tree\\nrepresent moves.\\nIt is usual to represent the two players’ moves on alternate levels of the\\ngame tree, so that all edges leading from the root node to the first level rep-\\nresent possible moves for the first player, and edges from the first level to\\nthe second represent moves for the second player, and so on.\\nLeaf nodes in the tree represent final states, where the game has been won,\\nlost, or drawn. In simple games, a goal node might represent a state in\\nwhich the computer has won, but for more complex games such as chess\\nand Go, the concept of a goal state is rarely of use.\\nOne approach to playing a game might be for the computer to use a tree\\nsearch algorithm such as depth-first or breadth-first search, looking for a\\ngoal state (i.e., a final state of the game where the computer has won).\\nUnfortunately, this approach does not work because there is another intel-\\nligence involved in the game. We will consider this to be a rational,\\ninformed opponent who plays to win. Whether this opponent is human or\\nanother computer does not matter—or should not matter—but for the\\npurposes of this section of the book, we will refer to the opponent as being\\nhuman, to differentiate him or her from the computer.\\nConsider the game tree shown in Figure 6.1. This partial tree represents the\\ngame of tic-tac-toe, in which the computer is playing noughts, and the\\nhuman opponent is playing crosses. The branching factor of the root node'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 171, 'page_label': '172'}, page_content='6.2 Game Trees 145\\no o o\\no o\\no\\nooo\\nox\\noox o\\no\\nx\\no\\no\\nxx\\no\\noo\\nxx o\\no\\no\\nxx\\no\\no\\nx\\nx\\no\\no\\nx\\nx\\no\\no\\nx\\nox o\\nx\\nFigure 6.1\\nA partial game tree for the game tic-tac-toe\\nis 9 because there are nine squares in which the computer can place its first\\nnought. The branching factor of the next level of the tree is 8, then 7 for the\\nnext level, and so on. The tree shown in Figure 6.1 is clearly just a part of\\nthat tree and has been pruned to enable it to fit comfortably on the page.\\nFor a computer to use this tree to make decisions about moves in a game of\\ntic-tac-toe, it needs to use an evaluation function , which enables it to\\ndecide whether a given position in the game is good or bad. If we use\\nexhaustive search, then we only need a function that can recognize a win, a\\nloss, and a draw. Then, the computer can treat “win” states as goal nodes\\nand carry out search in the normal way.\\n6.2.1 Rationality, Zero Sum, and Other Assumptions\\nAll the methods discussed in this chapter are designed for games with two\\nplayers. In most of the games, there is no element of chance (in other\\nwords, no dice are thrown, or cards drawn), and the players have complete'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 172, 'page_label': '173'}, page_content='146 CHAPTER 6 Game Playing\\nknowledge of the state of the game, which means that the players do not\\nconceal information (apart from their strategies and plans) from their\\nopponents. This sets games such as chess and Go aside from games such as\\npoker, in which there is an element of chance, and it is also important that\\nplayers conceal information from each other.\\nMost of the games we will consider in this chapter are zero-sum games,\\nwhich means that if the overall score at the end of a game for each player\\ncan be 1 (a win), 0 (a draw), or /H110021 (a loss), then the total score for both\\nplayers for any game must always be 0. In other words, if one player wins,\\nthe other must lose. The only other alternative is that both players draw.\\nFor this reason, we consider the search techniques that are discussed here to\\nbe adversarial methods because each player is not only trying to win but to\\ncause the opponent to lose. In the algorithms such as Minimax and\\nalpha–beta that are discussed later, it is important that the computer can\\nassume that the opponent is rational and adversarial. In other words, the\\ncomputer needs to assume that the opponent will play to win.\\nIn discussing game trees, we use the concept of ply, which refers to the\\ndepth of the tree. In particular, we refer to the ply of lookahead. When a\\ncomputer evaluates a game tree to ply 5, it is examining the tree to a depth\\nof 5. The 4th ply in a game tree is the level at depth 4 below the root node.\\nBecause the games we are talking about involve two players, sequential plies\\nin the tree will alternately represent the two players. Hence, a game tree\\nwith a ply of 8 will represent a total of eight choices in the game, which cor-\\nresponds to four moves for each player. It is usual to use the word ply to\\nrepresent a single level of choice in the game tree, but for the word move to\\nrepresent two such choices—one for each player.\\n6.2.2 Evaluation Functions\\nEvaluation functions (also known as static evaluators because they are\\nused to evaluate a game from just one static position) are vital to most\\ngame-playing computer programs. This is because it is almost never possi-\\nble to search the game tree fully due to its size. Hence, a search will rarely\\nreach a leaf node in the tree at which the game is either won, lost, or drawn,\\nwhich means that the software needs to be able to cut off search and evalu-\\nate the position of the board at that node. Hence, an evaluation function is\\nused to examine a particular position of the board and estimate how well\\nthe computer is doing, or how likely it is to win from this position. Due to'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 173, 'page_label': '174'}, page_content='6.2 Game Trees 147\\nthe enormous number of positions that must be evaluated in game playing,\\nthe evaluation function usually needs to be extremely efficient, to avoid\\nslowing down game play.\\nOne question is how the evaluation function will compare two positions. In\\nother words, given positions A and B, what relative values will it give those\\npositions? If A is a clearly better position than B, perhaps A should receive a\\nmuch higher score than B. In general, as we will see elsewhere, to be suc-\\ncessful, the evaluation function does not need to give values that linearly\\nrepresent the quality of positions: T o be effective, it just needs to give a\\nhigher score to a better position.\\nAn evaluation function for a chess game might look at the number of\\npieces, taking into account the relative values of particular pieces, and\\nmight also look at pawn development, control over the center of the board,\\nattack strength, and so on. Such evaluation functions can be extremely\\ncomplex and, as we will see, are essential to building successful chess-play-\\ning software.\\nEvaluation functions are usually weighted linear functions, meaning that a\\nnumber of different scores are determined for a given position and simply\\nadded together in a weighted fashion. So, a very simplistic evaluation func-\\ntion for chess might count the number of queens, the number of pawns,\\nthe number of bishops, and so on, and add them up using weights to indi-\\ncate the relative values of those pieces:\\nq = number of queens\\nr = number of rooks\\nn = number of knights\\nb = number of bishops\\np = number of pawns\\nscore = 9q + 5r + 3b + 3n + p\\nIf two computer programs were to compete with each other at a game such as\\ncheckers, and the two programs had equivalent processing capabilities and\\nspeeds, and used the same algorithms for examining the search tree, then the\\ngame would be decided by the quality of the programs’ evaluation functions.\\nIn general, the evaluation functions for game-playing programs do not\\nneed to be perfect but need to give a good way of comparing two positions\\nto determine which is the better. Of course, in games as complex as chess'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 174, 'page_label': '175'}, page_content='148 CHAPTER 6 Game Playing\\nand Go, this is not an easy question: two grandmasters will sometimes dif-\\nfer on the evaluation of a position.\\nAs we will see, one way to develop an accurate evaluation function is to\\nactually play games from each position and see who wins. If the play is per-\\nfect on both sides, then this will give a good indication of what the evalua-\\ntion of the starting position should be.\\nThis method has been used successfully for games such as checkers, but for\\ngames such as chess and Go, the number of possible positions is so huge\\nthat evaluating even a small proportion of them is not feasible. Hence, it is\\nnecessary to develop an evaluation function that is dynamic and is able to\\naccurately evaluate positions it has never seen before.\\n6.2.3 Searching Game Trees\\nEven for the game of tic-tac-toe, a part of whose game tree is illustrated in\\nFigure 6.1, it can be inefficient for the computer to exhaustively search the\\ntree because it has a maximum depth of 9 and a maximum branching fac-\\ntor of 9, meaning there are approximately 9 /H110038 /H110037 /H11003... /H110032 /H110031 nodes\\nin the tree, which means more than 350,000 nodes to examine. Actually,\\nthis is a very small game tree compared with the trees used in games like\\nchess or Go, where there are many more possible moves at each step and\\nthe tree can potentially have infinite depth.\\nIn fact, using exhaustive search on game trees is almost never a good idea\\nfor games with any degree of complexity. Typically, the tree will have very\\nhigh branching factors (e.g., a game tree representing chess has an average\\nbranching factor of 38) and often will be very deep. Exhaustively searching\\nsuch trees is just not possible using current computer technology, and so in\\nthis chapter, we will explore methods that are used to prune the game tree\\nand heuristics that are used to evaluate positions.\\nThere is another problem with using exhaustive search to find goal nodes\\nin the game tree. When the computer has identified a goal state, it has sim-\\nply identified that it can win the game, but this might not be the case\\nbecause the opponent will be doing everything he or she can to stop the\\ncomputer from winning. In other words, the computer can choose one arc\\nin the game tree, but the opponent will choose the next one. It may be that\\ndepth-first search reveals a path to a leaf node where the computer wins,\\nbut the computer must also assume that the opponent will be attempting\\nto choose a different path, where the computer loses.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 175, 'page_label': '176'}, page_content='6.3 Minimax 149\\nSo, as we see later in this chapter, the computer can use methods like depth-\\nfirst or breadth-first search to identify the game tree, but more sophisti-\\ncated methods need to be used to choose the correct moves.\\n6.3 Minimax\\nWhen evaluating game trees, it is usual to assume that the computer is\\nattempting to maximize some score that the opponent is trying to minimize.\\nNormally we would consider this score to be the result of the evaluation\\nfunction for a given position, so we would usually have a high positive score\\nmean a good position for the computer, a score of 0 mean a neutral position,\\nand a high negative score mean a good position for the opponent.\\nThe Minimax algorithm is used to choose good moves. It is assumed that a\\nsuitable static evaluation function is available, which is able to give an over-\\nall score to a given position. In applying Minimax, the static evaluator will\\nonly be used on leaf nodes, and the values of the leaf nodes will be filtered\\nup through the tree, to pick out the best path that the computer can achieve.\\nThis is done by assuming that the opponent will play rationally and will\\nalways play the move that is best for him or her, and thus worst for the\\ncomputer. The principle behind Minimax is that a path through the tree is\\nchosen by assuming that at its turn (a max node), the computer will choose\\nthe move that will give the highest eventual static evaluation, and that at\\nthe human opponent’s turn (a min node), he or she will choose the move\\nthat will give the lowest static evaluation. So the computer’s aim is to max-\\nimize the lowest possible score that can be achieved.\\nFigure 6.2 shows how Minimax works on a very simple game tree. Note that\\nthe best result that max can achieve is a score of 6. If max chooses the left\\nbranch as its first choice, then min will inevitably choose the right branch,\\nwhich leaves max a choice of 1 or 3. In this case, max will choose a score of\\n3. If max starts by choosing the right branch, min will have a choice between\\na path that leads to a score of 7 or a path that leads to a score of 6. It will\\ntherefore choose the left branch, leaving max a choice between 2 and 6.\\nFigure 6.2 shows how Minimax can use depth-first search to traverse the\\ngame tree. The arrows start from the root node at the top and go down to\\nthe bottom of the left branch.\\nThis leads to a max node, which will get a score of 5. The value 5 is there-\\nfore passed up to the parent of this max node. Following the right path\\nfrom this min node leads to another max node, this time getting a score of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 176, 'page_label': '177'}, page_content='150 CHAPTER 6 Game Playing\\n6\\n6\\n6\\n6\\n6\\n62 0\\n0\\n7\\n73\\n3\\n3\\n3125\\n5\\n5\\nMIN\\nMAX\\nMAXSTART\\nFigure 6.2\\nIllustrating how minimax works on a very simple game tree. The arrows show the order in which the nodes are examined by\\nthe algorithm, and the values that are passed through the tree.\\n3. This comes back up to the min node, which now chooses the minimum\\nof 3 and 5, and selects 3. Eventually, having traversed the whole tree, the\\nbest result for max comes back up to the root node: 6.\\nThe Minimax function provides a best available score for a given node\\nas follows:\\nFunction minimax (current_node)\\n{\\nif is_leaf (current_node)\\nthen return static_evaluation (current_node);\\nif is_min_node (current_node)\\nthen return min (minimax (children_of \\n(current_node)));\\nif is_max_node (current_node)\\nthen return max (minimax (children_of \\n(current_node)));\\n// this point will never be reached since\\n// every node must be a leaf node, a min node or a\\n// max node.\\n}'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 177, 'page_label': '178'}, page_content='6.3 Minimax 151\\nThis is a recursive function because to evaluate the scores for the children\\nof the current node, the Minimax algorithm must be applied recursively to\\nthose children until a leaf node is reached.\\nMinimax can also be performed nonrecursively, starting at the leaf nodes\\nand working systematically up the tree, in a reverse breadth-first search.\\n6.3.1 Bounded Lookahead\\nMinimax, as we have defined it, is a very simple algorithm and is unsuitable\\nfor use in many games, such as chess or Go, where the game tree is\\nextremely large. The problem is that in order to run Minimax, the entire\\ngame tree must be examined, and for games such as chess, this is not possi-\\nble due to the potential depth of the tree and the large branching factor.\\nIn such cases, bounded lookaheadis very commonly used and can be com-\\nbined with Minimax. The idea of bounded lookahead is that the search tree is\\nonly examined to a particular depth. All nodes at this depth are considered to\\nbe leaf nodes and are evaluated using a static evaluation function. This corre-\\nsponds well to the way in which a human plays chess. Even the greatest grand-\\nmasters are not able to look forward to see every possible move that will occur\\nin a game. Chess players look forward a few moves, and good chess players\\nmay look forward a dozen or more moves. They are looking for a move that\\nleads to as favorable a position as they can find and are using their own static\\nevaluator to determine which positions are the most favorable.\\nHence, the Minimax algorithm with bounded lookahead is defined as follows:\\nFunction bounded_minimax (current_node, max_depth)\\n{\\nif is_leaf (current_node)\\nthen return static_evaluation (current_node);\\nif depth_of (current_node) == max_depth\\nthen return static_evaluation (current_node);\\nif is_min_node (current_node)\\nthen return min (minimax (children_of \\n(current_node)));\\nif is_max_node (current_node)\\nthen return max (minimax (children_of \\n(current_node)));\\n// this point will never be reached since\\n// every node must be a leaf node, a min node or a\\n// max node.\\n}'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 178, 'page_label': '179'}, page_content='152 CHAPTER 6 Game Playing\\nFigure 6.3\\nChess position with black\\nto move\\nIn fact, it is not necessarily sensible to apply a fixed cut-off point for search.\\nThe reason for this can be seen from the chess position shown in Figure 6.3.\\nIf bounded Minimax search cut off search at this node, it might consider\\nthe position to be reasonably even because the two players have the same\\npieces, which are roughly equally well developed. In fact, although it is\\nblack’s turn to move, white will almost certainly take black’s queen after\\nthis move, meaning that the position is extremely strong for white.\\nThis problem must be avoided if a computer program is to play chess or\\nany other game successfully. One way to avoid the problem is to only cut off\\nsearch at positions that are deemed to be quiescent. A quiescent position is\\none where the next move is unlikely to cause a large change in the relative\\npositions of the two players. So, a position where a piece can be captured\\nwithout a corresponding recapture is not quiescent.\\nAnother problem with bounded Minimax search is the horizon problem.\\nThis problem involves an extremely long sequence of moves that clearly lead\\nto a strong advantage for one player, but where the sequence of moves,\\nalthough potentially obvious to a human player, takes more moves than is\\nallowed by the bounded search. Hence, the significant end of the sequence\\nhas been pushed over the horizon. This was a particular problem for Chi-\\nnook, the checkers-playing program that we learn more about in Section 6.5.\\nThere is no universal solution to the horizon problem, but one method to\\nminimize its effects is to always search a few ply deeper when a position is'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 179, 'page_label': '180'}, page_content='6.4 Alpha–Beta Pruning 153\\nfound that appears to be particularly good. The singular-extension heuris-\\ntic is defined as follows: if a static evaluation of a move is much better than\\nthat of other moves being evaluated, continue searching.\\n6.4 Alpha–Beta Pruning\\nBounded lookahead can help to make smaller the part of the game tree that\\nneeds to be examined. In some cases, it is extremely useful to be able to\\nprune sections of the game tree. Using alpha–beta pruning, it is possible to\\nremove sections of the game tree that are not worth examining, to make\\nsearching for a good move more efficient.\\nThe principle behind alpha–beta pruning is that if a move is determined to\\nbe worse than another move that has already been examined, then further\\nexamining the possible consequences of that worse move is pointless.\\nConsider the partial game tree in Figure 6.4.\\nThis very simple game tree has five leaf nodes. The top arc represents a\\nchoice by the computer, and so is a maximizing level (in other words, the\\ntop node is a max node). After calculating the static evaluation function for\\nthe first four leaf nodes, it becomes unnecessary to evaluate the score for\\nthe fifth. The reason for this can be understood as follows:\\nIn choosing the left-hand path from the root node, it is possible to achieve\\na score of 3 or 5. Because this level is a minimizing level, the opponent can\\nbe expected to choose the move that leads to a score of 3. So, by choosing\\nthe left-hand arc from the root node, the computer can achieve a score of 3.\\nBy choosing the right-hand arc, the computer can achieve a score of 7 or 1,\\nor a mystery value. Because the opponent is aiming to minimize the score,\\n53 7 1\\nFigure 6.4\\nA partial game tree'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 180, 'page_label': '181'}, page_content='154 CHAPTER 6 Game Playing\\nhe or she could choose the position with a score of 1, which is worse than\\nthe value the computer could achieve by choosing the left-hand path. So,\\nthe value of the rightmost leaf node doesn’t matter—the computer must\\nnot choose the right-hand arc because it definitely leads to a score ofat best\\n1 (assuming the opponent does not irrationally choose the 7 option).\\n6.4.1 The Effectiveness of Alpha–Beta Pruning\\nIn this contrived example, alpha–beta pruning removes only one leaf node\\nfrom the tree, but in larger game trees, it can result in fairly valuable reduc-\\ntions in tree size. However, as Winston (1993) showed, it will not necessar-\\nily remove large portions of a game tree. In fact, in the worst case,\\nalpha–beta pruning will not prune any searches from the game tree, but\\neven in this case it will compute the same result as Minimax and will not\\nperform any less efficiently.\\nThe alpha–beta pruning method provides its best performance when the\\ngame tree is arranged such that the best choice at each level is the first one\\n(i.e., the left-most choice) to be examined by the algorithm. With such a\\ngame tree, a Minimax algorithm using alpha–beta cut-off will examine a\\ngame tree to double the depth that a Minimax algorithm without\\nalpha–beta pruning would examine in the same number of steps.\\nThis can be shown as follows:\\nIf a game tree is arranged optimally, then the number of nodes that must\\nbe examined to find the best move using alpha–beta pruning can be\\nderived as follows:\\nwhere\\nb = branching factor of game tree\\nd = depth of game tree\\ns = number of nodes that must be examined\\nThis means that approximately\\ns = 2b\\nd/2\\nWithout alpha–beta pruning, where all nodes must be examined:\\ns = bd\\nS b if d is even\\nb b if d is odd\\nd\\ndd= −\\n+−\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4 +() −()\\n21\\n1\\n2\\n12 12\\n/\\n//'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 181, 'page_label': '182'}, page_content='6.4 Alpha–Beta Pruning 155\\nHence, we can consider that using alpha–beta pruning reduces the effective\\nbranching factor from b to /H20857b/H33526, meaning that in a fixed period of time,\\nMinimax with alpha–beta pruning can look twice as far in the game tree as\\nMinimax without pruning.\\nThis represents a significant improvement—for example, in chess it\\nreduces the effective branching factor from around 38 to around 6—but it\\nmust be remembered that this assumes that the game tree is arranged opti-\\nmally (such that the best choice is always the left-most choice). In reality, it\\nmight provide far less improvement.\\nIt was found that in implementing the Deep Blue chess computer (see Sec-\\ntion 6.6), use of the alpha–beta method did in fact reduce the average\\nbranching factor of the chess game tree from 38 to around 6.\\n6.4.2 Implementation\\nThe alpha-beta pruning algorithm is implemented as follows:\\n■ The game tree is traversed in depth-first order. At each non-leaf\\nnode a value is stored. For max nodes, this value is called alpha,\\nand for min nodes, the value is beta.\\n■ An alpha value is the maximum (best) value found so far in the\\nmax node’s descendants.\\n■ A beta value is minimum (best) value found so far in the min\\nnode’s descendants.\\nIn the following pseudo-code implementation, we use the function call\\nbeta_value_of (min_ancestor_of (current_node)) , which returns the beta\\nvalue of some min node ancestor of the current node to see how it com-\\npares with the alpha value of the current node. Similarly,\\nalpha_value_of\\n(max_ancestor_of (current_node)) returns the alpha value of some max\\nnode ancestor of the current node in order that it be compared with the\\nbeta value of the current node.\\nFunction alpha_beta (current_node)\\n{\\nif is_leaf (current_node)\\nthen return static_evaluation (current_node);\\nif is_max_node (current_node) and\\nalpha_value_of (current_node) >=\\nbeta_value_of (min_ancestor_of (current_node))\\nthen cut_off_search_below (current_node);'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 182, 'page_label': '183'}, page_content='156 CHAPTER 6 Game Playing\\nif is_min_node (current_node) and\\nbeta_value_of (current_node) <=\\nalpha_value_of (max_ancestor_of (current_node))\\nthen cut_off_search_below (current_node);\\n}\\nT o avoid searching back up the tree for ancestor values, values are propa-\\ngated down the tree as follows:\\n■ For each max node, the minimum beta value for all its min node\\nancestors is stored as beta.\\n■ For each min node, the maximum alpha value for all its max node\\nancestors is stored as alpha.\\n■ Hence, each non-leaf node will have a beta value and an alpha\\nvalue stored.\\n■ Initially, the root node is assigned an alpha value of negative infin-\\nity and a beta value of infinity.\\nSo, the \\nalpha_beta function can be modified as follows. In the following\\npseudo-code implementation, the variable children is used to represent all\\nof the children of the current node, so the following line:\\nalpha = max (alpha, alpha_beta (children, alpha, beta));\\nmeans that alpha is set to the greatest of the current value of alpha, and the\\nvalues of the current node’s children, calculated by recursively calling\\nalpha_beta.\\nFunction alpha_beta (current_node, alpha, beta)\\n{\\nif is_root_node (current_node)\\nthen\\n{\\nalpha = -infinity\\nbeta = infinity\\n}\\nif is_leaf (current_node)\\nthen return static_evaluation (current_node);\\nif is_max_node (current_node)\\nthen\\n{\\nalpha = max (alpha, alpha_beta (children, alpha, beta));\\nif alpha >= beta\\nthen cut_off_search_below (current_node);\\n}'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 183, 'page_label': '184'}, page_content='6.4 Alpha–Beta Pruning 157\\n12 3 457 10 2615\\nde f g\\ncb\\na\\nMAX\\nMIN\\nMAX\\nFigure 6.5\\nA simple game tree\\nif is_min_node (current_node)\\nthen\\n{\\nbeta = min (beta, alpha_beta (children, alpha, beta));\\nif beta <= alpha\\nthen cut_off_search_below (current_node);\\n}\\n}\\nT o see how alpha–beta pruning works in practice, let us examine the game\\ntree shown in Figure 6.5.\\nThe non-leaf nodes in the tree are labeled from a to g, and the leaf nodes\\nhave scores assigned to them by static evaluation: a is a max node; b and c\\nare min nodes; and d, e, f, and g are max nodes.\\nFollowing the tree by depth-first search, the first step is to follow the path\\na,b,d and then to the three children of d. This gives an alpha value for d of\\n3. This is passed up to b, which now has a beta value of 3, and an alpha\\nvalue that has been passed down from a of negative infinity.\\nNow, the first child of e is examined and has a score of 4. In this case, clearly\\nthere is no need to examine the other children of e because the minimizing\\nchoice at node b will definitely do worse by choosing e rather than d. So cut-\\noff is applied here, and the nodes with scores of 5 and 7 are never examined.\\nThe full analysis of the tree is shown in Table 6.1, which shows how the\\nscores move through the tree from step to step of the process.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 184, 'page_label': '185'}, page_content='158 CHAPTER 6 Game Playing\\nTABLE 6.1 Analysis of alpha–beta pruning for the game tree in Figure 6.5\\nStep Node Alpha Beta Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\na\\nb\\nd\\nd\\nd\\nd\\nb\\ne\\ne\\na\\nc\\nf\\nc\\n/H11002/H11009\\n/H11002/H11009\\n/H11002/H11009\\n1\\n2\\n3\\n/H11002/H11009\\n/H11002/H11009\\n4\\n3\\n3\\n3\\n3\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n3\\n3\\n3\\n/H11009\\n/H11009\\n/H11009\\n3\\nAlpha starts as /H11002/H11009and beta starts as /H11009.\\nAt this stage, we have examined the three children of d\\nand have obtained an alpha value of 3, which is passed\\nback up to node b.\\nAt this min node, we can clearly achieve a score of 3 or\\nbetter (lower). Now we need to examine the children of\\ne to see if we can get a lower score.\\nCUT-OFF . A score of 4 can be obtained from the first child\\nof e. Min clearly will do better to choose d rather than e\\nbecause if he chooses e, max can get at least 4, which is\\nworse for min than 3. Hence, we can now ignore the\\nother children of e.\\nThe value of 3 has been passed back up to the root node,\\na. Hence, max now knows that he can score at least 3. He\\nnow needs to see if he can do better.\\nWe now examine the three children of f and find that\\nnone of them is better than 3. So, we pass back a value of\\n3 to c.\\nCUT-OFF . Max has already found that by taking the left-\\nhand branch, he can achieve a score of 3. Now it seems\\nthat if he chooses the right-hand branch, min can choose\\nf, which will mean he can only achieve a score of 2. So\\ncut-off can now occur because there is no need to exam-\\nine g or its children.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 185, 'page_label': '186'}, page_content='6.5 Checkers 159\\n43 87 21 6 5\\nMAX\\nMIN\\nMAX\\nFigure 6.6\\nA game tree optimized for alpha–beta search\\n1Draughts is another name for the common variety of checkers, which is played on an 8/H110038\\nboard. International checkers is another variety, which is played on a 10/H1100310 board.\\nHence, out of the 12 leaf nodes in the tree, the algorithm has needed to\\nexamine only 7 to conclude that the best move for max to make is b, in\\nwhich case min will choose d and max will choose the right hand node,\\nending with a static evaluation of 3.\\nAt this stage, we can easily see that this is the right answer because if max\\nchooses c, then min will clearly choose f, resulting in max being able to\\nachieve a score of only 2.\\nAn ideal tree for alpha–beta pruning is shown in Figure 6.6.\\nIn this case, the Minimax algorithm with alpha–beta cut-off will need to\\nexamine only five of the eight leaf nodes.\\n6.5 Checkers\\nThe game of checkers (or draughts 1) has proved to be an excellent chal-\\nlenge for the methods of Artificial Intelligence and one that has been met\\nwith a reasonable degree of success.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 186, 'page_label': '187'}, page_content='160 CHAPTER 6 Game Playing\\nIn his paper of 1959, Some Studies in Machine Learning Using the Game of\\nCheckers, Arthur Samuel described a computer system that could play\\ncheckers to a reasonable level, using Minimax with alpha–beta pruning.\\nThis system used a weighted linear function of a variety of heuristics that\\nmeasured how strong a particular position was.\\nIf a particular strategy was played and found to lose, the system would\\nadjust its weights to avoid making such a mistake in the future. In this way,\\nby playing many games it was able to learn to play checkers to a fairly high\\nlevel. Samuel’s system was a significant milestone in the Artificial Intelli-\\ngence research. Unfortunately, it was widely reported by the media that\\nSamuel’s system had solved the game of checkers and was able to beat a\\ntop-ranked human player. Neither of these claims was true, but it caused\\nthe Artificial Intelligence community to believe that there was nothing\\nmore to be learned about checkers, and so until Chinook, very little further\\nresearch was done on the game.\\n6.5.1 Chinook\\nChinook is a checkers-playing computer that was developed by a team led\\nby Dr. Jonathan Schaeffer of the University of Alberta in Canada.\\nChinook uses Minimax search with alpha–beta pruning. It also applies iter-\\native deepening (see Section 4.11) and a number of heuristics to maximize\\nthe efficiency of the search and to minimize the size of the game tree that the\\nprogram needs to examine for each move. Chinook also has a database of\\nendgames consisting of hundreds of billions of possible positions. Such a\\ndatabase would not be practical in chess, due to the enormous number of\\npossible positions, but is possible with checkers because there are fewer legal\\nsquares on the board (32 instead of 64), fewer piece types (2 as opposed to\\n6), and a smaller average branching factor (8 compared with 38).\\nChinook also has a large amount of knowledge about the game and built-in\\nheuristics to help it evaluate positions and choose better moves.\\nOn average, Chinook examines the game tree to a depth of 20 ply and is\\nable to examine around 1000 positions per second per MIPS (millions of\\ninstructions per second) of processing power.\\nIn 1990, Chinook was beaten by the world champion checkers player,\\nMarion Tinsley, by a margin of 7.5 to 6.5. Tinsley had been world cham-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 187, 'page_label': '188'}, page_content='6.5 Checkers 161\\npion for 40 years and is commonly recognized as the greatest checkers\\nplayer of all time.\\nIn 1992, Tinsley beat Chinook in the World Checkers Championship. Forty\\ngames were played; Chinook won two and Tinsley won four. Thirty-three\\ngames were drawn. This was the first World Championship match of any\\ngame in which a computer had taken part. In a rematch in 1994, Tinsley\\nand Chinook played six games, all of which were drawn, after which Tins-\\nley resigned due to ill health, surrendering the world championship title to\\nChinook. Sadly, Marion Tinsley succumbed to cancer in 1995 before a fur-\\nther rematch could be played. Chinook did beat Don Lafferty, who was\\nthen the world’s second-best human player, in 1995 to retain the title. The\\ncurrent world champion, Ron King, has been beaten many times by Chi-\\nnook, but never in a championship match.\\nThe version of Chinook that played in 1994 was a significantly more pow-\\nerful player (running on a much more powerful computer and using\\nmore sophisticated algorithms). One might imagine that with increasing\\ncomputer power, a checkers computer could be developed that would\\n“solve” the game of checkers—that is, a computer program that could\\nexamine the entire game tree for checkers and determine the outcome\\nbefore a game was started. This is a difficult challenge and one that does\\nnot seem likely to be achieved in the near future. The main problem is\\nthat as the depth of analysis increases, the improvement in play increases\\nless than linearly, meaning that a much deeper analysis is needed to pro-\\nvide small improvements (compare with chess—Section 6.6). Addition-\\nally, although the complete game tree for checkers would be vastly smaller\\nthan that for chess, it would still be extremely large (around 10\\n20 possible\\nmoves to examine).\\n6.5.2 Chinook’ s Databases\\nOne of the secrets of Chinook’s success at playing checkers lies in its data-\\nbases of endgame positions and opening moves. Chinook uses the opening\\nbook of a commercial checkers program, Colossus. This book had been\\ndeveloped over years, mainly from the published literature on the game.\\nChinook spent a number of months examining each position in the book\\nto a depth of at least 19 ply to ensure that the opening moves were correct.\\nThis process was also used to identify unusual opening moves that might\\nsurprise an experienced player such as Tinsley.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 188, 'page_label': '189'}, page_content='162 CHAPTER 6 Game Playing\\nIn fact, to make Chinook play more imaginatively, the developers chose to\\navoid the use of the opening book in most situations. Instead, they built an\\nanti-book that contains moves that Chinook should avoid, and in most\\nother cases, Chinook uses search to decide on good moves to make. Schaef-\\nfer and his team have found that this actually tends to lead to better play\\nthan following the opening book.\\nThe main power of Chinook lies in its databases of endgame positions. In\\n1992, Chinook’s database contained an evaluation (win, lose, or draw) for\\nevery possible position involving seven pieces—around 40 billion positions.\\nBy 1994, the databases had been extended to cover all positions involving\\neight pieces—another 440 billion positions.\\nIn most games, within a few moves from the start, Chinook’s search has\\nreached a position stored in its database, meaning that it can usually deter-\\nmine the outcome of a game within 10 moves of the start. Most possible\\ngames lead to a draw, but Chinook has been programmed to look for draws\\nthat involve complicated strategies that the opponent is likely to miss.\\nHence, in evaluating the game tree, Chinook does not give all draws a score\\nof zero, but gives them a score depending on how likely it is that the oppo-\\nnent will make a mistake and lose the game.\\n6.5.3 Chinook’ s Evaluation Function\\nChinook uses a linear weighted static evaluation function, based on a num-\\nber of heuristics. These heuristics include piece count, king count, balance\\n(the distribution of pieces between the left and right sides of the board), the\\nnumber of trapped kings, and so on.\\nChinook divides the game up into four phases, which are identified by the\\nnumber of pieces left on the board.\\nPhase Number of pieces\\n1 20–24\\n2 14–19\\n3 10–13\\n4 less than 10'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 189, 'page_label': '190'}, page_content='6.5 Checkers 163\\nChinook has different sets of weights for its 25 heuristic values for each\\nphase of the game, so that, for example, it places more importance on kings\\nin later phases of the game than in the first phase.\\n6.5.4 Forward Pruning\\nAnother technique used by Chinook, and other game-playing systems, is\\nforward pruning. The idea behind forward pruning is that if a line of play\\n(i.e., a path through the game tree) is being examined in which a number of\\npieces are lost without any way to recapture, then examination of the path\\nis terminated and the tree is pruned at this point.\\nHence, unlike alpha–beta pruning, no static evaluation is needed. It is pos-\\nsible, but unlikely, that such pruning could miss a useful strategy, but this is\\nless likely with checkers than with chess, where sacrifices of important\\npieces can often lead to a win.\\n6.5.5 Limitations of Minimax\\nSchaeffer’s work with Chinook has revealed some serious limitations with\\nalpha–beta search.\\nIn some cases, Minimax with alpha–beta pruning might show that a num-\\nber of different paths lead to a draw. As was mentioned in the previous sec-\\ntion, some draws can be harder to achieve than others. In particular, one\\nposition might lead to a draw no matter what moves a player makes,\\nwhereas another position might lead to a draw only if a player makes the\\ncorrect choice on each move for 40 moves. Clearly the latter draw is much\\nharder to achieve. Chinook was programmed to take advantage of this,\\nafter it drew a game that it could probably have won because it chose a draw\\nthat was easy for its opponent to achieve and neglected a draw that looked\\nvery much like a win to most human players.\\nSimilarly, in one game against Marion Tinsley, Chinook discovered a long,\\ncomplicated win for Tinsley, and made a sacrifice to try to avoid this. As a\\nresult, it lost the game easily. In fact, it is quite possible that Tinsley would\\nhave missed the win without the sacrifice.\\nA modified version of Minimax would take advantage of the difficulty of\\nparticular paths through the game tree, as well as the final outcome.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 190, 'page_label': '191'}, page_content='164 CHAPTER 6 Game Playing\\n6.5.6 Blondie 24\\nIn 2000, a new kind of checkers program was developed, called Blondie 24.\\nUnlike Chinook, Blondie 24 does not have any knowledge of the game\\n(other than the basic rules) built in. The program, developed by Dr. David\\nFogel of Natural Selection Inc. based in San Diego, uses evolutionary tech-\\nniques (described in Chapter 13) to develop neural networks (see Chapter\\n11) that can learn how to play checkers, and how to win.\\nIn fact, Blondie uses standard Minimax evaluation to search the game tree.\\nThe evolutionary method was used to develop the static evaluation function.\\nThe static evaluation function is evaluated by neural networks that were\\ndeveloped by starting with programs that would play the game randomly\\nand breeding together the ones that played the most successfully. Repeating\\nthis over many generations, and allowing random mutations, led to the\\nfinal version of the software, which proved able to beat many human play-\\ners. This program is not nearly at the level of Chinook, which currently is\\nthe checkers world champion, but it does represent a fascinating example\\nof a combination of artificial life techniques with Artificial Intelligence,\\ngenerating a solution that performs extremely well, without ever having\\nhad any help from humans.\\n6.6 Chess\\nOne of the best-known applications of Artificial Intelligence is the develop-\\nment of computer programs that can play chess against human players.\\nChess programs typically use Minimax algorithms with alpha–beta prun-\\ning and are almost always programmed with large libraries of opening\\nmoves (similar to the databases used in Chinook for endgame moves).\\nIt has been shown that there is a more or less linear relationship between\\nthe depth to which a program can examine the game tree for chess and its\\nskill in playing the game. In other words, a system that is able to examine\\nthe tree to 12 ply is likely to beat a system that can only examine the tree to\\n10 ply (see Newborn 2002, pages 291–294). As mentioned in Section 6.5,\\nthis is not true for checkers, where a law of diminishing returns applies. In\\nfact, Schaeffer (1991) claims that this relationship is the same for chess and\\ncheckers, but that the relationship is more or less linear up to a depth of\\naround 15, tailing off after this. Because the best chess programs tend to'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 191, 'page_label': '192'}, page_content='6.7 Go 165\\nanalyze to a depth of around 12, they have not yet reached the nonlinear\\nstage of the graph.\\nAs a result of this relationship, great advances in the ability of computers to\\nplay chess have been made simply as a result of improvement in speed of\\ncomputers and in the use of parallel computing techniques.\\nIn 1997, a chess-playing computer system developed by IBM called Deep\\nBlue beat world champion Garry Kasparov, who is generally considered to be\\nthe strongest chess player of all time. The final score after six games was 3.5 to\\n2.5. Kasparov won one game, Deep Blue won two, and three were drawn.\\nIn 2002, Vladimir Kramnik, one of the highest-ranking chess players in the\\nworld, played a match against a German computer program, Deep Fritz.\\nThe eight-game match ended in a draw. There are a number of other chess\\ncomputers that can play at a comparable level, and at the time of writing\\nthis book, the competition between human and computer chess players is\\nvery close. In January 2003, Garry Kasparov played a six-game match\\nagainst Deep Junior, an Israeli computer program. The match ended in a\\ndraw, with Kasparov and Deep Junior each winning one game, and the\\nother four games being drawn.\\nIt is certainly not the case as it is with games such as Othello and checkers\\nthat the best computers are unbeatable by humans, and neither is it the case\\nas with Go or bridge that computers cannot beat the best humans.\\nIt seems likely that given the linear relationship between depth of search\\nand quality of play that with the improvement in computer power, it will\\nsoon be the case that the best computers are unbeatable by even the very\\nbest human players.\\n6.7 Go\\nGo is an ancient Japanese game, which is considered by many to be the final\\nfrontier for research in game-playing computers. It is certainly more com-\\nplex than chess: Go is played on a 19/H1100319 board, with far greater freedom of\\nchoice in playing most moves than chess, resulting in an enormous branch-\\ning factor (on average around 360, compared with around 38 for chess).\\nIt has thus been impossible to develop a system that searches the game tree\\nfor Go in the same way as for chess or checkers. Systems have been developed\\nthat play Go, but the best of these can compete only at the level of a weak club'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 192, 'page_label': '193'}, page_content='166 CHAPTER 6 Game Playing\\nplayer. None have shown any possibility yet of approaching the level of the\\nbest human players. Methods usually involve extremely selective search—\\nusing constraints to eliminate many options at each stage of the game tree.\\nSome success has also been had with pattern recognition techniques.\\nA Taiwanese business man offered one million dollars to the first person\\nwho could write a computer program that could beat a professional Go\\nplayer. Although the business man died in 1997, his bet looks as though it\\nwill remain safe for a while yet.\\n6.7.1 Go-Moku\\nGo-moku is a far simpler version of Go, usually played on a 15 /H1100315 board.\\nThis game is often used for teaching and illustrating Artificial Intelligence\\nbecause it is fairly simple. Alternate players place stones on the board, and\\nthe first player to place five stones in a row wins the game.\\nGo-Moku belongs to a group of games including Connect-4 and tic-tac-toe\\n(noughts and crosses) that have been solved. That is to say, the complete\\ngame tree has been evaluated such that the outcome of the game can be\\ndetermined from the start. Assuming both players play correctly, the player\\nwho starts the game will always win.\\n6.8 Othello (Reversi)\\nOthello is a simpler game than chess, and typical Othello computer pro-\\ngrams can now examine the search tree to a depth of around 50 ply. The best\\nhuman players are now unable to beat this level of play. In 1997, the human\\nworld champion Takeshi Murakami of Japan was beaten 6–0 by an American\\nOthello computer program, developed by Michael Buro, called Logistello.\\n6.9 Games of Chance\\nUnlike the games we have considered so far, many games involve an ele-\\nment of chance—often introduced by a dice roll or a draw of cards. With\\nthe exception of simple games like Chutes and Ladders and Ludo, most\\ngames of chance still involve a reasonable degree of skill because the chance\\nelement merely restricts the choices that can be made.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 193, 'page_label': '194'}, page_content='6.10 Chapter Summary 167\\nGames such as backgammon, scrabble, and bridge are popular games that\\ninvolve chance. Computer programs have been developed that can play\\nbackgammon and Scrabble at a level where they can beat all but the best\\nhuman players in the world. Bridge is rather more complex, with its bidding\\nsystem presenting real problems for Artificial Intelligence system develop-\\ners. Bridge systems have been developed that can play at an intermediate\\nlevel but are not yet close to playing at the level of the best human players.\\n6.9.1 Expectiminimax\\nExpectiminimax is a version of the Minimax algorithm that has been extended\\nto take into account the probability of each successor node being reached. In\\ngames that involve the throw of a single die, the successor nodes at each ply will\\nall have equal probabilities (one-sixth), but in more complex games, the prob-\\nabilities are not so straightforward. For example, in backgammon, where two\\ndice are rolled for each move (or rather, for each ply in the game tree), the like-\\nlihood of achieving a double (1 and 1, 2 and 2, 3 and 3, 4 and 4, 5 and 5, or 6\\nand 6) is 1 in 36, whereas the likelihood of rolling any other pair is 1 in 18.\\nRather than giving each position in the game tree a particular Minimax\\nvalue, the Expectiminimax algorithm, which is described in more detail in\\nRussell and Norvig (1995) assigns an expected value to each node, which is\\nthe average value that could be obtained on the node, taking into account\\nthe probabilities of each possible outcome.\\n6.10 Chapter Summary\\n■ Game trees can be used to represent two-player games.\\n■ Searching game trees is very hard for all but the simplest games.\\n■ Minimax is an algorithm that identifies the best move to make in a\\ntwo-player game with perfect knowledge, assuming the entire tree\\ncan be examined.\\n■ When the entire tree cannot be examined, a static evaluator is\\nneeded that can assign a score to any given position according to\\nhow well each player is doing and how likely each player is to win\\nthe game from that position.\\n■ Alpha–beta pruning enables Minimax to run more efficiently\\nbecause it removes unnecessary branches from the game tree.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 194, 'page_label': '195'}, page_content='168 CHAPTER 6 Game Playing\\n■ In the best case, alpha–beta pruning enables Minimax to search the\\ngame tree to double the depth that it would be able to search with-\\nout pruning.\\n■ Games such as Go-moku and tic-tac-toe have been solved, mean-\\ning that the result of a game is known from the start.\\n■ Computers are able to beat the best players in the world at games\\nsuch as Othello and checkers.\\n■ Computers cannot compete with humans at games such as Go\\nand bridge.\\n■ Although Deep Blue beat Garry Kasparov at chess in 1997, comput-\\ners and humans are fairly evenly matched at chess at the moment.\\n6.11 Review Questions\\n6.1 Discuss the current state of the art of game-playing computer sys-\\ntems in relation to the following games: chess, checkers, Go, bridge,\\nOthello, tic-tac-toe. What advances are likely in the near future?\\nDo you believe there are any fundamental limitations?\\n6.2 Discuss the approach you would take to building a system for play-\\ning Scrabble or another word game of the sort. What limitations\\ndoes your system have? How likely do you think it is that your sys-\\ntem would be able to beat the best human players in the world?\\n6.3 What problems did the developers of Chinook face? What new\\ntechniques did they add to simple Minimax with alpha–beta prun-\\ning? Would these techniques extend well to other games?\\n6.4 Explain why the alpha–beta procedure will always generate the\\nsame answer as Minimax without pruning. Why is it useful?\\n6.5 Show the steps that would be taken in running the Minimax algo-\\nrithm on the game tree in Figure 6.7. Now run through the same\\ntree using alpha–beta pruning. How do the two compare?\\n6.6 Why might it be particularly difficult to program a computer to\\nsuccessfully play card games like bridge or poker? What sort of\\nalgorithms might you use to play these games?\\n6.7 What does it mean to say that a game has been solved? How likely is\\nit that games like Go and chess will ever be solved? Is it always the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 195, 'page_label': '196'}, page_content='6.12 Exercises 169\\n54 10 89 67 37 28 19 02\\na\\nbc\\ngfed\\nhi j k l m no\\ncase that the player who goes first will win a game that has been\\nsolved? Even if both players play correctly?\\n6.8 Most commercially available chess programs for home users are\\ndesigned to play at a range of levels from beginner up to grand-\\nmaster. Consider the additional difficulties involved in program-\\nming a computer to play suboptimally. Would alpha–beta pruning\\nstill be appropriate? What methods might a programmer use to\\nprogram the computer to play over such a range of abilities?\\n6.12 Exercises\\n6.1 For a game tree of depth d, and branching factor b, show that iter-\\native deepening does not increase by a great deal the number of\\nstatic evaluations needed to examine the tree.\\n6.2 Write an algorithm in pseudo-code, or a programming language of\\nyour choice, that evaluates a position in tic-tac-toe. Y our static\\nFigure 6.7\\nGame tree for question 6.5'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 196, 'page_label': '197'}, page_content='170 CHAPTER 6 Game Playing\\nevaluator should give 0 for a drawn position, 1 for a won position\\nfor crosses, /H110021 for won position for noughts.\\n6.3 Extend the algorithm you designed for Exercise 6.2 so that it is able\\nto evaluate positions that are nonterminal—in other words, posi-\\ntions where the game has not yet finished. Y our score should be\\npositive for an advantage to crosses, and negative for an advantage\\nto noughts.\\n6.4 Implement a Minimax algorithm using your static evaluator for\\ntic-tac-toe, and write a simple program that plays the game. Have\\nthe program output how many nodes in the game tree it had to\\nexamine as well as its choice of move.\\n6.5 Add alpha–beta pruning to your program, and see what difference\\n(if any) it makes to the number of nodes the program has to exam-\\nine when playing a game.\\n6.6 Implement an Expectiminimax algorithm for a game of chance\\n(you might use backgammon, or another dice game).\\n6.7 Is it possible to add alpha–beta pruning to your Expectiminimax\\nprogram? If so, do so. If not, can you find another way of pruning\\nthe tree that improves the performance of the program? How can\\nyou tell if it is improving the performance?\\n6.13 Further Reading\\nThere is a great deal of fascinating literature on the subject of game playing\\nusing Artificial Intelligence. The Chinook website is well worth visiting and\\ncontains a great deal of insight into the way game-playing systems are\\ndeveloped and improved. It can be found using any search engine.\\nArthur Samuel’s articles on his checkers-playing system are also worth reading.\\nA number of books and articles have been published on the subject of Deep\\nBlue and other chess computers. Monty Newborn’s book does not contain\\na great deal of computer science but does make a fascinating read, particu-\\nlarly for anyone interested in the game of chess.\\nBlondie 24: Playing at the Edge of AI by David B. Fogel (2001 – Morgan\\nKaufmann)\\nBehind Deep Blue: Building the Computer That Defeated the World Chess\\nChampion by Feng-Hsiung Hsu (2002 – Princeton University Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 197, 'page_label': '198'}, page_content='6.13 Further Reading 171\\nAn analysis of alpha beta pruning , by Donald Knuth and R. W. Moore \\n(1975 - in Artificial Intelligence, Vol. 6(4), pp. 293–326)\\nDeep Blue: An Artificial Intelligence Milestone by Monty Newborn (2003 –\\nSpringer V erlag)\\nKasparov Versus Deep Blue: Computer Chess Comes of Age by Monty New-\\nborn (1997 – Springer V erlag)\\nKasparov and Deep Blue by Bruce Pandolfini (1997 – Fireside)\\nSome Studies in Machine Learning Using the Game of Checkers by Arthur\\nSamuel (1959–in Computation & Intelligence – Collected Readings edited by\\nGeorge F. Luger - MIT Press)\\nOne Jump Ahead: Challenging Human Supremacy in Checkers by Jonathan\\nSchaeffer (1997 – Springer V erlag)\\nA Re-examination of Brute-force Search by Jonathan Schaeffer, Paul Lu,\\nDuane Szafron, and Robert Lake (1993 – in Games: Planning and Learning,\\nAAAI 1993 Fall Symposium, Report FS9302, pp. 51–58)\\nA World Championship Caliber Checkers Program by Jonathan Schaeffer,\\nJoseph Culberson, Norman Treloar, Brent Knight, Paul Lu, and Duane\\nSzafron – (1992 – in Artificial Intelligence, Vol. 53(2–3), pp. 273–290)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 198, 'page_label': '199'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 199, 'page_label': '200'}, page_content='Knowledge Representation\\nand Automated Reasoning\\n3\\nIntroduction to Part 3\\nPart 3 is divided into three chapters:\\nPropositional and Predicate Logic\\nIn Chapter 7, the basic concepts behind propositional calculus\\nand predicate calculus are introduced. Truth tables and the ideas\\nbehind proofs by deduction are explained. The concept of tau-\\ntology is introduced, as is satisfiability and logical equivalence.\\nProperties of logical systems such as soundness, completeness,\\nand decidability are discussed. Logical systems other than\\nthose of classical logic are briefly introduced.\\nInference and Resolution for Problem Solving\\nIn Chapter 8, we introduce in detail the ideas behind proof by\\nrefutation and resolution for automated theorem proving.\\nThe chapter explains the steps needed to automate resolution,\\nincluding: converting expressions to conjunctive normal\\nform, Skolemization, and unification. The use of resolution\\nand Horn Clauses in Prolog is discussed, as are other practical\\napplications of resolution, such as for solving combinatorial\\nsearch problems.\\nRules and Expert Systems\\nChapter 9 discusses how rules and frames are used to build\\nexpert systems and discusses the practicalities of implement-\\ning such systems. Methods such as forward and backward\\nchaining and conflict resolution are discussed, as is the Rete\\nAlgorithm for a more efficient rule-based approach.\\nThe ideas behind inheritance and multiple inheritance are dis-\\ncussed in relation to frames, and the relationship between\\nframes and object-oriented programming languages such as\\nC++ and Java is explored.\\nPART\\n7\\nCHAPTER\\n8\\nCHAPTER\\n9\\nCHAPTER'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 200, 'page_label': '201'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 201, 'page_label': '202'}, page_content='7CHAPTER\\nPropositional and\\nPredicate Logic\\nIf, dear Reader, you will faithfully observe these Rules, and so give my little book\\na really fair trial, I promise you, most confidently, that you will find Symbolic\\nLogic to be one of the most, if not the most, fascinating of mental recreations!\\n—Lewis Carroll, from the Introduction to Symbolic Logic\\nIf it was so, it might be; and if it were so, it would be: but as it isn’t, it ain’t.\\nThat’s logic.\\n—Lewis Carroll, from Through The Looking Glass\\nOf science and logic he chatters\\nAs fine and as fast as he can;\\nThough I am no judge of such matters,\\nI’m sure he’s a talented man.\\n—Winthrop Mackworth Praed, from The T alented Man\\n7.1 Introduction\\nIn this chapter, we introduce propositional calculus and first-order predi-\\ncate calculus, the languages of logic. We introduce methods that can be\\nused to carry out deductions and prove whether or not a conclusion fol-\\nlows from a set of premises.\\nWe introduce the ideas of logical equivalence, tautologies, and satisfiability.\\nThis chapter also discusses some important properties of logical systems,\\nincluding soundness, completeness, monotonicity, and decidability.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 202, 'page_label': '203'}, page_content='176 CHAPTER 7 Propositional and Predicate Logic\\nThis chapter assumes no previous knowledge of logic, so readers who are\\nalready familiar with the ideas of propositional logic and the predicate cal-\\nculus may wish to skim this chapter.\\n7.2 What Is Logic?\\nLogic is concerned with reasoning and the validity of arguments. In gen-\\neral, in logic, we are not concerned with the truth of statements, but rather\\nwith their validity. That is to say, although the following argument is\\nclearly logical, it is not something that we would consider to be true:\\nAll lemons are blue\\nMary is a lemon\\nTherefore, Mary is blue\\nThis set of statements is considered to be valid because the conclusion\\n(Mary is blue) follows logically from the other two statements, which we\\noften call the premises.\\nThe reason that validity and truth can be separated in this way is simple: a\\npiece of a reasoning is considered to be valid if its conclusion is true in cases\\nwhere its premises are also true. Hence, a valid set of statements such as the\\nones above can give a false conclusion, provided one or more of the prem-\\nises are also false.\\nWe can say:a piece of reasoning is valid if it leads to a true conclusion in every\\nsituation where the premises are true.\\nLogic is concerned with truth values. The possible truth values are true and\\nfalse. These can be considered to be the fundamental units of logic, and\\nalmost all logic is ultimately concerned with these truth values.\\n7.3 Why Logic Is Used in Artificial Intelligence\\nLogic is widely used in computer science, and particularly in Artificial\\nIntelligence. Logic is widely used as a representational method for Artificial\\nIntelligence. Unlike some other representations (such as frames, which are\\ndescribed in detail in Chapter 3), logic allows us to easily reason about neg-\\natives (such as, “this book is not red”) and disjunctions (“or”—such as,\\n“He’s either a soldier or a sailor”).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 203, 'page_label': '204'}, page_content='7.4 Logical Operators 177\\nLogic is also often used as a representational method for communicating\\nconcepts and theories within the Artificial Intelligence community. In\\naddition, logic is used to represent language in systems that are able to\\nunderstand and analyze human language.\\nAs we will see, one of the main weaknesses of traditional logic is its inabil-\\nity to deal with uncertainty. Logical statements must be expressed in terms\\nof truth or falsehood—it is not possible to reason, in classical logic, about\\npossibilities. We will see different versions of logic such as modal logics that\\nprovide some ability to reason about possibilities, and also probabilistic\\nmethods and fuzzy logic that provide much more rigorous ways to reason\\nin uncertain situations.\\n7.4 Logical Operators\\nIn reasoning about truth values, we need to use a number of operators,\\nwhich can be applied to truth values. We are familiar with several of these\\noperators from everyday language:\\nI like apples and oranges.\\nY ou can have an ice cream or a cake.\\nIf you come from France, then you speak French.\\nI am not stupid!\\nHere we see the four most basic logical operators being used in everyday\\nlanguage. The operators are:\\n■ and\\n■ or\\n■ not\\n■ if . . . then . . . (usually called implies)\\nThese operators work more or less as we expect them to. One important\\npoint to note is that or is slightly different from the way we usually use it. In\\nthe sentence, “Y ou can have an icecream or a cake, ” the mother is usually\\nsuggesting to her child that he can only have one of the items, but not both.\\nThis is referred to as an exclusive-or in logic because the case where both\\nare allowed is excluded. The version of or that is used in logic is called\\ninclusive-or and allows the case with both options.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 204, 'page_label': '205'}, page_content='178 CHAPTER 7 Propositional and Predicate Logic\\nThe operators are usually written using the following symbols, although\\nother symbols are sometimes used, according to the context:\\nand ∧\\nor ∨\\nnot ¬\\nimplies →\\niff ↔\\nIff is an abbreviation that is commonly used to mean “if and only if. ” We see\\nlater that this is a stronger form of implies that holds true if one thing\\nimplies another, and also the second thing implies the first.\\nFor example, “you can have an ice-cream if and only if you eat your din-\\nner. ” It may not be immediately apparent why this is different from “you\\ncan have an icecream if you eat your dinner. ” This is because most mothers\\nreally mean iff when they use if in this way.\\n7.5 Translating between English and Logic Notation\\nT o use logic, it is first necessary to convert facts and rules about the real\\nworld into logical expressions using the logical operators described in Sec-\\ntion 7.4. Without a reasonable amount of experience at this translation, it\\ncan seem quite a daunting task in some cases.\\nLet us examine some examples.\\nFirst, we will consider the simple operators,\\n∧, ∨, and ¬.\\nSentences that use the word and in English to express more than one con-\\ncept, all of which is true at once, can be easily translated into logic using the\\nAND operator,\\n∧. For example:\\n“It is raining and it is Tuesday. ”\\nmight be expressed as:\\nR ∧ T\\nWhere R means “it is raining” and T means “it is Tuesday. ” Note that we\\nhave been fairly arbitrary in our choice of these terms. This is all right, as\\nlong as the terms are chosen in such a way that they represent the problem\\nadequately. For example, if it is not necessary to discuss where it is raining,\\nR is probably enough. If we need to write expressions such as “it is raining'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 205, 'page_label': '206'}, page_content='7.5 Translating between English and Logic Notation 179\\nin New Y ork” or “it is raining heavily” or even “it rained for 30 minutes on\\nThursday, ” then R will probably not suffice.\\nT o express more complex concepts like these, we usually use predicates.\\nHence, for example, we might translate “it is raining in New Y ork” as:\\nN(R)\\nWe might equally well choose to write it as:\\nR(N)\\nThis depends on whether we consider the rain to be a property of New\\nY ork, or vice versa. In other words, when we write N(R), we are saying that\\na property of the rain is that it is in New Y ork, whereas with R(N) we are\\nsaying that a property of New Y ork is that it is raining.\\nWhich we use depends on the problem we are solving. It is likely that if we\\nare solving a problem about New Y ork, we would use R(N), whereas if we\\nare solving a problem about the location of various types of weather, we\\nmight use N(R).\\nLet us return now to the logical operators. The expression “it is raining in New\\nY ork, and I’m either getting sick or just very tired” can be expressed as follows:\\nR(N)\\n∧ (S(I) ∨ T(I))\\nHere we have used both the ∧ operator, and the ∨ operator to express a col-\\nlection of statements. The statement can be broken down into two sections,\\nwhich is indicated by the use of parentheses. The section in the parentheses\\nis S(I)\\n∨ T(I), which means “I’m either getting sick OR I’m very tired” . This\\nexpression is “AND’ed” with the part outside the parentheses, which isR(N).\\nFinally, the ¬ operator is applied exactly as you would expect—to express\\nnegation. For example,\\nIt is not raining in New Y ork,\\nmight be expressed as\\n¬R(N)\\nIt is important to get the ¬ in the right place. For example: “I’m either not\\nwell or just very tired” would be translated as\\n¬W(I) ∨ T(I)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 206, 'page_label': '207'}, page_content='180 CHAPTER 7 Propositional and Predicate Logic\\nThe position of the ¬ here indicates that it is bound to W(I) and does not\\nplay any role in affecting T(I). This idea of precedence is explained further\\nin Section 7.7.\\nNow let us see how the → operator is used. Often when dealing with logic\\nwe are discussing rules, which express concepts such as “if it is raining then\\nI will get wet. ”\\nThis sentence might be translated into logic as\\nR → W(I)\\nThis is read “R implies W(I)” or “IF R THEN W(I)” . By replacing the sym-\\nbols R and W(I) with their respective English language equivalents, we can\\nsee that this sentence can be read as\\n“raining implies I’ll get wet”\\nor “IF it’s raining THEN I’ll get wet. ”\\nImplication can be used to express much more complex concepts than this.\\nFor example, “Whenever he eats sandwiches that have pickles in them, he\\nends up either asleep at his desk or singing loud songs” might be translated as\\nS(y)\\n∧ E(x, y) ∧ P(y) → A(x) ∨ (S(x, z) ∧ L(z))\\nHere we have used the following symbol translations:\\nS(y) means that y is a sandwich.\\nE(x, y) means that x (the man) eats y (the sandwich).\\nP(y) means that y (the sandwich) has pickles in it.\\nA(x) means that x ends up asleep at his desk.\\nS(x, z) means that x (the man) sings z (songs).\\nL(z) means that z (the songs) are loud.\\nIn fact, there are better ways to express this kind of sentence, as we will see\\nwhen we examine the quantifiers ∃ and ∀ in Section 7.13.\\nThe important thing to realize is that the choice of variables and predicates\\nis important, but that you can choose any variables and predicates that map\\nwell to your problem and that help you to solve the problem. For example, in\\nthe example we have just looked at, we could perfectly well have used instead\\nS → A\\n∨ L\\nwhere S means “he eats a sandwich which has pickles in it, ” A means “he\\nends up asleep at his desk, ” and L means “he sings loud songs. ”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 207, 'page_label': '208'}, page_content='7.6 Truth Tables 181\\nThe choice of granularity is important, but there is no right or wrong way\\nto make this choice. In this simpler logical expression, we have chosen to\\nexpress a simple relationship between three variables, which makes sense if\\nthose variables are all that we care about—in other words, we don’t need to\\nknow anything else about the sandwich, or the songs, or the man, and the\\nfacts we examine are simply whether or not he eats a sandwich with pickles,\\nsleeps at his desk, and sings loud songs. The first translation we gave is\\nmore appropriate if we need to examine these concepts in more detail and\\nreason more deeply about the entities involved.\\nNote that we have thus far tended to use single letters to represent logical\\nvariables. It is also perfectly acceptable to use longer variable names, and\\nthus to write expressions such as the following:\\nFish (x) \\n∧ living (x) → has_scales (x)\\nThis kind of notation is obviously more useful when writing logical expres-\\nsions that are intended to be read by humans but when manipulated by a\\ncomputer do not add any value.\\n7.6 Truth Tables\\nWe can use variables to represent possible truth values, in much the same\\nway that variables are used in algebra to represent possible numerical val-\\nues. We can then apply logical operators to these variables and can reason\\nabout the way in which they behave.\\nIt is usual to represent the behavior of these logical operators using truth\\ntables. A truth table shows the possible values that can be generated by\\napplying an operator to truth values.\\n7.6.1 Not\\nFirst of all, we will look at the truth table for not, ¬.\\nNot is a unary operator, which means it is applied only to one variable. Its\\nbehavior is very simple:\\n¬ true is equal to false\\n¬ false is equal to true\\nIf variable A has value true, then ¬A has value false.\\nIf variable B has value false, then ¬B has value true.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 208, 'page_label': '209'}, page_content='182 CHAPTER 7 Propositional and Predicate Logic\\nThese can be represented by a truth table,\\nA ¬ A\\ntrue false\\nfalse true\\n7.6.2 And\\nNow, let us examine the truth table for our first binary operator—one\\nwhich acts on two variables:\\nABA  ∧ B\\nfalse false false\\nfalse true false\\ntrue false false\\ntrue true true\\n∧ is also called theconjunctive operator. A ∧ B is theconjunctionof A and B.\\nY ou can see that the only entry in the truth table for which A ∧ B is true is\\nthe one where A is true and B is true.I f A is false, or if B is false, then A ∧ B\\nis false. If both A and B are false, then A ∧ B is also false.\\nWhat do A and B mean? They can represent any statement, or proposition,\\nthat can take on a truth value. For example, A might represent “It’s sunny,”\\nand B might represent “It’s warm outside. ” In this case,A ∧ B would mean\\n“It is sunny and it’s warm outside, ” which clearly is true only if the two\\ncomponent parts are true (i.e., if it is true that it is sunny and it is true that\\nit is warm outside).\\n7.6.3 Or\\nThe truth table for the or operator, ∨, should need little explanation.\\nABA  ∨ B\\nfalse false false\\nfalse true true\\ntrue false true\\ntrue true true'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 209, 'page_label': '210'}, page_content='7.6 Truth Tables 183\\n∨ is also called thedisjunctive operator. A ∨ B is the disjunction of A and B.\\nClearly A ∨ B is true for any situation except when both A and B are false.I f\\nA is true, or if B is true, or if both A and B are true, A ∨ B is true.\\nY ou should notice that this table represents the inclusive-or operator. A\\ntable to represent exclusive-or would have false in the final row. In other\\nwords, while A ∨ B is true if A and B are both true, A EOR B (A exclusive-or\\nB) is false if A and B are both true.\\nY ou may also notice a pleasing symmetry between the truth tables for ∧\\nand ∨. This will become useful later, as will a number of other symmetrical\\nrelationships.\\n7.6.4 Implies\\nThe truth table for implies (→) is a little less intuitive.\\nABA  → B\\nfalse false true\\nfalse true true\\ntrue false false\\ntrue true true\\n(This form of implication is also known as material implication.)\\nIn the statement A → B, A is the antecedent, and B is the consequent.\\nThe bottom two lines of the table should be obvious. If A is true and B is\\ntrue, then A → B seems to be a reasonable thing to believe. For example, if\\nA means “you live in France” and B means “Y ou speak French, ” then A → B\\ncorresponds to the statement “if you live in France, then you speak French. ”\\nClearly, this statement is true (A → B is true) if I live in France and I speak\\nFrench (A is true and B is true).\\nSimilarly, if I live in France, but I don’t speak French ( A is true,b u t  B is\\nfalse), then it is clear that A → B is not true.\\nThe situations where A is false are a little less clear. If I do not live in France\\n(A is not true), then the truth table tells us that regardless of whether I\\nspeak French or not (the value of B), the statement A → B is true.\\nA → B is usually read as “A implies B” but can also be read as “If A then B”\\nor “If A is true then B is true. ” Hence, ifA is false, the statement is not really'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 210, 'page_label': '211'}, page_content='184 CHAPTER 7 Propositional and Predicate Logic\\nsaying anything about the value of B, so B is free to take on any value (as\\nlong as it is true or false, of course!).\\nThis can lead to some statements being valid that might at first glance\\nappear absurd. All of the following statements are valid:\\n52 = 25 → 4 = 4 (true → true)\\n9 /H110039 = 123 → 8 > 3 (false → true)\\n52 = 25 → 0 = 2 (false → false)\\nIn fact, in the second and third examples, the consequent could be given\\nany meaning, and the statement would still be true. For example, the fol-\\nlowing statement is valid:\\n52 = 25 → Logic is weird\\nNotice that when looking at simple logical statements like these, there does\\nnot need to be any real-world relationship between the antecedent and the\\nconsequent. For logic to be useful, though, we tend to want the relation-\\nships being expressed to be meaningful as well as being logically true.\\n7.6.5 iff\\nThe truth table for iff (if and only if {↔}) is as follows:\\nABA  ↔ B\\nfalse false true\\nfalse true false\\ntrue false false\\ntrue true true\\nIt can be seen that A\\n↔ B is true as long as A and B have the same value. In\\nother words, if one is true and the other false, then A ↔ B is false. Other-\\nwise, if A and B have the same value, A ↔ B is true.\\n7.7 Complex Truth Tables\\nTruth tables are not limited to showing the values for single operators. For\\nexample, a truth table can be used to display the possible values forA ∧(B ∨C).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 211, 'page_label': '212'}, page_content='7.7 Complex Truth Tables 185\\nABCA  ∧ (B ∨ C)\\nfalse false false false\\nfalse false true false\\nfalse true false false\\nfalse true true false\\ntrue false false false\\ntrue false true true\\ntrue true false true\\ntrue true true true\\nNote that for two variables, the truth table has four lines, and for three vari-\\nables, it has eight. In general, a truth table for n variables will have 2\\nn lines.\\nThe use of brackets in this expression is important. A ∧ (B ∨ C) is not the\\nsame as (A ∧ B) ∨ C.\\nT o avoid ambiguity, the logical operators are assigned precedence, as with\\nmathematical operators. The order of precedence that is used is as follows:\\n¬, ∧, ∨, →, ↔\\nHence, in a statement such as\\n¬A ∨¬ B ∧ C\\nthe ¬ operator has the greatest precedence, meaning that it is most closely\\ntied to its symbols.∧ has a greater precedence than ∨, which means that the\\nsentence above can be expressed as\\n(¬A) ∨ ((¬B) ∧ C)\\nSimilarly, when we write\\n¬A ∨ B\\nthis is the same as\\n(¬A) ∨ B\\nrather than\\n¬(A ∨ B)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 212, 'page_label': '213'}, page_content='186 CHAPTER 7 Propositional and Predicate Logic\\nIn general, it is a good idea to use brackets whenever an expression might\\notherwise be ambiguous.\\n7.8 Tautology\\nConsider the following truth table:\\nAA  ∨ ¬ A\\nfalse true\\ntrue true\\nThis truth table has a property that we have not seen before: the value of\\nthe expression A\\n∨¬ A is true regardless of the value ofA. An expression like\\nthis that is always true is called a tautology.\\nIf A is a tautology, we write:\\n|=A\\nTautologies may seem like rather uninteresting entities, but in fact they are\\nextremely useful for logic, as we see later.\\nA logical expression that is a tautology is often described as being valid.A\\nvalid expression is defined as being one that is true under any interpreta-\\ntion. In other words, no matter what meanings and values we assign to the\\nvariables in a valid expression, it will still be true. For example, the follow-\\ning sentences are all valid:\\nIf wibble is true, then wibble is true.\\nEither wibble is true, or wibble is not true.\\nIn the language of logic, we can replace wibble with the symbol A, in which\\ncase these two statements can be rewritten as\\nA → A\\nA\\n∨¬ A\\nIf an expression is false in any interpretation, it is described as being con-\\ntradictory. The following expressions are contradictory:\\nA ∧¬ A\\n(A ∨¬ A) → (A ∧¬ A)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 213, 'page_label': '214'}, page_content='7.9 Equivalence 187\\nIt doesn’t matter whatA means in these expressions, the result cannot betrue.\\nSome expressions are satisfiable, but not valid. This means that they are\\ntrue under some interpretation, but not under all interpretations. The fol-\\nlowing expressions are satisfiable:\\nA\\n∨ B\\n(A ∧ B ∨¬ C) → (D ∧ E)\\nA contradictory expression is clearly not satisfiable and so is described as\\nbeing unsatisfiable.\\n7.9 Equivalence\\nConsider the following two expressions:\\nA ∧ B\\nB ∧ A\\nIt should be fairly clear that these two expressions will always have the same\\nvalue for a given pair of values forA and B. In other words, we say that the first\\nexpression islogically equivalentto the second expression. We write this as\\nA ∧ B /H11013B ∧ A\\nThis means that the ∧ operator is commutative.\\nNote that this is not the same as implication:\\nA ∧ B → B ∧ A\\nalthough this second statement is also true. The difference is that if for two\\nexpressions e1 and e2:\\ne1 /H11013e2\\nthen e1 will always have the same value as e2 for a given set of variables. On\\nthe other hand, as we have seen, e1 → e2 is true if e1 is false and e2 is true.\\nThere are a number of logical equivalences that are extremely useful. The\\nfollowing is a list of a few of the most common:\\nA ∨ A /H11013A\\nA ∧ A /H11013A\\nA ∧ (B ∧ C) /H11013(A ∧ B) ∧ C( ∧ is associative)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 214, 'page_label': '215'}, page_content='188 CHAPTER 7 Propositional and Predicate Logic\\nA ∨ (B ∨ C) /H11013(A ∨ B) ∨ C( ∨ is associative)\\nA ∧ (B ∨ C) /H11013(A ∧ B) ∨ (A ∧ C) ( ∧ is distributive over ∨)\\nA ∧ (A ∨ B) /H11013A\\nA ∨ (A ∧ B) /H11013A\\nA ∧ true /H11013A\\nA ∧ false /H11013false\\nA ∨ true /H11013true\\nA ∨ false /H11013A\\nAll of these equivalences can be proved by drawing up the truth tables for\\neach side of the equivalence and seeing if the two tables are the same. Y ou\\nmay want to try this to satisfy yourself that all of the equivalences are cor-\\nrect, particularly for some of the less intuitive ones.\\nThe following is a very important equivalence:\\nA → B /H11013\\n¬A ∨ B\\nY ou can verify this by checking the truth tables. The reason that this is use-\\nful is that it means we do not need to use the → symbol at all—we can\\nreplace it with a combination of ¬ and ∨. Similarly, the following equiva-\\nlences mean we do not need to use ∧ or ↔:\\nA ∧ B /H11013¬(¬A ∨¬ B)\\nA ↔ B /H11013¬(¬(¬A ∨ B) ∨¬ (¬B ∨ A))\\nIn fact, any binary logical operator can be expressed using ¬ and ∨. This is a\\nfact that is employed in electronic circuits, wherenor gates, based on an oper-\\nator callednor,a r eu s e d .Nor is represented by ↓, and is defined as follows:\\nA ↓ B /H11013¬(A ∨ B)\\nFinally, the following equivalences are known as DeMorgan’s Lawsand will\\nbe used later in this chapter:\\nA ∧ B /H11013¬(¬A ∨¬ B)\\nA ∨ B /H11013¬(¬A ∧¬ B)\\nBy using these and other equivalences, logical expressions can be simpli-\\nfied. For example,\\n(C ∧ D) ∨ ((C ∧ D) ∧ E)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 215, 'page_label': '216'}, page_content='7.10 Propositional Logic 189\\ncan be simplified using the following rule:\\nA ∨ (A ∧ B) /H11013A\\nhence,\\n(C ∧ D) ∨ ((C ∧ D) ∧ E) /H11013C ∧ D\\nIn this way, it is possible to eliminate subexpressions that do not contribute\\nto the overall value of the expression.\\n7.10 Propositional Logic\\nThere are a number of possible systems of logic. The system we have been\\nexamining so far in this chapter is called propositional logic. The language\\nthat is used to express propositional logic is called the propositional calcu-\\nlus (although in practice, many people use the expressions logic and calcu-\\nlus interchangeably in this context).\\nA logical system can be defined in terms of its syntax (the alphabet of\\nsymbols and how they can be combined), its semantics (what the sym-\\nbols mean), and a set of rules of deduction that enable us to derive one\\nexpression from a set of other expressions and thus make arguments\\nand proofs.\\n7.10.1 Syntax\\nWe have already examined the syntax of propositional calculus. The alpha-\\nbet of symbols, /H9018is defined as follows\\n/H9018= {true, false,\\n¬, →,( ,) , ∧, ∨, ↔,p 1,p 2,p 3,...,p n,...}\\nHere we have used set notation to define the possible values that are con-\\ntained within the alphabet /H9018. Note that we allow an infinite number of\\nproposition letters,o r  propositional symbols , p1, p2, p3,..., and so on.\\nMore usually, we will represent these by capital letters P, Q, R, and so on,\\nalthough if we need to represent a very large number of them, we will use\\nthe subscript notation (e.g., p\\n1).\\nAn expression is referred to as a well-formed formula (often abbreviated as\\nwff) or a sentence if it is constructed correctly, according to the rules of the\\nsyntax of propositional calculus, which are defined as follows. In these'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 216, 'page_label': '217'}, page_content='190 CHAPTER 7 Propositional and Predicate Logic\\nrules, we use A, B, C to represent sentences. In other words, we define a sen-\\ntence recursively, in terms of other sentences. The following are well-\\nformed sentences:\\nP ,Q ,R ...\\ntrue, false\\n(A)\\n¬A\\nA ∧ B\\nA ∨ B\\nA → B\\nA ↔ B\\nHence, we can see that the following is an example of a wff:\\nP ∧ Q ∨ (B ∧¬ C) → A ∧ B ∨ D ∧ (¬E)\\nThis is not to make any claims about the validity or otherwise of the expres-\\nsion, simply that it is allowed within the syntax of propositional calculus.\\n7.10.2 Semantics\\nThe semantics of the operators of propositional calculus can be defined in\\nterms of truth tables. As we have seen, the meaning of P\\n∧ Q is defined as\\n“true when P is true and Q is also true.”\\nThe meaning of symbols such as P and Q is arbitrary and could be ignored\\naltogether if we were reasoning about pure logic. In other words, reasoning\\nabout sentences such as P\\n∨ Q ∧¬ R is possible without considering what P,\\nQ, and R mean.\\nBecause we are using logic as a representational method for artificial intel-\\nligence, however, it is often the case that when using propositional logic, the\\nmeanings of these symbols are very important. The beauty of this represen-\\ntation is that it is possible for a computer to reason about them in a very\\ngeneral way, without needing to know much about the real world.\\nIn other words, if we tell a computer, “I like ice cream, and I like chocolate, ”\\nit might represent this statement as A\\n∧ B, which it could then use to reason\\nwith, and, as we will see, it can use this to make deductions.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 217, 'page_label': '218'}, page_content='7.11 Deduction 191\\n7.11 Deduction\\nIf we have a set of assumptions { A1, A2,..., An}, and from those assump-\\ntions we are able to derive a conclusion, C, then we say that we have\\ndeduced C from the assumptions, which is written\\n{A1,A 2,...,A n} ⊢C\\nIf C can be concluded without any assumptions, then we write\\n⊢C\\nT o derive a conclusion from a set of assumptions, we apply a set of infer-\\nence rules. T o distinguish an inference rule from a sentence, we often write\\nA ⊢B as follows:\\nSome of the most useful inference rules for propositional logic are as fol-\\nlows. In these rules, A, B, and C stand for any logical expressions.\\n7.11.1\\nThis rule is very straightforward. It says: Given A and B, we can deduce A ∧\\nB. This follows from the definition of ∧.\\n7.11.2\\nSimilarly,\\nThese rules say that given A ∧ B, we can deduce A and we can also deduce B\\nseparately. Again, these follow from the definition of∧.\\nAB\\nB\\n∧\\nAB\\nA\\n∧\\n-Elimination\\nA   B\\nAB∧\\n-Introduction\\nA/H5007B'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 218, 'page_label': '219'}, page_content='192 CHAPTER 7 Propositional and Predicate Logic\\n7.11.3 Or-Introduction\\nThese rules say that from A we can deduce the disjunction of A with any\\nexpression. For example, from the statement “I like logic, ” we can deduce\\nexpressions such as “I like logic or I like cheese, ”“I like logic or I do not like\\nlogic, ” “I like logic or fish can sing, ” “I like logic or 2 + 2 = 123, ” and so on.\\nThis follows because true\\n∨ B is true for any value of B.\\n7.11.4 → Elimination\\nThis rule is usually known as modus ponens and is one of the most com-\\nmonly used rules in logical deduction. It is expressed as follows:\\nIn other words, ifA is true and A implies B is true, then we know thatB is true.\\nFor example, if we replace A with “it is raining” and B with “I need an\\numbrella, ” then we produce the following:\\nIt is raining. If it’s raining, I need an umbrella. Therefore, I need\\nan umbrella.\\nThis kind of reasoning is clearly valid.\\n7.11.5 Reductio Ad Absurdum\\nWe need to introduce a new notation for this rule:\\nThe symbol ⊥ is called falsum, which is used to indicate an absurdity, or a\\ncontradiction. For example, ⊥ can be deduced from A ∧¬ A.\\n¬\\n⊥\\nA\\nA\\nM\\nA    A B\\nB\\n→\\nB\\nAB∨\\nA\\nAB∨'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 219, 'page_label': '220'}, page_content='7.11 Deduction 193\\nThe reductio ad absurdum rule simply says that if we assume that A is false\\n(¬A) and this leads to a contradiction ( ⊥), then we can deduce that A is\\ntrue. This is known as proof by contradiction.\\nAs we will see, this is an extremely powerful concept and is widely used in\\nlogical systems.\\n7.11.6 → Introduction\\nThis rule shows that if in carrying out a proof we start from an assumption\\nA and derive a conclusion C, then we can conclude that A → C.\\n7.11.7 ¬¬ Elimination\\nThis rule states that if we have a sentence that is negated twice, we can con-\\nclude the sentence itself, without the negation. Clearly, this rule follows\\nfrom the definition of\\n¬.\\n7.11.8 Example 1\\nT o carry out a proof that one set of sentences follows logically from\\nanother, we selectively apply the rules presented above to the assumptions\\nuntil we arrive at the conclusions.\\nFor example, it would be useful to prove the following:\\n{A,\\n¬A} ⊢⊥\\nIn other words, if we start from the set of assumptions A and ¬A, we can\\nconclude falsum.\\nFirst, note that\\n¬A /H11013A → ⊥\\nThis can be seen by comparing the truth tables for ¬A and for A → ⊥.\\n¬¬A\\nA\\nA\\nC\\nAC\\nM\\n→'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 220, 'page_label': '221'}, page_content='194 CHAPTER 7 Propositional and Predicate Logic\\nHence, we can take as our set of assumptions\\n{A, A → ⊥}\\nThus, our proof using modus ponens (the → ELIMINATION rule pre-\\nsented in Section 7.11.2) is as follows:\\n7.11.9 Example 2\\nLet us prove the following:\\n{A ∧ B} ⊢A ∨ B\\nThe proof is as follows:\\nA ∧ B assumption\\nAb y  ∧ elimination\\nA ∨ Bb y  ∨ introduction\\n7.11.10 Example 3\\nWe will use reductio ad absurdum to prove the following:\\n⊢(¬A → B) → (¬B → A)\\nThe usual method for carrying out such proofs is based on the idea that in\\norder to prove something of the form A → B, it is a good idea to start by\\nassuming A.\\nWe will start with two assumptions: ¬A and (¬A → B). After the first step,\\nwhich uses modus ponens, on our original assumptions to prove B, we\\nintroduce a new assumption, which is ¬B. The proof is as follows:\\n¬A ¬A → B assumptions\\nB ¬B modus ponens\\nBB  →⊥ rewriting ¬B\\n⊥ modus ponens\\nA reductio ad absurdum\\n¬B → A → introduction\\n(¬A → B) → (¬B → A) → introduction\\nA  A →⊥\\n⊥'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 221, 'page_label': '222'}, page_content='7.12 The Deduction Theorem 195\\nIn carrying out this proof, we have used the relationship between ¬B and B\\n→ ⊥ as we did in Example 1. We have also used reductio absurdum to show\\nthat if we start by assuming ¬A, we end up with a contradiction ( ⊥), and\\ntherefore our initial assumption,¬A, was false.H e n c e ,A must be true.\\n7.11.11 Example 4\\nLet us now aim to prove the following:\\n⊢(A → B) → ((B → C) → ((C → D) → (A → D)))\\nT o prove this, we will need to make a series of assumptions. We will start by\\nmaking two assumptions, A and A → B. Hence, our proof is as follows:\\nAA  → B assumptions\\nBB  → C modus ponens\\nCC  → D modus ponens\\nD modus ponens\\nA → D → introduction\\n(C → D) → (A → D) → introduction\\n(B → C) → ((C → D) → (A → D)) → introduction\\n(A → B) → ((B → C) → ((C → D) → (A → D))) → introduction\\n7.12 The Deduction Theorem\\nA useful rule known as the deduction theorem provides us with a way to\\nmake propositional logic proofs easier. The rule is as follows:\\nif A ⋃ {B} ⊢C then A ⊢(B → C)\\nHere A is a set of wff’s, which makes up our assumptions. Note that this\\nrule is true even if A is the empty set. A ⋃ {B} means the union of the set A\\nwith the set consisting of one element, B.\\nThe rule also holds in reverse:\\nif A ⊢(B → C) then A ⋃ {B} ⊢C\\nLet us see an example of a proof using the deduction theorem.\\nOur aim is to prove the following:\\n{A → B}\\n⊢A → (C → B)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 222, 'page_label': '223'}, page_content='196 CHAPTER 7 Propositional and Predicate Logic\\nRecall the axiom that was presented earlier:\\nA → (B → A)\\nBecause propositional logic is monotonic (see Section 7.18), we can add in\\nan additional assumption, that A is true:\\nA\\nNow, by applying modus ponens to this assumption and our hypothesis, A\\n→ B, we arrive at\\nB\\nWe can now apply our axiom\\nB → (C → B)\\nAnd by modus ponens on the above two lines, we get\\nC → B\\nHence, we have shown that\\n{A → B} ⋃ A ⊢(C → B)\\nAnd, therefore, by the deduction theorem\\n{A → B} ⊢A → (C → B)\\n7.13 Predicate Calculus\\n7.13.1 Syntax\\nPredicate calculus allows us to reason about properties of objects and rela-\\ntionships between objects. In propositional calculus, we could express the\\nEnglish statement “I like cheese” by A. This enables us to create constructs\\nsuch as \\n¬A, which means “I do not like cheese, ” but it does not allow us to\\nextract any information about the cheese, or me, or other things that I like.\\nIn predicate calculus, we use predicates to express properties of objects. So\\nthe sentence “I like cheese” might be expressed as\\nL(me, cheese)\\nwhere L is a predicate that represents the idea of “liking. ” Note that as well\\nas expressing a property of me, this statement also expresses a relationship\\nbetween me and cheese. This can be useful, as we will see, in describing\\nenvironments for robots and other agents. For example, a simple agent may'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 223, 'page_label': '224'}, page_content='7.13 Predicate Calculus 197\\nbe concerned with the location of various blocks, and a statement about\\nthe world might be\\nT(A,B)\\nwhich could mean: Block A is on top of Block B.\\nThus far we have expressed ideas about specific objects. It is also possible to\\nmake more general statements using the predicate calculus. For example, to\\nexpress the idea that everyone likes cheese, we might say\\n(∀x)(P(x) → L(x, C))\\nThe symbol ∀ is read “for all, ” so the statement above could be read as “for\\nevery x it is true that if property P holds for x, then the relationship L holds\\nbetween x and C, ” or in plainer English: “every x that is a person likes\\ncheese. ” (Here we are interpreting P(x) as meaning “x is a person” or, more\\nprecisely, “x has property P.” )\\nNote that we have used brackets rather carefully in the statement above.\\nThis statement can also be written with fewer brackets:\\n∀x P(x) → L(x, C)\\n∀ is called the universal quantifier.\\nThe quantifier ∃ can be used to express the notion that some values do have\\na certain property, but not necessarily all of them:\\n(∃x)(L(x,C))\\nThis statement can be read “there exists an x such that x likes cheese. ” This\\ndoes not make any claims about the possible values of x, so x could be a\\nperson, or a dog, or an item of furniture. When we use the existential quan-\\ntifier in this way, we are simply saying that there is at least one value of x for\\nwhich L(x,C) holds.\\nNote, therefore, that the following is true:\\n(∀x)(L(x,C)) → (∃x)(L(x,C))\\nbut the following is not:\\n(∃x)(L(x,C)) → (∀x)(L(x,C))\\n7.13.2 Relationships between ∀ and ∃\\nIt is also possible to combine the universal and existential quantifiers, such\\nas in the following statement:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 224, 'page_label': '225'}, page_content='198 CHAPTER 7 Propositional and Predicate Logic\\n(∀x) (∃y) (L(x,y))\\nThis statement can be read “for all x, there exists a y such that L holds for x\\nand y, ” which we might interpret as “everyone likes something. ”\\nA useful relationship exists between ∀ and ∃. Consider the statement “not\\neveryone likes cheese. ” We could write this as\\n¬(∀x)(P(x) → L(x,C)) (1)\\nAs we have already seen, A → B is equivalent to ¬A ∨ B. Using DeMorgan’s\\nlaws, we can see that this is equivalent to ¬(A ∧¬ B). Hence, the statement\\n(1) above, can be rewritten:\\n¬(∀x)¬(P(x) ∧¬ L(x,C)) (2)\\nThis can be read as “It is not true that for allx the following is not true: x is\\na person and x does not like cheese. ” If you examine this rather convoluted\\nsentence carefully, you will see that it is in fact the same as “there exists anx\\nsuch that x is a person andx does not like cheese. ” Hence we can rewrite it as\\n(∃x)(P(x) ∧¬ L(x,C)) (3)\\nIn making this transition from statement (2) to statement (3), we have uti-\\nlized the following equivalence:\\n∃x /H11013¬(∀x)¬\\nIn an expression of the form ( ∀x)(P(x, y)), the variable x is said to be\\nbound, whereas y is said to be free. This can be understood as meaning that\\nthe variable y could be replaced by any other variable because it is free, and\\nthe expression would still have the same meaning, whereas if the variable x\\nwere to be replaced by some other variable in P(x,y), then the meaning of\\nthe expression would be changed:\\n(∀x)(P(y, z))\\nis not equivalent to (∀x)(P(x, y)), whereas (∀x)(P(x, z)) is. Note that a vari-\\nable can occur both bound and free in an expression, as in\\n(∀x)(P(x,y,z) → (∃y)(Q(y,z)))\\nIn this expression, x is bound throughout, and z is free throughout; y is free\\nin its first occurrence but is bound in (∃y)(Q(y,z)). (Note that both occur-\\nrences of y are bound here.)\\nMaking this kind of change is known as substitution. Substitution is\\nallowed of any free variable for another free variable.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 225, 'page_label': '226'}, page_content='7.14 First-Order Predicate Logic 199\\n7.13.3 Functions\\nIn much the same way that functions can be used in mathematics, we can\\nexpress an object that relates to another object in a specific way using\\nfunctions. For example, to represent the statement “my mother likes\\ncheese, ” we might use\\nL(m(me),cheese)\\nHere the function m(x) means the mother of x. Functions can take more\\nthan one argument, and in general a function with n arguments is rep-\\nresented as\\nf(x\\n1,x 2,x 3,...,x n)\\n7.14 First-Order Predicate Logic\\nThe type of predicate calculus that we have been referring to is also calledfirst-\\norder predicate logic (FOPL). A first-order logic is one in which the quantifiers\\n∀ and ∃ can be applied to objects orterms, but not to predicates or functions.\\nSo we can define the syntax of FOPL as follows. First, we define a term:\\nA constant is a term.\\nA variable is a term.\\nf(x\\n1,x 2,x 3,...,x n) is a term if x1,x 2,x 3,...,x n are all terms.\\nAnything that does not meet the above description cannot be a term. For\\nexample, the following is not a term:∀xP (x). This kind of construction we\\ncall a sentence or a well-formed formula (wff), which is defined as follows.\\nIn these definitions, P is a predicate,x\\n1, x2, x3,..., xn are terms, and A,B are\\nwff’s. The following are the acceptable forms for wff’s:\\nP(x1,x 2,x 3,...,x n)\\n¬A\\nA ∧ B\\nA ∨ B\\nA → B\\nA ↔ B\\n(∀x)A\\n(∃x)A\\nAn atomic formula is a wff of the form P(x1, x2, x3,..., xn).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 226, 'page_label': '227'}, page_content='200 CHAPTER 7 Propositional and Predicate Logic\\nHigher order logics exist in which quantifiers can be applied to predicates\\nand functions, and where the following expression is an example of a wff:\\n(∀P)(∃x)P(x)\\nIn this book, we will stick with first-order logics, in which quantifiers can\\nonly be applied to variables, not predicates or functions.\\n7.15 Soundness\\nWe have seen that a logical system such as propositional logic consists of a\\nsyntax, a semantics, and a set of rules of deduction. A logical system also\\nhas a set of fundamental truths, which are known as axioms. The axioms\\nare the basic rules that are known to be true and from which all other theo-\\nrems within the system can be proved.\\nAn axiom of propositional logic, for example, is\\nA → (B → A)\\nA theorem of a logical system is a statement that can be proved by applying\\nthe rules of deduction to the axioms in the system.\\nIf A is a theorem, then we write\\n⊢A\\nA logical system is described as being sound if every theorem is logically\\nvalid, or a tautology.\\nIt can be proved by induction that both propositional logic and FOPL\\nare sound.\\n7.16 Completeness\\nA logical system is complete if every tautology is a theorem—in other\\nwords, if every valid statement in the logic can be proved by applying the\\nrules of deduction to the axioms. Both propositional logic and FOPL are\\ncomplete. The proofs that these systems are complete are rather complex.\\n7.17 Decidability\\nA logical system is decidable if it is possible to produce an algorithm that\\nwill determine whether any wff is a theorem. In other words, if a logical\\nsystem is decidable, then a computer can be used to determine whether\\nlogical expressions in that system are valid or not.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 227, 'page_label': '228'}, page_content='7.19 Abduction and Inductive Reasoning 201\\nWe can prove that propositional logic is decidable by using the fact that it is\\ncomplete. Thanks to the completeness of propositional logic, we can prove\\nthat a wff A is a theorem by showing that it is a tautology. T o show if a wff\\nis a tautology, we simply need to draw up a truth table for that wff and\\nshow that all the lines have true as the result. This can clearly be done algo-\\nrithmically because we know that a truth table for n values has 2\\nn lines and\\nis therefore finite, for a finite number of variables.\\nFOPL, on the other hand, is not decidable. This is due to the fact that it is\\nnot possible to develop an algorithm that will determine whether an arbi-\\ntrary wff in FOPL is logically valid.\\n7.18 Monotonicity\\nA logical system is described as being monotonic if a valid proof in the sys-\\ntem cannot be made invalid by adding additional premises or assumptions.\\nIn other words, if we find that we can prove a conclusion C by applying\\nrules of deduction to a premise B with assumptions A, then adding addi-\\ntional assumptions A/H11032and B/H11032will not stop us from being able to deduce C.\\nBoth propositional logic and FOPL are monotonic. Elsewhere in this book,\\nwe learn about probability theory, which is not a monotonic system.\\nMonotonicity of a logical system can be expressed as follows:\\nIf we can prove {A, B} \\n⊢C,\\nthen we can also prove: {A, B, A/H11032,B /H11032} ⊢C.\\nNote that A/H11032and B/H11032can be anything, including ¬A and ¬B. In other words,\\neven adding contradictory assumptions does not stop us from making the\\nproof in a monotonic system. In fact, it turns out that adding contradictory\\nassumptions allows us to prove anything, including invalid conclusions.\\nThis makes sense if we recall the line in the truth table for →, which shows\\nthat false → true. By adding a contradictory assumption, we make our\\nassumptions false and can thus prove any conclusion.\\n7.19 Abduction and Inductive Reasoning\\nThe kind of reasoning that we have seen so far in this chapter has been\\ndeductive reasoning, which in general is based on the use of modus ponens\\nand the other deductive rules of reasoning. This kind of reasoning assumes'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 228, 'page_label': '229'}, page_content='202 CHAPTER 7 Propositional and Predicate Logic\\nthat we are dealing with certainties and does not allow us to reason about\\nthings of which we are not certain. As we see elsewhere in this book, there is\\nanother kind of reasoning, inductive reasoning, which does not have the\\nsame logical basis but can be extremely powerful for dealing with situations\\nin which we lack certainty.\\nStrangely, another form of reasoning, abduction, is based on a common\\nfallacy, which can be expressed as\\nNote that abduction is very similar to modus ponens but is not logically\\nsound. A typical example of using this rule might be “When Jack is sick, he\\ndoesn’t come to work. Jack is not at work today. Therefore Jack is sick.”\\nIn fact, Jack may be having a holiday, or attending a funeral, or it may be\\nSunday or Christmas Day.\\nGiven that this type of reasoning is invalid, why are we discussing it here?\\nIt turns out that although abduction does not provide a logically sound\\nmodel for reasoning, it does provide a model that works reasonably well\\nin the real world because it allows us to observe a phenomenon and pro-\\npose a possible explanation or cause for that phenomenon without com-\\nplete knowledge. Abductive reasoning is discussed in more detail in\\nChapter 17.\\nInductive reasoning enables us to make predictions about what will hap-\\npen, based on what has happened in the past. Humans use inductive rea-\\nsoning all the time without realizing it. In fact, our entire lives are based\\naround inductive reasoning, for example, “the sun came up yesterday and\\nthe day before, and every day I know about before that, so it will come up\\nagain tomorrow. ” It’s possible it won’t, but it seems fairly unlikely. This kind\\nof reasoning becomes more powerful when we apply probabilities to it, as\\nin “I’ve noticed that nearly every bird I see is a swallow. Therefore, it’s quite\\nlikely that that bird is a swallow. ”\\nAs we will see, these kinds of reasoning are extremely useful for dealing\\nwith uncertainty and are the basis of most of the learning techniques used\\nin Artificial Intelligence.\\nB      A B\\nA\\n→'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 229, 'page_label': '230'}, page_content='7.20 Modal Logics and Possible Worlds 203\\n7.20 Modal Logics and Possible Worlds\\nThe forms of logic that we have dealt with so far deal with facts and prop-\\nerties of objects that are either true or false. In these classical logics,w e  d o\\nnot consider the possibility that things change or that things might not\\nalways be as they are now.\\nModal logics are an extension of classical logic that allow us to reason\\nabout possibilities and certainties. In other words, using a modal logic, we\\ncan express ideas such as “although the sky is usually blue, it isn’t always”\\n(for example, at night). In this way, we can reason about possible worlds. A\\npossible world is a universe or scenario that could logically come about.\\nThe following statements may not be true in our world, but they are possible,\\nin the sense that they are not illogical, and could be true in a possible world:\\nTrees are all blue.\\nDogs can fly.\\nPeople have no legs.\\nIt is possible that some of these statements will become true in the future,\\nor even that they were true in the past. It is also possible to imagine an\\nalternative universe in which these statements are true now. The following\\nstatements, on the other hand, cannot be true in any possible world:\\nA\\n∧¬ A\\n(x > y) ∧ (y > z) ∧ (z > x)\\nThe first of these illustrates the law of the excluded middle , which simply\\nstates that a fact must be either true or false: it cannot be both true and\\nfalse. It also cannot be the case that a fact is neither true nor false. This is a\\nlaw of classical logic, and as we see in Chapter 18, it is possible to have a log-\\nical system without the law of the excluded middle, and in which a fact can\\nbe both true and false.\\nThe second statement cannot be true by the laws of mathematics. We are\\nnot interested in possible worlds in which the laws of logic and mathemat-\\nics do not hold.\\nA statement that may be true or false, depending on the situation, is called\\ncontingent. A statement that must always have the same truth value,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 230, 'page_label': '231'}, page_content='204 CHAPTER 7 Propositional and Predicate Logic\\nregardless of which possible world we consider, is noncontingent.H e n c e ,\\nthe following statements are contingent:\\nA ∧ B\\nA ∨ B\\nI like ice cream.\\nThe sky is blue.\\nThe following statements are noncontingent:\\nA\\n∨¬ A\\nA ∧¬ A\\nIf you like all ice cream, then you like this ice cream.\\nClearly, a noncontingent statement can be either true or false, but the fact\\nthat it is noncontingent means it will always have that same truth value.\\nIf a statement A is contingent, then we say that A is possibly true, which\\nis written\\n/H17003A\\nIf A is noncontingent, then it is necessarily true, which is written\\nA\\n7.20.1 Reasoning in Modal Logic\\nIt is not possible to draw up a truth table for the operators /H17003and . (Con-\\nsider the four possible truth tables for unary operators—it should be clear\\nthat none of these matches these operators.) It is possible, however, to rea-\\nson about them.\\nThe following rules are examples of the axioms that can be used to reason\\nin this kind of modal logic:\\nA → /H17003A\\n¬A → ¬/H17003A\\n/H17003A → ¬ A\\nAlthough truth tables cannot be drawn up to prove these rules, you should\\nbe able to reason about them using your understanding of the meaning of\\nthe and /H17003operators.\\nModal logic, and other nonclassical logics, are discussed in Chapter 17.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 231, 'page_label': '232'}, page_content='7.23 Review Questions 205\\n7.21 Dealing with Change\\nAs we have seen, classical logics do not deal well with change. They assume\\nthat if an object has a property, then it will always have that property and\\nalways has had it. Of course, this is not true of very many things in the real\\nworld, and a logical system that allows things to change is needed. The sit-\\nuation and event calculi are covered in more detail in Chapters 17 and 19.\\n7.22 Chapter Summary\\n■ Logic is primarily concerned with the logical validity of state-\\nments, rather than with truth.\\n■ Logic is widely used in Artificial Intelligence as a representa-\\ntional method.\\n■ Abduction and inductive reasoning are good at dealing with\\nuncertainty, unlike classical logic.\\n■ The main operators of propositional logic are ∧, ∨, ¬, →, and ↔\\n(and, or, not, implies, and iff).\\n■ The behavior of these logical operators can be expressed in truth\\ntables. Truth tables can also be used to solve complex problems.\\n■ Propositional logic deals with simple propositions such as “I like\\ncheese. ” First-order predicate logic allows us to reason about more\\ncomplex statements such as “All people who eat cheese like cats, ”\\nusing the quantifiers ∀ and ∃ (“for all” , and “there exists”).\\n■ A statement that is always true in any situation is called a tautology.\\nA ∨¬ A is an example of a tautology.\\n■ Two statements are logically equivalent if they have the same\\ntruth tables.\\n■ First-order predicate logic is sound and complete, but not decid-\\nable. Propositional logic is sound, complete, and decidable.\\n■ Modal logics allow us to reason about certainty.\\n7.23 Review Questions\\n7.1 Explain the meanings of the following terms in the context of logic:\\na. truth\\nb. validity'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 232, 'page_label': '233'}, page_content='206 CHAPTER 7 Propositional and Predicate Logic\\nc. equivalent\\nd. uncertainty\\ne. tautology\\nf. satisfiable\\ng. sound\\nh. complete\\ni. decidable\\nj. modal logic\\n7.2 “Inductive reasoning is a reasonable way to think about everyday\\nlife, but it does not provide the logical structure that propositional\\nlogic does. ” Discuss.\\n7.3 Explain what is meant by the following: “Classical logics are not\\ngood at dealing with uncertainty. ”\\n7.4 Explain why the addition of the quantifiers ∀ and ∃ makes predi-\\ncate calculus so powerful.\\n7.5 Explain the rule of modus ponens. Explain how it is used in every-\\nday life.\\n7.6 Explain in layman’s terms what the law of the excluded middle\\nmeans. What difficulties might you encounter in logical deduction\\nif you ignored the law of the excluded middle?\\n7.7 Assume the law of the excluded middle is not true, and use this to\\nprove the equality 1 = 0.\\n7.8 What does it mean to say that a logic is monotonic? Is proposi-\\ntional logic monotonic? What complexities do you think nonmo-\\nnotonicity would add to the process of logical deduction? Would\\nmodus ponens still hold in a nonmonotonic logic?\\n7.24 Exercises\\n7.1 Translate the following sentences into logical statements, using\\neither propositional or predicate logic as appropriate:\\na. I like apples and pears.\\nb. When I eat apples and pears, I usually like to have a walk.\\nc. Every apple that I have ever eaten has been delicious.\\nd. The fact that some pears are not delicious will not stop me eat-\\ning them.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 233, 'page_label': '234'}, page_content='7.24 Exercises 207\\ne. I can only eat an apple if I have first eaten a pear, and I can only\\neat a pear if I eat an apple immediately afterward.\\nf. There exists a book that includes details of every book.\\ng. There exists somewhere in the world a book that lists every\\nsingle person who doesn’t appear in any other book.\\nh. If you haven’t read the book that lists all other books, then you\\nhaven’t read any book, unless you’ve read the book that lists\\nbooks that do not exist, in which case you’ve read every book.\\n7.2 Draw a truth table for the following expression:\\n¬A ∧ (A ∨ B) ∧ (B ∨ C)\\n7.3 (Hard) Prove that propositional logic and first-order predicate\\nlogic are sound and complete.\\n7.4 Write expressions in propositional calculus to represent the follow-\\ning statements:\\na. If you go to Mexico, you will be far away.\\nb. I cannot hear you when you are far away.\\nc. When I can’t hear you, I forget what you look like.\\nd. If I come to Mexico, and I don’t know what you look like, I\\nwon’t be able to find you.\\ne. Therefore, if you go to Mexico, and I follow you, I won’t be\\nable to find you.\\nProve whether the conclusion follows from the premises or not.\\n7.5 Write expressions in first-order predicate logic to represent the fol-\\nlowing statements, and prove whether the conclusion follows from\\nthe premises or not:\\na. All dancers love to dance.\\nb. Everyone who sings and plays an instrument loves to dance.\\nc. Therefore, all dancers sing and play an instrument.\\n7.6 Prove the following:\\na.\\n⊢A → A\\nb. ⊢(( ¬A → ¬B) → A) → ((¬B → ¬A) → ¬B)\\nc. ⊢(¬¬¬ A → ¬¬¬ B) → (¬A → ¬B)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 234, 'page_label': '235'}, page_content='208 CHAPTER 7 Propositional and Predicate Logic\\n7.25 Further Reading\\nMost textbooks on Artificial Intelligence provide good coverage of logic.\\nAn excellent, short introduction to logic that provides more detail on most\\nof the subject than has been provided here is Kelly (1997). Lewis Carroll’s\\nwork, though over 100 years old, still makes for an interesting and relevant\\nread on the subject of logic and reasoning, although his approach was\\nrather different from that usually found today.\\nThe idea of abduction was introduced by C. S. Peirce, in his 1878 paper\\nHow to Make Our Ideas Clear, published in Popular Science Monthly.\\nFrancis Bacon introduced the idea of inductive reasoning in 1620. His writ-\\nings on the subject can be found in The New Organon, and Related Writ-\\nings, published in 1960.\\nFrancis Bacon: The New Organon by Francis Bacon, edited by Lisa Jardine\\nand Michael Silverthorne (2002 – Cambridge University Press)\\nPropositional Logic: Deduction and Algorithms by Hans Kleine Büning and\\nTheodor Lettmann (1999 – Cambridge University Press)\\nSymbolic Logic and Game of Logic by Lewis Carroll (published in one vol-\\nume – 1958 – Dover Books).\\nPredicate Logic: The Semantic Foundations of Logic by Richard L. Epstein\\n(2000 – Wadsworth Publishing)\\nPropositional Logics: The Semantic Foundations of Logic by Richard L.\\nEpstein (2000 – Wadsworth Publishing)\\nThe Essence of Logic by John Kelly (1997 – Prentice Hall)\\nIntroduction to Logic: Propositional Logic by Howard Pospesel (1999 – Pren-\\ntice Hall)\\nLogic for Computer Science by Steve Reeves and Michael Clarke (1993 –\\nAddison Wesley)\\nLogical Forms: An Introduction to Philosophical Logic by Mark Sainsbury\\n(1991 – Blackwell)\\nLogic and Prolog by Richard Spencer-Smith (1991 – Harvester Wheatsheaf)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 235, 'page_label': '236'}, page_content='8CHAPTER\\nInference and Resolution for\\nProblem Solving\\nEarly work in theorem proving programs for quantified logics culminated in 1965\\nwith Alan Robinson’s development of machine-oriented formulation of first-\\norder logic called Resolution (Robinson, 1965). There followed an immensely pro-\\nductive period of exploration of resolution-based theorem-proving.\\n—Alan Newell, The Knowledge Level\\nWhen you have eliminated the impossible, whatever remains, no matter how\\nimprobable, must be the truth.\\n—Sir Arthur Conan Doyle, The Sign of Four\\nAt thirty a man suspects himself a fool;\\nKnows it at forty, and reforms his plan;\\nAt fifty chides his infamous delay,\\nPushes his prudent purpose to resolve;\\nIn all the magnanimity of thought\\nResolves; and re-resolves; then dies the same.\\n—Edward Y oung,Night Thoughts\\n8.1 Introduction\\nThis chapter introduces the main ideas behind automated reasoning, or theo-\\nrem proving. The method of resolution is discussed in some detail because it\\npertains to both propositional logic and first-order predicate logic (FOPL).\\nT o explain resolution, ideas such as unification, normal forms, and Her-\\nbrand universes are introduced. This chapter is somewhat more advanced'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 236, 'page_label': '237'}, page_content='210 CHAPTER 8 Inference and Resolution for Problem Solving\\nthan Chapter 7 and assumes an understanding of propositional calculus\\nand first-order predicate calculus.\\nThis chapter also briefly explains how PROLOG uses resolution to process\\ndata and to provide solutions to problems. In fact, resolution is fundamen-\\ntal to the way that PROLOG works. Resolution is an important part of Arti-\\nficial Intelligence research and provides a common method for systems to\\nreason logically and to prove theorems in an automated manner. This has\\nadvantages over other theorem-proving methods that depend more on\\nintuition and experience and are thus best applied by humans. In this chap-\\nter, we show how resolution can be entirely automated and an algorithm\\ngenerated for using resolution to prove theorems.\\nThe representational methods discussed in this chapter and Chapter 7 pro-\\nvide a powerful tool for reasoning logically and, in particular, for enabling\\ncomputer systems to automatically reason about a database of facts. This\\nrepresentational method, though, is not suitable for all problems, and in\\nmany situations it is entirely inadequate.\\nIn particular, predicate logic does not have a mechanism for dealing with\\nchange or with time. We will discuss a number of alternative representations in\\nChapters 17 and 18 that overcome some of these difficulties. As we see in these\\nchapters, and in Chapter 19, intelligent agents and other Artificial Intelligence\\nsystems often need to reason about events, situations, and other time-based\\nfactors. The logic we are looking at here is better suited to static environments,\\nwhich is certainly appropriate for solving a number of problems.\\n8.2 Resolution in Propositional Logic\\nIn Chapter 7, we introduced a method of deductive reasoning in order to\\nmake proofs in predicate and propositional logic. It is not clear how this\\nprocess might be automated because at each stage an amount of initiative\\nwas required to choose the right next step. We now introduce a proof\\nmethod, resolution, which can be automated because it involves a fixed set\\nof steps. Before we examine resolution, we must introduce some key ideas.\\n8.2.1 Normal Forms\\nA sentence or well-formed formula (wff) is inconjunctive normal formif it\\nis of the following form:\\nA1 ∧ A2 ∧ A3 ∧ ... ∧ An'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 237, 'page_label': '238'}, page_content='8.2 Resolution in Propositional Logic 211\\nwhere each clause, Ai, is of the form\\nB1 ∨ B2 ∨ B3 ∨ ... ∨ Bn\\nEach Bi is a literal, where a literal is a basic symbol of propositional logic. In\\nfact, a literal can be more accurately defined as an atom or an atom that is\\nnegated, where an atom is one of the basic object symbols in propositional\\nlogic. Hence, in the following expression:\\nA\\n∧ B ∨ (¬C ∧ D)\\nA is an atom, as are B, C, and D. The literals are A, B, ¬C, and D.\\nSo, an expression is in conjunctive normal form (often written CNF) if it\\nconsists of a set of or phrases anded together, such as:\\nA ∧ (B ∨ C) ∧ (¬A ∨¬ B ∨¬ C ∨ D)\\nTrivially, a literal is also in CNF.\\nA sentence is in disjunctive normal form (DNF) if it consists of a set of\\nand phrases ored together, as in\\nA\\n∨ (B ∧ C) ∨ (¬A ∧¬ B ∧¬ C ∧ D)\\nAny wff can be converted to CNF by using the following equivalences,\\nwhich we have encountered previously:\\n1. A ↔ B /H11013(A → B) ∧ (B → A)\\n2. A → B /H11013¬A ∨ B\\n3. ¬(A ∧ B) /H11013¬A ∨¬ B\\n4. ¬(A ∨ B) /H11013¬A ∧¬ B\\n5. ¬¬A /H11013A\\n6. A ∨ (B ∧ C) /H11013(A ∨ B) ∧ (A ∨ C)\\nFor example, we will convert (A → B) → C to CNF:\\n(A → B) → C\\n¬(A → B) ∨ C (2)\\n¬(¬A ∨ B) ∨ C (3)\\n(A ∧¬ B) ∨ C (4)\\n(A ∨ C) ∧ (¬B ∨ C) (6)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 238, 'page_label': '239'}, page_content='212 CHAPTER 8 Inference and Resolution for Problem Solving\\nA further example follows:\\nA ↔ (B ∧ C)\\n(A → (B ∧ C)) ∧ ((B ∧ C) → A) (1)\\n(¬A ∨ (B ∧ C)) ∧ (¬(B ∧ C) ∨ A) (2)\\n(¬A ∨ (B ∧ C)) ∧ (¬B ∨¬ C ∨ A) (3)\\n(¬A ∨ B) ∧ (¬A ∨ C) ∧ (¬B ∨¬ C ∨ A) (6)\\nNote that this process can be automated, as the equivalences can always be\\napplied in the order they were listed, replacing symbols and constructions\\nas they are encountered. As we will see, this is a useful fact: an algorithm\\ncan be expressed that will convert any wff into CNF.\\nHaving converted a wff into CNF, we can now express it as a set of clauses.\\nSo our expression above\\n(\\n¬A ∨ B) ∧ (¬A ∨ C) ∧ (¬B ∨¬ C ∨ A)\\nwould be represented in clause form as\\n{(¬A, B), (¬A, C), (¬B, ¬C, A)}\\n8.2.2 The Resolution Rule\\nNow we introduce a new rule to sit alongside the rules presented in Section\\n7.10, which is called the resolution rule:\\nThis rule is not as immediately obvious as the rules in Section 7.10, but it\\ndoes prove to be extremely useful. It can also be written as follows:\\nIn this form, the rule can be seen to be saying that implication is transitive,\\nor in other words, if A implies B and B implies C, then A implies C.\\nThis can be applied to wff’s in clause form, as follows:\\nIf a wff contains a clause that contains literal L and another clause that con-\\ntains literal \\n¬L, then these two clauses can be combined together, and L and\\n¬L can be removed from those clauses. For example,\\n¬→ →\\n¬→\\nA B       B C\\nAC\\nA B       B C\\nAC\\n∨¬ ∨\\n∨'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 239, 'page_label': '240'}, page_content='8.2 Resolution in Propositional Logic 213\\n{(A, B), (¬B, C)}\\ncan be resolved to give\\n{(A, C)}\\nSimilarly,\\n{(A, B, C), D, (¬A, D, E), (¬D, F)}\\ncan be resolved to give\\n{ ( B ,C ,D ,E ) ,D ,(¬D, F)}\\nwhich can be further resolved to give either\\n{ ( B ,C ,D ,E ) ,F }\\nor\\n{ ( B ,C ,E ,F ) ,D }\\nNote that at the first step, we also had a choice and could have resolved to\\n{(A, B, C), D, (¬A, E, F)}\\nwhich can be further resolved to give\\n{ ( B ,C ,E ,F ) ,D }\\nNow, if wff P resolves to give wff Q, we write\\nP |= Q\\nFor example, we can resolve (A ∨ B) ∧ (¬A ∨ C) ∧ (¬B ∨ C) as follows:\\n{(A, B), (¬A, C), (¬B, C)}\\n{(B, C), (¬B, C)}\\n{C}\\nWe can express this as\\n(A ∨ B) ∧ (¬A ∨ C) ∧ (¬B ∨ C) |= C\\nIf we resolve two clauses, we produce the resolvent of those clauses. The\\nresolvent is a logical consequence of the two clauses.\\n8.2.3 Resolution Refutation\\nNow let us resolve the following clauses:\\n{(¬A, B), (¬A, ¬B, C), A,¬C}'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 240, 'page_label': '241'}, page_content='214 CHAPTER 8 Inference and Resolution for Problem Solving\\nWe begin by resolving the first clause with the second clause, thus eliminat-\\ning B and ¬B:\\n{(¬A, C), A,¬C}\\n{C, ¬C}\\n⊥\\nThe fact that this resolution has resulted in falsum means that the original\\nclauses were inconsistent. We have refuted the original clauses, using reso-\\nlution refutation. We can write\\n{(¬A, B), (¬A, ¬B, C), A,¬C} |= ⊥\\nThe idea behind resolution refutation is explained in more detail in Sec-\\ntion 8.2.4.\\n8.2.4 Proof by Refutation\\nProof by refutation (also known as proof by contradiction ), as used in\\nresolution refutation, is a powerful method for solving problems. For\\nexample, let us imagine that we want to determine whether the following\\nlogical argument is valid:\\nIf it rains and I don’t have an umbrella, then I will get wet.\\nIt is raining, and I don’t have an umbrella.\\nTherefore, I will get wet.\\nWe can rewrite this in propositional calculus as follows:\\n(A\\n∧¬ B) → C\\nA ∧¬ B\\n∴C\\nT o prove this by refutation, we first negate the conclusion and convert the\\nexpressions into clause form. The first expression is the only one that is not\\nalready in CNF, so first we convert this to CNF as follows:\\n(A\\n∧¬ B) → C\\n/H11013¬(A ∧¬ B) ∨ C\\n/H11013¬A ∨ B ∨ C'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 241, 'page_label': '242'}, page_content='8.2 Resolution in Propositional Logic 215\\nNow, to prove that our conclusion is valid, we need to show that\\n{(¬A, B, C), A,¬B, ¬C} |= ⊥\\nWe resolve these clauses as follows:\\n{(B, C), ¬B, ¬C}\\n{C, ¬C}\\n⊥\\nHence, in showing that by negating our conclusion we lead to a contradic-\\ntion, we have shown that our original conclusion must have been true.\\nIf this process leads to a situation where some clauses are unresolved, and\\nfalsum cannot be reached, we have shown that the clauses with the negated\\nconclusion are not contradictory and that therefore the original conclusion\\nwas not valid.\\nBecause following resolution explanations in this way can be confusing, it is\\noften preferable to present a resolution proof in the form of a tree, where pairs\\nof resolved clauses are connected together, as shown in the following proof:\\nA → B\\nB → C\\nC → D\\nD → E\\n∨ F\\n∴ A → F\\nFirst we negate the conclusion, to give: ¬(A → F). Next we convert to\\nclause form:\\nD → E ∨ F\\n/H11013¬D ∨ (E ∨ F)\\nand\\n¬(A → F)\\n/H11013¬(¬A ∨ F)\\n/H11013A ∧¬ F\\nSo, our clauses are\\n{(¬A, B), (¬B, C), (¬C, D), (¬D, E, F), A,¬F)}'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 242, 'page_label': '243'}, page_content='216 CHAPTER 8 Inference and Resolution for Problem Solving\\nOur proof in tree form is as follows:\\nWe have not been able to reach falsum because we are left with a single\\nclause {E}, which cannot be resolved with anything. Hence, we can con-\\nclude that our original conclusion was not valid. Y ou can prove this for\\nyourself using a truth table.\\n8.3 Applications of Resolution\\nClearly, resolution can be used to automate the process of proving whether\\na conclusion can be derived in a valid way from a set of premises or not.\\nThis is certainly useful, but resolution is not limited in its applications to\\njust proving logical arguments.\\nA combinatorial search problem is a problem where there are a number of\\nvariables, each of which can be assigned a particular value. For example, a\\njigsaw puzzle can be seen as a problem where each piece is represented by a\\nvariable, and the position that each piece is placed in is the value assigned\\nto that variable. Of course, the point of the puzzle is that there is only one\\ncorrect way to place the pieces, so correspondingly, there would only be one\\nset of assignments of values to variables that would be correct. We have\\nalready examined combinatorial search problems and seen ways to solve\\nthem in Chapter 5. Although resolution cannot help us to solve such prob-\\nlems, it can help by telling us whether a solution exists or not.\\nAn example of a combinatorial search problem is the three-coloring prob-\\nlem: Given a map, is it possible to color the countries using three colors\\nsuch that no two countries that are next to each other have the same color?\\n(¬ A, B)\\n(¬ A, C)\\n(¬ A, D)\\n(¬ B, C) ( ¬ C, D) ( ¬ D, E, F)\\n(¬ A, E, F)\\n(E, F)\\n¬ FA\\nE'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 243, 'page_label': '244'}, page_content='8.3 Applications of Resolution 217\\nA\\nBC D\\nFigure 8.1\\nA graph representing a\\nthree-coloring problem\\nA\\nB\\nC\\nD\\nFigure 8.2\\nGraph that cannot be\\nthree-colored\\nA slightly more general version of the three-coloring problem for maps is\\nto represent the countries in the three-coloring problem as nodes on a\\ngraph, as in Figure 8.1. The problem now is to assign values from a set of\\nthree possible values to each of the nodes in the graph, such that no two\\nnodes that are joined by an edge have been assigned the same value.\\nFor example, in the graph in Figure 8.1, the following assignments would\\nbe a suitable three-coloring:\\nA = red\\nB = green\\nC = blue\\nD = green\\nNote that in the graph shown in Figure 8.2, no suitable three-coloring exists.\\nAlso note that some graphs do not represent maps of countries, as in the\\ncase of a graph with five nodes where every pair of nodes is connected by\\nan edge. However, any map of countries can be represented by a graph, and\\nsolving the three-coloring problem for that graph is equivalent to solving\\nthe three-coloring problem for the map.\\nThe three-coloring problem for graphs can be solved using resolution and\\npropositional logic, by representing the graph as a set of clauses and deter-\\nmining whether the clauses can be satisfied.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 244, 'page_label': '245'}, page_content='218 CHAPTER 8 Inference and Resolution for Problem Solving\\nThe representation is generated as follows:\\nFirst we can represent which color has been assigned to a vertex, as follows:\\nAr means that vertex A has been colored red.\\nAg means that vertex A has been colored green.\\nAb means that vertex A has been colored blue.\\nAnd so on for all vertices.\\nHence, for each vertex, we can generate the following set of clauses to repre-\\nsent that it must be given a color, but cannot be given more than one color:\\nAr ∨ Ag ∨ Ab\\n¬Ar ∨¬ Ag (/H11013Ar → ¬Ag)\\n¬Ag ∨¬ Ab\\n¬Ab ∨¬ Ar\\nSimilarly, for each edge, we can represent the fact that the two nodes at\\neither end of the edge must have different colors:\\nIf (A,B) is an edge, then:\\n¬Ar ∨¬ Br\\n¬Ab ∨¬ Bb\\n¬Ag ∨¬ Bg\\nNow, if we ∧ these sets of clauses together, and apply resolution to the\\nresult, we can see if the set is satisfiable. If so, then a three-coloring solution\\ndoes exist for the graph. If not, then it does not.\\nIn the same way, any instance of a combinatorial search problem can be\\nrepresented as a set of clauses, and their satisfiability can be tested using\\nresolution. This method tells us if a solution exists but does not tell us what\\nthat solution is. We have already seen in Part 2 of this book how solutions\\nto such problems can be found using search.\\n8.4 Resolution in Predicate Logic\\nResolution as applied to propositional logic can also be applied to FOPL by\\nreducing FOPL expressions to a suitable normal form. The methods used\\nto carry out this process are described in the following sections.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 245, 'page_label': '246'}, page_content='8.5 Normal Forms for Predicate Logic 219\\n8.5 Normal Forms for Predicate Logic\\nT o apply resolution to FOPL expressions, we first need to deal with the\\npresence of the quantifiers ∀ and ∃. The method that is used is to move\\nthese quantifiers to the beginning of the expression, resulting in an expres-\\nsion that is in prenex normal form.\\nIn converting a wff to prenex normal form, we use the same rules as we\\nused to convert a wff to CNF:\\n1. A ↔ B /H11013(A → B)\\n∧ (B → A)\\n2. A → B /H11013¬A ∨ B\\n3. ¬(A ∧ B) /H11013¬A ∨¬ B\\n4. ¬(A ∨ B) /H11013¬A ∧¬ B\\n5. ¬¬A /H11013A\\n6. A ∨ (B ∧ C) /H11013(A ∨ B) ∧ (A ∨ C)\\nIn addition, we use the following rules to move the quantifiers to the front:\\n7. ¬(∀x)A(x) /H11013(∃x)¬A(x)\\n8. ¬(∃x)A(x) /H11013(∀x)¬A(x)\\n9. ( ∀x)A(x) ∧ B /H11013(∀x)(A(x) ∧ B)\\n10. ( ∀x)A(x) ∨ B /H11013(∀x)(A(x) ∨ B)\\n11. ( ∃x)A(x) ∧ B /H11013(∃x)(A(x) ∧ B)\\n12. ( ∃x)A(x) ∨ B /H11013(∃x)(A(x) ∨ B)\\n13. ( ∀x)A(x) ∧ (∀y)B(y) /H11013(∀x)(∀y)(A(x) ∧ B(y))\\n14. ( ∀x)A(x) ∧ (∃ y)B(y) /H11013(∀x)(∃y)(A(x) ∧ B(y))\\n15. ( ∃x)A(x) ∧ (∀y)B(y) /H11013(∃x)(∀y)(A(x) ∧ B(y))\\n16. ( ∃x)A(x) ∧ (∃y)B(y) /H11013(∃x)(∃y)(A(x) ∧ B(y))\\nNote that rules 9, 10, 11, and 12 can be used only if x does not occur in B.\\nLet us briefly examine why some of these rules make logical sense. Rule 7,\\nfor example, can be stated in layman’s terms as “if it is not true that all x’s\\nhave property A, then there must exist some x for which A is not true. ” This\\nis clearly valid.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 246, 'page_label': '247'}, page_content='220 CHAPTER 8 Inference and Resolution for Problem Solving\\nSimilarly, rule 8 states “if there does not exist an x for which A is true, then\\nA is not true for any x.”\\nRules 9 through 12 take advantage of the fact that x is not present in B.\\nHence, if B is true, then it is also true for all x:\\nB →∀ xB\\nprovided x is not free in B.\\nRules 13 through 16 similarly take advantage of the fact that x is not pres-\\nent in B(y) and that y is not present in A(x).\\nA further set of rules (17–20) can be generated by replacing ∧ with ∨ in\\nrules 13 through 16.\\nFor example, let us convert the following wff to prenex normal form:\\n(∀x)(A(x) → B(x)) → (∃y)(A(y) ∧ B(y))\\n¬(∀x)(A(x) → B(x)) ∨ (∃y)(A(y) ∧ B(y)) (2)\\n¬(∀x)(¬A(x) ∨ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (2)\\n(∃x)¬(¬A(x) ∨ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (7)\\n(∃x)(¬¬A(x) ∧¬ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (4)\\n(∃x)(A(x) ∧¬ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (5)\\n(∃x)(∃y)((A(x) ∧¬ B(x)) ∨ (A(y) ∧ B(y))) (19)\\n(∃x)(∃y)(((A(x) ∨ A(y)) ∧ (¬B(x) ∨ A(y)) ∧ (A(x) ∨ B(y)) ∧\\n(¬B(x) ∨ B(y)))) (6)\\n8.6 Skolemization\\nBefore resolution can be carried out on a wff, we need to eliminate all the\\nexistential quantifiers, ∃, from the wff.\\nThis is done by replacing a variable that is existentially quantified by a con-\\nstant, as in the following case:\\n∃(x) P(x)\\nwould be converted to\\nP(c)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 247, 'page_label': '248'}, page_content='8.6 Skolemization 221\\nwhere c is a constant that has not been used elsewhere in the wff . Although\\nP(c) is not logically equivalent to ∃(x) P(x), we are able to make this substi-\\ntution in the process of resolution because we are interested in seeing\\nwhether a solution exists. If there exists some x for which P(x) holds, then\\nwe may as well select such an x and name it c. This process is called skolem-\\nization, and the variable c is called a skolem constant.\\nThe variable c can be thought of as an example of a suitable value for x. It is\\nextremely important that c not appear anywhere else in the expression\\nbecause that would create a conflict. Imagine, for example, replacing x with\\nb in the following expression:\\n∃x(x\\n∨ b)\\nThis would leave us with\\nb ∨ b\\nwhich reduces to\\nb\\nThis clearly is not the same expression, and, in fact, it should be skolemized\\nusing a different constant, such as:\\nc ∨ b\\nSkolemization must proceed slightly differently in cases where the ∃ follows\\na ∀ quantifier, as in the following example:\\n(∀x)(∃y)(P(x,y))\\nIn this case, rather than replacing y with a skolem constant, we must replace\\nit with a skolem function, such as in the following:\\n(∀x)(P(x,f(x))\\nNote that the skolem function is a function of the variable that is univer-\\nsally quantified, in this case, x.\\nHaving removed the existential quantifiers in this way, the wff is said to be\\nin skolem normal form, and has been skolemized.\\n8.6.1 Example of Skolemization\\nThe following expression\\n(∀x)(∃y)(∀z)(P(x) ∧ Q(y, z))'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 248, 'page_label': '249'}, page_content='222 CHAPTER 8 Inference and Resolution for Problem Solving\\nwould be skolemized as follows:\\n(∀x)(∀z)(P(x) ∧ Q(f(x), z)\\nNote that y has been replaced by f(x) because ∃y occurred after ∀x.\\n8.6.2 Second Example of Skolemization\\nThe following expression:\\n(∀w)(∀x)(∃y)(∀z)(P(x) ∧ Q(w, y, z))\\nwould be skolemized as follows:\\n(∀w)(∀x)(∀z)(P(x) ∧ Q(w, f(w,x), z)\\nHere y has been replaced by a function of both w and x: f(w,x) because ∃y\\noccurred after ∀w and ∀x.\\nT o proceed with resolution, this wff must now be represented as set of\\nclauses. T o do this, we first drop the universal quantifiers. Hence, our\\nexpression above will be represented as the following set of two clauses:\\n{(P(x)), (Q(w, f(w,x), z))}\\n8.6.3 Unification\\nT o carry out resolution on wff’s in FOPL, we need to carry out one final\\nstage, which is to make substitutions.\\nFor example, if we had the following set of clauses:\\n{(P(w,x)), (¬P(y,z))}\\nIt seems clear that we should be able to resolve P with ¬P, but they are not\\ncurrently identical because they have different arguments. These clauses\\ncan be resolved by making a substitution. We will replace w with y and x\\nwith z, to result in the following clauses:\\n{(P(y,z)), (\\n¬P(y,z))}\\nThese can now clearly be resolved to give falsum.\\nThe substitution that was made here can be written as\\n{y/w, z/x}\\nIn this case, it was easy to see which substitution to make, but with more\\ncomplex clauses, it can be harder. A formal process exists for determining'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 249, 'page_label': '250'}, page_content='8.6 Skolemization 223\\nhow to make these substitutions, which of course means the process can be\\nautomated.\\nIn general, we use the symbol /H9268to indicate a substitution, and we can write\\n/H9268= {y/w, z/x}\\nA = P(w,x)\\nB = \\n¬P(y,z)\\nA/H9268= P(y,z)\\nB/H9268= ¬P(y,z)\\nIn general, if a substitution can be applied to a set of clauses { A, B, C,\\n...}s u c h that\\nA/H9268= B/H9268= C/H9268=  ...\\nThen /H9268is called a unifier for the set {A, B, C,...} .\\nIn some cases, more than one substitution needs to be applied to produce a\\nform that can be resolved. The operator, o, can be applied to two substitu-\\ntions to provide thecomposition of those two substitutions, which produces\\na third substitution that is effectively the same as applying both substitutions.\\nLet us take two substitutions /H92681 and /H92682, defined as follows:\\n/H92681 = {a\\n1/x1,a 2/x2,...,a m/xm}\\n/H92682 = {b1/y1,b 2/y2,...,b n/yn}\\nNow we define the composition of these two substitutions as follows:\\n/H92681 o /H92682 = {a1/H92682/x1,a 2/H92682/x2,...,a n/H92682/xn, b1/y1,b 2/y2,...,b n/yn}\\nCertain elements on the composite set can be eliminated:\\nIf yi = xj then bi/yi can be eliminated.\\nIf ai/H92682 = xi then ai/H92682/xi can be eliminated.\\nFor example, let us form the composition of the following two substitutions:\\n/H92681 = {a/x, x/y, f(a)/z}\\n/H92682 = {y/x, f(z)/y}\\n/H92681 o /H92682 = {a/H92682/x, x/H92682/y, f(a)/H92682/z, y/x, f(z)/y}\\n= {a/x, y/y, f(a)/z, y/x, f(z)/y}\\n= {a/x, f(a)/z, f(z)/y}'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 250, 'page_label': '251'}, page_content='224 CHAPTER 8 Inference and Resolution for Problem Solving\\nNotice that y/y is removed because this is a form of ai/H92682 = xi and that y/x is\\nremoved because this matches the elimination rule where yi = xj.\\nNow let us find /H92682 o /H92681:\\n/H92681 = {a/x, x/y, f(a)/z}\\n/H92682 = {y/x, f(z)/y}\\n/H92682 o /H92681 = {y/H92681/x, f(z)/H92681/y, a/x, x/y, f(a)/z}\\n= {x/x, f(f(a))/y, a/x, x/y, f(a)/z}\\n= {f(f(a))/y, f(a)/z}\\nHence, the o operator is not commutative, as /H92681 o /H92682 ≠ /H92682 o /H92681.\\nSome rules determine the way in which unifiers can be applied:\\n1. A constant, such as a, cannot be replaced by a variable.\\n2. A variable x cannot be replaced by a term that contains x.\\nHence, for example, the following substitutions are not valid unifiers:\\n{P(x)/x}\\n{x/a}\\n8.6.4 Most General Unifiers\\nA unifier u1 is called a most general unifier for a set S = {A, B, C,...}  i f\\nany other unifier u2 can be expressed as the composition of u 1 with some\\nother substitution (i.e., u2 = u1 o u3).\\nA most general unifier ( mgu) is a unique unifier that provides the most\\ngeneral set of substitutions to unify a set of clauses. By most general, we\\nmean where possible variables are used in place of constants because con-\\nstants are specific and variables are general.\\n8.6.5 Unification Algorithm\\nT o automate the process of resolution in FOPL, we need an algorithm for\\ngenerating a most general unifier, in order to put clauses in a form that can\\nbe resolved.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 251, 'page_label': '252'}, page_content='8.6 Skolemization 225\\nThe unification algorithm is expressed as follows:\\nT o unify a set of clauses,S0:\\nInitialize: /H92680 = {}\\ni = 0\\nLoop: If Si has only one element, terminate and report that /H9268i is\\nthe mgu for S0.\\nIf Si has more than one element, find the disagreement set\\nDi of Si (i.e., the substitutions that need to be made).\\nIf Di contains a variable x and a term t where x is not con-\\ntained within t, then we say:\\n/H9268i/H110011= /H9268i o {t/x}\\nSi/H110011 = Si {t/x}\\nIncrement i, and repeat the loop.\\n8.6.6 Unification Example\\nLet us find the mgu for the following set of clauses:\\nS0 = {P(a, x, y, z), P (x, y, z, w)}\\n(a is a constant, and x, y, z are variables).\\nWe initialize /H92680 = {} and i = 0.\\nNow we proceed as follows:\\nD0 = {a,x}\\n/H92681= /H9268o o {a/x} = {a/x}\\nS1 = So {a/x} = {P(a, a, y, z), P (a, y, z, w)}\\nD1 = {a,y}\\n/H92682 = {a/x} o {a/y} = {a/x, a/y}\\nS2 = S1 {a/x, a/y} = {P(a, a, a, z), P (a, a, z, w)}\\nD2 = {a, z}\\n/H92683 = {a/x, a/y} o {a/z} = {a/x, a/y, a/z}\\nS3 = S2 {a/x, a/y, a/z} = {P(a, a, a, a), P(a, a, a, w)}'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 252, 'page_label': '253'}, page_content='226 CHAPTER 8 Inference and Resolution for Problem Solving\\nD3 = {a, w}\\n/H92684 = {a/x, a/y, a/z} o {a/w} = {a/x, a/y, a/z, a/w}\\nS4 = S3 {a/x, a/y, a/z, a/w} = {P(a, a, a, a), P(a, a, a, a)}\\n= {P(a, a, a, a)}\\nNow we stop because S4 has just one element, and we have found a mgu,/H92684,\\nwhich is {a/x, a/y, a/z, a/w}.\\nThe following is also a unifier of S0:\\n/H92685 = {x/a, x/y, x/z, x/w}\\nHowever, this is not a mgu because we can find another substitution /H92686\\nsuch that the composition /H92685 o /H92686 gives the mgu, /H92684:\\n{x/a, x/y, x/z, x/w} o {a/x}\\ngives\\n{x{a/x}/a, x{a/x}/y, x{a/x}/z, x{a/x}/w}\\nwhich in turn leads to\\n{a/a, a/x, a/y, a/z, a/w}\\nWe can eliminate a/a, which gives us\\n{a/x, a/y, a/z, a/w}\\nwhich is the mgu /H92684. because /H92684 is a mgu, we could find another substitu-\\ntion /H9268x for any other unifier /H9268u, such that /H9268u o /H9268x = /H92684.\\n8.7 Resolution Algorithm\\nNow we have all the tools we need to produce an automated system for gen-\\nerating proofs using resolution on FOPL expressions. Given a set of\\nassumptions and a conclusion, we can prove whether the assumption logi-\\ncally follows from the assumptions as follows:\\n■ First, negate the conclusion and add it to the list of assumptions.\\n■ Now convert the assumptions into prenex normal form.\\n■ Next, skolemize the resulting expression.\\n■ Now convert the expression into a set of clauses.\\nNow we resolve the clauses using the following method:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 253, 'page_label': '254'}, page_content='8.8 Horn Clauses and PROLOG 227\\nIf our clauses are {A, B, C,...}  a n d A has a literal LA and B has a literal LB\\nsuch that LA and ¬LB have a mgu /H9268, then we can resolve A and B to give the\\nresolvent of these clauses, which is:\\n(A/H9268/H11002LA/H9268) ⋃ (B/H9268/H11002LB/H9268)\\nwhere ∪ is the set union operator, and /H11002is the set difference operator. For\\nexample, in resolving the following clauses:\\n{A(x,y), B(f(y)), C(x, y, z)}\\n{A(f(x), z),\\n¬B(z), C(f(a), b, z)}\\nwe note that B(f(y)) and B(z) have a mgu that is { f(y)/z}. Hence, we apply\\nthis unifier to the two clauses to give:\\n{A(x,y), B(f(y)), C(x, y, z)}{f(y)/z} /H11002B(f(y)){f(y)/z}\\n⋃ (A(f(x), z), ¬B(z), C(f(a), b, z)} /H11002¬B(z){f(y)/z}\\n= {A(x,y), C(x, y, z)} U {A(f(x), z), C(f(a), b, z)}\\n= {A(x,y), C(x, y, z), A(f(x), z), C(f(a), b, z)}\\nIn this way, we have removed the two literals \\n¬B(z) and B(f(y)), and by con-\\ntinuing this process until no further literals can be resolved, a set of clauses\\ncan be tested for contradiction. Note that each stage of this process can be\\nexpressed as an algorithm, and so the process of resolution can be used by\\ncomputers to prove the validity of deductions in FOPL.\\n8.8 Horn Clauses and PROLOG\\nA Horn clause,o r  Horn sentence, is a clause that has, at most, one positive\\nliteral. Hence, the following Horn clause takes the following form:\\nA ∨¬ B ∨¬ C ∨¬ D ∨¬ E  ...\\nwhere A, B, C, D, E, and so on are positive literals.\\nThis Horn clause can also be written as an implication:\\nB ∧ C ∧ D ∧ E → A\\nIn the programming language PROLOG, this would be written in reverse,\\nwith the conclusion on the left, as follows:\\nA :− B, C, D, E\\nHorn clauses can take three forms. The type we have seen above, where there is\\none positive literal, and one or more negative literals is called arule relation.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 254, 'page_label': '255'}, page_content='228 CHAPTER 8 Inference and Resolution for Problem Solving\\nA clause with no negative literals is called a fact:\\nA :−\\nFinally, a clause with no positive literal is called a goal, or a headless clause:\\n:− B, C, D, E\\nUnfortunately, not all expressions can be represented as Horn clauses (e.g.,A ∨\\nB). However, this representation does have the benefit of efficiency, and it also\\nmeans that if a set of clauses has a contradiction, then resolution bydepth-first\\nsearch(see Chapter 4) is guaranteed to result in the empty clause\\n:−\\nthus proving the original assertion.\\nA program in PROLOG consists of a set of Horn clauses. PROLOG applies\\nunification and resolution to attempt to derive conclusions from a set of\\nrules that are defined in the language.\\nIn fact, the programmer is able to define rules and facts, and to set up a\\ngoal that the PROLOG system must attempt to prove, using unification and\\nresolution.\\nRules and their uses in programming are briefly mentioned in Chapter 3\\nand are examined in more detail in Chapter 9.\\nThe rule for resolution in PROLOG can be expressed as follows:\\nHence, for example, a PROLOG programmer might start by establishing\\nthe following facts:\\nspeaks (Bob, French).\\nspoken_in (French, France).\\nlocated_in (Task1, France).\\nNext, the programmer writes a rule:\\nassign_task (X, P) \\n:− located_in (X, C),\\nspoken_in (L, C),\\nspeaks (P , L)\\nD :— A,  B,          G:— D,  E,  \\nG :— A,  B,  E,  \\nKK\\nK'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 255, 'page_label': '256'}, page_content='8.9 Herbrand Universes 229\\nFinally, the goal is set up:\\nassign_task (Task1, Bob)\\nThis is simply asking, should Task1 be assigned to Bob?\\nFirst, the system needs to use unification to be able to carry out resolution.\\nClearly, our goal could be resolved with the rule if we made the following\\nsubstitution:\\n{Task1 / X, Bob / P}\\nAfter applying this substitution and resolution, we are left with a new goal:\\n:− located_in (Task1, C),\\nspoken_in (L, C),\\nspeaks (Bob, L)\\nNow we apply a further substitution:\\n{France / C, French / L}\\nwhich enables us to resolve this new goal with the three established facts to\\nresult in the empty clause:\\n:−\\nHence, the goal has been proved.\\n8.9 Herbrand Universes\\nFor a set of clauses,S, the Herbrand universe, HS, is defined as being the set\\nof constants that are contained within S, and the set of functions in S\\napplied to those constants. These constants and functions are known as\\nground terms because they do not contain variables.\\nFor example,\\nS is {{A(x), B(y, a), C(z)}, {D(x, a, b),\\n¬E(y, c, b)}}\\nthen HS, the Herbrand universe of S, is defined as follows:\\nHS = {a, b, c}\\nBecause there are no functions in S, the Herbrand universe consists just of\\nthe constants in S, which are a, b, and c.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 256, 'page_label': '257'}, page_content='230 CHAPTER 8 Inference and Resolution for Problem Solving\\nA further example follows:\\nS is {{A(x), B(y, a), C(z)}, {D(x, a, b),¬E(y, f(x, y))}}\\nIn this case, where S contains a function, the Herbrand universe is infinite:\\nHS = {a, b, c, f(a, a), f(a, b), f (b, a), f(b, b), f(f(a), a), f(f(a), f(a)) ...}\\nIn the following case, S contains no functions or constants:\\nS is {{A(x), B(y, z), C(z)}, {D(x, y, z), ¬E(y, x)}}\\nIn such cases, we define the Herbrand universe to contain the constant a:\\nHS = {a}\\nA ground instance of a clause in S is a version of that clause in which any\\nvariables it contains have been replaced by ground terms from HS.\\nFor instance, given the following definition of S:\\nS is {{A(x), B(y, a), C(z)}, {D(x, a, b),¬E(y, c, b)}}\\na ground instance of the first clause could be\\n{A(a), B(b, a), C(a)}\\nAnother ground instance of the same clause is\\n{A(c), B(a, a), C(c)}\\n8.9.1 The Herbrand Base\\nThe Herbrand base of S, HS(S), is defined as the set of ground atoms that\\ncan be obtained by replacing variables in S by members of HS. A ground\\natom is a formula that contains no variables, only ground terms.\\nFor example,\\nS is {{A(x), B(y, a), C(z)}, {D(x, a, b), ¬E(y, c, b)}}\\nthen\\nHS = {a, b, c}\\nand\\nHS(S) = {{A(a), B(a, a), C(a), D(a, a, b), ¬E(a, c, b),\\nA(b), B(b, a), C(b), D(b, a, b), ¬E(b, c, b),\\n... }'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 257, 'page_label': '258'}, page_content='8.9 Herbrand Universes 231\\nIn this case, where there are no functions in S, the Herbrand base is finite\\n(although it will consist of a large number of clauses). As with the Her-\\nbrand universe, if the clauses contain functions, then the Herbrand base\\nwill be infinite.\\n8.9.2 Herbrand Interpretations\\nA Herbrand interpretation for S is defined as a set of assignments of true\\nand false to the elements of the Herbrand base, HS(S).\\nFor example,\\nS is {{A(x, y), B(a, x)}, {¬C(z, a), D(a, z)}\\nHS = {a}\\nHS(S) = {A(a, a), B(a, a), C(a, a), D(a, a)}\\nThen two possible Herbrand interpretations of S are\\n{¬A(a, a), B(a, a), C(a, a), D(a, a)}\\n{A(a, a), ¬B(a, a), C(a, a),¬D(a, a)}\\nIn the first interpretation, all the elements of HS(S) have been assigned the\\nvalue true, apart from A(a, a), which has been assigned false. In general, if\\nwe assign a value true to A, we write A, and if we assign the value false to A,\\nwe write ¬A. In the second interpretation, the second and fourth elements\\nof HS(S) have been assigned the value false, and the first and third have\\nbeen assigned the value true.\\nFor this set, S, there will be 16 possible Herbrand interpretations because\\nthere are four elements in the Herbrand base. In general, if there are n ele-\\nments in the Herbrand base, there will be 2n Herbrand interpretations.\\nNow we come back to the idea of satisfiability. In some cases, a given Her-\\nbrand interpretation of a set S will be set to satisfy S.\\nLet us use our previous example again:\\nS is {{A(x, y), B(a, x)}, {¬C(z, a), D(a, z)}\\nWe presented two Herbrand interpretations for this S:\\n{¬A(a, a), B(a, a), C(a, a), D(a, a)}\\n{A(a, a), ¬B(a, a), C(a, a),¬D(a, a)}\\nNow we say that a given interpretation for S satisfies S if the assignments\\nfrom the interpretation make each clause in S true.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 258, 'page_label': '259'}, page_content='232 CHAPTER 8 Inference and Resolution for Problem Solving\\nFor example, replacing the variables in S with the constants from the Her-\\nbrand universe, we get the following:\\n{{A(a, a), B(a, a)}, {¬C(a, a), D(a, a)}\\nNow, if we use the first interpretation where A(a, a) was the only element\\nthat had been assigned the value false, we see that the clauses become\\n{{false, true}, {false, true}}\\nNow, recall that clauses are a set of conjunctions of disjunctions, and so we\\ncan rewrite this as\\n(false ∨ true) ∧ (false ∨ true)\\nwhich clearly is true. Hence, this interpretation satisfies S.\\nUsing the second interpretation, the clauses become\\n{{true, false}, {false, false}}\\nwhich is not true. So the second interpretation does not satisfy S.\\nIt can be shown that if no Herbrand interpretation exists for a set of clauses\\nS that satisfies S, then S is not satisfiable. This is, in fact, the basis for reso-\\nlution because it can further be shown that if a set of clauses is unsatisfi-\\nable, then resolving that set of clauses will lead to falsum.\\n8.9.3 Example\\nWe have seen that the satisfiability of a set of clauses can be proved or dis-\\nproved by examining the Herbrand interpretations for the set. We now\\npresent an example:\\nS = {{\\n¬A(y, a), B(y)}, {¬B(x)}, {B(a), A(x, a)}}\\nThen we can define the Herbrand universe as\\nHS = {a}\\nNext, we define the Herbrand base:\\nHS(S) = {A(a, a), B(a)}'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 259, 'page_label': '260'}, page_content='8.10 Resolution for Problem Solving 233\\nThus, there are four possible Herbrand interpretations for S:\\n1. {A(a, a), B(a)}\\n2. {A(a, a),\\n¬B(a)}\\n3. { ¬A(a, a), B (a)}\\n4. { ¬A(a, a), ¬B(a)}\\nY ou should be able to see that none of these interpretations satisfies S.\\nHence, we have proved that S is unsatisfiable.\\n8.10 Resolution for Problem Solving\\nAlthough it seems complex, resolution is made up a series of simple steps.\\nBecause each of those steps can be readily automated, resolution is widely\\nused. As has already been explained, PROLOG systems use resolution to\\nsolve problems.\\nLet us now see how resolution can be used to solve a simple logic problem.\\nConsider the following set of premises:\\n1. Some children will eat any food.\\n2. No children will eat food that is green.\\n3. All children like food made by Cadbury’s.\\nWe now wish to prove that the following conclusion follows from these\\npremises:\\nNo food made by Cadbury’s is green.\\nFirst, we need to represent the premises and the conclusion in predicate\\ncalculus. We will use the following symbols:\\nC(x) means “x is a child. ”\\nF(x) means “x is food. ”\\nL(x, y) means “x likes y. ”\\nG(x) means “x is green. ”\\nM(x, y) means “x makes y. ”\\nc means “Cadbury’s. ”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 260, 'page_label': '261'}, page_content='234 CHAPTER 8 Inference and Resolution for Problem Solving\\nSo our premises can be represented as:\\n1. ( ∃x)(C(x) ∧ (∀y)(F(y) → L(x,y)))\\n2. ( ∀x)(C(x) → (∀y)((F(y) ∧ G(y)) → ¬L(x,y)))\\n3. ( ∀x)((F(x) ∧ M(c,x)) → (∀y)(C(y) → L(y,x)))\\nOur conclusion can be represented as follows:\\n(∀x)((F(x) ∧ M(c,x)) → ¬G(x))\\nFirst, we must negate the conclusion and add it to the set of premises,\\nwhich means we must now prove that the following expression cannot be\\nsatisfied:\\n(∃x)(C(x)\\n∧ (∀y)(F(y) → L(x,y)))\\n∧ (∀x)(C(x) → (∀y)((F(y) ∧ G(y)) → ¬L(x,y)))\\n∧ (∀x)((F(x) ∧ M(c,x)) → (∀y)(C(y) → L(y,x)))\\n∧¬ ((∀x)((F(x) ∧ M(c,x)) → ¬G(x)))\\nWe will convert this into a set of clauses, starting with expression 1:\\n(∃x)(C(x) ∧ (∀y)(F(y) → L(x,y)))\\n→ must be eliminated first:\\n(∃x)(C(x) ∧ (∀y)(¬F(y) ∨ L(x,y)))\\nNext, we bring the quantifiers to the front of the expression:\\n(∃x)(∀y)(C(x) ∧ (¬F(y) ∨ L(x,y)))\\nNow we skolemize this expression, to eliminate the existential quantifier:\\n(∀y)(C(a) ∧ (¬F(y) ∨ L(a,y)))\\nThis can be expressed as the following clauses:\\n{C(a)}, {¬F(y), L(a,y)}\\nNext, we deal with expression 2 in the same way:\\n(∀x)(C(x) → (∀y)((F(y) ∧ G(y)) → ¬L(x,y)))\\n→ is eliminated first:\\n(∀x)(¬C(x) ∨ (∀y)(¬(F(y) ∧ G(y)) ∨¬ L(x,y)))'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 261, 'page_label': '262'}, page_content='8.10 Resolution for Problem Solving 235\\nNow DeMorgan’s law is applied:\\n(∀x)(¬C(x) ∨ (∀y)(¬F(y) ∨¬ G(y) ∨¬ L(x,y)))\\nQuantifiers are moved to the front:\\n(∀x)(∀y)(¬C(x) ∨¬ F(y) ∨¬ G(y) ∨¬ L(x,y))\\nThis can be written as the following single clause:\\n{¬C(x), ¬F(y), ¬(G(y), ¬L(x,y)}\\nNow, for expression 3:\\n(∀x)((F(x) ∧ M(c,x)) → (∀y)(C(y) → L(y,x)))\\nWe first eliminate →:\\n(∀x)(¬(F(x) ∧ M(c,x)) ∨ (∀y)(¬C(y) ∨ L(y,x)))\\nNext, we apply DeMorgan’s law:\\n(∀x)(¬F(x) ∨¬ M(c,x) ∨ (∀y)(¬C(y) ∨ L(y,x)))\\nNow we bring the quantifiers to the front of the expression:\\n(∀x)(∀y)(¬F(x) ∨¬ M(c,x) ∨¬ C(y) ∨ L(y,x))\\nThis can be expressed as the following single clause:\\n{¬F(x), ¬M(c,x), ¬C(y), L(y,x)}\\nNow we deal with the conclusion, which has been negated:\\n¬(∀x)((F(x) ∧ M(c,x)) → ¬G(x))\\nFirst, we eliminate →:\\n¬(∀x)(¬(F(x) ∧ M(c,x)) ∨¬ G(x))\\nNow we apply the quantifier equivalence to move the ¬ from the front of\\nthe expression:\\n(∃x)¬(¬(F(x) ∧ M(c,x)) ∨¬ G(x))\\nDeMorgan’s law can now be applied:\\n(∃x)(¬¬(F(x) ∧ M(c,x)) ∧¬ ¬G(x))\\nWe can now remove ¬¬:\\n(∃x)(F(x) ∧ M(c,x) ∧ G(x))'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 262, 'page_label': '263'}, page_content='236 CHAPTER 8 Inference and Resolution for Problem Solving\\nThis expression is now skolemized:\\nF(b) ∧ M(c,b) ∧ G(b))\\nThis can be expressed as the following set of clauses:\\n{{F(b)}, {M(c,b)}, {G(b)}}\\nNow we have arrived at a set of clauses, upon which resolution can be\\napplied. The clauses we have are the following:\\n1. {C(a)}\\n2. { ¬F(y), L(a,y)}\\n3. { ¬C(x), ¬F(y), ¬(G(y), ¬L(x,y)}\\n4. { ¬F(x), ¬M(c,x), ¬C(y), L(y,x)}\\n5. {F(b)}\\n6. {M(c,b)}\\n7. {G(b)}\\nWe now apply resolution as follows:\\nFirst, we unify lines 1 and 3 using {a/x} and resolve these two, to give\\n8. {\\n¬F(y), ¬(G(y), ¬L(a,y)}\\nSimilarly, the unifier {b/y} can be applied to lines 2 and 5, which are then\\nresolved to give\\n9. {L(a,b)}\\nNow we apply {b/y} to resolve line 5 with line 8 to give\\n10. { ¬(G(b), ¬L(a,b)}\\nNow lines 9 and 10 can be resolved to give\\n11. { ¬(G(b)}\\nFinally, line 7 can be resolved with line 11 to give\\n12. ⊥\\nHence, we have proven that the set of clauses derived from the premises 1,\\n2, and 3, and the negation of the conclusion, are unsatisfiable. Thus we have\\nsuccessfully proved that the conclusion does indeed follow from the prem-\\nises, and so the argument is a valid one.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 263, 'page_label': '264'}, page_content='8.11 Chapter Summary 237\\n8.11 Chapter Summary\\n■ Any well-formed formula (wff) can be expressed in conjunctive\\nnormal form (CNF) or disjunctive normal form (DNF). An\\nexpression in CNF is a conjunction of disjunctions, and an expres-\\nsion in DNF is a disjunction of conjunctions.\\n■ An algorithm can be generated that will convert any wff into CNF.\\n■ The resolution rule says that if you know ( A ∨ B) and you know\\n(¬B ∨ C), then you can eliminate the instances of B from these two\\nexpressions, to produce (A ∨ C).\\n■ By negating a conclusion, and proving that the resultant set of\\nexpressions is unsatisfiable, one can prove that an argument is\\nvalid. Such reasoning is called proof by refutation (or proof by\\ncontradiction). The traditional method for using resolution is to\\nprove arguments valid by refutation.\\n■ Any combinatorial problem can be represented as a set of clauses,\\nand the existence of a solution to the problem can be determined\\nusing resolution on those clauses.\\n■ T o apply resolution in first-order predicate logic, an expression\\nneeds to be first converted to prenex normal form (where the\\nquantifiers are at the beginning) and then skolemized, which\\ninvolves removing variables that are existentially quantified by\\nreplacing them with constants and functions.\\n■ Unification involves using a unifier to resolve two similar clauses\\nthat do not have the same variables. An algorithm can be generated\\nto unify any sets of clauses that can be unified.\\n■ A unifier u1 is a most general unifier if any other unifier, u2, can\\nbe expressed as the composition of u1 with another unifier, u3: u2\\n= u1 o u3.\\n■ Resolution can be applied to a set of clauses that have been skolem-\\nized. This process can be automated because each step can be\\nexpressed algorithmically.\\n■ A Horn clause is one with, at most, one positive literal. PROLOG\\nuses resolution on Horn clauses to solve problems.\\n■ If no Herbrand interpretation exists for a set of clauses that satis-\\nfies that set, then the clauses are not satisfiable.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 264, 'page_label': '265'}, page_content='238 CHAPTER 8 Inference and Resolution for Problem Solving\\n8.12 Review Questions\\n8.1 Explain the concept of proof by refutation.\\n8.2 Explain how and to what extent combinatorial problems can be\\nsolved using resolution.\\n8.3 Explain what is meant by prenex normal form and skolem normal\\nform.\\n8.4 Explain each step of the algorithm for resolution in first-order\\npredicate logic.\\n8.5 Explain the following terms:\\n■ Herbrand universe\\n■ Herbrand base\\n■ Herbrand interpretation\\n8.13 Exercises\\n8.1 Convert the following expression to CNF and to DNF:\\nA ∨ (B ∧ C) ∨ (D ∧ E ∧¬ (A ∨ B))\\n8.2 Convert the following expressions to a set of clauses:\\n(∀x)(P(x) → (A(x) ∧ B(x) ∨¬ C(x, a))\\n(∃y)(Q(y, a) ∧ ((∀z) A(z) → ¬ B(y))).\\n8.3 Prove that the resolution rule is valid.\\n8.4 Generate the full set of clauses for the map-coloring graph in Fig-\\nure 8.1. Resolve these clauses to prove that a three-coloring solu-\\ntion does exist for the graph.\\n8.5 Use resolution to determine whether the following is valid:\\n(((∀x)(A (x) → B(x)))\\n∧ (¬ B(x))) → ¬ A(x)\\n8.6 Use resolution to determine whether the following is valid:\\n(∀x)(∃y) (((A (x) ∧ B(y)) → (A (y) ∧ B(x))) → (A(x) → B(x)))\\n8.7 Use resolution to prove that the following logical argument,\\ndevised by Lewis Carroll, is valid:\\nNo shark ever doubts that it is well fitted out'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 265, 'page_label': '266'}, page_content='8.14 Further Reading 239\\nA fish, that cannot dance a minuet, is contemptible\\nNo fish is quite certain that it is well fitted out, unless it has three rows\\nof teeth\\nAll fishes, except sharks, are kind to children\\nNo heavy fish can dance a minuet\\nA fish with three rows of teeth is not to be despised\\nConclusion:\\nNo heavy fish is unkind to children\\n8.14 Further Reading\\nResolution is covered by most of the standard Artificial Intelligence texts,\\nbut you may need to look in the specialized books such as Chang and Lee\\n(1973) to find deeper coverage of the subject. Reeves (1990) provides a\\ngood introduction to resolution.\\nA Resolution Principle for a Logic With Restricted Quantifiers by H. J. Burck-\\nert (1992 – Springer V erlag)\\nThe Resolution Calculus by Alexander Leitsch (1997 – Springer V erlag)\\nAutomated Theorem Proving: A Logical Basis by Donald W. Loveland (1978\\n– Elsevier Science – out of print)\\nAutomated Theorem Proving: Theory and Practice by Monty Newborn\\n(2001 – Springer V erlag)\\nLogic, Form and Function: The Mechanization of Deductive Reasoning by\\nJohn Alan Robinson (1980 – Elsevier Science)\\nUsing Sophisticated Models in Resolution Theorem Proving (Lecture Notes in\\nComputer Science, Vol. 90) by David M. Sandford (1981 – Springer V erlag)\\nResolution Proof Systems: An Algebraic Theory (Automated Reasoning Series,\\nVol. 4)by Zbigniew Stachniak (1996 – Kluwer Academic Publishers)\\nSymbolic Logic and Mechanical Theorem Proving by Chin-Liang Chang and\\nRichard Char-Tung Lee (1973 – Academic Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 266, 'page_label': '267'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 267, 'page_label': '268'}, page_content='9CHAPTER\\nRules and Expert Systems\\nAny problem that can be solved by your in-house expert in a 10–30 minute\\ntelephone call can be developed as an expert system.\\n—M. Firebaugh, Artificial Intelligence: A Knowledge-Based Approach\\n‘Rule Forty-two. All persons more than a mile high to leave the court. ’\\n‘That’s not a regular rule: you invented it just now. ’\\n‘It’s the oldest rule in the book, ’ said the King.\\n‘Then it ought to be number one, ’ said Alice.\\n—Lewis Carroll, Alice’s Adventures in Wonderland\\nThese so-called expert systems were often right, in the specific areas for which\\nthey had been built, but they were extremely brittle. Given even a simple prob-\\nlem just slightly beyond their expertise, they would usually get a wrong\\nanswer. Ask a medical program about a rusty old car, and it might blithely\\ndiagnose measles.\\n—Douglas B. Lenat, Programming Artificial Intelligence\\n9.1 Introduction\\nIn this chapter, we introduce the ideas behind production systems, or\\nexpert systems, and explain how they can be built using rule-based systems,\\nframes, or a combination of the two.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 268, 'page_label': '269'}, page_content='242 CHAPTER 9 Rules and Expert Systems\\nThis chapter explains techniques such as forward and backward chaining,\\nconflict resolution, and the Rete algorithm. It also explains the architecture\\nof an expert system and describes the roles of the individuals who are\\ninvolved in designing, building, and using expert systems.\\n9.2 Rules for Knowledge Representation\\nOne way to represent knowledge is by using rules that express what must\\nhappen or what does happen when certain conditions are met. Rules are\\nusually expressed in the form of IF . . . THEN . . . statements, such as:\\nIF A THEN B\\nThis can be considered to have a similar logical meaning as the following:\\nA → B\\nAs we saw in Chapter 7, A is called the antecedent and B is the consequent\\nin this statement. In expressing rules, the consequent usually takes the form\\nof an action or a conclusion. In other words, the purpose of a rule is usu-\\nally to tell a system (such as an expert system) what to do in certain cir-\\ncumstances, or what conclusions to draw from a set of inputs about the\\ncurrent situation.\\nIn general, a rule can have more than one antecedent, usually combined\\neither by AND or by OR (logically the same as the operators \\n∧ and ∨ we\\nsaw in Chapter 7). Similarly, a rule may have more than one consequent,\\nwhich usually suggests that there are multiple actions to be taken.\\nIn general, the antecedent of a rule compares anobject with a possiblevalue,\\nusing anoperator. For example, suitable antecedents in a rule might be\\nIF x > 3\\nIF name is “Bob”\\nIF weather is cold\\nHere, the objects being considered are x, name, and weather; the operators\\nare “>” and “is” , and the values are 3, “Bob, ” and cold. Note that an object is\\nnot necessarily an object in the real-world sense—the weather is not a real-\\nworld object, but rather a state or condition of the world. An object in this\\nsense is simply a variable that represents some physical object or state in the\\nreal world.\\nAn example of a rule might be'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 269, 'page_label': '270'}, page_content='9.3 Rule-Based Systems 243\\nIF name is “Bob”\\nAND weather is cold\\nTHEN tell Bob ‘Wear a coat’\\nThis is an example of a recommendation rule, which takes a set of inputs\\nand gives advice as a result. The conclusion of the rule is actually an action,\\nand the action takes the form of a recommendation to Bob that he should\\nwear a coat. In some cases, the rules provide more definite actions such as\\n“move left” or “close door, ” in which case the rules are being used to repre-\\nsent directives.\\nRules can also be used to represent relations such as:\\nIF temperature is below 0\\nTHEN weather is cold\\n9.3 Rule-Based Systems\\nRule-based systems or production systems are computer systems that use\\nrules to provide recommendations or diagnoses, or to determine a course\\nof action in a particular situation or to solve a particular problem.\\nA rule-based system consists of a number of components:\\n■ a database of rules (also called a knowledge base)\\n■ a database of facts\\n■ an interpreter,o r  inference engine\\nIn a rule-based system, the knowledge base consists of a set of rules that rep-\\nresent the knowledge that the system has. The database of facts represents\\ninputs to the system that are used to derive conclusions, or to cause actions.\\nThe interpreter, or inference engine, is the part of the system that controls\\nthe process of deriving conclusions. It uses the rules and facts, and com-\\nbines them together to draw conclusions.\\nAs we will see, these conclusions are often derived using deduction,\\nalthough there are other possible approaches. Using deduction to reach a\\nconclusion from a set of antecedents is called forward chaining. An alter-\\nnative method, backward chaining, starts from a conclusion and tries to\\nshow it by following a logical path backward from the conclusion to a set of\\nantecedents that are in the database of facts.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 270, 'page_label': '271'}, page_content='244 CHAPTER 9 Rules and Expert Systems\\n9.3.1 Forward Chaining\\nForward chaining employs the same deduction method that we saw in\\nChapter 7. In other words, the system starts from a set of facts, and a set of\\nrules, and tries to find a way of using those rules and facts to deduce a con-\\nclusion or come up with a suitable course of action.\\nThis is known as data-driven reasoning because the reasoning starts from\\na set of data and ends up at the goal, which is the conclusion.\\nWhen applying forward chaining, the first step is to take the facts in the fact\\ndatabase and see if any combination of these matches all the antecedents of\\none of the rules in the rule database. When all the antecedents of a rule are\\nmatched by facts in the database, then this rule is triggered. Usually, when\\na rule is triggered, it is then fired, which means its conclusion is added to\\nthe facts database. If the conclusion of the rule that has fired is an action or\\na recommendation, then the system may cause that action to take place or\\nthe recommendation to be made.\\nFor example, consider the following set of rules that is used to control an\\nelevator in a three-story building:\\nRule 1\\nIF on first floor and button is pressed on first floor\\nTHEN open door\\nRule 2\\nIF on first floor\\nAND button is pressed on second floor\\nTHEN go to second floor\\nRule 3\\nIF on first floor\\nAND button is pressed on third floor\\nTHEN go to third floor\\nRule 4\\nIF on second floor\\nAND button is pressed on first floor'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 271, 'page_label': '272'}, page_content='9.3 Rule-Based Systems 245\\nAND already going to third floor\\nTHEN remember to go to first floor later\\nThis represents just a subset of the rules that would be needed, but we can\\nuse it to illustrate how forward chaining works.\\nLet us imagine that we start with the following facts in our database:\\nFact 1\\nAt first floor\\nFact 2\\nButton pressed on third floor\\nFact 3\\nT oday is Tuesday\\nNow the system examines the rules and finds that Facts 1 and 2 match the\\nantecedents of Rule 3. Hence, Rule 3 fires, and its conclusion\\nGo to third floor\\nis added to the database of facts. Presumably, this results in the elevator\\nheading toward the third floor. Note that Fact 3 was ignored altogether\\nbecause it did not match the antecedents of any of the rules.\\nNow let us imagine that the elevator is on its way to the third floor and has\\nreached the second floor, when the button is pressed on the first floor. The fact\\nButton pressed on first floor\\nIs now added to the database, which results in Rule 4 firing. Now let us imag-\\nine that later in the day the facts database contains the following information:\\nFact 1\\nAt first floor\\nFact 2\\nButton pressed on second floor\\nFact 3\\nButton pressed on third floor\\nIn this case, two rules are triggered—Rules 2 and 3. In such cases where\\nthere is more than one possible conclusion,conflict resolution needs to be\\napplied to decide which rule to fire.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 272, 'page_label': '273'}, page_content='246 CHAPTER 9 Rules and Expert Systems\\n9.3.2 Conflict Resolution\\nIn a situation where more than one conclusion can be deduced from a set\\nof facts, there are a number of possible ways to decide which rule to fire\\n(i.e., which conclusion to use or which course of action to take).\\nFor example, consider the following set of rules:\\nIF it is cold\\nTHEN wear a coat\\nIF it is cold\\nTHEN stay at home\\nIF it is cold\\nTHEN turn on the heat\\nIf there is a single fact in the fact database, which is “it is cold, ” then clearly\\nthere are three conclusions that can be derived. In some cases, it might be\\nfine to follow all three conclusions, but in many cases the conclusions are\\nincompatible (for example, when prescribing medicines to patients).\\nIn one conflict resolution method, rules are given priority levels, and when\\na conflict occurs, the rule that has the highest priority is fired, as in the fol-\\nlowing example:\\nIF patient has pain\\nTHEN prescribe painkillers priority 10\\nIF patient has chest pain\\nTHEN treat for heart disease priority 100\\nHere, it is clear that treating possible heart problems is more important\\nthan just curing the pain.\\nAn alternative method is thelongest-matching strategy. This method involves\\nfiring the conclusion that was derived from the longest rule. For example:\\nIF patient has pain\\nTHEN prescribe painkiller\\nIF patient has chest pain'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 273, 'page_label': '274'}, page_content='9.3 Rule-Based Systems 247\\nAND patient is over 60\\nAND patient has history of heart conditions\\nTHEN take to emergency room\\nHere, if all the antecedents of the second rule match, then this rule’s con-\\nclusion should be fired rather than the conclusion of the first rule because\\nit is a more specific match.\\nA further method for conflict resolution is to fire the rule that has matched\\nthe facts most recently added to the database.\\nIn each case, it may be that the system fires one rule and then stops (as in\\nmedical diagnosis), but in many cases, the system simply needs to choose a\\nsuitable ordering for the rules (as when controlling an elevator) because\\neach rule that matches the facts needs to be fired at some point.\\n9.3.3 Meta Rules\\nIn designing an expert system, it is necessary to select the conflict resolu-\\ntion method that will be used, and quite possibly it will be necessary to use\\ndifferent methods to resolve different types of conflicts. For example, in\\nsome situations it may make most sense to use the method that involves fir-\\ning the most recently added rules. This method makes most sense in situa-\\ntions in which the timeliness of data is important. It might be, for example,\\nthat as research in a particular field of medicine develops, new rules are\\nadded to the system that contradict some of the older rules. It might make\\nmost sense for the system to assume that these newer rules are more accu-\\nrate than the older rules.\\nIt might also be the case, however, that the new rules have been added by an\\nexpert whose opinion is less trusted than that of the expert who added the\\nearlier rules. In this case, it clearly makes more sense to allow the earlier\\nrules priority.\\nThis kind of knowledge is called meta knowledge —knowledge about\\nknowledge. The rules that define how conflict resolution will be used, and\\nhow other aspects of the system itself will run, are called meta rules.\\nThe knowledge engineer who builds the expert system is responsible for\\nbuilding appropriate meta knowledge into the system (such as “expert A is'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 274, 'page_label': '275'}, page_content='248 CHAPTER 9 Rules and Expert Systems\\nto be trusted more than expert B” or “any rule that involves drug X is not to\\nbe trusted as much as rules that do not involve X”).\\nMeta rules are treated by the expert system as if they were ordinary rules\\nbut are given greater priority than the normal rules that make up the\\nexpert system. In this way, the meta rules are able to override the normal\\nrules, if necessary, and are certainly able to control the conflict resolu-\\ntion process.\\n9.3.4 Backward Chaining\\nForward chaining applies a set of rules and facts to deduce whatever con-\\nclusions can be derived, which is useful when a set of facts are present, but\\nyou do not know what conclusions you are trying to prove. In some cases,\\nforward chaining can be inefficient because it may end up proving a num-\\nber of conclusions that are not currently interesting. In such cases, where a\\nsingle specific conclusion is to be proved, backward chaining is more\\nappropriate.\\nIn backward chaining, we start from a conclusion, which is the hypothesis\\nwe wish to prove, and we aim to show how that conclusion can be reached\\nfrom the rules and facts in the database.\\nThe conclusion we are aiming to prove is called a goal, and so reasoning in\\nthis way is known as goal-driven reasoning.\\nAs we see in Chapter 16, backward chaining is often used in formulating\\nplans. A plan is a sequence of actions that a program (such as an intelli-\\ngent agent) decides to take to solve a particular problem. Backward chain-\\ning can make the process of formulating a plan more efficient than\\nforward chaining.\\nBackward chaining in this way starts with the goal state, which is the set of\\nconditions the agent wishes to achieve in carrying out its plan. It now\\nexamines this state and sees what actions could lead to it. For example, if\\nthe goal state involves a block being on a table, then one possible action\\nwould be to place that block on the table. This action might not be possible\\nfrom the start state, and so further actions need to be added before this\\naction in order to reach it from the start state. In this way, a plan can be for-\\nmulated starting from the goal and working back toward the start state.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 275, 'page_label': '276'}, page_content='9.3 Rule-Based Systems 249\\nThe benefit in this method is particularly clear in situations where the first\\nstate allows a very large number of possible actions. In this kind of situa-\\ntion, it can be very inefficient to attempt to formulate a plan using forward\\nchaining because it involves examining every possible action, without pay-\\ning any attention to which action might be the best one to lead to the goal\\nstate. Backward chaining ensures that each action that is taken is one that\\nwill definitely lead to the goal, and in many cases this will make the plan-\\nning process far more efficient.\\n9.3.5 Comparing Forward and Backward Chaining\\nLet us use an example to compare forward and backward chaining. In this\\ncase, we will revert to our use of symbols for logical statements, in order to\\nclarify the explanation, but we could equally well be using rules about ele-\\nvators or the weather.\\nRules:\\nRule 1 A ∧ B → C\\nRule 2 A → D\\nRule 3 C ∧ D → E\\nRule 4 B ∧ E ∧ F → G\\nRule 5 A ∧ E → H\\nRule 6 D ∧ E ∧ H → I\\nFacts:\\nFact 1 A\\nFact 2 B\\nFact 3 F\\nGoal:\\nOur goal is to prove H.\\nFirst let us use forward chaining. As our conflict resolution strategy, we will\\nfire rules in the order they appear in the database, starting from Rule 1.\\nIn the initial state, Rules 1 and 2 are both triggered. We will start by firing\\nRule 1, which means we add C to our fact database. Next, Rule 2 is fired,\\nmeaning we add D to our fact database.\\nWe now have the facts A, B, C, D, F, but we have not yet reached our goal,\\nwhich is G.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 276, 'page_label': '277'}, page_content='250 CHAPTER 9 Rules and Expert Systems\\nNow Rule 3 is triggered and fired, meaning that fact E is added to the data-\\nbase. As a result, Rules 4 and 5 are triggered. Rule 4 is fired first, resulting in\\nFact G being added to the database, and then Rule 5 is fired, and Fact H is\\nadded to the database. We have now proved our goal and do not need to go\\non any further.\\nThis deduction is presented in the following table:\\nFacts Rules triggered Rule fired\\nA, B, F 1, 2 1\\nA, B, C, F 2 2\\nA, B, C, D, F 3 3\\nA, B, C, D, E, F 4, 5 4\\nA, B, C, D, E, F , G 5 5\\nA, B, C, D, E, F , G, H 6 STOP\\nNow we will consider the same problem using backward chaining. T o do so,\\nwe will use a goals database in addition to the rule and fact databases. In\\nthis case, the goals database starts with just the conclusion, H, which we\\nwant to prove. We will now see which rules would need to fire to lead to this\\nconclusion. Rule 5 is the only one that has H as a conclusion, so to prove H,\\nwe must prove the antecedents of Rule 5, which are A and E.\\nFact A is already in the database, so we only need to prove the other\\nantecedent, E. Therefore, E is added to the goal database. Once we have\\nproved E, we now know that this is sufficient to prove H, so we can remove\\nH from the goals database.\\nSo now we attempt to prove Fact E. Rule 3 has E as its conclusion, so to\\nprove E, we must prove the antecedents of Rule 3, which are C and D. Nei-\\nther of these facts is in the fact database, so we need to prove both of them.\\nThey are both therefore added to the goals database. D is the conclusion of\\nRule 2 and Rule 2’s antecedent, A, is already in the fact database, so we can\\nconclude D and add it to the fact database.\\nSimilarly, C is the conclusion of Rule 1, and Rule 1’s antecedents, A and B,\\nare both in the fact database. So, we have now proved all the goals in the\\ngoal database and have therefore proved H and can stop.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 277, 'page_label': '278'}, page_content='9.4 Rule-Based Expert Systems 251\\nThis process is represented in the table below:\\nFacts Goals Matching rules\\nA, B, F H 5\\nA, B, F E 3\\nA, B, F C, D 1\\nA, B, C, F D 2\\nA, B, C, D, F STOP\\nIn this case, backward chaining needed to use one fewer rule. If the rule data-\\nbase had had a large number of other rules that had A, B, and F as their\\nantecedents, then forward chaining might well have been even more inefficient.\\nIn many situations, forward chaining is more appropriate, particularly in\\na situation where a set of facts is available, but the conclusion is not\\nalready known.\\nIn general, backward chaining is appropriate in cases where there are few\\npossible conclusions (or even just one) and many possible facts, not very\\nmany of which are necessarily relevant to the conclusion. Forward chaining\\nis more appropriate when there are many possible conclusions.\\nThe way in which forward or backward chaining is usually chosen is to\\nconsider which way an expert would solve the problem. This is particularly\\nappropriate because rule-based reasoning is often used in expert systems.\\n9.4 Rule-Based Expert Systems\\nAn expert system is one designed to model the behavior of an expert in\\nsome field, such as medicine or geology. Rule-based expert systems are\\ndesigned to be able to use the same rules that the expert would use to draw\\nconclusions from a set of facts that are presented to the system.\\n9.4.1 The People Involved in an Expert System\\nThe design, development, and use of expert systems involves a number of\\npeople. The end-user of the system is the person who has the need for the\\nsystem. In the case of a medical diagnosis system, this may be a doctor, or it\\nmay be an individual who has a complaint that they wish to diagnose.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 278, 'page_label': '279'}, page_content='252 CHAPTER 9 Rules and Expert Systems\\nFact\\nDatabase\\nKnowledge\\nBase\\nInference\\nEngine\\nExplanation\\nSystem\\nKnowledge\\nBase Editor\\nExpert\\nSystem\\nShell\\nUser Interface\\nUser\\nFigure 9.1\\nArchitecture of an expert\\nsystem\\nThe knowledge engineer is the person who designs the rules for the sys-\\ntem, based on either observing the expert at work or by asking the expert\\nquestions about how he or she works.\\nThe domain expert is very important to the design of an expert system. In\\nthe case of a medical diagnosis system, the expert needs to be able to\\nexplain to the knowledge engineer how he or she goes about diagnosing\\nillnesses.\\n9.4.2 Architecture of an Expert System\\nA typical expert system architecture is shown in Figure 9.1.\\nThe knowledge base contains the specific domain knowledge that is used by\\nan expert to derive conclusions from facts. In the case of a rule-based expert\\nsystem, this domain knowledge is expressed in the form of a series of rules.\\nThe explanation system provides information to the user about how the\\ninference engine arrived at its conclusions. This can often be essential, par-\\nticularly if the advice being given is of a critical nature, such as with a med-\\nical diagnosis system. If the system has used faulty reasoning to arrive at its'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 279, 'page_label': '280'}, page_content='9.4 Rule-Based Expert Systems 253\\nconclusions, then the user may be able to see this by examining the data\\ngiven by the explanation system.\\nThe fact database contains the case-specific data that are to be used in a\\nparticular case to derive a conclusion. In the case of a medical expert sys-\\ntem, this would contain information that had been obtained about the\\npatient’s condition.\\nThe user of the expert system interfaces with it through a user interface,\\nwhich provides access to the inference engine, the explanation system, and\\nthe knowledge-base editor. The inference engine is the part of the system\\nthat uses the rules and facts to derive conclusions. The inference engine will\\nuse forward chaining, backward chaining, or a combination of the two to\\nmake inferences from the data that are available to it.\\nThe knowledge-base editor allows the user to edit the information that is\\ncontained in the knowledge base. The knowledge-base editor is not usually\\nmade available to the end user of the system but is used by the knowledge\\nengineer or the expert to provide and update the knowledge that is con-\\ntained within the system.\\n9.4.3 The Expert System Shell\\nNote that in Figure 9.1, the parts of the expert system that do not contain\\ndomain-specific or case-specific information are contained within the\\nexpert system shell. This shell is a general toolkit that can be used to build\\na number of different expert systems, depending on which knowledge base\\nis added to the shell.\\nAn example of such a shell is CLIPS (C Language Integrated Production\\nSystem), which is described in more detail in Section 9.4. Other examples\\nin common use include OPS5, ART, JESS, and Eclipse.\\n9.4.4 The Rete Algorithm\\nOne potential problem with expert systems is the number of comparisons\\nthat need to be made between rules and facts in the database. In some cases,\\nwhere there are hundreds or even thousands of rules, running comparisons\\nagainst each rule can be impractical.\\nThe Rete Algorithm is an efficient method for solving this problem and is\\nused by a number of expert system tools, including OPS5 and Eclipse.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 280, 'page_label': '281'}, page_content='254 CHAPTER 9 Rules and Expert Systems\\nThe Rete is a directed, acyclic, rooted graph (or a search tree, which was\\ndiscussed in great detail in Chapters 3 and 4).\\nEach path from the root node to a leaf in the tree represents the left-hand\\nside of a rule. Each node stores details of which facts have been matched by\\nthe rules at that point in the path.\\nAs facts are changed, the new facts are propagated through the Rete from\\nthe root node to the leaves, changing the information stored at nodes\\nappropriately. This could mean adding a new fact, or changing information\\nabout an old fact, or deleting an old fact.\\nIn this way, the system only needs to test each new fact against the rules,\\nand only against those rules to which the new fact is relevant, instead of\\nchecking each fact against each rule.\\nThe Rete algorithm depends on the principle that in general, when using\\nforward chaining in expert systems, the values of objects change relatively\\ninfrequently, meaning that relatively few changes need to be made to the\\nRete. In such cases, the Rete algorithm can provide a significant improve-\\nment in performance over other methods, although it is less efficient in\\ncases where objects are continually changing.\\n9.4.5 Knowledge Engineering\\nKnowledge engineering is a vital part of the development of any expert sys-\\ntem. The knowledge engineer does not need to have expert domain knowl-\\nedge but does need to know how to convert such expertise into the rules\\nthat the system will use, preferably in an efficient manner. Hence, the\\nknowledge engineer’s main task is communicating with the expert, in order\\nto understand fully how the expert goes about evaluating evidence and\\nwhat methods he or she uses to derive conclusions.\\nHaving built up a good understanding of the rules the expert uses to draw\\nconclusions, the knowledge engineer must encode these rules in the expert\\nsystem shell language that is being used for the task.\\nIn some cases, the knowledge engineer will have freedom to choose the\\nmost appropriate expert system shell for the task. In other cases, this deci-\\nsion will have already been made, and the knowledge engineer must work\\nwith what he is given.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 281, 'page_label': '282'}, page_content='9.5 CLIPS (C Language Integrated Production System) 255\\n9.5 CLIPS (C Language Integrated Production System)\\nCLIPS is a freely available expert system shell that has been implemented in\\nC. It provides a language for expressing rules and mainly uses forward\\nchaining to derive conclusions from a set of facts and rules.\\nThe notation used by CLIPS is very similar to that used by LISP . The fol-\\nlowing is an example of a rule specified using CLIPS:\\n(defrule birthday\\n(firstname ?r1 John)\\n(surname ?r1 Smith)\\n(haircolor ?r1 Red)\\n=>\\n(assert (is-boss ?r1)))\\n?r1 is used to represent a variable, which in this case is a person. Assert is\\nused to add facts to the database, and in this case the rule is used to draw a\\nconclusion from three facts about the person: If the person has the first\\nname John, has the surname Smith, and has red hair, then he is the boss.\\nThis can be tried in the following way:\\n(assert (firstname x John))\\n(assert (surname x Smith))\\n(assert (haircolor x Red))\\n(run)\\nAt this point, the command (facts) can be entered to see the facts that are\\ncontained in the database:\\nCLIPS> (facts)\\nf-0 (firstname x John)\\nf-1 (surname x Smith)\\nf-2 (haircolor x Red)\\nf-3 (is-boss x)\\nSo CLIPS has taken the three facts that were entered into the system and\\nused the rule to draw a conclusion, which is that x is the boss. Although this\\nis a simple example, CLIPS, like other expert system shells, can be used to\\nbuild extremely sophisticated and powerful tools.\\nFor example, MYCIN is a well-known medical expert system that was\\ndeveloped at Stanford University in 1984. MYCIN was designed to assist\\ndoctors to prescribe antimicrobial drugs for blood infections. In this way,\\nexperts in antimicrobial drugs are able to provide their expertise to other\\ndoctors who are not so expert in that field. By asking the doctor a series of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 282, 'page_label': '283'}, page_content='256 CHAPTER 9 Rules and Expert Systems\\nquestions, MYCIN is able to recommend a course of treatment for the\\npatient. Importantly, MYCIN is also able to explain to the doctor which\\nrules fired and therefore is able to explain why it produced the diagnosis\\nand recommended treatment that it did.\\nMYCIN has proved successful: for example, it has been proven to be able to\\nprovide more accurate diagnoses of meningitis in patients than most doctors.\\nMYCIN was developed using LISP , and its rules are expressed as LISP\\nexpressions. The following is an example of the kind of rule used by\\nMYCIN, translated into English:\\nIF the infection is primary-bacteria\\nAND the site of the culture is one of the sterile sites\\nAND the suspected portal of entry is the gastrointestinal tract\\nTHEN there is suggestive evidence (0.7) that infection is bacteroid\\nIn Chapter 17, we learn more about how MYCIN uses certainty factors to\\naid its diagnosis process.\\nThe following is a very simple example of a CLIPS session where rules are\\ndefined to operate an elevator:\\nCLIPS> (defrule rule1\\n(elevator ?floor_now)\\n(button ?floor_now)\\n=>\\n(assert (open_door)))\\nCLIPS> (defrule rule2\\n(elevator ?floor_now)\\n(button ?other_floor)\\n=>\\n(assert (goto ?other_floor)))\\nCLIPS> (assert (elevator floor1))\\n==> f-0 (elevator floor1)\\n<Fact-0>\\nCLIPS> (assert (button floor3))\\n==> f-1 (button floor3)\\n<Fact-1>\\n<CLIPS> (run)\\n==>f-2 (goto floor3)\\nThe segments in bold are inputs by the knowledge engineer, and the plain\\ntext sections are CLIPS.\\nNote that ?floor_now is an example of a variable within CLIPS, which\\nmeans that any object can match it for the rule to trigger and fire. In our'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 283, 'page_label': '284'}, page_content='9.6 Backward Chaining in Rule-Based Expert Systems 257\\nexample, the first rule simply says: If the elevator is on a floor, and the but-\\nton is pressed on the same floor, then open the door. The second rule says:\\nIf the elevator is on one floor, and the button is pressed on a different floor,\\nthen go to that floor.\\nAfter the rules, two facts are inserted into the database. The first fact says\\nthat the elevator is on floor 1, and the second fact says that the button has\\nbeen pressed on floor 3.\\nWhen the (run) command is issued to the system, it inserts a new fact into\\nthe database, which is a command to the elevator to go to floor 3.\\n9.6 Backward Chaining in Rule-Based Expert Systems\\nA common method for building expert systems is to use a rule-based sys-\\ntem with backward chaining. Typically, a user enters a set of facts into the\\nsystem, and the system tries to see if it can prove any of the possible\\nhypotheses using these facts. In some cases, it will need additional facts, in\\nwhich case the expert system will often ask the user questions, to ascertain\\nfacts that could enable further rules to fire.\\nThe algorithm is applied as follows:\\nT o prove a conclusion, we must prove a set of hypotheses, one of which is\\nthe conclusion. For each hypothesis, H:\\nIf H is in the facts database, it is proved.\\nOtherwise, if H can be determined by asking a question, then enter\\nthe user’s answer in the facts database. Hence, it can be determined\\nwhether H is true or false, according to the user’s answer.\\nOtherwise, find a rule whose conclusion is H. Now apply this algo-\\nrithm to try to prove this rule’s antecedents.\\nIf none of the above applies, we have failed to prove H.\\nTypically, backward chaining is used in combination with forward chain-\\ning. Whenever a new fact is added to the database, forward chaining is\\napplied to see if any further facts can be derived. Backward chaining is then\\nused to try to prove each possible hypothesis.\\nLet us imagine a simple medical expert system that is designed to diagnose\\nand treat patients’ diseases. The rules might be as follows:\\nRule 1: If headache then prescribe painkiller'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 284, 'page_label': '285'}, page_content='258 CHAPTER 9 Rules and Expert Systems\\nRule 2: If headache and sore throat and coughing then diagnose flu\\nRule 3: If tired and headache then diagnose glandular fever\\nRule 4: If tired and sore throat then diagnose tonsillitis\\nRule 5: If tired then diagnose stress\\nNow we might imagine a session between the user (the patient) and the\\nexpert system as follows:\\nPatient: I have a headache\\nSystem: I suggest you take a painkiller. Do you have a sore throat?\\nPatient: Y es, I do.\\nSystem: Have you been coughing?\\nPatient: No, why do you ask?\\nSystem: If you had a headache and a sore throat and had been\\ncoughing, I would have concluded you might have flu. It\\ndoesn’t sound as though you have flu. Have you been feel-\\ning tired recently?\\nPatient: Y es, I have.\\nSystem: It sounds as though you may have glandular fever or tonsillitis.\\nThis was a rather simplistic example, but it illustrates the way that the sys-\\ntem uses forward chaining to derive new facts and asks questions in order\\nto carry out its backward chaining analysis. For example, having been told\\nthat the patient has a headache, the system first applies forward chaining\\nwith Rule 1 and, as a result, adds “prescribe painkiller” to its database of\\nfacts. This fact has an action, or a recommendation, associated with it, and\\nso the system advises the patient that she should take a painkiller.\\nNext, the system tries to see if it can prove any other hypotheses. The possi-\\nble hypotheses are flu, tonsillitis, glandular fever, and stress. First, the sys-\\ntem uses backward chaining to try to prove the hypothesis that the patient\\nhas the flu.\\nT o prove this hypothesis, the antecedents of Rule 2 must be proved: that the\\npatient has a headache and a sore throat and has been coughing. The\\npatient has already said that she has a headache, so this fact is already in the\\nfact database. Next, the system must establish whether the patient has a\\nsore throat. She says that she does, so this fact is added to the fact database.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 285, 'page_label': '286'}, page_content='9.7 CYC 259\\nShe has not been coughing, though, so the system concludes that she does\\nnot have flu.\\nAt this point also note that the patient asks why the system asked the last\\nquestion. The system is able to use its explanation facility to provide an\\nexplanation for why it asked the question and what conclusion it was able\\nto draw from the answer.\\nFinally, the patient says that she has been feeling tired, and as a result of this\\nfact being added to the database, Rules 3, 4, and 5 are all triggered. In this\\ncase, conflict resolution has been applied in a rather simplistic way, such\\nthat Rules 3 and 4 both fire, but 5 does not. In a real medical expert system,\\nit is likely that further questions would be asked, and more sophisticated\\nrules applied to decide which condition the patient really had.\\n9.7 CYC\\nCYC is an example of a frame-based representational system of knowledge,\\nwhich is, in a way, the opposite of an expert system. Whereas an expert sys-\\ntem has detailed knowledge of a very narrow domain, the developers of CYC\\nhave fed it information on over 100,000 different concepts from all fields of\\nhuman knowledge. CYC also has information of over 1,000,000 different\\npieces of “common sense” knowledge about those concepts. The system has\\nover 4000 different types of links that can exist between concepts, such as\\ninheritance, and the “is–a” relationship that we have already looked at.\\nThe idea behind CYC was that humans function in the world mainly on the\\nbasis of a large base of knowledge built up over our lifetimes and our ances-\\ntors’ lifetimes. By giving CYC access to this knowledge, and the ability to\\nreason about it, they felt they would be able to come up with a system with\\ncommon sense. Ultimately, they predict, the system will be built into word\\nprocessors. Then word processors will not just correct your spelling and\\ngrammar, but will also point out inconsistencies in your document. For\\nexample, if you promise to discuss a particular subject later in your docu-\\nment, and then forget to do so, the system will point this out to you. They\\nalso predict that search engines and other information retrieval systems\\n(see Chapter 20) will be able to find documents even though they do not\\ncontain any of the words you entered as your query.\\nCYC’s knowledge is segmented into hundreds of different contexts to avoid\\nthe problem of many pieces of knowledge in the system contradicting each'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 286, 'page_label': '287'}, page_content='260 CHAPTER 9 Rules and Expert Systems\\nother. In this way, CYC is able to know facts about Dracula and to reason\\nabout him, while also knowing that Dracula does not really exist.\\nCYC is able to understand analogies, and even to discover new analogies for\\nitself, by examining the similarities in structure and content between differ-\\nent frames and groups of frames. CYC’s developers claim, for example, that\\nit discovered an analogy between the concept of “family” and the concept of\\n“country. ”\\n9.8 Chapter Summary\\n■ IF . . . THEN . . . rules can be used to represent knowledge\\nabout objects.\\n■ Rule-based systems, or production systems, use rules to attempt to\\nderive diagnoses or to provide instructions.\\n■ Rule systems can work using forward chaining, backward chaining,\\nor both. Forward chaining works from a set of initial facts, and\\nworks toward a conclusion. Backward chaining starts with a\\nhypothesis and tries to prove it using the facts and rules that are\\navailable.\\n■ Conflict resolution methods are used to determine what to do\\nwhen more than one solution is provided by a rule-based system.\\n■ Knowledge from a domain expert is translated by a knowledge\\nengineer into a form suitable for an expert system, which is then\\nable to help an end-user solve problems that would normally\\nrequire a human expert.\\n■ In many cases, expert systems use backward chaining and ask ques-\\ntions of the end user to attempt to prove a hypothesis.\\n■ Expert systems are usually built on an expert system shell (such as\\nCLIPS), which provides a generic toolkit for building expert systems.\\n■ The Rete algorithm provides an efficient method for chaining in\\nrule-based systems where there are many rules and where facts do\\nnot change frequently.\\n■ Semantic nets and frame-based systems are also used as the basis\\nfor expert systems.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 287, 'page_label': '288'}, page_content='9.11 Further Reading 261\\n■ CYC is a system built with knowledge of over 100,000 objects and\\nis able to make complex deductions about the real world.\\n9.9 Review Questions\\n9.1 Explain why expert systems are so called.\\n9.2 Explain the difference between forward chaining and backward\\nchaining. Explain the advantages and disadvantages of each method.\\n9.3 Explain the various methods of conflict resolution that can be used\\nin rule-based expert systems. For each of these, give an example of\\na scenario where using it would not give the correct result.\\n9.4 What is the purpose of meta rules? Would an expert system have\\nany advantages if it knew the difference between meta rules and\\nnormal rules?\\n9.5 Describe the architecture of an expert system, and describe the\\nroles of the various people involved in it.\\n9.6 What is the purpose of the Rete algorithm? Describe how it works.\\n9.7 Explain the relationships between rules, logic, semantic nets, and\\nframes. Give an example of a situation where you might use each of\\nthem. Which system has the greatest representational adequacy?\\nWhich has the least? Why?\\n9.10 Exercises\\n9.1 Extend the CLIPS rules given in Section 9.5 to produce a more use-\\nful system for running an elevator on a building with five floors.\\n9.2 Implement a rule-based or frame-based expert system shell in the\\nprogramming language of your choice. Implement an expert sys-\\ntem in your expert system shell to solve problems in an area in\\nwhich you have expertise (for example, solving computer science\\nproblems, or identifying films or songs).\\n9.11 Further Reading\\nNegnevitsky (2002) provides an excellent overview of rule- and frame-\\nbased expert systems.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 288, 'page_label': '289'}, page_content='262 CHAPTER 9 Rules and Expert Systems\\nFor a more detailed description of the Rete algorithm, see Rete: A Fast Algo-\\nrithm for the Many Pattern / Many Object Pattern Match Problem by C. L.\\nForgy from Artificial Intelligence, Vol. 19, pp. 17–37, 1982.\\nWinston (1993) also covers the Rete algorithm.\\nA brief description of the CYC expert system, and some of the difficulties\\nfaced in building it can be found in Douglas B. Lenat’s article,Programming\\nArtificial Intelligence , which was first published in Scientific American in\\nSeptember 1995 and can also be found in Fritz (2002).\\nDeeper coverage of CYC can be found in Lenat and Guha (1990).\\nDiscussion of productions systems and an example of an expert system\\nimplementation in PROLOG can be found in Logic and Prolog by Richard\\nSpencer-Smith (1991).\\nAn interesting discussion of some of the limitations of MYCIN can be\\nfound in McCarthy (1983). In particular, he points out that the system is\\nnot aware of its own limitations and so is liable to make confident recom-\\nmendations that are potentially dangerous.\\nFundamentals of Expert System Technology: Principles and Concepts by\\nSamuel J. Biondo (1990 – Intellect)\\nRule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuris-\\ntic Programming Project by B. G. Buchanan and E. H. Shortliffe (1984 –\\nAddison Wesley)\\nProlog Programming for Students: With Expert Systems and Artificial Intelli-\\ngence Topics by David Callear (2001 – Continuum)\\nArtificial Intelligence: A Knowledge-Based Approach by Morris W. Firebaugh\\n(1988 – Boyd & Fraser Publishing Company – out of print)\\nUnderstanding Artificial Intelligence (Science Made Accessible) compiled by\\nSandy Fritz (2002 – Warner Books)\\nExpert Systems: Principles and Programming by Joseph C. Giarratano (1998\\n– Brooks Cole)\\nManaging Uncertainty in Expert Systems by Jerzy W. Grzymala-Busse (1991\\n– Kluwer Academic Publishers)\\nExpert Systems: Artificial Intelligence in Business by Paul Harmon (1985 –\\nJohn Wiley & Sons – out of print)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 289, 'page_label': '290'}, page_content='9.11 Further Reading 263\\nIntroduction to Expert Systems by Peter Jackson (1999 – Addison Wesley)\\nKnowledge Acquisition for Expert Systems: A Practical Handbookby Alison L.\\nKidd (1987 – Plenum Publishing Corporation)\\nBuilding Large Knowledge-Based Systems: Representation and Inference in the\\nCYC Project by Douglas B. Lenat and R. V . Guha (1990 – Addison Wesley)\\nThe Logic of Knowledge Bases by Hector J. Levesque and Gerhard Lakemeyer\\n(2001 – MIT Press)\\nSome Expert Systems Need Common Sense by John McCarthy (1983 – in\\nComputer Culture: The Scientific, Intellectual and Social Impact of the Com-\\nputer edited by Heinz Pagels, Vol. 426)\\nBuilding Expert Systems in Prologby Dennis Merritt (1995 – Springer V erlag)\\nArtificial Intelligence: A Guide to Intelligent Systems by Michael Negnevitsky\\n(2002 – Addison Wesley)\\nComputer Based Medical Consultations: Mycinby Edward Shortliffe (1976 –\\nElsevier Science, out of print)\\nLogic and Prolog by Richard Spencer-Smith (1991 – Harvester Wheatsheaf)\\nManaging Expert Systems edited by Efraim Turban and Jay Liebowitz (1992\\n– Idea Group Publishing)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 290, 'page_label': '291'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 291, 'page_label': '292'}, page_content='Machine Learning\\n4\\nIntroduction to Part 4\\nPart 4 is divided into five chapters:\\nIntroduction to Machine Learning\\nThis chapter introduces a number of techniques for\\nmachine learning, such as ID3 for learning decision trees,\\nversion spaces, and the nearest neighbor algorithm. It also\\nintroduces the ideas behind neural networks, which are\\ncovered in more detail in Chapter 11.\\nThis chapter explains the idea of inductive bias and why it\\nis important in machine learning.\\nNeural Networks\\nThis chapter expands on the ideas introduced in Chapter 10\\nand gives a more detailed coverage of neural networks. It\\nexplains the relationship between artificial neurons and\\nbiological neurons, and introduces perceptions. The chap-\\nter then explains multilayer networks and introduces back-\\npropagation as a way to train multilayer networks. It also\\nintroduces recurrent networks, such as Hopfield networks.\\nThis chapter explains unsupervised neural networks (such\\nas Kohonen maps) as well as supervised ones.\\nFinally, this chapter briefly introduces the idea of evolving\\nneural networks, combining ideas from this chapter with\\nideas from Chapters 13 and 14.\\nPART\\n10\\nCHAPTER\\n11\\nCHAPTER'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 292, 'page_label': '293'}, page_content='Probabilistic Reasoning and Bayesian Belief Networks\\nThis chapter introduces probabilistic reasoning and\\nexplains how it can be used to deal with situations in which\\nthere is uncertainty about some variables. The chapter\\nexplains the notation that is used and introduces condi-\\ntional probability. It explains Bayes’ theorem and shows\\nhow it can be used with some practical examples. It then\\nintroduces methods that use Bayesian reasoning to learn,\\nincluding the optimal Bayes’ classifier, which provides the\\nbest possible classification of unseen data.\\nArtificial Life: Learning through Emergent Behavior\\nChapter 13 provides a broad overview of the subject of arti-\\nficial life. It starts with a question: What is life? It then\\nexplains how artificial life techniques model nature. A\\nnumber of techniques and experimental findings are dis-\\ncussed, including cellular automata, genetic programming,\\nevolutionary programming, and L-systems.\\nThe chapter also discusses the idea of emergent behavior\\nand the reason that evolution is such an important concept.\\nSome concepts relating to genetic algorithms are intro-\\nduced in this chapter and explored further in Chapter 14.\\nGenetic Algorithms\\nChapter 14 builds on the ideas introduced in Chapter 13,\\nby explaining in more detail the ideas behind genetic algo-\\nrithms. The chapter explains the methods used in building\\ngenetic algorithms, such as crossover and mutation, and\\nattempts to provide a basis for understanding why genetic\\nalgorithms work.\\nA concrete example is given of how a genetic algorithm could\\nbe built to play a simple game—the Prisoner’s Dilemma.\\n12\\nCHAPTER\\n13\\nCHAPTER\\n14\\nCHAPTER'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 293, 'page_label': '294'}, page_content='10CHAPTER\\nIntroduction to \\nMachine Learning\\nO, what learning is!\\n—William Shakespeare, Romeo and Juliet\\nMuch learning doth make thee mad.\\n—The Acts of the Apostles, Chapter 26, V erse 24\\nWhence is thy learning? Hath thy toil\\nO’er books consumed the midnight oil?\\n—John Gay,Fables\\nLearning and intelligence are intimately related to each other. It is usually\\nagreed that a system capable of learning deserves to be called intelligent; and\\nconversely, a system being considered as intelligent is, among other things,\\nusually expected to be able to learn. Learning always has to do with the self-\\nimprovement of future behaviour based on past experience.\\n—Sandip Sen and Gerhard Weiss,Learning in Multiagent Systems\\n10.1 Introduction\\nMachine learning is an extremely important part of Artificial Intelligence.\\nThis chapter provides a brief overview of some of the main methods and\\nideas that are used in machine learning and also provides a very brief intro-\\nduction to neural networks, which are covered in more detail in Chapter 11.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 294, 'page_label': '295'}, page_content='268 CHAPTER 10 Introduction to Machine Learning\\nIn this chapter, concept learning methods are explored, which are able to\\ngeneralize from a set of training data to be able to correctly classify data\\nthat has not been seen before. Decision-tree learning is examined, and the\\nID3 algorithm is explained.\\n10.2 Training\\nIn most learning problems, the task is to learn to classify inputs according\\nto a finite (or sometimes infinite) set of classifications. Typically, a learning\\nsystem is provided with a set of training data, which have been classified by\\nhand. The system then attempts to learn from these training data how to\\nclassify the same data (usually a relatively easy task) and also how to classify\\nnew data that it has not seen.\\nLearning to classify unseen data clearly assumes that there is some relation-\\nship between the data and the classifications—in other words, some function\\nf can be generated such that if a piece of datax belongs in classificationy, then\\nf(x) = y\\nFor example, if the equality function were used, the learning task would be\\nrelatively simple because each datum would be classified as itself. Clearly\\nmost real-world problems are not so simple, and producing a function that\\napproximates the correct mapping is one of the main challenges of\\nmachine learning.\\nIn fact, in most learning problems, the input data consist of more than\\none variable.\\nFor example, let us consider a system that is to learn how to evaluate static\\nchess positions.\\nFirst, we will consider a number of variables:\\nx\\n1: Number of white pieces on the board\\nx2: Number of black pieces on the board\\nx3: Number of black pieces threatened by white pieces\\nx4: Number of white pieces threatened by black pieces\\nx5: Can white checkmate on the next go?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 295, 'page_label': '296'}, page_content='10.2 Training 269\\nx6: Can black checkmate on the next go?\\nx7: Number of different moves white can make\\nx8: Number of different moves black can make\\nClearly, this is an oversimplification because a real chess system would need\\nto use a much more complex set of variables to evaluate a position.\\nNote that the variables are not all of the same type: most of the variables are\\nnumeric, but two of them are Boolean (can each side achieve checkmate on\\nthe next go). Many learning problems will involve data of a number of dif-\\nferent types.\\nThe evaluation of each position is to be calculated as a high positive in the\\nevent that white has the better position and a high negative if black has the\\nbetter position. A value of 0 indicates a level position, and a score of /H11006100\\nindicates that one side has won the game, or is about to win.\\nIt seems probable that a simple linear weighted function of these variables\\nwill suffice: We will write our evaluation function f as follows:\\nf(x\\n1,x 2,x 3,x 4,x 5,x 6,x 7,x 8) =\\nw1x1 + w2x2 + w3x3 + w4x4 + w5x5 + w6x6 + w7x7 + w8x8\\nwhere w1 to w8 are the weights associated with the eight variables. The aim\\nof the system is to determine suitable values for these weights, based on the\\ntraining data that are provided.\\nAn item of training data might be\\nf(10, 2, 1, 0, true, false, 10, 1) = 100\\nThis suggests that the position described by the training data is a definite\\nwin for white.\\nClearly, there are an extraordinarily large number of possible sets of train-\\ning data for this function, and it may not even be the case that a suitable\\nfunction exists for this representation. A superior representation, for which\\na suitable function certainly exists, would be to map the positions of all 32\\npieces to the 64 squares on the board. In this case, a system could certainly\\nbe trained to determine whether any given position was better for white or'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 296, 'page_label': '297'}, page_content='270 CHAPTER 10 Introduction to Machine Learning\\nfor black, but the enormous number of possible input data makes the prob-\\nlem somewhat harder.\\nIn Chapter 11, we see how artificial neural networks can be used to provide\\nextremely accurate mappings from input data to classifications for prob-\\nlems such as this.\\nIn this chapter, we will look at methods that are primarily used to learn\\nsomewhat simpler mappings, although these methods can certainly be\\nextended to work with more complex sets of data.\\n10.3 Rote Learning\\nThe simplest way for a computer to learn from experience is simply to learn\\nby rote. Training involves storing each piece of training data and its classi-\\nfication. Thereafter, a new item of data is classified by looking to see if it is\\nstored in memory. If it is, then the classification that was stored with that\\nitem is returned. Otherwise, the method fails.\\nHence, a rote learner is able to classify only data that it has already seen,\\nand no attempt is made to approximate the mapping function, which is a\\nmajor weakness.\\n10.4 Learning Concepts\\nWe will now look at a number of methods that can be used to learn con-\\ncepts. Concept learning involves determining a mapping from a set of\\ninput variables to a Boolean value.\\nThe methods described here are known as inductive-learning methods .\\nThese methods are based on the principle that if a function is found that\\ncorrectly maps a large set of training data to classifications, then it will also\\ncorrectly map unseen data. In doing so, a learner is able to generalize from\\na set of training data.\\nT o illustrate these methods, we will use a simple toy problem, as follows:\\nOur learning task will be to determine whether driving in a particular man-\\nner in particular road conditions is safe or not. We will use the following\\nattributes:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 297, 'page_label': '298'}, page_content='10.4 Learning Concepts 271\\nAttribute Possible values\\nSpeed slow, medium, fast\\nWeather wind, rain, snow, sun\\nDistance from car in front 10ft, 20ft, 30ft, 40ft, 50ft, 60ft\\nUnits of alcohol driver has drunk 0, 1, 2, 3, 4, 5\\nTime of day morning, afternoon, evening, night\\nTemperature cold, warm, hot\\nWe will consider a hypothesis to be a vector of values for these attributes. A\\npossible hypothesis is\\nh\\n1 = <slow, wind, 30ft, 0, evening, cold>\\nWe also want to represent in a hypothesis that we do not care what value an\\nattribute takes. This is represented by “?” , as in the following hypothesis:\\nh2 = <fast, rain, 10ft, 2, ?, ?>\\nh2 represents the hypothesis that driving quickly in rainy weather, close to\\nthe car in front after having drunk two units of alcohol is safe, regardless of\\nthe time of day or the temperature. Clearly, this hypothesis is untrue and\\nwould be considered by the learner to be a negative training example.\\nIn other cases, we need to represent a hypothesis that no value of a particu-\\nlar attribute will provide a positive example. We write this as “∅” , as in the\\nfollowing hypothesis:\\nh\\n3 = <fast, rain, 10ft, 2, ∅, ∅>\\nh3 states the opposite of h2—that driving quickly in rainy weather, close to\\nthe car in front after having drunk two units of alcohol cannot be safe,\\nregardless of the time of day or the temperature.\\nThe task of the concept learner is to examine a set of positive and negative\\ntraining data and to use these to determine a hypothesis that matches all\\nthe training data, and which can then be used to classify instances that have\\nnot previously been seen.\\nConcept learning can be thought of as search through a search space that\\nconsists of all possible hypotheses, where the goal is the hypothesis that\\nmost closely represents the correct mapping.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 298, 'page_label': '299'}, page_content='272 CHAPTER 10 Introduction to Machine Learning\\n10.5 General-to-Specific Ordering\\nConsider the following two hypotheses:\\nhg = <?, ?, ?, ?, ?, ?>\\nhs = <∅, ∅, ∅, ∅, ∅, ∅>\\nhg is the hypothesis that it is safe to drive regardless of the conditions—this\\nis the most general hypothesis.\\nhs is the most specific hypothesis, which states that it is never safe to drive,\\nunder any circumstances.\\nThese hypotheses represent two extremes, and clearly a useful hypothesis\\nthat accurately represents the mapping from attribute values to a Boolean\\nvalue will be somewhere in between these two.\\nOne method for concept learning is based on the idea that a partial order\\nexists over the space of hypotheses. This partial order is represented by the\\nrelationship “more general than”:\\n≥\\ng\\nWe w r ite\\nh1 ≥g h2\\nwhich states that h1 is more general than (or as general as) h2. Similarly, we\\ncould write\\nh1 >g h2\\nin the case where h1 is certainly more general than h2.\\n≥g defines a partial order over the hypothesis space, rather than a total\\norder, because some hypotheses are neither more specific nor more general\\nthan other hypotheses. For example, consider the following hypotheses:\\nh\\n1 = <?, ?, ?, ?, evening, cold>\\nh2 = <medium, snow, ?, ?, ?, ?>\\nWe cannot express any relationship betweenh1 and h2 in terms of generality.\\nOne hypothesis is more general than (or equally general as) another\\nhypothesis if every instance that is matched by the second hypothesis is also\\nmatched by the first hypothesis.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 299, 'page_label': '300'}, page_content='10.5 General-to-Specific Ordering 273\\nFor example,\\n<slow, ?, ?, ?, ?, ?> ≥ g <slow, ?, ?, ?, ?, cold>\\nIt should be clear that a more general hypothesis matches more instances\\nthan a less general hypothesis.\\n10.5.1 A Simple Learning Algorithm\\nThe following algorithm uses the general-to-specific ordering of hypotheses to\\nsearch the hypothesis space for a suitable hypothesis. The method is as follows:\\nStart with the most specific hypothesis. In our example above, this would\\nbe <∅, ∅, ∅, ∅, ∅, ∅, ∅, ∅>.\\nNow, for each positive training example, determine whether each attribute in\\nthe example is matched by the current hypothesis. If it is not, replace the\\nattributes in the hypothesis with the next more general value that does match.\\nFor example, let us consider the following set of positive training data:\\n<slow, wind, 30ft, 0, evening, cold>\\n<slow, rain, 20ft, 0, evening, warm>\\n<slow, snow, 30ft, 0, afternoon, cold>\\nFirst, let us compare the first item of training data with the current hypoth-\\nesis, which is <∅, ∅, ∅, ∅, ∅, ∅, ∅, ∅>. Clearly, none of the attributes are\\nmatched by this hypothesis. The next most general value for each attribute\\nthan ∅ that matches the training data is the value contained in the training\\ndata. So we replace our hypothesis with the following hypothesis:\\n<slow, wind, 30ft, 0, evening, cold>\\nClearly, the hypothesis <?, ?, ?, ?, ?, ?, ?, ?> would have been more general than\\nthe initial hypothesis, but the method we are using is to select thenext more\\ngeneral value for each attribute. In this way, we move from a hypothesis that\\nis too specific to one that is general enough to match all the training data.\\nWe will now consider the second item of training data:\\n<slow, rain, 20ft, 0, evening, warm>\\nNow we compare each attribute value with the corresponding value in our\\ncurrent hypothesis. Where the values match, we do not need to make any\\nchange. Where they do not match, we need to replace the value with “?” so'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 300, 'page_label': '301'}, page_content='274 CHAPTER 10 Introduction to Machine Learning\\nthat the hypothesis matches both items of training data. Hence, our new\\nhypothesis is\\n<slow, ?, ?, 0, evening, ?>\\nBy comparing with our final item of training data, we arrive at the follow-\\ning hypothesis:\\n<slow, ?, ?, 0, ?, ?>\\nThis hypothesis states that it is only safe to drive if one drives slowly and\\nhas not drunk any alcohol, and that this is true regardless of the road or\\nweather conditions.\\nThis hypothesis is consistent with the training examples, which means that\\nit maps each of them to the correct classification.\\nThis algorithm will generate the most specific hypothesis that matches\\nall of the training data. There are a number of problems with this algo-\\nrithm: first of all, it may not be desirable to identify the most specific\\nhypothesis—it may be that the most general hypothesis that matches the\\ntraining data provides a better solution. Secondly, the most specific\\nhypothesis identified by the algorithm may not be the only solution—\\nthere may be other most specific hypotheses that match the data, one of\\nwhich may be a preferable solution. Additionally, this algorithm does\\nnot make any use of negative examples. As we will see, most useful\\nlearning methods are able to make use of negative as well as positive\\ntraining examples.\\nFinally, the method does not deal well with inconsistent or incorrect train-\\ning data. In real-world problems, an ability to deal with such errors is vital,\\nas we see later in this part of the book.\\n10.6 Version Spaces\\nGiven a set of training examples (positive and negative), the set of hypothe-\\nses that correctly map each of the training examples to its classification is\\ncalled the version space.\\nOne method for learning from a set of data is thus to start from a complete\\nversion space that contains all hypotheses and systematically remove all\\nthe hypotheses that do not correctly classify each training example.\\nAlthough this method might work well on small problems, for problems of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 301, 'page_label': '302'}, page_content='10.7 Candidate Elimination 275\\nany reasonable size, the task of enumerating all hypotheses would be\\nimpractical.\\n10.7 Candidate Elimination\\nWe now explore another method that uses version spaces to learn. The\\naim of these methods is to identify a single hypothesis, if possible, that\\ncorrectly describes the problem. The more training data that are avail-\\nable, the fewer hypotheses are contained in the version space. If all the\\ntraining data have been used, and the version space contains just a single\\nhypothesis, then this matches all the training data and should also match\\nunseen data.\\nThe candidate elimination learning method operates in a similar manner\\nto the simple algorithm presented in Section 10.5.1. Unlike the earlier sim-\\nple method, the candidate elimination method stores not just a single\\nhypothesis, but two sets of hypotheses. In addition to maintaining a set of\\nmost specific hypotheses that match the training data, this method also\\nmaintains a set of hypotheses that starts out as a set with the single item \\n<?, ?, ?, ?, ?, ?, ?, ?> and ends up being a set of the most general hypotheses\\nthat match all the training data. This algorithm is thus able to make use of\\nnegative training data as well as positive training data.\\nThe method operates as follows: Two sets are maintained of hypotheses, h\\ns\\nand hg: hs is initialized as {<∅, ∅, ∅, ∅, ∅, ∅, ∅, ∅>} and hg is initialized\\nas {<?, ?, ?, ?, ?, ?, ?, ?>}.\\nWhen a positive training example is encountered, it is compared with the\\nhypotheses contained in hg. If any of these hypotheses does not match the\\ntraining example, it is removed from hg. The positive training data are then\\ncompared with the hypotheses contained in hs. If one of these hypotheses\\ndoes not match the training data, it is replaced by the set of slightly more\\ngeneral hypotheses that are consistent with the data, and such that there is\\nat least one hypothesis in h\\ng that is more general.\\nThis method is applied in reverse for negative training data. By applying\\nthis method to each item of training data, the sets hg and hs move closer to\\neach other and eventually between them contain the full version space of\\nhypotheses that match all the training data.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 302, 'page_label': '303'}, page_content='276 CHAPTER 10 Introduction to Machine Learning\\n10.8 Inductive Bias\\nAll learning methods have an inductive bias . Inductive bias refers to the\\nrestrictions that are imposed by the assumptions made in the learning\\nmethod. For example, in the above discussions we have been assuming\\nthat the solution to the problem of road safety can be expressed as a con-\\njunction of a set of eight concepts. This does not allow for more complex\\nexpressions that cannot be expressed as a conjunction. This inductive\\nbias means that there are some potential solutions that we cannot\\nexplore, and which are, therefore, not contained within the version space\\nwe examine.\\nThis may seem like an unfortunate limitation, but in fact inductive bias is\\nessential for learning. In order to have an unbiased learner, the version\\nspace would have to contain every possible hypothesis that could possibly\\nbe expressed. This would impose a severe limitation: the solution that the\\nlearner produced could never be any more general than the complete set of\\ntraining data. In other words, it would be able to classify data that it had\\npreviously seen (as the rote learner could) but would be unable to general-\\nize in order to classify new, unseen data.\\nThe inductive bias of the candidate elimination algorithm is that it is only\\nable to classify a new piece of data if all the hypotheses contained within its\\nversion space give the data the same classification. Hence, the inductive bias\\ndoes impose a limitation on the learning method.\\nIn the 14th century, William of Occam proposed his famous “ Occam’s\\nrazor, ” which simply states that it is best to choose the simplest hypothesis\\nto explain any phenomenon. We can consider this to be a form of inductive\\nbias, which states that the best hypothesis to fit a set of training data is the\\nsimplest hypothesis. We will see later how this inductive bias can be useful\\nin learning decision trees.\\n10.9 Decision-Tree Induction\\nIn Chapter 3, we see a tree that was used to determine which species a par-\\nticular bird belonged to, based on various observed features of the bird. A\\nvariation of this kind of tree, where the leaf nodes are all Boolean values is'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 303, 'page_label': '304'}, page_content='10.9 Decision-Tree Induction 277\\nCountry of\\nOrigin\\nUSA Europe Rest of World\\nComedy Romance\\nScience FictionYe s N o\\nGenreBig Star\\nfalsefalsetruefalsetrue\\nfalse\\nFigure 10.1\\nA simple decision tree \\nfor determining whether\\nor not a film will be a \\nbox-office success\\ncalled a decision tree. A decision tree takes in a set of attribute values and\\noutputs a Boolean decision.\\nAn example of a decision tree is shown in Figure 10.1. This decision tree can be\\nused to determine whether or not a given film will be a success at the box office.\\nT o use the decision tree, we start at the top and apply the question to the\\nfilm. If the film is made in the United States, we move down the first branch\\nof the tree; if it is made in Europe the second; and if elsewhere then we\\nexplore the third branch. The final boxes represent the Boolean value, true\\nor false, which expresses whether a film is a success or not.\\nAccording to this extremely simplistic (and possibly somewhat con-\\ntentious) decision tree, a film can only be a box-office success if it is made\\nin the United States and has a big star, or if it is a European comedy.\\nWhereas version spaces are able to represent expressions that consist solely\\nof conjunctions, decision trees can represent more complex expressions,\\ninvolving disjunctions and conjunctions. For example, the decision tree in\\nFigure 10.1 represents the following expression:\\n((Country = USA) ∧ (Big Star = yes)) ∨ ((Country = Europe) ∧\\n(Genre = comedy))\\nDecision-tree induction (or decision-tree learning) involves using a set of\\ntraining data to generate a decision tree that correctly classifies the training\\ndata. If the learning has worked, this decision tree will then correctly clas-\\nsify new input data as well.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 304, 'page_label': '305'}, page_content='278 CHAPTER 10 Introduction to Machine Learning\\nThe best-known decision tree induction algorithm is ID3, which was devel-\\noped by Quinlan in the 1980s.\\nThe ID3 algorithm builds a decision tree from the top down. The nodes are\\nselected by choosing features of the training data set that provide the most\\ninformation about the data and turning those features into questions. For\\nexample, in the above example, the first feature to be noted might be that\\nthe country of origin is a significant determinant of whether a film will be\\na success or not. Hence, the first question to be placed into the decision tree\\nis “what is the film’s country of origin?” .\\nThe most important feature of ID3 is how the features are chosen. It would\\nbe possible to produce a decision tree by selecting the features in an arbi-\\ntrary order, but this would not necessarily produce the most efficient deci-\\nsion tree. The ID3 algorithm finds the shortest possible decision tree that\\ncorrectly classifies the training data.\\n10.9.1 Information Gain\\nThe method used by ID3 to determine which features to use at each stage of\\nthe decision tree is to select, at each stage, the feature that provides the\\ngreatest information gain. Information gain is defined as the reduction in\\nentropy. The entropy of a set of training data, S, is defined as\\nH(S) = /H11002p\\n1 log2 p1 /H11002p0 log2 p0\\nwhere p1 is defined as the proportion of the training data that includes\\npositive examples, and p0 is defined as the proportion that includes neg-\\native examples. The entropy of S is zero when all the examples are posi-\\ntive, or when all the examples are negative. The entropy reaches its\\nmaximum value of 1 when exactly half of the examples are positive and\\nhalf are negative.\\nThe information gain of a particular feature tells us how closely that feature\\nrepresents the entire target function, and so at each stage, the feature that\\ngives the highest information gain is chosen to turn into a question.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 305, 'page_label': '306'}, page_content='10.9 Decision-Tree Induction 279\\n10.9.2 Example\\nWe will start with the training data given below:\\nFilm Country of origin Big star Genre Success\\nFilm 1 United States yes Science Fiction true\\nFilm 2 United States no Comedy false\\nFilm 3 United States yes Comedy true\\nFilm 4 Europe no Comedy true\\nFilm 5 Europe yes Science fiction false\\nFilm 6 Europe yes Romance false\\nFilm 7 Rest of World yes Comedy false\\nFilm 8 Rest of World no Science fiction false\\nFilm 9 Europe yes Comedy true\\nFilm 10 United States yes Comedy true\\nWe will now calculate the information gain for the three different attributes\\nof the films, to select which one to use at the top of the tree.\\nFirst, let us calculate the information gain of the attribute “country of ori-\\ngin. ” Our collection of training data consists of five positive examples and\\nfive negative examples, so currently it has an entropy value of 1.\\nFour of the training data are from the United States, four from Europe, and\\nthe remaining two from the rest of the world.\\nThe information gain of this attribute is the reduction in entropy that it\\nbrings to the data. This can be calculated as follows:\\nFirst, we calculate the entropy of each subset of the training data as broken\\nup by this attribute. In other words, we calculate the entropy of the items\\nthat are from the United States, the entropy of the items from Europe, and\\nthe entropy of the items from the rest of the world.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 306, 'page_label': '307'}, page_content='280 CHAPTER 10 Introduction to Machine Learning\\nOf the films from the United States, three were successes and one was not.\\nHence, the entropy of this attribute is\\nH(USA) = /H11002(3/4) log2 (3/4) /H11002(1/4) log2 (1/4)\\n= 0.311 + 0.5\\n= 0.811\\nSimilarly, we calculate the entropies of the other two subsets as divided by\\nthis attribute:\\nH(Europe) = 1\\n(since half of the European films were successes, and half were not).\\nH(Rest of world) = 0\\n(since none of these films were successes).\\nThe total information gain is now defined as the original entropy of the set\\nminus the weighted sum of these entropies, where the weight applied to\\neach entropy value is the proportion of the training data that fell into that\\ncategory. For example, four-tenths of the training data were from the\\nUnited States, so the weight applied to H(USA) is 4/10 = 0.4.\\nThe information gain is defined as:\\nGain = 1 /H11002(0.4 /H110030.811) /H11002(0.4 /H110031) /H11002(0.2 /H110030)\\n= 1 /H110020.3244 /H110020.4 /H110020\\n= 0.2756\\nHence, at this stage, the information gain for the “country of origin” attrib-\\nute is 0.2756.\\nFor the “Big star” attribute\\nH(yes) = 0.9852\\nH(no) = 1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 307, 'page_label': '308'}, page_content='10.9 Decision-Tree Induction 281\\nso, the information gain for this attribute is\\nGain = 1 /H11002(0.7 /H110030.9852) /H11002(0.3 /H110031)\\n= 1 /H110020.68964 /H110020.3\\n= 0.01\\nFor the “Genre” attribute\\nH(science fiction) = 0.918296\\nH(comedy) = 0.918296\\nH(romance) = 0\\n(note that we treat 0 /H11003log\\n20 as 0)\\nhence, the information gain for this attribute is\\nGain = 1 /H11002(0.3 /H110030.918296) /H11002(0.6 /H110030.918296) /H11002(0.1 /H110030)\\n= 1 /H110020.2754888 /H110020.5509776 /H110020\\n= 0.17\\nHence, at this stage, the category “Country of origin” provides the greatest\\nentropy gain and so is placed at the top of the decision tree. This method is\\nthen applied recursively to the sub-branches of the tree, examining the\\nentropy gain achieved by subdividing the training data further.\\n10.9.3 Inductive Bias of ID3\\nID3’s inductive bias is that it tends to produce the shortest decision tree\\nthat will correctly classify all of the training data. This fits very well with\\nOccam’s razor, which was briefly introduced in Section 10.8. It is not the\\ncase that Occam’s razor can be applied in all situations to provide the opti-\\nmal solution: it is, however, the case that ID3 tends to produce adequate\\nresults. Additionally, a smaller decision tree is clearly easier for humans to\\nunderstand, which in some circumstances can be very useful, for example if\\nthe need arises to debug the learner and find out why it makes a mistake on\\na particular piece of unseen data.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 308, 'page_label': '309'}, page_content='282 CHAPTER 10 Introduction to Machine Learning\\nFigure 10.2\\nIllustration of the problem\\nof overfitting\\n10.10 The Problem of Overfitting\\nIn some situations, decision trees (and other learning methods) can run\\ninto the problem of overfitting. Overfitting usually occurs when there is\\nnoise in the training data, or when the training data do not adequately rep-\\nresent the entire space of possible data. In such situations, it can be possible\\nfor one decision tree to correctly classify all the training data, but to per-\\nform less well at classifying unseen data than some other decision tree that\\nperforms poorly at classifying the training data. In other words, if the train-\\ning data do not adequately and accurately represent the entire data set, the\\ndecision tree that is learned from it may not match unseen data.\\nThis problem does not just apply to decision trees, but also to other learn-\\ning methods. It can best be understood by examining the illustration in\\nFigure 10.2.\\nIn the first diagram in Figure 10.2, black dots are positive training data, and\\nwhite dots are negative training data. The two lines represent two hypothe-\\nses that have been developed to distinguish the training data. The thin line\\nis a relatively simple hypothesis, which incorrectly classifies some of the\\ntraining data—it should have all positive examples below it and all negative\\nexamples above it. The thicker line correctly classifies all the training data,\\nusing a more complex hypothesis, which is somewhat warped by noise in\\nthe data. In the next diagram, the thin line is shown to map reasonably\\neffectively the full set of data. It does make some errors, but it reasonably'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 309, 'page_label': '310'}, page_content='10.11 The Nearest Neighbor Algorithm 283\\nclosely represents the trend in the data. The third diagram, however, shows\\nthat the more complex solution does not at all represent the full set of data.\\nThis hypothesis has been overfitted to the training data, allowing itself to\\nbe warped by noise in the training data.\\nOverfitting is, perhaps, a good illustration of why Occam’s razor can some-\\ntimes be a useful inductive bias: selecting a complex solution to accommo-\\ndate all of the training data can be a bad idea when the training data\\ncontain errors.\\n10.11 The Nearest Neighbor Algorithm\\nThe nearest neighbor algorithm is an example of instance-based learning.\\nInstance-based learning methods do not attempt to generalize from train-\\ning data to produce a hypothesis to match all input data, instead, they store\\nthe training data and use these data to determine a classification for each\\nnew piece of data as it is encountered.\\nThe nearest neighbor algorithm operates in situations where each instance\\ncan be defined by an n-dimensional vector, where n is the number of attrib-\\nutes used to describe each instance, and where the classifications are dis-\\ncrete numerical values. The training data are stored, and when a new\\ninstance is encountered it is compared with the training data to find its\\nnearest neighbors. This is done by computing the Euclidean distance\\nbetween the instances in n-dimensional space. In two-dimensional space,\\nfor example, the distance between <x\\n1, y1> and <x2, y2> is\\nTypically, the nearest neighbor algorithm obtains the classifications of the\\nnearest k neighbors to the instance that is to be classified and assigns it the\\nclassification that is most commonly returned by those neighbors.\\nAn alternative approach is to weight the contribution of each of the neigh-\\nbors according to how far it is from the instance that is to be classified. In\\nthis way, it is possible to allow every instance of training data to contribute\\nto the classification of a new instance. When used in this way, the algorithm\\nis known as Shepard’s method (see Shepard 1968).\\nUnlike decision-tree learning, the nearest neighbor algorithm performs\\nvery well with noisy input data. Its inductive bias is to assume that\\nxx yy12\\n2\\n12\\n2−( ) +−( )( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 310, 'page_label': '311'}, page_content='284 CHAPTER 10 Introduction to Machine Learning\\ninstances that are close to each other in terms of Euclidean distance will\\nhave similar classifications. In some cases, this can be an erroneous\\nassumption; for example, in a situation where 10 attributes are used to\\ndefine each instance, but only 3 of those attributes play any part in deter-\\nmining the classification of the instance. In this situation, instances can be\\nvery far apart from each other in 10-dimensional space and yet have the\\nsame classification. This problem can be avoided to some extent by neglect-\\ning to include unimportant attributes from the calculations.\\n10.12 Learning Neural Networks\\nAn artificial neural network is a network of simple processing nodes,\\nwhich is roughly modeled on the human brain. The human brain is a mas-\\nsively parallel computation device, which achieves its power through the\\nenormous connectivity between its neurons. Each neuron is a very simple\\ndevice that can either fire or not fire, but by combining billions of these\\nneurons together, the brain is able to achieve levels of complexity as yet\\nunattainable by machines.\\nThe word artificial is often used to describe neural networks to differentiate\\nthem from the biological neural networks that make up the human brain,\\nbut in this book we shall simply refer to them as neural networks because it\\nshould be clear from the context which type of network we are referring to.\\nNeural networks consist of a number of nodes, each of which can be\\nthought of as representing a neuron. Typically, these neurons are arranged\\ninto layers, and the neurons from one layer are connected to the neurons in\\nthe two layers on either side of it.\\nTypically, the network is arranged such that one layer is the input layer,\\nwhich receives inputs that are to be classified. These inputs cause some of the\\nneurons in the input layer to fire, and these neurons in turn pass signals to\\nthe neurons to which they are connected, some of which also fire, and so on.\\nIn this way, a complex pattern of firings is arranged throughout the network,\\nwith the final result being that some neurons in the final output layer fire.\\nThe connections between neurons are weighted, and by modifying these\\nweights, the neural network can be arranged to perform extremely complex\\nclassification tasks such as handwriting analysis and face recognition.\\nAs we see in Chapter 11 where we discuss them in more detail, neural net-\\nworks have a number of advantages over other learning methods. Many of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 311, 'page_label': '312'}, page_content='10.14 Unsupervised Learning 285\\nthese advantages derive from features of the human brain. For example,\\nneural networks are extremely robust, both to errors in any training data\\nand to damage that may be caused to the network itself.\\n10.13 Supervised Learning\\nSupervised learning networks learn by being presented with preclassified\\ntraining data. The techniques we have discussed so far in this chapter use\\nforms of supervised learning. Neural networks that use supervised learning\\nlearn by modifying the weights of the connections within their networks to\\nmore accurately classify the training data. In this way, neural networks are\\nable to generalize extremely accurately in many situations from a set of\\ntraining data to the full set of possible inputs.\\nOne of the most commonly used methods for supervised learning is back-\\npropagation, which will be discussed in Chapter 11.\\n10.14 Unsupervised Learning\\nUnsupervised learning methods learn without any human intervention. A\\ngood example of an unsupervised learning network is a Kohonen map.A\\nKohonen map is a neural network that is able to learn to classify a set of\\ninput data without being told what the classifications are and without\\nbeing given any training data. This method is particularly useful in situa-\\ntions where data need to be classified, or clustered, into a set of classifica-\\ntions but where the classifications are not known in advance.\\nFor example, given a set of documents retrieved from the Internet (perhaps\\nby an intelligent information agent), a Kohonen map could cluster similar\\ndocuments together and automatically provide an indication of the distinct\\nsubjects that are covered by the documents.\\nAnother method for unsupervised learning in neural networks was pro-\\nposed by Donald Hebb in 1949 and is known as Hebbian learning. Hebbian\\nlearning is based on the idea that if two neurons in a neural network are\\nconnected together, and they fire at the same time when a particular input\\nis given to the network, then the connection between those two neurons\\nshould be strengthened. It seems likely that something not dissimilar from\\nHebbian learning takes place in the human brain when learning occurs\\n(Edelman 1987).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 312, 'page_label': '313'}, page_content='286 CHAPTER 10 Introduction to Machine Learning\\n10.15 Reinforcement Learning\\nClassifier systems, which are discussed in Chapter 13, use a form of rein-\\nforcement learning. A system that uses reinforcement learning is given a\\npositive reinforcement when it performs correctly and a negative reinforce-\\nment when it performs incorrectly. For example, a robotic agent might\\nlearn by reinforcement learning how to pick up an object. When it success-\\nfully picks up the object, it will receive a positive reinforcement.\\nThe information that is provided to the learning system when it performs\\nits task correctly does not tell it why or how it performed it correctly, simply\\nthat it did.\\nSome neural networks learn by reinforcement. The main difficulty with\\nsuch methods is the problem of credit assignment. The classifier systems\\n(which are discussed in Chapter 13) use a bucket brigade algorithm for\\ndeciding how to assign credit (or blame) to the individual components of\\nthe system. Similar methods are used with neural networks to determine to\\nwhich neurons to give credit when the network performs correctly and\\nwhich to blame when it does not.\\n10.16 Chapter Summary\\n■ Many learning methods use some form of training to learn to gen-\\neralize from a set of preclassified training data to be able to cor-\\nrectly classify unseen data.\\n■ Rote learning involves simply memorizing the classifications of\\ntraining data. A rote learning system is not able to generalize and\\nso is only able to classify data it has seen before.\\n■ A general-to-specific ordering of hypotheses can be used to learn\\nto generalize from a set of training data to a hypothesis that\\nmatches all input data. This is known as concept learning.\\n■ A version space, which consists of all possible hypotheses that\\nmatch a given set of training data, can be used to generalize from\\nthose training data to learn to classify unseen data.\\n■ Candidate elimination is a method that uses the general-to-specific\\nordering to produce a set of hypotheses that represent the entire\\nversion space for a problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 313, 'page_label': '314'}, page_content='10.17 Review Questions 287\\n■ The inductive bias of a learning method is the assumptions it\\nmakes about the possible hypotheses that can be used. A learning\\nsystem with no inductive bias is not capable of generalizing beyond\\nthe training data it is given.\\n■ Decision-tree induction can be used to learn a decision tree that\\nwill correctly classify a set of input data. The inductive bias of deci-\\nsion-tree induction is to prefer shorter trees.\\n■ The problem of overfitting occurs when there is noise in the train-\\ning data that causes a learning method to develop a hypothesis that\\ncorrectly matches the training data but does not perform well on\\nother input data.\\n■ The nearest neighbor algorithm simply memorizes the classifica-\\ntions of the training data, and when presented with a new piece of\\ndata gives the majority answer given by the closest neighbors to\\nthis piece of data in n-dimensional space.\\n■ Neural networks are based on biological networks of neurons con-\\ntained within the human brain.\\n■ Supervised learning methods learn from manually classified\\ntraining data.\\n■ Unsupervised learning methods such as Kohonen maps learn\\nwithout any manual intervention.\\n■ A system that uses reinforcement learning is given a positive rein-\\nforcement when it performs correctly. Credit and blame assign-\\nment are important features of such methods.\\n10.17 Review Questions\\n10.1 Explain the idea behind learning by generalization.\\n10.2 What is meant by inductive bias? Is it a good thing? What is the\\ninductive bias of the ID3 algorithm?\\n10.3 Explain how candidate elimination uses version spaces to learn.\\n10.4 Explain how a system can learn by building decision trees, using\\nthe ID3 algorithm.\\n10.5 How does the nearest neighbor algorithm work?\\n10.6 Explain the problem of overfitting and how it can be avoided.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 314, 'page_label': '315'}, page_content='288 CHAPTER 10 Introduction to Machine Learning\\n10.7 Explain the differences and similarities between the following\\nthree types of learning methods:\\nsupervised\\nunsupervised\\nreinforcement\\n10.18 Exercises\\n10.1 Use the ID3 algorithm to build the full decision tree for the data set\\ngiven in Section 10.9.2.\\n10.2 Implement the nearest neighbor algorithm in the programming\\nlanguage of your choice. The algorithm should work with vectors\\nof up to 10 integer values and allow up to 10 integer classifications.\\nBy mapping each value to a number, use your program to learn\\nfrom the training data given in Section 10.9.2. Have your program\\nnow classify the following films:\\nFilm Country of origin Big star Genre\\nFilm 11 United States no Science fiction\\nFilm 12 United States yes Romance\\nFilm 13 United States no Romance\\nFilm 14 Europe no Science fiction\\nFilm 15 Rest of world no Romance\\nComment on the results.\\n10.19 Further Reading\\nMitchell (1997) provides an excellent coverage of many aspects of machine\\nlearning. An excellent background from a biological perspective is pro-\\nvided by Pfeifer and Scheier (1999). Winston (1992) developed many of the\\nconcepts that are used today in machine learning.\\nFurther references on neural networks are given in the Further Reading\\nsection of Chapter 11 of this book.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 315, 'page_label': '316'}, page_content='10.19 Further Reading 289\\nLearning from Data: Concepts, Theory, and Methods by Vladimir Cher-\\nkassky and Filip Mulier (1998 – Wiley Interscience)\\nNeural Darwinism: The Theory of Neuronal Group Selection by Gerald M.\\nEdelman (1990 – Oxford University Press)\\nLearning and Soft Computing: Support Vector Machines, Neural Networks,\\nand Fuzzy Logic Models (Complex Adaptive Systems) by Vojislav Kecman\\n(2001 – MIT Press)\\nMachine Learning by T om M. Mitchell (1997 – McGraw Hill)\\nMachine Learning: A Theoretical Approach by Balas K. Natarajan (1991 –\\nMorgan Kaufmann)\\nUnderstanding Intelligence by Rolf Pfeifer and Christian Scheier (1999 –\\nMIT Press)\\nInduction of Decision Treesby J. R. Quinlan (1986 – from Machine Learning,\\nVol. 1, pp. 81–106)\\nA Two Dimensional Interpolation Function for Irregularly Spaced Data by\\nD. Shepard (1968 - Proceedings of the 23rd National Conference of the ACM,\\npp. 517–523)\\nReinforcement Learning: An Introduction (Adaptive Computation and Machine\\nLearning) by Richard S. Sutton and Andrew G. Barto (1998 – MIT Press)\\nStatistical Learning Theoryby Vladimir N.Vapnik (1998 – Wiley Interscience)\\nAn Introduction to Computational Learning Theory by Michael J. Kearns\\nand Umesh V . Vazirani (1994 – MIT Press)\\nLearning and Generalization: With Applications to Neural Networks by\\nMathukumalli Vidyasagar (2002 – Springer V erlag)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 316, 'page_label': '317'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 317, 'page_label': '318'}, page_content='11CHAPTER\\nNeural Networks\\nMan, unlike any other thing organic or inorganic in the universe, grows\\nbeyond his work, walks up the stairs of his concepts, emerges ahead of his\\naccomplishments.\\n—John Steinbeck, The Grapes of Wrath\\nBehind a pot of ferns the wagging clock\\nTells me the hour’s word, the neural meaning\\nFlies on the shafted disc, declaims the morning\\nAnd tells the windy weather in the cock.\\n—Dylan Thomas, Especially When the October Wind\\nHis high pitched voice already stood out above the general murmur of well-\\nbehaved junior executives grooming themselves for promotion within the Bell\\ncorporation. Then he was suddenly heard to say: ‘No, I’m not interested in\\ndeveloping a powerful brain. All I’m after is just a mediocre brain, something\\nlike the President of the American Telephone and Telegraph Company. ’\\n—Alan Turing, quoted in Alan Turing the \\nEnigma of Intelligence by A. Hodge\\n11.1 Introduction\\nThis chapter introduces the relationship between biological neurons, which\\nmake up human brains, and artificial neurons, which are used in artificial\\nneural networks. McCulloch and Pitts neurons are explained, and the capa-\\nbilities and limitations of perceptrons are examined. Multilayer neural'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 318, 'page_label': '319'}, page_content='292 CHAPTER 11 Neural Networks\\nOUTPUT\\nAXONSOMA\\nSYNAPSE\\nDENDRITES\\nFigure 11.1\\nA neuron in the human\\nbrain\\nnetworks are explored, and the backpropagation algorithm for supervised\\nlearning in multilayer networks is explained. Recurrent networks, such as\\nHopfield networks and other bidirectional associative memories, are also\\nexplained. Unsupervised learning is explained through the use of Kohonen\\nmaps and Hebb’s law.\\nAlthough the neural networks presented in this chapter are very simplistic,\\nreal-world networks can be extremely complex, consisting of hundreds or\\neven thousands of neurons. Networks of this size can often appear like a\\n“black box, ” in the sense that it is not clear why they behave in the way they\\ndo. In fact, the behavior of complex neural networks is often emergent.\\n11.2 Neurons\\n11.2.1 Biological Neurons\\nThe human brain contains over ten billion neurons, each of which is con-\\nnected, on average, to several thousand other neurons. These connections\\nare known as synapses, and the human brain contains about 60 trillion\\nsuch connections.\\nNeurons are in fact very simple processing elements. Each neuron contains\\na soma, which is the body of the neuron, an axon, and a number of den-\\ndrites. A simplified diagram of a biological neuron is shown in Figure 11.1.\\nThe neuron receives inputs from other neurons along its dendrites, and\\nwhen this input signal exceeds a certain threshold, the neuron “fires”—in'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 319, 'page_label': '320'}, page_content='11.2 Neurons 293\\nfact, a chemical reaction occurs, which causes an electrical pulse, known as\\nan action potential, to be sent down the axon (the output of the neuron),\\ntoward synapses that connect the neuron to the dendrites of other neurons.\\nAlthough each neuron individually is extremely simple, this enormously\\ncomplex network of neurons is able to process information at a great rate\\nand of extraordinary complexity. The human brain far exceeds in terms of\\ncomplexity any device created by man, or indeed, any naturally occurring\\nobject or structure in the universe, as far as we are aware today.\\nThe human brain has a property known asplasticity, which means that neu-\\nrons can change the nature and number of their connections to other neu-\\nrons in response to events that occur. In this way, the brain is able to learn. As\\nis explained in Chapter 10, the brain uses a form of credit assignment to\\nstrengthen the connections between neurons that lead to correct solutions to\\nproblems and weakens connections that lead to incorrect solutions. The\\nstrength of a connection, or synapse, determines how much influence it will\\nhave on the neurons to which it is connected, and so if a connection is weak-\\nened, it will play less of a role in subsequent computations.\\n11.2.2 Artificial Neurons\\nArtificial neural networks are modeled on the human brain and consist of\\na number of artificial neurons. Neurons in artificial neural networks tend\\nto have fewer connections than biological neurons, and neural networks\\nare all (currently) significantly smaller in terms of number of neurons than\\nthe human brain.\\nThe neurons that we examine in this chapter were invented by McCulloch\\nand Pitts (1943) and so are often referred to as McCulloch and Pitts neurons.\\nEach neuron (or node) in a neural network receives a number of inputs. A\\nfunction called the activation function is applied to these input values,\\nwhich results in the activation level of the neuron, which is the output\\nvalue of the neuron. There are a number of possible functions that can be\\nused in neurons. Some of the most commonly used activation functions\\nare illustrated in Figure 11.2.\\nIn Figure 11.2, thex-axis of each graph represents the input value to the neu-\\nron, and they-axis represents the output, or the activation level, of the neuron.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 320, 'page_label': '321'}, page_content='294 CHAPTER 11 Neural Networks\\nYY\\n+1 +1\\nt XX\\nY\\nX\\n–1\\n(a) Step function (b) Sigmoid function (c) Linear function\\nFigure 11.2\\nThree activation functions\\nOne of the most commonly used functions is the step function, or linear\\nthreshold function. In using this function, the inputs to the neuron are\\nsummed (having each been multiplied by a weight), and this sum is com-\\npared with a threshold, t. If the sum is greater than the threshold, then the\\nneuron fires and has an activation level of +1. Otherwise, it is inactive and has\\nan activation level of zero. (In some networks, when the sum does not exceed\\nthe threshold, the activation level is considered to be /H110021 instead of 0).\\nHence, the behavior of the neuron can be expressed as follows:\\nX is the weighted sum of the n inputs to the neuron, x\\n1 to xn, where each\\ninput, xn is multiplied by its corresponding weight wn. For example, let us\\nconsider a simple neuron that has just two inputs. Each of these inputs has\\na weight associated with it, as follows:\\nw\\n1 = 0.8\\nw2 = 0.4\\nThe inputs to the neuron are x1 and x2:\\nx1 = 0.7\\nx2 = 0.9\\nSo, the summed weight of these inputs is\\n(0.8 /H110030.7) + (0.4 /H110030.9) = 0.92\\nThe activation level Y, is defined for this neuron as\\nY\\nfor X t\\nfor X t=\\n+>\\n≤\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\n0\\nXw x ii\\ni\\nn\\n=\\n=\\n∑\\n1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 321, 'page_label': '322'}, page_content='11.3 Perceptrons 295\\nHence, if t is less than or equal to 0.92, then this neuron will fire with this\\nparticular set of inputs. Otherwise, it will have an activation level of zero.\\nA neuron that uses the linear activation function simply uses the weighted\\nsum of its inputs as its activation level. The sigmoid function converts inputs\\nfrom a range of/H11002/H11009to +/H11009into an activation level in the range of 0 to +1.\\nA neural network consists of a set of neurons that are connected together.\\nLater in this chapter we explore the ways in which neurons are usually con-\\nnected together. The connections between neurons have weights associated\\nwith them, and each neuron passes its output on to the inputs of the neu-\\nrons to which it is connected. This output depends on the application of\\nthe activation function to the inputs it receives. In this way, an input signal\\nto the network is processed by the entire network and an output (or multi-\\nple outputs) produced. There is no central processing or control mecha-\\nnism—the entire network is involved in every piece of computation that\\ntakes place.\\nThe way in which neurons behave over time is particularly interesting.\\nWhen an input is given to a neural network, the output does not appear\\nimmediately because it takes some finite period of time for signals to pass\\nfrom one neuron to another. In artificial neural networks this time is usu-\\nally very short, but in the human brain, neural connections are surprisingly\\nslow. It is only the enormously parallel nature of the brain that enables it to\\ncalculate so quickly.\\nFor neural networks to learn, the weight associated with each connection\\n(equivalent to a synapse in the biological brain) can be changed in response\\nto particular sets of inputs and events. As is mentioned in Chapter 10, Heb-\\nbian learning involves increasing the weight of a connection between two\\nneurons if both neurons fire at the same time. We learn more about this\\nlater in the chapter.\\n11.3 Perceptrons\\nThe perceptron, which was first proposed by Rosenblatt (1958), is a simple\\nneuron that is used to classify its inputs into one of two categories.\\nThe perceptron can have any number of inputs, which are sometimes\\narranged into a grid. This grid can be used to represent an image, or a field\\nof vision, and so perceptrons can be used to carry out simple image classi-\\nfication or recognition tasks.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 322, 'page_label': '323'}, page_content='296 CHAPTER 11 Neural Networks\\nA perceptron uses a step function that returns +1 if the weighted sum of the\\ninputs, X, is greater than a threshold,t, and /H110021i f X is less than or equal tot:\\nThis function is often written as Step (X):\\nin which case, the activation function for a perceptron can be written as\\nNote that here we have allowed i to run from 0 instead of from 1. This\\nmeans that we have introduced two new variables: w0 and x0. We define x0\\nas 1, and w0 as /H11002t.\\nA single perceptron can be used to learn a classification task, where it\\nreceives an input and classifies it into one of two categories: 1 or 0. We can\\nconsider these to represent true and false, in which case the perceptron can\\nlearn to represent a Boolean operator, such as AND or OR.\\nThe learning process for a perceptron is as follows:\\nFirst, random weights are assigned to the inputs. Typically, these weights\\nwill be chosen between /H110020.5 and +0.5.\\nNext, an item of training data is presented to the perceptron, and its output\\nclassification observed. If the output is incorrect, the weights are adjusted\\nto try to more closely classify this input. In other words, if the perceptron\\nincorrectly classifies a positive piece of training data as negative, then the\\nweights need to be modified to increase the output for that set of inputs.\\nThis can be done by adding a positive value to the weight of an input that\\nhad a negative input value, and vice versa.\\nThe formula for this modification, as proposed by Rosenblatt (Rosenblatt\\n1960) is as follows:\\nY Step w x ii\\ni\\nn\\n= \\uf8eb\\n\\uf8ed\\uf8ec\\n\\uf8f6\\n\\uf8f8\\uf8f7\\n=\\n∑\\n0\\nStep X\\nfor X t\\nfor X t( ) =\\n+>\\n≤\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\n0\\nXw x\\nY\\nfor X t\\nfor X t\\nii\\ni\\nn\\n=\\n=\\n+>\\n≤\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n=\\n∑\\n1\\n1\\n0'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 323, 'page_label': '324'}, page_content='11.3 Perceptrons 297\\nwi ← wi + (a /H11003xi /H11003e)\\nwhere e is the error that was produced, anda is the learning rate,w h e r e0<\\na <1 ; e is defined as 0 if the output is correct, and otherwise it is positive if\\nthe output is too low and negative if the output is too high. In this way, if the\\noutput is too high, a decrease in weight is caused for an input that received\\na positive value. This rule is known as theperceptron training rule.\\nOnce this modification to the weights has taken place, the next piece of\\ntraining data is used in the same way. Once all the training data have been\\napplied, the process starts again, until all the weights are correct and all\\nerrors are zero. Each iteration of this process is known as an epoch.\\nLet us examine a simple example: we will see how a perceptron can learn to\\nrepresent the logical-OR function for two inputs. We will use a threshold of\\nzero (t = 0) and a learning rate of 0.2.\\nFirst, the weight associated with each of the two inputs is initialized to a\\nrandom value between /H110021 and +1:\\nw\\n1 = /H110020.2\\nw2 = 0.4\\nNow, the first epoch is run through. The training data will consist of the\\nfour combinations of 1’s and 0’s possible with two inputs.\\nHence, our first piece of training data is\\nx1 = 0\\nx2 = 0\\nand our expected output is x1 ∨ x2 = 0.\\nWe apply our formula for Y:\\nHence, the output Y is as expected, and the error, e, is therefore 0. So the\\nweights do not change.\\nNow, for x1 = 0 and x2 = 1:\\nY Step w x\\nStep\\nii\\ni\\nn\\n= \\uf8eb\\n\\uf8ed\\uf8ec\\n\\uf8f6\\n\\uf8f8\\uf8f7\\n=× − ( ) +×( )( )\\n=\\n=\\n∑\\n0\\n00 20 0 4\\n0\\n..'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 324, 'page_label': '325'}, page_content='298 CHAPTER 11 Neural Networks\\nY = Step ((0 /H11003/H110020.2) + (1 /H110030.4))\\n= Step (0.4)\\n= 1\\nAgain, this is correct, and so the weights do not need to change.\\nFor x1 = 1 and x2 = 0:\\nY = Step ((1 /H11003/H110020.2) + (0 /H110030.4))\\n= Step (/H110020.2)\\n= 0\\nThis is incorrect because 1 \\n∨ 0 = 1, so we should expect Y to be 1 for this set\\nof inputs. Hence, the weights are adjusted.\\nWe will use the perceptron training rule to assign new values to the weights:\\nwi ← wi + (a /H11003xi /H11003e)\\nOur learning rate is 0.2, and in this case, the e is 1, so we will assign the fol-\\nlowing value to w1:\\nw1 = /H110020.2 + (0.2 /H110031 /H110031)\\n= /H110020.2 + (0.2)\\n= 0\\nWe now use the same formula to assign a new value to w2:\\nw2 = 0.4 + (0.2 /H110030 /H110031)\\n= 0.4\\nBecause w2 did not contribute to this error, it is not adjusted.\\nThe final piece of training data is now used (x1 = 1 and x2= 1):\\nY = Step ((0 /H110031) + (0.4 /H110031))\\n= Step (0 + 0.4)\\n= Step (0.4)\\n= 1\\nThis is correct, and so the weights are not adjusted.\\nThis is the end of the first epoch, and at this point the method runs again\\nand continues to repeat until all four pieces of training data are classified\\ncorrectly.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 325, 'page_label': '326'}, page_content='11.3 Perceptrons 299\\nTable 11.1 A sample run showing how the weights change for a simple perceptron \\nwhen it learns to represent the logical OR function\\nEpoch X1 X2 Expected Y Actual Y Error w1 w2\\n1 000 0 0 /H110020.2 0.4\\n1 011 1 0 /H110020.2 0.4\\n1 101 0 1 0 0 . 4\\n1 111 1 0 0 0 . 4\\n2 000 0 0 0 0 . 4\\n2 011 1 0 0 0 . 4\\n2 1 0 1 0 1 0.2 0.4\\n2 1 1 1 1 0 0.2 0.4\\n3 0 0 0 0 0 0.2 0.4\\n3 0 1 1 1 0 0.2 0.4\\n3 1 0 1 1 0 0.2 0.4\\n3 1 1 1 1 0 0.2 0.4\\nTable 11.1 shows the complete sequence—it takes just three epochs for the\\nperceptron to correctly learn to classify input values. Lines in which an\\nerror was made are marked in bold.\\nAfter just three epochs, the perceptron learns to correctly model the logi-\\ncal-OR function.\\nIn the same way, a perceptron can be trained to model other logical func-\\ntions such as AND, but there are some functions that cannot be modeled\\nusing a perceptron, such as exclusive OR.\\nThe reason for this is that perceptrons can only learn to model functions\\nthat are linearly separable. A linearly separable function is one that can be\\ndrawn in a two-dimensional graph, and a single straight line can be drawn\\nbetween the values so that inputs that are classified into one classification\\nare on one side of the line, and inputs that are classified into the other are\\non the other side of the line. Figure 11.3 shows how such a line can be\\ndrawn for the OR function, but not for the exclusive-OR function. Four'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 326, 'page_label': '327'}, page_content='300 CHAPTER 11 Neural Networks\\nx2\\n1\\nx1\\n10\\nx2\\n1\\nx1\\n10\\nFigure 11.3\\nIllustrating the difference\\nbetween a linearly separa-\\nble function and one\\nwhich is not\\npoints are plotted on each graph, and a solid dot represents true, and a hol-\\nlow dot represents a value of false. It should be clear that no dashed line\\ncould be drawn in the second case, for the exclusive OR function, that\\nwould separate solid dots from hollow ones.\\nThe reason that a single perceptron can only model functions that are lin-\\nearly separable can be seen by examining the following function:\\nUsing these functions, we are effectively dividing the search space using a\\nline for which X = t. Hence, in a perceptron with two inputs, the line that\\ndivides one class from the other is defined as follows:\\nw\\n1x1 + w2x2 = t\\nThe perceptron functions by identifying a set of values for wi, which gener-\\nates a suitable function. In cases where no such linear function exists, the\\nperceptron cannot succeed.\\n11.4 Multilayer Neural Networks\\nMost real-world problems are not linearly separable, and so although per-\\nceptrons are an interesting model for studying the way in which artificial\\nneurons can work, something more powerful is needed.\\nAs has already been indicated, neural networks consist of a number of neu-\\nrons that are connected together, usually arranged in layers.\\nXw x\\nY\\nfor X t\\nfor X t\\nii\\ni\\nn\\n=\\n=\\n+>\\n−≤\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n=\\n∑\\n1\\n1\\n1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 327, 'page_label': '328'}, page_content='11.4 Multilayer Neural Networks 301\\nFigure 11.4\\nA simple three-layer feed-\\nforward neural network\\nA single perceptron can be thought of as a single-layer perceptron. Multi-\\nlayer perceptrons are capable of modeling more complex functions, includ-\\ning ones that are not linearly separable, such as the exclusive-OR function.\\nT o see that a multilayer network is capable of modeling a function that is\\nnot linearly separable, such as exclusive-OR, note that the functions NOR\\nand NAND are both linearly separable and so can be represented by a sin-\\ngle perceptron. By combining these functions together, all other Boolean\\nfunctions can be generated. Hence, by combining single perceptrons in just\\ntwo layers, any binary function of two inputs can be generated.\\nA typical architecture for a multilayer neural network is shown in Figure 11.4.\\nThe network shown in Figure 11.4 is a feed-forward network, consisting of\\nthree layers.\\nThe first layer is the input layer . Each node (or neuron) in this layer\\nreceives a single input signal. In fact, it is usually the case that the nodes in\\nthis layer are not neurons, but simply act to pass input signals on to the\\nnodes in the next layer, which is in this case a hidden layer.\\nA network can have one or more hidden layers, which contain the neurons\\nthat do the real work. Note that each input signal is passed to each of the\\nnodes in this layer and that the output of each node in this layer is passed to\\neach node in the final layer, which is the output layer. The output layer car-\\nries out the final stage of processing and sends out output signals.\\nThe network is called feed-forward because data are fed forward from the\\ninput nodes through to the output nodes. This is in contrast with recur-\\nrent networks, which we examine in Section 11.5, where some data are\\npassed back from the output nodes to the input nodes.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 328, 'page_label': '329'}, page_content='302 CHAPTER 11 Neural Networks\\nA typical feed-forward neural network consists of an input layer, one or\\ntwo hidden layers, and an output layer, and may have anywhere between 10\\nand 1000 neurons in each layer.\\n11.4.1 Backpropagation\\nMultilayer neural networks learn in much the same way as single percep-\\ntrons. The main difference is that in a multilayer network, each neuron has\\nweights associated with its inputs, and so there are a far greater number of\\nweights to be adjusted when an error is made with a piece of training data.\\nClearly, an important question is how to assign blame (or credit) to the var-\\nious weights. One method that is commonly used is backpropagation.\\nRather than using the simple Step function that single perceptrons use,\\nmultilayer backpropagation networks usually use the sigmoid function,\\nwhich is illustrated in Figure 11.2(b).\\nThe sigmoid function is defined as follows:\\nThis function is easy to differentiate because\\nThis is in contrast with the Step function used by perceptrons, which has\\nno simple derivative.\\nAs with the single perceptron, the backpropagation algorithm starts by ini-\\ntializing the weights in the network to random values, which are usually set\\nto small values, say in the range of /H110020.5 to 0.5. Alternatively, the weights\\ncan be normally distributed over the range from /H110022.4/n to 2.4/n, where n is\\nthe number of inputs to the input layer.\\nEach iteration of the algorithm involves first feeding data through the net-\\nwork from the inputs to the outputs. The next phase, which gives the algo-\\nrithm its name, involves feeding errors back from the outputs to the inputs.\\nThese error values feed back through the network, making changes to the\\nweights of nodes along the way. The algorithm repeats in this way until the\\noutputs produced for the training data are sufficiently close to the desired\\nvalues—in other words, until the error values are sufficiently small.\\ndx\\ndx xxσ σσ( ) = ( ) ⋅− ( )( )1\\nσ x\\ne x( ) =\\n+ −\\n1\\n1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 329, 'page_label': '330'}, page_content='11.4 Multilayer Neural Networks 303\\nBecause the sigmoid function cannot actually reach 0 or 1, it is usual to\\naccept a value such as 0.9 as representing 1 and 0.1 as representing 0.\\nNow we shall see the formulae that are used to adjust the weights in the\\nbackpropagation algorithm. We will consider a network of three layers and\\nwill use i to represent nodes in the input layer, j to represent nodes in the\\nhidden layer, and k to represent nodes in the output layer. Hence, for exam-\\nple, w\\nij refers to the weight of a connection between a node in the input\\nlayer and a node in the hidden layer.\\nThe function that is used to derive the output value for a node j in the net-\\nwork is as follows:\\nwhere n is the number of inputs to node j;w ij is the weight of the connec-\\ntion between each node i and node j; /H9258j is the threshold value being used for\\nnode j, which is set to a random value between 0 and 1;xi is the input value\\nfor input node I; and yj is the output value produced by node j.\\nOnce the inputs have been fed through the network to produce outputs, an\\nerror gradient is calculated for each node k in the output layer.\\nThe error signal for k is defined as the difference between the desired value\\nand the actual value for that node:\\nek = dk /H11002yk\\ndk is the desired value for node k, and yk is the actual value, in this iteration.\\nThe error gradient for output node k is defined as the error value for this\\nnode multiplied by the derivative of the activation function:\\nxk is the weighted sum of the input values to the node k.\\nBecause y is defined as a sigmoid function of x, we can use the formula that\\nwas given above for the derivative of the sigmoid function to obtain the fol-\\nlowing formula for the error gradient:\\nδk\\nk\\nk\\nk\\ny\\nx e= ∂\\n∂ ⋅\\nXx w\\nY\\ne\\nj i ij j\\ni\\nn\\nj X j\\n=⋅ −\\n=\\n+\\n=\\n−\\n∑ θ\\n1\\n1\\n1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 330, 'page_label': '331'}, page_content='304 CHAPTER 11 Neural Networks\\nSimilarly, we calculate an error gradient for each node j in the hidden layer,\\nas follows:\\nwhere n is the number of nodes in the output layer, and thus the number of\\noutputs from each node in the hidden layer.\\nNow each weight in the network,wij or wjk, is updated according to the fol-\\nlowing formula:\\nwhere xi is the input value to input node i, and /H9251is the learning rate, which\\nis a positive number below 1, and which should not be too high.\\nThis method is known as gradient descent because it involves following\\nthe steepest path down the surface that represents the error function to\\nattempt to find the minimum in the error space, which represents the set of\\nweights that provides the best performance of the network.\\nIn fact, the iteration of the backpropagation algorithm is usually termi-\\nnated when the sum of the squares of the errors of the output values for all\\ntraining data in an epoch is less than some threshold, such as 0.001.\\nNote that this method assigns blame to individual nodes within the net-\\nwork by comparing the weights attached to each node with the error asso-\\nciated with that node. In the case of hidden nodes, there is no error value\\nbecause there is no specific desired output value for these nodes. In this\\ncase, the weight of each connection between a hidden layer node and an\\noutput node is multiplied by the error of that output node to attempt to\\ndistribute the blame between the nodes in the hidden layer according to\\nhow much each one contributes to the error.\\nUnlike Hebbian learning, which is discussed in more detail in Section\\n11.6.3, backpropagation does not appear to occur in the human brain.\\nAdditionally, it is rather inefficient and tends to be too slow for use in solv-\\nww x\\nww y\\nij ij i j\\njk jk j k\\n←+ ⋅ ⋅\\n←+ ⋅ ⋅\\nαδ\\nαδ\\nδδjj j j k k\\nk\\nn\\nyy w=⋅ − ( )\\n=\\n∑1\\n1\\nδkk k kyy e=⋅ − ( ) ⋅1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 331, 'page_label': '332'}, page_content='11.4 Multilayer Neural Networks 305\\ning real-world problems. With some simple problems it can take hundreds\\nor even thousands of epochs to reach a satisfactorily low level of error.\\n11.4.2 Improving the Performance of Backpropagation\\nA common method used to improve the performance of backpropagation\\nis to include momentum in the formula that is used to modify the weights.\\nThe momentum takes into account the extent to which a particular weight\\nwas changed on the previous iteration. We shall use t to represent the cur-\\nrent iteration, and t /H110021 to represent the previous iteration. Hence, we can\\nwrite our learning rules as follows:\\n/H9004w\\nij(t) is the amount that is added to the weight of the connection between\\nnodes i and j, wij at iteration t; /H9252is the momentum value, which is a positive\\nnumber between 0 and 1. Typically, a fairly high value such as 0.95 is used.\\nIf /H9252is zero, this is the same as the backpropagation algorithm without\\nusing momentum.\\nThis rule, including the momentum value, is known as the generalized\\ndelta rule.\\nThe inclusion of the momentum value has the benefit of enabling the\\nbackpropagation method to avoid local minima and also to move more\\nquickly through areas where the error space is not changing.\\nAn alternative method of speeding up backpropagation is to use the hyper-\\nbolic tangent function, tanh, instead of the sigmoid function, which tends\\nto enable the network to converge on a solution in fewer iterations. The\\ntanh function is defined as:\\nwhere a and b are constants, such as a = 1.7 and b = 0.7.\\nA final way to improve the performance of backpropagation is to vary the\\nvalue of the learning rate, /H9251during the course of training the network. Two\\nheuristics proposed by R. A. Jacobs (1988) use the direction of change\\ntanh x a\\ne\\nabx( ) =\\n+\\n−−\\n2\\n1\\n∆∆\\n∆∆\\nwt x wt\\nwt y wt\\nij i j ij\\njk j k jk\\n( ) =⋅⋅ + − ( )\\n( ) =⋅ ⋅ + − ( )\\nαδ β\\nαδ β\\n1\\n1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 332, 'page_label': '333'}, page_content='306 CHAPTER 11 Neural Networks\\n(increase or decrease) of the sum of the square of the errors from one\\nepoch to the next to determine the change in learning rate:\\nIf for several epochs the sum of the square of the errors changes in the same\\ndirection, increase the learning rate.\\n1. If for several epochs the sum of the square of the errors changes in\\nthe same direction, increase the learning rate.\\n2. If the sum of the square of the errors alternates its change in\\ndirection over several epochs, decrease the learning rate.\\nBy using these heuristics in combination with the generalized delta rule,\\nthe performance of the backpropagation algorithm can be significantly\\nimproved.\\n11.5 Recurrent Networks\\nThe neural networks we have been studying so far are feed-forward net-\\nworks. A feed-forward network is acyclic, in the sense that there are no\\ncycles in the network, because data passes from the inputs to the outputs,\\nand not vice versa,. Once a feed-forward network has been trained, its state\\nis fixed and does not alter as new input data is presented to it. In other\\nwords, it does not have memory.\\nA recurrent network can have connections that go backward from output\\nnodes to input nodes and, in fact, can have arbitrary connections between\\nany nodes. In this way, a recurrent network’s internal state can alter as sets\\nof input data are presented to it, and it can be said to have a memory.\\nThis is particularly useful in solving problems where the solution depends\\nnot just on the current inputs, but on all previous inputs. For example,\\nrecurrent networks could be used to predict the stock market price of a par-\\nticular stock, based on all previous values, or it could be used to predict what\\nthe weather will be like tomorrow, based on what the weather has been.\\nClearly, due to the lack of memory, feed-forward networks are not able to\\nsolve such tasks.\\nWhen learning, the recurrent network feeds its inputs through the net-\\nwork, including feeding data back from outputs to inputs, and repeats this\\nprocess until the values of the outputs do not change. At this point, the net-\\nwork is said to be in a state of equilibrium or stability. For this reason,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 333, 'page_label': '334'}, page_content='11.5 Recurrent Networks 307\\nrecurrent networks are also known as attractor networks because they are\\nattracted to certain output values. The stable values of the network, which\\nare also known as fundamental memories, are the output values used as\\nthe response to the inputs the network received.\\nHence, a recurrent network can be considered to be a memory, which is\\nable to learn a set of states—those that act as attractors for it. Once such a\\nnetwork has been trained, for any given input it will output the attractor\\nthat is closest to that input.\\nFor example, a recurrent network can be used as an error-correcting net-\\nwork. If only a few possible inputs are considered “valid, ” the network can\\ncorrect all other inputs to the closest valid input.\\nIt is not always the case that a recurrent network will reach a stable state:\\nsome networks are unstable, which means they oscillate between different\\noutput values.\\n11.5.1 Hopfield Networks\\nIn the 1980s, John Hopfield invented a form of recurrent network that has\\ncome to be known as a Hopfield network.\\nThe activation function used by most Hopfield networks is the sign activa-\\ntion function, which is defined as:\\nNote that this definition does not provide a value for Sign(0). This is\\nbecause when a neuron that uses the sign activation function receives an\\ninput of 0, it stays in the same state—in other words, it continues to output\\n1 if it was outputting 1 in the previous iteration, and continues to output\\n/H110021 if it was outputting /H110021.\\nWhen considering the operation of a Hopfield network, it is usual to use\\nmatrix arithmetic. The weights of the network are represented by a matrix,\\nW, which is calculated as follows:\\nWX X N ii\\nt\\ni\\nN\\n=−\\n=\\n∑ I\\n1\\nSign X\\nfor X\\nfor X( ) =\\n+>\\n−<\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n10\\n10'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 334, 'page_label': '335'}, page_content='308 CHAPTER 11 Neural Networks\\nwhere each Xi is an input vector, representing the m input values to the\\nnetwork; Xit is the matrix transposition of Xi; I is the m /H11003m identity\\nmatrix; N is the number of states ( Xi) that are to be learned. The trans-\\nposition of a matrix is simply one where the rows and columns are\\nswapped. If\\nthen the transposition of X\\n1 is\\nThe identity matrix, I, is a matrix with zeros in every row and column, but\\nwith 1s along the leading diagonal. For example,\\nNow let us examine an example. We will imagine a single-layer Hopfield\\nnetwork with five nodes and three training inputs that are to be learned by\\nthe network. We will have our network learn the following three states:\\nWe thus have three states (vectors) that are to be learned, each of which\\nconsists of five input values. The inputs can be either 1 or /H110021; similarly, the\\noutput values can be either 1 or /H110021, and so the output can be represented\\nas a similar vector of five values, each of which is either 1 or /H110021.\\nThe weight matrix is calculated as follows:\\nXX X12 3\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nI =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n100\\n010\\n001\\nXi\\nt =−[]11 1\\nX1\\n1\\n1\\n1\\n=−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 335, 'page_label': '336'}, page_content='11.5 Recurrent Networks 309\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n01331\\n10113\\n31031\\n31301\\n13110\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n+\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n+\\n−−\\n−− −\\n−−\\n−−\\n−\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11 1 11\\n11 1 11\\n11 1 11\\n11 1 11\\n11 −−−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n11 1\\n30000\\n03000\\n00300\\n00030\\n00003\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n[] +\\n−\\n−\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−−−−−\\n[] +\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−−\\n[] −\\n1\\n1\\n1\\n1\\n1\\n11111\\n1\\n1\\n1\\n1\\n1\\n11111\\n1\\n1\\n1\\n1\\n1\\n1 111 1 3\\n10000\\n01000\\n00100 0\\n00010\\n00001\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nWX X\\nXX X X XX\\nii\\nt\\ni\\ni\\nt\\ni\\nt\\ni\\nt\\n=−\\n=++−\\n=\\n∑ 3\\n3\\n1\\n3\\n123\\nI\\nI'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 336, 'page_label': '337'}, page_content='310 CHAPTER 11 Neural Networks\\nNote that the weight matrix has zeros along its leading diagonal. This\\nmeans that each node in the network is not connected to itself (i.e., wii = 0\\nfor all i). A further property of a Hopfield network is that the two connec-\\ntions between a pair of nodes have the same weight. In other words, wij =\\nwji for any nodes i and j.\\nThe three training states used to produce the weight matrix will be stable\\nstates for the network. We can test this by determining the output vectors\\nfor each of them.\\nThe output vector is defined by\\nwhere \\nθ is the threshold matrix, which contains the thresholds for each of\\nthe five inputs. We will assume that the thresholds are all set at zero.\\nHence, the first input state is a stable state for the network. Similarly, we can\\nshow that Y2 = X2 and that Y3 = X3.\\nNow let us see how the network treats an input that is different from the\\ntraining data. We will use\\nY Sign\\nSign\\n1\\n01331\\n10113\\n31031\\n31301\\n13110\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n8\\n6\\n8\\n8\\n6\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n1\\n1\\n1\\n1\\n1\\n1X\\nY Sign Xii =− ( )W θ'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 337, 'page_label': '338'}, page_content='11.5 Recurrent Networks 311\\nNote that this vector differs from X1 in just one value, so we would expect\\nthe network to converge on X1 when presented with this input.\\nNow we will try an input that is very different from the training data:\\nX5\\n1\\n1\\n1\\n1\\n1\\n=\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nY Sign\\nSign\\n4\\n01331\\n10113\\n31031\\n31301\\n13110\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n2\\n4\\n8\\n2\\n4\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n1\\n1\\n1\\n1\\n1\\n1X\\nX4\\n1\\n1\\n1\\n1\\n1\\n= −\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 338, 'page_label': '339'}, page_content='312 CHAPTER 11 Neural Networks\\nLet us apply the network to this input data:\\nBecause this is different from X5 and is not one of the attractors, we need to\\napply the rule again:\\nThe use of the Hopfield network involves three stages. In the first stage, the\\nnetwork is trained to learn the set of attractor states. This can be thought of\\nas a storage or memorization stage. This is done by setting the weights of\\nthe network according to the values given by the weights matrix, W, which\\nis calculated as described above.\\nThe second phase involves testing the network, by providing the attractor\\nstates as inputs, and checking that the outputs are identical. The final stage\\nY Sign\\nSign\\n5\\n01331\\n10113\\n31031\\n31301\\n13110\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n2\\n4\\n2\\n8\\n4\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n1\\n1\\n1\\n1\\n1\\n1X\\nY Sign\\nSign\\n5\\n01331\\n10113\\n31031\\n31301\\n13110\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n2\\n2\\n2\\n4\\n2\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n1\\n1\\n1\\n1\\n1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 339, 'page_label': '340'}, page_content='11.5 Recurrent Networks 313\\ninvolves using the network, in which the network, in acting as a memory, is\\nrequired to retrieve data from its memory.\\nIn each case, the network will retrieve the attractor closest to the input that\\nit is given. In this case, the nearest attractor is X1, which differs in just two\\ninputs. The measure of distance that is usually used for such vectors is the\\nHamming distance. The Hamming distance measures the number of ele-\\nments of the vectors that differ. The Hamming distance between two vec-\\ntors, X and Y, is written \\n||X, Y ||.\\nFor the vectors we have used\\nHence, the Hopfield network is a memory that usually maps an input vec-\\ntor to the memorized vector whose Hamming distance from the input vec-\\ntor is least.\\nIn fact, although a Hopfield network always converges on a stable state, it\\ndoes not always converge on the state closest to the original input. No\\nmethod has yet been found for ensuring that a Hopfield network will\\nalways converge on the closest state.\\nA Hopfield network is considered to be an autoassociative memory, which\\nmeans that it is able to remember an item itself, or a similar item that might\\nhave been modified slightly, but it cannot use one piece of data to remem-\\nber another. The human brain is fully associative, or heteroassociative,\\nwhich means one item is able to cause the brain to recall an entirely differ-\\nent item. A piece of music or a smell will often cause us to remember an old\\nmemory: this is using the associative nature of memory. A Hopfield net-\\nwork is not capable of making such associations.\\n11.5.2 Bidirectional Associative Memories (BAMs)\\nA Bidirectional Associative Memory , or BAM, is a neural network first\\ndiscussed by Bart Kosko (1988) that is similar in structure to the Hopfield\\nnetwork and which can be used to associate items from one set to items in\\nanother set.\\nXX\\nXX\\n14\\n15\\n1\\n2\\n,\\n,\\n=\\n='),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 340, 'page_label': '341'}, page_content='314 CHAPTER 11 Neural Networks\\nThe network consists of two layers of nodes, where each node in one layer\\nis connected to every other node in the other layer—this means that the\\nlayers are fully connected . This is in contrast to the Hopfield network,\\nwhich consists of just a single layer of neurons: in the Hopfield network,\\neach neuron is connected to every other neuron within the same layer,\\nwhereas in the BAM, each neuron is connected just to neurons in the other\\nlayer, not to neurons in its own layer.\\nAs with Hopfield networks, the weight matrix is calculated from the items\\nthat are to be learned. In this case, two sets of data are to be learned, so that\\nwhen an item from set X is presented to the network, it will recall a corre-\\nsponding item from set Y.\\nThe weights matrix W is defined as:\\nThe BAM uses a neuron with a sign activation function, which is also used\\nby a Hopfield network.\\nWhen the network is given a vectorX\\ni as an input, it will recall the correspon-\\nding vectorYi, and similarly, when presented withYi, the network will recallXi.\\nLet us examine a simple example:\\nWe are using our network to learn two sets of vectors. The network has two\\nlayers: the input layer has two neurons, and the output layer has three neurons.\\nThe weights matrix is calculated as follows:\\nW = \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa[] + −\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa −−−[]\\n= \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n1\\n1 111 1\\n1 111\\n222\\n222\\nX\\nY\\n12\\n12\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n= \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa =\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nX\\nY\\nWX Y= ∑ ii\\nt\\ni\\nn'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 341, 'page_label': '342'}, page_content='11.5 Recurrent Networks 315\\nNow we will test the network. When presented with input X1, the network\\nwill output the following vector:\\nIf the network is functioning correctly, this should be equal to Y1:\\nSo the network has correctly recalled Y1 when presented with X1.\\nSimilarly, the association should work in reverse: when presented with Y1,\\nthe network should recall X1:\\nNote that in this case, we are using the output layer as if it were an input\\nlayer, and vice versa—hence, the network is bidirectional.\\nLike a Hopfield network, the BAM is guaranteed to produce a stable output for\\nany given inputs and for any training data. In fact, a Hopfield network is a type\\nof BAM, with the additional requirement that the weight matrix be square and\\nSign\\nSign\\nSign\\nWY\\nX\\n1\\n1\\n222\\n222\\n1\\n1\\n1\\n6\\n6\\n1\\n1\\n( )\\n= \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n= \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa = \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa =\\nSign Sign\\nSign\\ntWX 1\\n1\\n22\\n22\\n22\\n1\\n1\\n4\\n4\\n4\\n1\\n1\\n1\\n( ) =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n= Y\\nSign tWX 1( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 342, 'page_label': '343'}, page_content='316 CHAPTER 11 Neural Networks\\nthat each neuron not have a connection to itself (or to its corresponding neu-\\nron in the other layer). BAMs are extremely useful neural networks, although\\ntheir capabilities (and limitations) are not yet fully understood.\\n11.6 Unsupervised Learning Networks\\nThe networks we have studied so far in this chapter use supervised learn-\\ning: they are presented with preclassified training data before being asked\\nto classify unseen data. We will now look at a number of methods that are\\nused to enable neural networks to learn in an unsupervised manner.\\n11.6.1 Kohonen Maps\\nA Kohonen map,o r self-organizing feature map, is a form of neural net-\\nwork invented by Kohonen in the 1980s. The Kohonen map uses the win-\\nner-take-all algorithm , which leads to a form of unsupervised learning\\nknown as competitive learning . The winner-take-all algorithm uses the\\nprinciple that only one neuron provides the output of the network in\\nresponse to a given input: the neuron that has the highest activation level.\\nDuring learning, only connections to this neuron have their weights altered.\\nThe purpose of a Kohonen map is toclusterinput data into a number of clus-\\nters. For example, a Kohonen map could be used to cluster news stories into\\nsubject categories. A Kohonen map is not told what the categories are: it deter-\\nmines the most useful segmentation itself. Hence, a Kohonen map is particu-\\nlarly useful for clustering data where the clusters are not known in advance.\\nA Kohonen map has two layers: an input layer and a cluster layer, which\\nserves as the output layer. Each input node is connected to every node in\\nthe cluster layer, and typically the nodes in the cluster layer are arranged in\\na grid formation, although this is not essential.\\nThe method used to train a Kohonen map is as follows: Initially, all weights\\nare set to small random values. The learning rate, /H9251, is also set, usually to a\\nsmall positive value.\\nAn input vector is presented to the input layer of the map. This layer feeds\\nthe input data to the cluster layer. The neuron in the cluster layer that most\\nclosely matches the input data is declared the winner. This neuron provides\\nthe output classification of the map and also has its weights updated.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 343, 'page_label': '344'}, page_content='11.6 Unsupervised Learning Networks 317\\nT o determine which neuron wins, its weights are treated as a vector, and\\nthis vector is compared with the input vector. The neuron whose weight\\nvector is closest to the input vector is the winner.\\nThe Euclidean distance d\\ni from the input vector x of a neuron with weight\\nvector wi is calculated as follows:\\nwhere n is the number of neurons in the input layer and hence the number\\nof elements in the input vector.\\nFor example, let us calculate the distance between the following two vectors:\\nSo the Euclidean distance between these two vectors is 4.\\nThe neuron for which di is the smallest is the winner, and this neuron has\\nits weight vector updated as follows:\\nThis adjustment moves the weight vector of the winning neuron closer to\\nthe input vector that caused it to win.\\nIn fact, rather than just the winning neuron having its weights updated, a\\nneighborhood of neurons around the winner are usually updated. The\\nneighborhood is usually defined as a radius within the two-dimensional\\ngrid of neurons around the winning neuron.\\nww x wij ij j ij←+ − ( )α\\nwx\\nd\\ni\\ni\\n=\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n∴= −\\n( ) ++( ) +−−( )\\n=+ +\\n=\\n=\\n1\\n2\\n1\\n3\\n1\\n2\\n13 21 12\\n439\\n16\\n4\\n22 2\\ndw xii j j\\nj\\nn\\n=− ( )\\n=\\n∑\\n2\\n1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 344, 'page_label': '345'}, page_content='318 CHAPTER 11 Neural Networks\\n123\\n456\\n789\\nFigure 11.5\\nThe cluster layer of a\\nsimple Kohonen map\\nTypically, the radius decreases over time as the training data are examined,\\nending up fixed at a small value. Similarly, the learning rate is often reduced\\nduring the training phase.\\nThis training phase usually terminates when the modification of weights\\nbecomes very small for all the cluster neurons. At this point, the network has\\nextracted from the training data a set of clusters, where similar items are con-\\ntained within the same cluster, and similar clusters are near to each other.\\n11.6.2 Kohonen Map Example\\nLet us examine a simplified example of a Kohonen map.\\nOur Kohonen map has just two inputs and nine cluster neurons, which are\\narranged into a 3/H110033 grid, as shown in Figure 11.5.\\nFigure 11.5 shows how the neurons are arranged in a grid. Each node in the\\ncluster layer is connected to each of the two input nodes. The cluster layer\\nnodes are not connected to each other. The grid shown in Figure 11.5 does\\nnot represent physical connection, but rather spatial proximity—node 1 is\\nclose to nodes 2 and 4. This spatial proximity of neurons is used to calculate\\nthe neighborhood set that is used to determine which weights to update\\nduring the training phase.\\nNote that this square arrangement is by no means necessary. The nodes are\\noften arranged in a rectangular grid, but other shapes can be used equally\\nsuccessfully.\\nBecause there are two input nodes in the network, we can represent each\\ninput as a position in two-dimensional space. Figure 11.6 shows the nine\\ninput values that are to be used to train this network.\\nIn Figure 11.6, x\\n1 and x2 are the two input values that are to be presented to\\nthe input layer, which contains two neurons.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 345, 'page_label': '346'}, page_content='11.6 Unsupervised Learning Networks 319\\nX2\\nX1\\nFigure 11.6\\nTraining data for the\\nKohonen map shown in\\nFigure 11.5\\n1\\n2 3\\n456\\n7\\n8 9\\nw2\\nw1\\nFigure 11.7\\nInitial weight vectors for\\nthe Kohonen map\\nNote that the training data have been selected randomly from the available\\nspace, such that they fill as much of the space as possible. In this way the\\ndata will be as representative as possible of all available input data, and so\\nthe Kohonen map will be able to cluster the input space optimally.\\nBecause each neuron in the cluster layer has connections to the two input\\nlayer neurons, their weight vectors can be plotted in two-dimensional\\nspace. These weight vectors are initially set to random values, which are\\nshown in Figure 11.7. The connections between nodes in Figure 11.7 repre-\\nsent spatial proximity again, as in Figure 11.5.\\nBecause there are nine cluster nodes and nine pieces of training data, we\\nexpect the network to assign each neuron to one piece of training data.\\nMost real Kohonen maps consist of far more neurons, and many more\\ntraining data are usually used.\\nIn our simple example, by running a number of iterations of the Kohonen\\nmap, the weight vectors are modified to those shown in Figure 11.8.\\nIn this case, it is easy to see what the map has done: by modifying the\\nweight vector of each neuron so that it closely resembles one training vec-\\ntor, the nodes have been modified so that each node will respond extremely\\nwell to one of the input data. When a new piece of input data is presented,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 346, 'page_label': '347'}, page_content='320 CHAPTER 11 Neural Networks\\n12 3\\n54\\n6\\n7 8 9\\nw2\\nw1\\nFigure 11.8\\nWeight vectors after train-\\ning the Kohonen map\\nit will be classified by the node whose weight vector is closest to it. Addi-\\ntionally, that node’s weight vector will be moved slightly toward the new\\npiece of input data. In this way, the network continues to learn as new data\\nare presented to it. By decreasing the learning rate over time, the network\\ncan be forced to reach a stable state where the weights no longer change, or\\nchange only very slightly, when presented with new input data.\\nThis example illustrates the self-organizing nature of Kohonen maps.\\nThe space-filling shape shown in Figure 11.8 is typical of the behavior of\\nthese networks.\\n11.6.3 Hebbian Learning\\nHebbian learning is based on Hebb’s law, which was stated by D. O. Hebb\\nin 1949. Hebb’s law is stated as follows:\\nWhen an axon of cell A is near enough to excite a cell B and repeatedly or\\npersistently takes part in firing it, some growth process or metabolic change\\ntakes place in one or both cells such that A ’s efficiency, as one of the cells\\nfiring B, is increased.\\nIn terms of artificial neural networks, this rule can be restated as follows:\\nIf two neurons that are connected to each other fire at the same time, the\\nweight of the connection between those neurons is increased.\\nConversely, if the neurons fire at different times, the weight of the connec-\\ntion between them is decreased.\\nNeural networks use Hebbian learning to learn without needing to be given\\npreclassified training data.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 347, 'page_label': '348'}, page_content='11.7 Evolving Neural Networks 321\\nUsing Hebbian learning, the weight of a connection between neurons i and\\nj is increased according to the following rule:\\nwhere /H9251is the learning rate; xi is the input to node i, and yi is the output of\\nnode i (and thus the input contributed to node j by node i). This rule is\\nknown as the activity product rule.\\nBy treating the weights of neuron i as a vector, Wi, this rule can also be\\nwritten as\\nwhere Xi is the input vector to node i, and yi is the output of node i.\\nThe activity product rule does not allow for decreasing weights, which is\\nrequired by Hebb’s law. The rule can be modified to allow weights to be\\ndecreased by using a forgetting factor, /H9278, as follows:\\nWhen /H9278is zero, the network cannot “forget, ” and the weights are always\\nincreased during learning. If /H9278were set to 1, the network would not be able\\nto learn at all because it would forget everything. Usually a small value,\\nsuch as between 0.01 and 0.1, is used as the forgetting factor.\\nUsing Hebb’s law, a neural network is able to learn to associate one input\\nwith another input. This can be thought of as analogous to the experiment\\nconducted by Pavlov in which he rang a bell whenever he fed his dogs,\\nwhich led the dogs to salivate whenever they heard a bell ring.\\n11.7 Evolving Neural Networks\\nThe ideas that we cover in Chapter 14 on genetic algorithms can be applied\\nto neural networks. Genetic algorithms can be used to evolve suitable start-\\ning weight vectors for a network. This is useful because the initial weight\\nvector that is chosen for a network can significantly affect the ability of the\\nnetwork to solve a particular problem. Neural networks suffer from many\\nof the problems faced by search methods presented in Part 2 of this book,\\nww y x y wij ij i i i ij←+ ⋅ ⋅ − ⋅ ⋅αφ\\nWW Xii i i y←+ ⋅ ⋅ α\\nww y xij ij i i←+ ⋅ ⋅ α'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 348, 'page_label': '349'}, page_content='322 CHAPTER 11 Neural Networks\\nsuch as falling into local minima. By repeatedly running a full training ses-\\nsion on a neural network with different random starting weights, this prob-\\nlem can be avoided. Clearly, this problem can also be avoided by using\\nevolutionary methods to select starting weight vectors.\\nSimilarly, a genetic algorithm can be used to determine the connectivity of\\nthe network. In this way, the number of neurons and the connections\\nbetween those neurons can be evolved to produce an optimal architecture.\\n11.8 Chapter Summary\\n■ Biological neurons are the building blocks of the human brain.\\nEach neuron has a number of inputs, and one output, which fires\\ndepending on the inputs.\\n■ Artificial neurons are modeled on biological neurons and are used\\nto build artificial neural networks. Artificial neurons often use a\\nfunction such as a Step function to calculate their output based on\\nthe weighted sum of their inputs.\\n■ A perceptron is a very simple neuron that can model problems that\\nare linearly separable.\\n■ Multilayer neural networks, using backpropagation, can solve\\nproblems that are not linearly separable.\\n■ Recurrent networks, such as Hopfield networks, allow arbitrary con-\\nnections between neurons within the network, which is particularly\\nuseful for modeling functions such as the value of the stock market,\\nwhere the value at one point in time is dependent on previous values.\\n■ Unsupervised neural networks, such as Kohonen maps, learn to\\nclassify without being presented any preclassified training data.\\n■ Hebbian learning is an unsupervised learning technique based on\\nthe idea that if two neurons fire at the same time, then the connec-\\ntion between them should be strengthened.\\n11.9 Review Questions\\n11.1 Explain how the human brain uses neurons to learn. What are the\\nsimilarities and differences between artificial neurons and biologi-\\ncal neurons?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 349, 'page_label': '350'}, page_content='11.10 Exercises 323\\n11.2 How likely do you think it is that a neural network of the complex-\\nity of the human brain will ever be built in software? In hardware?\\n11.3 Explain how the backpropagation algorithm is used. Why is\\nmomentum used with backpropagation?\\n11.4 Explain the limitations of a perceptron. What kind of problems\\ncan they solve? Give a real-world example.\\n11.5 Explain how Hopfield networks operate.\\n11.6 Explain the difference between supervised and unsupervised learn-\\ning. When might each be most useful?\\n11.7 Explain what is meant by Hebbian learning. Why is forgetting\\nimportant to Hebbian learning?\\n11.8 Explain in detail how a Kohonen map might be used to cluster a set\\nof web documents in response to a user’s keyword query.\\n11.9 What are the advantages and disadvantages of applying evolution-\\nary techniques to neural networks? What could be the ultimate\\ngoal of such a combination?\\n11.10 Exercises\\n11.10 Run through the training process for a perceptron to calculate the\\nbinary AND function on three inputs.\\n11.11 Design a multilayer neural network with two inputs and one hid-\\nden layer that uses the backpropagation algorithm to learn to rep-\\nresent the logical exclusive-OR function for two inputs. Y our\\nnetwork should have two nodes in the input layer, two in the hid-\\nden layer, and one in the output layer. Initialize the weights to ran-\\ndom values, and run the algorithm (on paper) for three epochs.\\nComment on your results. Implement this network in the pro-\\ngramming language of your choice. Run it until the sum of the\\nsquares of the errors is less than 0.001. How many epochs does the\\nnetwork take to learn the exclusive-OR function? Try to modify\\nyour program so that it learns the function in fewer epochs.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 350, 'page_label': '351'}, page_content='324 CHAPTER 11 Neural Networks\\nFigure 11.9\\nThe 10 digits possible with\\na seven-segment display\\n11.12 On paper, calculate the weight matrix for a Hopfield network that\\nis to learn the following two input vectors:\\nNow calculate the behavior of the network when it is presented\\nwith X1 as an input. How does it behave when it is presented with\\nthe following input?\\n11.14 Design and implement a neural network system for recognizing\\nnumbers. Y ou could start by building a network to recognize the 10\\npossible digits represented in a seven-segment LED, as shown in\\nFigure 11.9. If you are feeling ambitious, extend your algorithm to\\nwork with numbers displayed in a dot-matrix display of 8-by-8\\ndots. What problems do you encounter?\\n11.11 Further Reading\\nThere are a number of excellent introductory texts on neural networks, as\\nwell as many more advanced ones. Introductions by Gurney (1997) and\\nX3\\n1\\n1\\n1\\n1\\n=\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nXX12\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 351, 'page_label': '352'}, page_content='11.11 Further Reading 325\\nCallan (1999) cover most of the material introduced in this chapter. For\\nmore advanced readings, consult the papers and books referenced below.\\nT o learn more about evolutionary neural networks, consult Negnevitsky\\n(2002) or Bäck et al. (1997).\\nThe Handbook of Brain Theory and Neural Networks: Second Edition edited\\nby Michael A. Arbib (2002 – MIT Press)\\nHandbook of Evolutionary Computation edited by T. Bäck, D. B. Fogel, and\\nZ. Michalewicz (1997 – Institute of Physics Publishing)\\nNeural Networks for Pattern Recognition by Christopher M. Bishop (1996 –\\nOxford University Press)\\nUnderstanding 99% of Artificial Neural Networks: Introduction & Tricks by\\nMarcelo Bosque (2002 – Writers Club Press)\\nThe Essence of Neural Networks by Robert Callan (1999 – Prentice Hall)\\nFundamentals of Neural Networksby Laurene V . Fausett (1994 – Prentice Hall)\\nAn Introduction to Neural Networks by Kevin Gurney (1997 – UCL Press)\\nNeural Networks: A Comprehensive Foundationby Simon S. Haykin (1998 –\\nPrentice Hall)\\nThe Organisation of Behavior: A Neuropsychological Theory by D. O. Hebb\\n(1949 – republished in 2002 by Lawrence Erlbaum Assoc.)\\nIncreased Rates of Convergence Through Learning Rate Adaptation by R. A.\\nJacobs (1987 – in Neural Networks, Vol. 1, pp. 295–307).\\nSelf-Organizing Maps by T euvo Kohonen (2000 – Springer V erlag)\\nBidirectional Associative Memories by Bart Kosko (1988 – in IEEE Transac-\\ntions Systems, Man & Cybernetics, Vol. 18, pp. 49–60).\\nA Logical Calculus of the Ideas Immanent in Nervous Activity by W. S.\\nMcCulloch and W. Pitts (1943 – inBulletin of Mathematical Biophysics, Vol.\\n5, pp. 115–137).\\nPerceptrons by Marvin Minsky and Seymour A. Papert (1969 – now avail-\\nable in an extended edition: Perceptrons - Expanded Edition: An Introduc-\\ntion to Computational Geometry. 1987 – MIT Press)\\nMachine Learning by T om M. Mitchell (1997 – McGraw Hill)\\nArtificial Intelligence: A Guide to Intelligent Systems by Michael Negnevitsky\\n(2002 – Addison Wesley)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 352, 'page_label': '353'}, page_content='326 CHAPTER 11 Neural Networks\\nComputational Explorations in Cognitive Neuroscience: Understanding the\\nMind by Simulating the Brain by Randall C. O’Reilly (Author) and Yuko\\nMunakata (2000 – MIT Press)\\nUnderstanding Intelligence by Rolf Pfeifer and Christian Scheier (2000 –\\nMIT Press)\\nThe Perceptron: A Probabilistic Model for Information Storage and Organiza-\\ntion in the Brain by F. Rosenblatt (1958 – in Psychological Review, Vol. 65,\\npp. 386–408)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 353, 'page_label': '354'}, page_content='12CHAPTER\\nProbabilistic Reasoning and\\nBayesian Belief Networks\\nBut to us, probability is the very guide of life.\\n—Joseph Butler,The Analogy of Religion\\nProbable Impossibilities are to be preferred to improbable possibilities.\\n—Aristotle, Poetics\\nDo not expect to arrive at certainty in every subject which you pursue. There\\nare a hundred things wherein we mortals must be content with probability,\\nwhere our best light and reasoning will reach no farther.\\n—Isaac Watts\\n12.1 Introduction\\nThis chapter introduces the ideas behind probabilistic reasoning, and in\\nparticular Bayes’ Theorem. Thomas Bayes was an English mathematician\\nand theologian who lived from 1702 to 1761. His theorem is used exten-\\nsively today in dealing with situations that lack certainty.\\nThis chapter explains the relationship between probability theory and the\\nlogic that we saw in Part 3. It explains joint probability distributions and\\ngoes on to explain Bayes’ theorem, using two examples.\\nThis chapter explains how Bayesian belief networks can be built and used\\nto learn from data about which certainty is lacking. Bayesian classifiers are\\nalso explained. The chapter also includes an introduction to the ideas'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 354, 'page_label': '355'}, page_content='328 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nbehind collaborative filtering and explains how this increasingly popular\\ntechnique relates to Bayesian reasoning.\\n12.2 Probabilistic Reasoning\\nIn this section, we will present a brief introduction to probability theory and\\nthe notation that is used to express it. Probability theory is used to discuss\\nevents, categories, and hypotheses about which there is not 100% certainty.\\nThe notation that we saw in Chapter 7 for making and analyzing logical\\nstatements does not function in situations that are lacking certainty.\\nFor example, we might write\\nA → B\\nwhich means that if A is true, then B is true. If we are unsure whether A is\\ntrue, then we cannot make use of this expression. In many real-world situ-\\nations, it is very useful to be able to talk about things that lack certainty. For\\nexample, what will the weather be like tomorrow? We might formulate a\\nvery simple hypothesis based on general observation, such as “it is sunny\\nonly 10% of the time, and rainy 70% of the time. ” We can use a notation\\nsimilar to that used for predicate calculus to express such statements:\\nP(S) = 0.1\\nP(R) = 0.7\\nThe first of these statements says that the probability of S (“it is sunny”) is\\n0.1. The second says that the probability of R is 0.7. Probabilities are always\\nexpressed as real numbers between 0 and 1. A probability of 0 means “defi-\\nnitely not” and a probability of 1 means “definitely so. ” Hence, P(S) = 1\\nmeans that it is always sunny.\\nMany of the operators and notations that are used in prepositional logic\\ncan also be used in probabilistic notation. For example, P(\\n¬S) means “the\\nprobability that it is not sunny”; P(S ∧ R) means “the probability that it is\\nboth sunny and rainy. ”\\nP(A ∨ B), which means “the probability that either A is true or B is true, ” is\\ndefined by the following rule:\\nP(A ∨ B) = P(A) + P(B) /H11002P(A ∧ B)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 355, 'page_label': '356'}, page_content='12.2 Probabilistic Reasoning 329\\nA B\\nA ∧ B\\nFigure 12.1\\nIllustrating the relation-\\nship between A ∧B and\\nA ∨B\\nThis rule can be seen to be true by examining the V enn diagram shown in\\nFigure 12.1.\\nThe notation P(B|A) can be read as “the probability of B, given A. ” This is\\nknown as conditional probability—it is conditional onA. In other words, it\\nstates the probability thatB is true, given that we already know thatA is true.\\nP(B|A) is defined by the following rule:\\nOf course, this rule cannot be used in cases where P(A) = 0.\\nFor example, let us suppose that the likelihood that it is both sunny and\\nrainy at the same time is 0.01. Then we can calculate the probability that it\\nis rainy, given that it is sunny as follows:\\nPR S PR S\\nPS( ) = ∧( )\\n( )\\n=\\n=\\n00 1\\n01\\n01\\n.\\n.\\n.\\nPB A PB A\\nPA( ) = ∧( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 356, 'page_label': '357'}, page_content='330 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nNote that the probability that it is sunny given that it is overcast—P(S|R)—\\nis different from this: 0.01/0.7 = 0.14; hence, P(A|B) ≠ P(B|A).\\n12.3 Joint Probability Distributions\\nA joint probability distribution (also known as a joint) can be used to\\nrepresent the probabilities of combined statements, such as A ∧ B.F o r\\nexample, the following table shows a joint probability distribution of two\\nvariables, A and B:\\nA ¬A\\nB 0.11 0.09\\n¬B 0.63 0.17\\nThis shows, for example, that P(A ∧ B) = 0.11, and that P(A ∧¬ B) = 0.63.\\nBy summing these two values, we can find P(A) = 0.11 + 0.63 = 0.74. Simi-\\nlarly, P(B) = 0.11 + 0.09 = 0.2.\\nWe can use this table to determine the probability of any logical combina-\\ntion of A and B. For example, P(A ∨ B) = 0.11 + 0.09 + 0.63 = 0.83. We\\ncould have obtained this result by noting that P(¬A ∧¬ B) = 0.17 and that\\nP(¬A ∧¬ B) = 1 /H11002P(A ∨ B) = 1 /H110020.17 = 0.83.\\nSimilarly, we can determine conditional probabilities, such as P(B|A) using\\nthe following rule:\\nIn this case, P(B ∧ A) = 0.11 and P(A) = 0.11 + 0.63 = 0.74, so P(B|A) =\\n0.11 / 0.74 = 0.15.\\nCalculations like this are easy when we use a joint probability of just two\\nvariables. Real-world problems will often involve much greater numbers of\\nvariables, and in these cases, drawing up probability distribution tables is\\nclearly much less straightforward.\\n12.4 Bayes’ Theorem\\nBayes’ theorem can be used to calculate the probability that a certain event\\nwill occur or that a certain proposition is true, given that we already know\\na related piece of information.\\nPB A PB A\\nPA( ) = ∧( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 357, 'page_label': '358'}, page_content='12.4 Bayes’ Theorem 331\\nThe theorem is stated as follows:\\nP(B) is called the prior probability of B. P(B|A), as well as being called the\\nconditional probability, is also known as the posterior probability of B.\\nLet us briefly examine how Bayes’ theorem is derived:\\nWe can deduce a further equation from the rule given in Section 12.2\\nabove. This rule is known as the product rule:\\nP(A\\n∧ B) = P(A|B)P(B)\\nNote that due to the commutativity of ∧, we can also write\\nP(A ∧ B) = P(B|A)P(A)\\nHence, we can deduce:\\nP(B|A)P(A) = P(A|B)P(B)\\nThis can then be rearranged to give Bayes’ theorem:\\n12.4.1 Example: Medical Diagnosis\\nLet us examine a simple example to illustrate the use of Bayes’ theorem for\\nthe purposes of medical diagnosis.\\nWhen one has a cold, one usually has a high temperature (let us say, 80% of\\nthe time). We can use A to denote “I have a high temperature” and B to\\ndenote “I have a cold.” Therefore, we can write this statement of posterior\\nprobability as\\nP(A\\n|B) = 0.8\\nNote that in this case, we are using A and B to represent pieces of data that\\ncould each either be a hypothesis or a piece of evidence. It is more likely\\nthat we would use A as a piece of evidence to help us prove or disprove the\\nhypothesis, B, but it could work equally well the other way around (at least,\\nmathematically speaking).\\nNow, let us suppose that we also know that at any one time around 1 in\\nevery 10,000 people has a cold, and that 1 in every 1000 people has a high\\ntemperature. We can write these prior probabilities as\\nPB A PA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )\\nPB A\\nPA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 358, 'page_label': '359'}, page_content='332 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nP(A) = 0.001\\nP(B) = 0.0001\\nNow suppose that you have a high temperature. What is the likelihood that\\nyou have a cold? This can be calculated very simply by using Bayes’ theorem:\\nHence, we have shown that just because you have a high temperature does\\nnot necessarily make it very likely that you have a cold—in fact, the chances\\nthat you have a cold are just 8 in 1000.\\nBayes’ theorem can be extended to express a conditional probability involv-\\ning more than two variables as follows:\\nProvided the n pieces of evidence E\\n1 ... En are independent of each other,\\ngiven the hypothesis H,1 then this can be rewritten as follows:\\n12.4.2 Example: Witness Reliability\\nLet us examine a further example. In the city of Cambridge, there are two\\ntaxi companies. One taxi company uses yellow taxis, and the other uses\\nwhite taxis. The yellow taxi company has 90 cars, and the white taxi com-\\npany has just 10 cars.\\nA hit-and-run incident has been reported, and an eye witness has stated\\nthat she is certain that the car was a white taxi.\\nPH E E PEH PE H PH\\nPE En\\nn\\nn\\n1\\n1\\n1\\n∧∧( ) = ( ) ⋅⋅ ( ) ⋅ ( )\\n∧∧( )\\nK K\\nK\\nPH E E PE E H PH\\nPE En\\nn\\nn\\n1\\n1\\n1\\n∧∧( ) = ∧∧( ) ⋅ ( )\\n∧∧( )\\nK K\\nK\\nPB A PA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )\\n= ⋅\\n=\\n0 8 0 0001\\n0 001\\n0 008\\n..\\n.\\n.\\n1In other words, if H is true, then the truth or otherwise of Ei should have no effect on the\\ntruth of Ej for any i and j.\\n∧ ∧\\n∧ ∧\\n∧ ∧\\n∧ ∧\\n∧ ∧'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 359, 'page_label': '360'}, page_content='12.4 Bayes’ Theorem 333\\nSo far, we have the following information:\\nP(Y) = 0.9 (the probability of any particular taxi being yellow)\\nP(W) = 0.1 (the probability of any particular taxi being white)\\nLet us further suppose that experts have asserted that given the foggy\\nweather at the time of the incident, the witness had a 75% chance of cor-\\nrectly identifying the taxi.\\nGiven that the lady has said that the taxi was white, what is the likelihood\\nthat she is right?\\nLet us denote by P(C\\nW) the probability that the culprit was driving a white\\ntaxi and by P(CY) the probability that it was a yellow car.\\nWe will use P(WW) to denote the probability that the witness says she saw a\\nwhite car and P(WY) to denote that she says she saw a yellow car. (We\\nassume the witness tells the truth!)\\nNow, if the witness really saw a yellow car, she would say that it was yellow\\n75% of the time, and if she says she saw a white car, she would say it was\\nwhite 75% of the time. Hence, we now know the following:\\nP(C\\nY) = 0.9\\nP(CW) = 0.1\\nP(WW | CW) =0.75\\nP(WY | CY) = 0.75\\nHence, we can apply Bayes’ theorem to find the probability, given that she is\\nsaying that the car was white, that she is correct:\\nWe now need to calculate P(WW)—the prior probability that the lady\\nwould say she saw a white car.\\nLet us imagine that the lady is later shown a random sequence of 1000 cars.\\nWe expect 900 of these cars to be yellow and 100 of them to be white. The\\nwitness will misidentify 250 of the cars: Of the 900 yellow cars, she will\\nincorrectly say that 225 are white. Of the 100 white cars, she will incorrectly\\nsay that 25 are yellow. Hence, in total, she will believe she sees 300 white\\nPC W PWWW\\nW\\n( ) = ⋅\\n( )\\n07 5 01..'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 360, 'page_label': '361'}, page_content='334 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\ncars—even though only 100 of them are really white. So, P(WW) is\\n300/1000 = 0.3.\\nWe can now complete our equation to find P(CW|WW):\\nIn other words, if the lady says that the car was white, the probability that it\\nwas in fact white is only 0.25—it is three times more likely that it was actu-\\nally yellow!\\nIn this example, Bayes’ theorem takes into account the actual number of\\neach color of taxi in the city. If the witness had said she saw a yellow taxi, it\\nwould be very likely that she was right—but this is likely anyway because\\nthere are so many more yellow taxis than white taxis. If the witness were a\\nperfect observer who made no errors, then the probability P(C\\nW|WW)\\nwould, of course, be 1.\\nThis example also helps to illustrate the fact that in many real-world situa-\\ntions we do have enough information to be able to use Bayes’ theorem. It\\ncan look as though Bayes’ theorem will apply only in contrived situations,\\nbut in fact it is usually the case that obtaining the data needed to use Bayes’\\ntheorem is easier than obtaining the posterior probability by other means.\\nThis is particularly true in cases where there are a large number of individ-\\nuals being discussed.\\n12.4.3 Comparing Conditional Probabilities\\nIn many situations, it can be useful to compare two probabilities. In partic-\\nular, in making a diagnosis from a set of evidence, one will often have to\\nchoose from a number of possible hypotheses.\\nFor example, let us extend the medical example given in Section 12.4.1.\\nThere we used A to represent the piece of evidence “I have a high tempera-\\nture” and B to represent the hypothesis “I have a cold, ” where\\nP(A) = 0.001\\nP(B) = 0.0001\\nPC WWW( ) = ⋅\\n=\\n07 5 01\\n03\\n02 5\\n..\\n.\\n.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 361, 'page_label': '362'}, page_content='12.4 Bayes’ Theorem 335\\nP(A|B) = 0.8\\nLet us further use C to represent the hypothesis “I have plague, ” where\\nP(C) = 0.000000001\\nP(A|C) = 0.99\\nIn other words, it is highly unlikely for anyone to have plague, but if they\\ndo, they will almost certainly have a high temperature.\\nIn this case, when carrying out a diagnosis of a patient that has a high tem-\\nperature, it will be useful to determine which is the more likely hypothe-\\nsis—B or C.\\nBayes’ theorem gives us the following:\\nClearly, to find the more likely of B and C,g i v e n  A, we can eliminate P(A)\\nfrom these equations and can determine the relative likelihood of B and C\\nas follows:\\nHence, it is hundreds of thousands of times more likely given that a patient\\nhas a high temperature that he has a cold than that he has plague.\\n12.4.4 Normalization\\nNormalization is the process whereby the posterior probabilities of a pair\\nof variables are divided by a fixed value to ensure that they sum to 1.\\nPB A\\nPCA\\nPA B PB\\nPA C P C\\n( )\\n( )\\n= ( ) ⋅ ( )\\n( ) ⋅ ( )\\n= ⋅\\n⋅\\n=\\n08 00 0 1\\n0 95 0 000000001\\n842 105\\n..\\n..\\n,\\nPB A PA B PB\\nPA\\nPCA PA C P C\\nPA\\n( ) = ( ) ⋅ ( )\\n( )\\n( ) = ( ) ⋅ ( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 362, 'page_label': '363'}, page_content='336 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nThis can be done by considering the following two equations:\\nGiven that A is true, B must either be true or false, which means that P(B|A)\\n+ P(¬B|A) = 1.\\nHence, we can add the two equations above to give\\nNow we can replace P(A) in the equation for Bayes’ theorem, to give\\nHence, it is possible to use Bayes’ theorem to obtain the conditional proba-\\nbility P(B|A) without needing to know or calculate P(A), providing we can\\nobtain P(A|¬B). [P(¬B) is simply 1–P(B)].\\nThis equation is often written as follows:\\nP(B|A) = /H9251/H11080P(A|B) /H11080P(B)\\nwhere /H9251represents the normalizing constant:\\nLet us examine our diagnosis example again. The facts we have are as follows:\\nP(A) = 0.001\\nP(B) = 0.0001\\nP(A\\n|B) = 0.8\\nα =\\n( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\n1\\nPA B PB PA B P B\\nPB A PA B PB\\nPA B PB PA B P B( ) = ( ) ⋅ ( )\\n( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\n1 = ( ) ⋅ ( )\\n( )\\n+ ¬( ) ⋅ ( )\\n( )\\n∴ ( ) = ( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\nPA B PB\\nPA\\nPA B PB\\nPA\\nPA PA B PB PA B P B\\nPB A PA B PB\\nPA\\nPB A PA B P B\\nPA\\n( ) = ( ) ⋅ ( )\\n( )\\n¬( ) = ¬( ) ⋅¬( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 363, 'page_label': '364'}, page_content='12.5 Simple Bayesian Concept Learning 337\\nLet us now suppose that P(A|¬B) = 0.00099. This conditional probability\\nstates the likelihood that a person will have a high temperature if she does\\nnot have a cold (\\n¬B). We can now thus use the following equation to calcu-\\nlate P(B|A):\\nSimilarly, we can calculate P(¬B|A):\\nThe net result of this normalization process has been to ensure that P(B|A)\\n+ P(¬B|A) = 1. We could now carry out a similar process to calculateP(C|A)\\nand P(¬C|A), which would enable us to ensure that they also sum to 1.\\n12.5 Simple Bayesian Concept Learning\\nA very simple model for learning can be developed using Bayes’ rule.\\nThroughout the above discussion we have been talking about probabilities\\nof hypotheses or of specific pieces of evidence. T o use probability theory in\\nlearning, it is useful to talk about the probability that some hypothesis is\\ntrue, given a particular set of evidence. We can use the same notation for\\nthis, and write\\nP(H\\n|E)\\nPB A PA B P B\\nPA B P B PA B PB¬( ) = ¬( ) ⋅¬( )\\n¬( ) ⋅¬( ) + ( ) ⋅ ( )\\n= ⋅\\n⋅+ ⋅\\n=\\n=\\n0 00099 0 9999\\n0 00099 0 9999 0 8 0 0001\\n0 000989901\\n0 001069901\\n0 925\\n..\\n.. . .\\n.\\n.\\n.\\nPB A PA B PB\\nPA B PB PA B P B( ) = ( ) ⋅ ( )\\n( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\n= ⋅\\n⋅+ ⋅\\n=\\n=\\n0 8 0 0001\\n0 8 0 001 0 00099 0 9999\\n0 00008\\n0 001069901\\n0 075\\n..\\n.. . .\\n.\\n.\\n.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 364, 'page_label': '365'}, page_content='338 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nHence, given a set of evidence, the learner can determine which hypothesis\\nto believe in by identifying the posterior probability of each. Let us suppose\\nthat there are n possible hypotheses, H\\n1 ... Hn.H e n c e ,f o r  e a c h  Hi\\nSo the algorithm could calculate P(Hi|E) for each possible hypothesis and\\nselect the one that has the highest probability. Similarly, the system could\\nuse this method to determine an action to take, where H\\ni is the hypothesis\\nthat the best action to take in the current situation is action Ai.\\nIn fact, the formula above can be simplified in this situation: because P(E)\\nis independent of Hi, it will have the same value for each hypothesis. So\\nbecause we are simply looking for the hypothesis with the maximum poste-\\nrior probability, we can eliminate P(E) from the calculation and simply aim\\nto maximize the following value:\\nP(E\\n|Hi) /H11080P(Hi)\\nIn fact, if we assume that all hypotheses are equally likely, given no addi-\\ntional information (i.e., P(Hi) = P(Hj) for any i and j), we can in fact reduce\\nthis further and simply choose the hypothesis for whom the value P(E|Hi)\\nis the highest. This value is known as the likelihood of the evidence E,g i v e n\\nhypothesis Hi. Of course, by learning from observations what the prior\\nprobabilities are of each of the hypotheses, more accurate results can be\\nobtained, but the simpler formula is more efficient in calculation time.\\nRecall the discussion from Chapter 7 of abduction and inductive reason-\\ning. These are really a form of learning: by observing the events that occur,\\nwe are able to make reasonable guesses about future events, and these\\nguesses can often guide our actions. For example, if a robot observed that\\nevery time it heard a particular noise, an enemy robot appeared, it might\\nlearn to hide when it heard that noise. In doing so, it is learning from expe-\\nrience and using Bayesian reasoning to decide upon the correct course of\\naction. The robot is not using rules of logical deduction, such as modus\\nponens, which was explained in Chapter 7, but a rather more probabilistic\\nform of reasoning, along the lines of “I have noticed in the past that when\\nthis noise occurs, an enemy appears. I have also noticed in the past that if I\\nPH E\\nPE H PH\\nPEi\\nii\\n( ) = ( ) ⋅ ( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 365, 'page_label': '366'}, page_content='12.6 Bayesian Belief Networks 339\\nA\\nC D\\nB\\nE\\nFigure 12.2\\nA simple belief network\\ndo not hide when an enemy appears, I get hurt by the enemy. Hence, I\\nshould probably hide when I hear the noise. ”\\nHumans use learning of this kind all the time, and it is essential for learning\\nin situations in which there is very little certainty, such as the real world.\\n12.6 Bayesian Belief Networks\\nThe concept of dependence is very important in probability theory. Two\\nevents, A and B,a r e  independent if the likelihood of occurrence of A is\\nentirely unrelated to whether or not B occurs.\\nFor example, in tossing two coins, the likelihood that the first coin will come\\nup heads and the likelihood that the second coin will come up heads are two\\nindependent probabilities because neither one depends on the other.\\nIf A and B are independent, then the probability that A and B will both\\noccur can be calculated very simply:\\nP(A\\n∧ B) = P(A).P(B)\\nWe know that this equation does not hold if A depends on B because we\\nhave already seen the following equation:\\nBy comparing these two equations, we can see that A and B are independ-\\nent if P(B|A) = P(B). In other words, the likelihood of B is unaffected by\\nwhether or not A occurs. B is independent of A.I f B is dependent on A,\\nthen P(B|A) will be different from P(B).\\nThese relationships can be expressed extremely succinctly in a belief net-\\nwork, such as the one shown in Figure 12.2.\\nPB A PB A\\nPA( ) = ∧( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 366, 'page_label': '367'}, page_content='340 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nA Bayesian belief network is an acyclic directed graph, where the nodes in\\nthe graph represent evidence or hypotheses, and where an arc that connects\\ntwo nodes represents a dependence between those two nodes.\\nThe belief network in Figure 12.2 contains five nodes that represent pieces\\nof evidence (A and B) and three hypotheses (C, D, and E). The arcs between\\nthese nodes represent the interdependence of the hypotheses. According to\\nthis diagram, C and D are both dependent on A, and D and E are both\\ndependent on B. Two nodes that do not have an arc between them are inde-\\npendent of each other. For example,B is independent of A.\\nEach node in the network has a set of probabilities associated with it, based on\\nthe values of the nodes on which it is dependent. Hence,A and B both have\\njust prior probabilities,P(A) andP(B), because they are not dependent on any\\nother nodes.C and E are each dependent on just one other node. Hence, for\\nexample,P(C) must be represented in the two cases—A is true andA is false.\\nP(D) must be represented in four cases, depending on the values ofA and B.\\nFor example, the following conditional probabilities might be used in the\\nnetwork shown in Figure 12.2:\\nP(A) = 0.1\\nP(B) = 0.7\\nP(C\\n|A) = 0.2\\nP(C|¬A) = 0.4\\nP(D|A ∧ B) = 0.5\\nP(D|A ∧¬ B) = 0.4\\nP(D|¬A ∧ B) = 0.2\\nP(D|¬A ∧¬ B) = 0.0001\\nP(E|B) = 0.2\\nP(E|¬B) = 0.1\\nThe above list of probabilities, combined with the diagram shown in Figure\\n12.2, represent a complete (rather simple) Bayesian belief network. The\\nnetwork states beliefs about a set of hypotheses or pieces of evidence and\\nthe ways that they interact.\\nThese probabilities can also be expressed in the form of conditional prob-\\nability tables, as follows:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 367, 'page_label': '368'}, page_content='12.6 Bayesian Belief Networks 341\\nCompare these tables with the logical truth tables described in Chapter 7.\\nIn those tables, a logical value ( true or false) was given for a variable that\\ndepended on the values of one or more other variables. Hence, a condi-\\ntional probability table is very similar to a truth table, except that it\\nexpresses the probability of one variable, given the truth values of one or\\nmore other variables.\\nA joint probability can be calculated from the Bayesian belief network\\nusing the definition of conditional probability:\\nHence,\\nP(A,B,C,D,E) = P(E\\n|A,B,C,D) /H11080P(A,B,C,D)\\nWe can apply this rule recursively to obtain\\nP(A,B,C,D,E)= P(E|A,B,C,D,) /H11080P(D|A,B,C,) /H11080P(C|A,B) /H11080P(B|A) /H11080P(A)\\nIn fact, the nature of our belief network allows us to simplify this expres-\\nsion, and because we know that, for example,E is not dependent on A, C,o r\\nD, we can reduce P(E|A,B,C,D) to P(E|B).\\nP(A,B,C,D,E) = P(E|B) /H11080P(D|A,B) /H11080P(C|A) /H11080P(B) /H11080P(A)\\nPB A PB A\\nPA( ) = ∧( )\\n( )\\nP(A) P(B)\\n0.1 0.7\\nA P(C) B P(E)\\ntrue 0.2 true 0.2\\nfalse 0.4 false 0.1\\nA B P(D)\\ntrue true 0.5\\ntrue false 0.4\\nfalse true 0.2\\nfalse false 0.0001'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 368, 'page_label': '369'}, page_content='342 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nWe have now greatly reduced the complexity of the calculation needed to\\ncompute the joint probability. This has only been possible due to the way in\\nwhich the nodes were ordered in the original expression. For example, if we\\nused the same method blindly on the expression\\nP(E,D,C,B,A)\\nwe would be left with the following expression:\\nP(E,D,C,B,A)= P(A\\n|E,D,C,B) /H11080P(B|E,D,C) /H11080P(C|E,D) /H11080P(D|E) /H11080P(E)\\nThis is not correct because E is dependent on B, and so we need to include\\nP(E|B). Similarly, D is dependent on A and B, which is not reflected in this\\nexpression.\\nIn other words, to calculate the joint probability, the nodes must be ordered\\nin the expression in such a way that if a node X is dependent on another\\nnode Y, then X appears before Y in the joint. Hence, we could have used any\\nordering in which A and B appear before C, D, and E; B,A,E,D,C would\\nhave worked equally well, for example.\\nAs a result of this, when constructing a Bayesian belief network, it is essential\\nthat the graph be constructed in the correct order—in other words, in an\\norder such that the connections between nodes makes logical sense. This usu-\\nally means starting with causes and then adding the events they cause, and\\nthen treating those events as causes, and adding any further events they cause.\\nThe nature of Bayesian belief networks means that in general they are an\\nefficient way of storing a joint probability distribution. The network does\\nnot store the conditional probability P(X\\n|Y) if X and Y are independent of\\neach other, given the parents ofX. In the network shown in Figure 12.2, for\\nexample, this means that P(E|A) does not need to be stored.\\n12.6.1 Example: Life at College\\nLet us examine the simple Bayesian belief network shown in Figure 12.3.\\nIn Figure 12.3, the five nodes represent the following statements:\\nC = that you will go to college\\nS = that you will study\\nP = that you will party\\nE = that you will be successful in your exams\\nF = that you will have fun'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 369, 'page_label': '370'}, page_content='12.6 Bayesian Belief Networks 343\\nC\\nS\\nE F\\nP\\nFigure 12.3\\nA Bayesian network to rep-\\nresent activities at college\\nThis network shows us at a glance that if you go to college, this will affect\\nthe likelihood that you will study and the likelihood that you will party.\\nStudying and partying affect your chances of exam success, and partying\\naffects your chances of having fun.\\nT o complete the Bayesian belief network, we need to include the condi-\\ntional probability tables. Let us define these as follows:\\nP(C)\\n0.2\\nC P(S)\\ntrue 0.8\\nfalse 0.2\\nC P(P)\\ntrue 0.6\\nfalse 0.5\\nS P P(E)\\ntrue true 0.6\\ntrue false 0.9\\nfalse true 0.1\\nfalse false 0.2'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 370, 'page_label': '371'}, page_content='344 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nNote that according to this belief network there is a dependence between F\\nand C, but because it is not a direct dependence, no information needs to\\nbe stored about it.\\nThese conditional probability tables give us all the information we need\\nto carry out any reasoning about this particular domain. For example, we\\ncan clearly obtain values such as P(\\n¬C) by using the fact that\\nP(¬C)=1  /H11002P(C)=1 /H110020.2 = 0.8.\\nWe can use the network to determine conditional probabilities, such as\\nP(F|P) by observing that in the final table, if P is true, then P(F ) = 0.9.\\nHence, P(F|P) = 0.9.\\nThe joint probability distribution for this domain represents the entire\\nstate of the domain. We can represent such a state using the notation as\\nused in the following example:\\nP(C = true, S = true, P = false, E = true, F = false)\\nWe can simplify this notation as follows:\\nP(C, S,\\n¬P, E ,¬F)\\nThis represents the probability that you will go to college and that you will\\nstudy and be successful in your exams, but will not party or have fun. This\\nprobability can be calculated using the following rule:\\nwhere E is the evidence on which each x\\ni is dependent—in other words, in\\nthe Bayesian belief network, E consists of the nodes that are parents of xi.\\nFor example, using the network shown in Figure 12.3, we can calculate the\\nfollowing probability:\\nPx x Px Eni\\ni\\nn\\n1\\n1\\n,,K( ) = ( )\\n=\\n∏\\nP P(F)\\ntrue 0.9\\nfalse 0.7'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 371, 'page_label': '372'}, page_content='12.6 Bayesian Belief Networks 345\\nP(C,S,¬P,E,¬F) = P(C) /H11080P(S|C) /H11080P(¬P|C) /H11080P(E|S ∧¬ P) /H11080P(¬F|¬P)\\n= 0.2 /H110800.8 /H110800.4 /H110800.9 /H110800.3\\n= 0.01728\\nHence, for S we need to include in the product P(S|C) because S is only\\ndependent on C, and C is true in the situation we are examining. Similarly,\\nfor E we need to include P(E|S ∧¬ P) because E is dependent on S and on P,\\nand S is true and P is not true in the scenario.\\nWe can also calculate more complex conditional probabilities. In fact, this\\nis an extremely simple process, due to the way in which the belief network\\nhas been created. For example, let us look at the following conditional\\nprobability:\\nP(E\\n|F ∧¬ P ∧ S ∧ C)\\nThis is the probability that you will have success in your exams if you have\\nfun and study at college, but don’t party.\\nThe assumption behind the Bayesian belief network is that because there is\\nno direct connection between E and C, E is independent of C,g i v e n  S and\\nP. In other words, if we wish to calculate the following:\\nP(E|C ∧ S ∧ P)\\nwe can in fact drop C from this altogether, and simply obtain\\nP(E|S ∧ P) = 0.6\\nSimilarly, the more complex conditional probability above can be simpli-\\nfied by dropping F and C to give\\nP(E|S ∧¬ P) = 0.9\\nHence, any calculation that we might need to make about this domain\\ncan be made simply using the conditional probability tables of the\\nbelief network.\\nSimilarly, we can make diagnoses about your college life by determining\\nposterior probabilities. For example, let us say that we know that you had\\nfun and studied hard while at college and we know that you succeeded in\\nyour exams, but we want to know whether you partied or not.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 372, 'page_label': '373'}, page_content='346 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nClearly, we know C, S, E, and F, but we do not know P. We need to deter-\\nmine the most likely value for P. Hence, we can compare the values of the\\nfollowing two expressions:\\nP(C ∧ S ∧ P ∧ E ∧ F ) = P(C) /H11080P(S|C) /H11080P(P|C) /H11080P(E|S ∧ P) /H11080P(F |P)\\n= 0.2 /H110800.8 /H110800.6 /H110800.6 /H110800.9\\n= 0.05184\\nP(C ∧ S ∧¬ P ∧ E ∧ F )= P(C) /H11080P(S|C) /H11080P(¬P|C) /H11080P(E|S ∧¬ P) /H11080P(F |¬P)\\n= 0.2 /H110800.8 /H110800.4 /H110800.9 /H110800.7\\n= 0.04032\\nHence, it is slightly more likely that you did party while at college than that\\nyou did not.\\n12.6.2 Example: Chapter Dependencies\\nWe will now examine a simple example of a slightly unusual Bayesian net-\\nwork. Rather than each node representing a hypothesis or a piece of diag-\\nnostic information, each node in the Bayesian network shown in Figure\\n12.4 represents a chapter of this book. The arcs between nodes represent\\nthe dependencies between chapters. For example, the network shows that if\\nyou plan to read Chapter 8, which covers logical proof by resolution, it is a\\ngood idea to have read Chapter 7 on propositional and predicate logic first.\\nT o see this as a more standard belief network, we can consider each node to\\nrepresent the likelihood that you have read a given chapter and that a\\ndependency from Chapter 8 to Chapter 7, for example, represents the fact\\nthat, if you have read Chapter 8, it is likely that you have also read Chapter\\n7. For this network to be useful to you in deciding which order to read the\\nchapters, you can think of the dependencies as being advice about whether\\nyou should read a particular chapter before reading another.\\n12.7 The Noisy-V Function\\nThus far, we have assumed that the probabilities contained with a joint\\nprobability distribution are unrelated to each other, in the sense that they\\nhave been determined by observing the way in which events occur. In\\nsome situations, it can be possible to use the fact that events in a Bayesian'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 373, 'page_label': '374'}, page_content='12.7 The Noisy-V Function 347\\n7\\nPropositional and\\nPredicate Logic\\n8\\nInference and Resolution\\nfor Problem Solving\\n9\\nRules and\\nExpert Systems\\n3\\nKnowledge\\nRepresentation\\n4\\nSearch\\nMethodologies\\n5\\nAdvanced\\nSearch\\n6\\nGame Playing\\n13\\nArtificial Life\\n14\\nGenetic\\nAlgorithms\\n19\\nIntelligent\\nAgents\\n10\\nIntroduction to\\nMachine Learning\\n11\\nNeural\\nNetworks\\n12\\nProbablilistic Reasoning and\\nBayesian Belief Networks\\n16\\nPlanning Methods\\n15\\nIntroduction to\\nPlanning\\n17\\nAdvanced Knowledge\\nRepresentation\\n18\\nFuzzy\\nReasoning\\n20\\nUnderstanding\\nLanguage\\n21\\nMachine\\nVision\\nFigure 12.4\\nA Bayesian belief network that shows dependencies between chapters in this book\\nbelief network are related to each other by some kind of mathematical or\\nlogical relation.\\nClearly, logical relations such as ∧ and ∨, as defined in propositional logic,\\nwill not do because they do not provide a way to handle probabilities.\\nFuzzy logic (which is described in Chapter 18) could provide suitable rela-\\ntions. Another useful class of relations is noisy logical relationships.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 374, 'page_label': '375'}, page_content='348 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nLet us return to our diagnosis example. We use P(A|B) to represent the\\nprobability that if one has a cold, then one will also have a high tempera-\\nture. Similarly, we used P(A\\n|C) to represent the probability that if one has\\nthe plague, then one will also have a high temperature.\\nWe have the following:\\nP(A|B) = 0.8\\nP(A|C) = 0.99\\nThe noisy- ∨ function is based on the assumption that the only possible\\ncauses of a high temperature are a cold and the plague (i.e., that P(A|B ∨ C)\\n= 1. Clearly this is not true for our example, but we can fix this by including\\na leak node in the network, which represents all other possible causes.\\nHence, we will further include P(A\\n|D) = 0.9, where D is the leak node,\\nwhich represents other causes of a high temperature.\\nLet us now define the noise parameters for these relationships. The noise\\nparameters are simply defined as the conditional probabilities for ¬A,\\nrather than for A, and can be obtained as follows:\\nP(¬A|B) = 1 /H11002P(A|B) = 0.2\\nP(¬A|C) = 1 /H11002P(A|C) = 0.01\\nP(¬A|D) = 1 /H11002P(A|D) = 0.1\\nA further assumption in using the noisy- ∨ function is that the causes of a\\nhigh temperature are independent of each other and, similarly, that the\\nnoise parameters (whatever it is that stops each illness from causing a high\\ntemperature) are independent of each other.\\nThe noisy-\\n∨ function for B, C, and D is defined as follows:\\nIf B, C, and D are all false, then P(A) = 0. Otherwise, P(¬A) is equal to the\\nproduct of the noise parameters for all the variables that are true. For\\nexample, if B is true and C and D are false, then P(\\n¬A) is equal to the noise\\nparameter for B, and so\\nP(A) = 1 /H110020.2\\n= 0.8\\nIf C and D are both true, and B is false, then P(¬A) is equal to the product\\nof the noise parameters for C and D, and so\\nP(A) = 1 /H11002(0.01 /H110030.1)\\n= 0.999'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 375, 'page_label': '376'}, page_content='12.8 Bayes’ Optimal Classifier 349\\nNow we can define the noisy-∨ function for our diagnosis example:\\nB C D P(A) P( ¬A)\\nfalse false false 0 1\\nfalse false true 0.9 0.1\\nfalse true false 0.99 0.01\\nfalse true true 0.999 0.01 /H110030.1 = 0.001\\ntrue false false 0.8 0.2\\ntrue false true 0.98 0.2 /H110030.1 = 0.02\\ntrue true false 0.998 0.2 /H110030.01 = 0.002\\ntrue true true 0.9998 0.2 /H110030.01 /H110030.1 = 0.0002\\nNote that this noisy logical function is defined by just three conditional\\nprobabilities, as opposed to needing to store eight values. For Bayesian\\nbelief networks used in the real world with hundreds or even thousands of\\nnodes, this can make a significant difference.\\n12.8 Bayes’ Optimal Classifier\\nIt is possible to use Bayesian reasoning to build a system that learns to clas-\\nsify data.\\nFor example, let us suppose that for a given piece of data, y, there are five\\npossible hypotheses, H\\n1 ... H5, each of which assigns a classification to y.\\nThe classification, c, can be any value from a set C. For this example, let us\\nassume that C consists of the values true and false.\\nOur classifier knows the posterior probabilities of each of the five hypothe-\\nses to be the following:\\nP(H1|x1,... ,x n) = 0.2\\nP(H2|x1,... ,x n) = 0.3\\nP(H3|x1,... ,x n) = 0.1\\nP(H4|x1,... ,x n) = 0.25\\nP(H5|x1,... ,x n) = 0.15\\nwhere x1 to xn are the training data.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 376, 'page_label': '377'}, page_content='350 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nThe probability that the new item of data, y, should be classified with clas-\\nsification cj is defined by the following:\\nwhere m is the number of available hypotheses, which in this case is 5. The\\noptimal classification for y is the classification cj for which P(cj|x1 ... xn) is\\nthe highest.\\nIn our case, there are two classifications:\\nc1 = true\\nc2 = false\\nLet us suppose that hypotheses H3 and H5 each define y as true, while H1,\\nH2, and H4 define y as false.\\nHence, we have the following posterior probabilities:\\nP(false|H1) = 0 P(true |H1) = 1\\nP(false|H2) = 0 P(true |H2) = 1\\nP(false|H3) = 1 P(true |H3) = 0\\nP(false|H4) = 0 P(true |H4) = 1\\nP(false|H5) = 1 P(true |H5) = 0\\nThus we can calculate the posterior probabilities for each of the two possi-\\nble classifications for y as follows:\\nHence, the optimal classification for y is true.\\nP false x x P false H P H x xni i n\\ni\\n11\\n1\\n5\\n01 01 5\\n02 5\\nKK( ) = ( ) ⋅ ( )\\n=+\\n=\\n=\\n∑\\n..\\n.\\nP true x x P true H P H x xni i n\\ni\\n11\\n1\\n5\\n02 03 02 5\\n07 5\\nKK( ) = ( ) ⋅ ( )\\n=++\\n=\\n=\\n∑\\n...\\n.\\nPc x x Pc h Ph x xjn j i in\\ni\\nm\\n11\\n1\\nKK( ) = ( ) ⋅ ( )\\n=\\n∑'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 377, 'page_label': '378'}, page_content='12.9 The Naïve Bayes Classifier 351\\nThis method is known as an optimal classifier because it provides the best\\npossible classification system. Another classification system, given the same\\ndata, can only hope to classify unseen data as well as this method—it can-\\nnot do better than the optimal classifier, on average.\\n12.9 The Naïve Bayes Classifier\\nThe naïve Bayes classifier is a simple but effective learning system. Each\\npiece of data that is to be classified consists of a set of attributes, each of\\nwhich can take on a number of possible values. The data are then classified\\ninto a single classification.\\nT o identify the best classification for a particular instance of data ( d\\n1,... ,\\ndn), the posterior probability of each possible classification is calculated:\\nP(ci| d1,... ,d n)\\nwhere ci is the ith classification, from a set of |c| classifications.\\nThe classification whose posterior probability is highest is chosen as the\\ncorrect classification for this set of data. The hypothesis that has the highest\\nposterior probability is often known as the maximum a posteriori ,o r\\nMAP hypothesis. In this case, we are looking for the MAP classification.\\nT o calculate the posterior probability, we can use Bayes’ theorem and\\nrewrite it as\\nBecause we are simply trying to find the highest probability, and because\\nP(d\\n1,... , dn) is a constant independent of ci, we can eliminate it and simply\\naim to find the classification ci, for which the following is maximized:\\nP(d1,... , dn|ci) /H11080P(ci)\\nThe naïve Bayes classifier now assumes that each of the attributes in the\\ndata item is independent of the others, in which case P(d1,... , dn|ci) can be\\nrewritten and the following value obtained:\\nPc Pd cij i\\nj\\nn\\n( ) ⋅ ( )\\n=\\n∏\\n1\\nPd d c Pc\\nPd d\\nni i\\nn\\n1\\n1\\n,,\\n,,\\nK\\nK\\n( ) ⋅ ( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 378, 'page_label': '379'}, page_content='352 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nThe naïve Bayes classifier selects a classification for a data set by finding the\\nclassification ci for which the above calculation is a maximum.\\nFor example, let us suppose that each data item consists of the attributes x,\\ny, and z, where x, y, and z are each integers in the range 1 to 4.\\nThe available classifications are A, B, and C.\\nThe example training data are as follows:\\nx y z Classification\\n232A\\n414B\\n132A\\n243A\\n424B\\n213C\\n124A\\n233B\\n224A\\n333C\\n321A\\n121B\\n214A\\n434C\\n224A\\nHence, we have 15 pieces of training data, each of which has been classified.\\nEight of the training data are classified as A, four as B, and three as C.\\nNow let us suppose that we are presented with a new piece of data, which is\\n(x = 2, y = 3, z = 4)\\nWe need to obtain the posterior probability of each of the three classifica-\\ntions, given this piece of training data. Note that if we were to attempt to cal-\\nculate P(c\\ni|x =2 , y =3 , z = 4) without having made the simplifying step that'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 379, 'page_label': '380'}, page_content='12.9 The Naïve Bayes Classifier 353\\nwe took above, in assuming that the attribute values are independent of each\\nother, then we would need to have had many more items of training data to\\nproceed. The naïve Bayes classifier requires far fewer items of training data.\\nWe must now calculate each of the following:\\nP(A) /H11080P(x = 2\\n|A) /H11080P(y = 3|A) /H11080P(z = 4|A)\\nP(B) /H11080P(x = 2|B) /H11080P(y = 3|B) /H11080P(z = 4|B)\\nP(C) /H11080P(x = 2|C) /H11080P(y = 3|C) /H11080P(z = 4|C)\\nHence, for classification A, we obtain the following:\\nThis was calculated by observing that of the 15 items of training data, 8 were\\nclassified as A, and so P(A) = 8/15. Similarly, of the eight items of training\\ndata that were classified asA,f i v eh a dx =2 ,t w oh a dy = 3, and four hadz =\\n4, and so P(x =2 |A) = 5/8, P(y =3 |A) = 2/8, and P(z =4 |A) = 4/8.\\nSimilarly, we obtain the posterior probability for category B:\\nand for category C:\\nHence, category A is chosen as the best category for this new piece of data,\\nwith category C as the second best choice.\\nLet us now suppose that we are to classify the following piece of unseen data:\\n(x = 1, y = 2, z = 2)\\nAs before, we would calculate the posterior probability for A.H o w e v e r ,i n\\ncalculating the probabilities for B and C, we would have problems. In the\\ncase of category B, we would have\\nP(x = 1|B) = 1/5\\nP(y = 2|B) = 1/5\\nP(z = 2|B) = 0\\nBecause there are no training examples withz = 2 that were classified asB,w e\\nhave a posterior probability of 0. Similarly, for categoryC, we end up with\\n3\\n15\\n1\\n3\\n2\\n3\\n1\\n3 0 015⋅⋅⋅ = .\\n4\\n15\\n1\\n4\\n1\\n4\\n2\\n4 0 0083⋅⋅⋅ = .\\n8\\n15\\n5\\n8\\n2\\n8\\n4\\n8 0 0417⋅⋅⋅ = .'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 380, 'page_label': '381'}, page_content='354 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nP(x = 1|C) = 0\\nP(y = 2|C) = 0\\nP(z = 2|C) = 0\\nIn this case, we clearly must select category A as the best choice for the data,\\nbut it appears to be based on a fairly inadequate comparison because insuf-\\nficient training data were available to properly compute posterior probabil-\\nities for the other categories.\\nThis problem can be avoided by using the m-estimate, as follows:\\nWe wish to determine the probability of a particular attribute value, given a\\nparticular classification, such as P(x = 1\\n|C). We will estimate this probabil-\\nity according to the following formula:\\nwhere a = the number of training examples that exactly match our require-\\nments (e.g., for P(x = 1|C ), a is the number of training examples where x =\\n1 and that have been categorized as C. In this example,a is 0); b = the num-\\nber of training examples that were classified in the current classification\\n(i.e., for P(x = 1\\n|C), b is the number of items of training data that were\\ngiven classification C ); p = an estimate of the probability that we are trying\\nto obtain (usually this is obtained by simply assuming that each possible\\nvalue is equally likely—hence, in our example, for P(x = 1\\n|C ), p = 1/4 =\\n0.25, as it would be for each of the other three possible values for x); m is a\\nconstant value, known as the equivalent sample size.\\nFor example, let us use an equivalent sample size of 5 and determine the\\nbest classification for (x = 1, y = 2, z = 2):\\nFor category A, we first need to calculate the probability for each of the\\nthree attributes.\\nHence, for x = 1:\\nFor y = 2:\\n2 54\\n85 02 5\\n+\\n+ = .\\nam p\\nbm\\n+\\n+'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 381, 'page_label': '382'}, page_content='12.9 The Naïve Bayes Classifier 355\\nFor z = 2:\\nHence, the posterior probability estimate for A is\\nSimilarly, we can now obtain posterior probability estimates for cate-\\ngories B and C:\\nFor category B, we obtain the following three probabilities:\\nThis gives us a posterior probability for category B as follows:\\nFinally, the posterior probability for category C can be obtained. We note\\nfirst that each of the three probabilities is the same because none of the\\nattribute values occur in the training data with category C.H e n c e ,t h e\\nprobability we use will be\\nHence, the posterior probability for category C is as follows:\\nHence, using this estimate for probability, we find that category B is the\\nbest match for the new data, and not category A as would have been\\nobtained using the simpler probability estimates.\\n3\\n15 0 156 0 156 0 156 0 0008⋅⋅⋅ =... .\\n0 54\\n35 0 156\\n+\\n+ = .\\n5\\n15 0 225 0 325 0 125 0 0091⋅⋅⋅ =... .\\n1 54\\n55 0 225\\n2 54\\n55 0 325\\n0 54\\n55 0 125\\n+\\n+ =\\n+\\n+ =\\n+\\n+ =., ., .\\n8\\n15 0 25 0 33 0 17 0 0076⋅⋅⋅ =... .\\n1 54\\n85 01 7\\n+\\n+ = .\\n3 54\\n85 03 3\\n+\\n+ = .'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 382, 'page_label': '383'}, page_content='356 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nIt is possible to further simplify the naïve Bayes classifier by considering the val-\\nues to be positionless within each item of data. In other words, when consider-\\ning a new item of data, rather than assigning values to three attributes, we can\\nsimply think of the data as consisting of three values, whose order is arbitrary.\\nFor example, consider the piece of new data (2, 3, 4).\\nIn this case, we use the same method as before, but rather than considering\\nthe probability that, for example, x = 2 when an item is classified as A,w e\\nsimply consider the probability that any attribute has value 2.\\nThis simplified version of the naïve Bayes classifier is often used in text clas-\\nsification applications. Here, the categories are often simply “relevant” and\\n“irrelevant, ” and the data to be classified consist of the words contained\\nwithin textual documents. For example, an item of data might be (“the, ”\\n“cat, ” “sat, ” “on, ” “the, ” “mat”). Training data would be presented in the\\nform of a set of documents that has been preclassified as relevant and a set\\nthat has been preclassified as irrelevant. This form of textual analysis is dis-\\ncussed in more detail in Chapter 20, which is concerned with information\\nretrieval and natural language processing.\\n12.10 Collaborative Filtering\\nA further practical use for Bayesian reasoning is in collaborative filtering.\\nCollaborative filtering is a technique that is increasingly used by online\\nstores (such as Amazon.com) to provide plausible suggestions to customers\\nbased on their previous purchases. The idea behind collaborative filtering\\ncan be stated very simply: if we know that Anne and Bob both like items A,\\nB, and C, and that Anne likes D, then it is reasonable to suppose that Bob\\nwould also like D.\\nCollaborative filtering can be implemented in a number of ways, and the\\nBayesian inference has proved to be a successful method. This involves\\nworking with posterior probabilities such as the following:\\nP(Bob Likes Z \\n| B o b  l i k e s  A ,B o b  l i k e s  B ,... ,B o b  L i k e s  Y )\\nClearly, for this mechanism to work accurately, large amounts of data must\\nbe collected. Information about thousands of individuals is needed, and\\ninformation is required about dozens or hundreds of items for each indi-\\nvidual. In the case of commerce sites, this information can be collected on'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 383, 'page_label': '384'}, page_content='12.11 Chapter Summary 357\\nP(Will Enjoy C) = 0.5\\nP(Will Enjoy C) = 0.7P(Will Enjoy C) = 0.9\\nYe s\\nEnjoys\\nBook A?\\nNo\\nYe s No\\nEnjoys\\nBook B?\\nFigure 12.5\\nA decision tree for collabo-\\nrative filtering\\nthe basis of assuming that if a user buys a book or a CD, then he probably\\nlikes it. More accurate data can be collected by asking users to rate products.\\nT o see how collaborative filtering works, consider the simple decision tree\\nshown in Figure 12.5.\\nThe decision tree in Figure 12.5 relates enjoyment of book C to informa-\\ntion about enjoyment of books A and B. It states that if you did not enjoy\\nbook A, then you will only have a 0.5 probability of enjoying book C.O n\\nthe other hand, if you did enjoy book A and also enjoyed book B, then you\\nwill have a 0.9 chance of enjoying book C.\\nA full collaborative filtering system would have one decision tree for each\\nitem. A full Bayesian belief network would then be built from these deci-\\nsion trees, which can be used to make inferences about a new person on the\\nbasis of their likes or dislikes.\\n12.11 Chapter Summary\\n■ Probabilistic reasoning uses a notation similar to first-order predi-\\ncate calculus, but with the addition of terms such as P(A) = 0.5,\\nwhich states that the probability that A is true (or that A will\\noccur) is 0.5.\\n■ Conditional probability is defined as\\nPB A PB A\\nPA( ) = ∧( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 384, 'page_label': '385'}, page_content='358 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\n■ This means the probability that B will occur, given that we\\nalready know A.\\n■ The joint probability distribution (or joint) is used to represent\\nprobabilities concerning more than one variable.\\n■ Bayes’ theorem can be used to determine the posterior (condi-\\ntional) probability:\\n■ Bayesian concept learning involves selecting the most likely\\nhypothesis to explain a piece of data, using Bayes’ theorem to cal-\\nculate posterior probabilities.\\n■ A Bayesian belief network is an acyclic directed graph, where the\\nnodes in the graph represent evidence or hypotheses, and where an\\narc that connects two nodes represents a dependence between\\nthose two nodes.\\n■ Bayes’ optimal classifier uses Bayes’ theorem to learn to classify\\nitems of data. No other classifier can perform better than the opti-\\nmal classifier, on average.\\n■ The naïve Bayes classifier uses the simplifying assumption that all\\nthe variables used to represent data for classification are independ-\\nent of each other.\\n■ Collaborative filtering is used to guess an individual’s likes or dis-\\nlikes based on prior information about other interests. One very\\nsuccessful method for collaborative filtering is to build a Bayesian\\nbelief network, based on a set of decision trees.\\n12.12 Review Questions\\n12.1 Explain what is meant by the conditional probability of an event.\\n12.2 “Bayes’ theorem uses a conditional probability and two prior prob-\\nabilities to calculate just one conditional probability. That doesn’t\\nsound like it’s very helpful. ” Discuss this comment.\\n12.3 Explain the purpose of the noisy-\\n∨ function.\\n12.4 Explain how Bayes’ theorem can be used to develop learning systems.\\nPB A PA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 385, 'page_label': '386'}, page_content='12.14 Further Reading 359\\n12.5 Explain how Bayes’ optimal classifier and the naïve Bayes classifier\\nwork.\\n12.6 Explain why collaborative filtering is such a useful technique. How\\nsuccessful do you believe it can be? What might limit its efficacy?\\n12.13 Exercises\\n12.1 Implement a Bayesian belief network in the programming lan-\\nguage of your choice to represent a subject in which you are inter-\\nested (for example, you might use it to diagnose medical\\nconditions from symptoms, or to deduce a band from a descrip-\\ntion of the band’s music).\\n12.2 Implement the naïve Bayes classifier in the programming language\\nof your choice, and use it to classify pages of text, based on which\\nwords appear on the page. T o do this, you will first need to train the\\nclassifier with preclassified examples of pages. Y ou should choose\\ntwo classifications: interesting and not interesting, and try to make\\nthe interesting category fairly narrow: for example, it might be\\n“pages about Bayesian reasoning. ” What happens if you make the\\ncategory very broad (such as “pages I find interesting”)?\\n12.3 Use the following facts to calculate normalized values for P(B\\n|A)\\nand P(¬B|A):\\nP(A) = 0.0025\\nP(B) = 0.015\\nP(A\\n|B) = 0.6\\nP(A|¬B) = 0.25\\n12.4 Examine the collaborative filtering mechanism used by an online\\nshopping system. How effective do you think it is? Might there be\\nmore effective methods to achieve the same goal? What kinds of\\nmistakes does the mechanism make? In what situations does it per-\\nform well?\\n12.14 Further Reading\\nIf you are interested in seeing the original proposal of Bayes’ theorem by\\nThomas Bayes from 1763, you can find it in Swinburne (2002). Y ou can read'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 386, 'page_label': '387'}, page_content='360 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nmore about collaborative filtering by exploring the writings of Patti Maes. A\\nless technical explanation can be found in Riedl and Konstan (2002).\\nModeling the Internet and the Web: Probabilistic Methods and Algorithms by\\nPierre Baldi, Paolo Frasconi, and Padhraic Smyth (2003 – John Wiley & Sons)\\nBayesian Theory by José M. Bernardo and Adrian F. M. Smith (2001 – John\\nWiley & Sons)\\nEmpirical Analysis of Predictive Algorithms for Collaborative Filtering by\\nJohn S. Breese, David Heckerman, and Carl Kadie (1998 – in Proceedings of\\nthe Fourteenth Conference on Uncertainty in Artificial Intelligence)\\nExpert Systems and Probabilistic Network Models by Enrique Castillo, Jose\\nManuel Gutierrez, and Ali S. Hadi (1997 – Springer V erlag)\\nProbabilistic Networks and Expert Systems edited by Robert G. Cowell (1999\\n– Springer V erlag)\\nBayesian Methods for Nonlinear Classification and Regression by David G. T.\\nDenison, Christopher C. Holmes, Bani K. Mallick, and Adrian F. M. Smith\\n(2002 – John Wiley & Sons)\\nBayesian Data Analysis by Andrew Gelman, Donald B. Rubin, and Hal S.\\nStern (2003 – CRC Press)\\nProbabilistic Theory of Pattern Recognition by Luc Devroye, Laszlo Gyorfi,\\nand Gabor Lugosi (1998 – Springer V erlag)\\nMaking Decisions by D. V . Lindley (1991 – John Wiley & Sons)\\nLearning Bayesian Networksby Richard E. Neapolitan (2003 – Prentice Hall)\\nProbabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference\\nby Judea Pearl (1997 – Morgan Kaufmann)\\nWord of Mouse: The Marketing Power of Collaborative Filtering by John\\nRiedl and Joseph Konstan (2002–Warner Books)\\nThe Bayesian Choice: From Decision-Theoretic Foundations to Computa-\\ntional Implementation by Christian P . Robert (2001 – Springer V erlag)\\nMonte Carlo Statistical Methods by Christian P . Robert and George Casella\\n(1999 – Springer V erlag)\\nThe Evidential Foundations of Probabilistic Reasoning by David A. Schum\\n(2001 – Northwestern University Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 387, 'page_label': '388'}, page_content='12.14 Further Reading 361\\nSocial Information Filtering: Algorithms for Automating “Word of Mouth by\\nU. Shardanand and P . Maes (1995 – in Proceedings of CHI’95—Human Fac-\\ntors in Computing Systems, pp. 210–217)\\nData Analysis: A Bayesian Tutorial by D. S. Sivia (1996 – Oxford Univer-\\nsity Press)\\nBayes’s Theorem (Proceedings of the British Academy, Vol. 113) edited by\\nRichard Swinburne (2002 – British Academy)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 388, 'page_label': '389'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 389, 'page_label': '390'}, page_content='13CHAPTER\\nArtificial Life: Learning\\nthrough Emergent Behavior\\nNatural Selection is the blind watchmaker, blind because it does not see\\nahead, does not plan consequences, has no purpose in view. Yet the living\\nresults of natural selection overwhelmingly impress us with the appearance of\\ndesign as if by a master watchmaker, impress us with the illusion of design\\nand planning.\\n—Richard Dawkins, The Blind Watchmaker\\nAgents can become more complex in two ways. First, a designer can identify a\\nfunctionality that the agent needs to achieve, then investigate possible behav-\\niors that could realize the functionality, and then introduce various mecha-\\nnisms that give rise to the behavior. Second, existing behavior systems in\\ninteraction with each other and the environment can show side effects, in other\\nwords, emergent behavior.\\n—Luc Steels, The Artificial Life Roots of Artificial Intelligence\\nAll things are artificial, for nature is the art of God.\\n—Sir Thomas Browne, Religio Medici\\n13.1 Introduction\\nThis chapter provides a broad introduction to the subject of Artificial Life.\\nArtificial Life techniques use methods modeled on the behavior of living\\nsystems in much the same way that Artificial Intelligence techniques use\\nmethods modeled on the way the human brain works. Many Artificial Life'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 390, 'page_label': '391'}, page_content='364 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\ntechniques (in particular, genetic algorithms) are an established part of the\\nfield of Artificial Intelligence.\\nThis chapter starts by attempting to define “life”—a difficult problem, but\\none that needs to be discussed in order to consider Artificial Life. Emergent\\nbehavior is one of the most important concepts of Artificial Life—the idea\\nthat systems that are defined in a simple way can produce their own behav-\\nior, which can be remarkably complex.\\nThe chapter introduces a number of Artificial Life techniques, many of which\\nillustrate emergent behavior. T echniques such as cellular automata, genetic\\nprogramming, evolutionary programming, and classifier systems are dis-\\ncussed. Discussion of classifier systems provides an introduction to the subject\\nof genetic algorithms, which is covered in much more detail in Chapter 14.\\nThe chapter also looks at the ways in which systems might be built that are\\nself-reproducing, and a number of systems are explored that model evolution.\\nAs you will see in this chapter and the next, Artificial Life (or A-Life) tech-\\nniques build on a number of Artificial Intelligence techniques and provide\\nways in which systems can adapt (or evolve) to changing conditions. Classi-\\nfier systems, which are explored in Section 13.12, show how the addition of\\nevolutionary techniques to production systems (which are discussed in\\nChapter 9) can enable them to respond to changes in their environment\\nand to learn to deal with unexpected situations.\\n13.2 What Is Life?\\nWhat does it mean to be alive? What differentiates living creatures from\\nnonliving things? This is a question to which there is still no satisfactory\\nanswer. Aristotle, the Greek philosopher, said that a thing was alive if it\\ncould “nourish itself and decay. ” The following is a list of properties that are\\nalso often considered to be indicative of life:\\n■ self-reproduction\\n■ ability to evolve by Darwinian natural selection\\n■ response to stimuli\\n■ ability to die\\n■ growth or expansion'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 391, 'page_label': '392'}, page_content='13.3 Emergent Behavior 365\\nEven this short list has problems. Mules are certainly alive, but they cannot\\nreproduce. The question of whether viruses are alive is not universally\\nagreed. Most lists of properties of life exclude some living creatures or\\ninclude some things that may not be alive.\\nIn other words, it is very difficult to define what life is. Hence, it is not nec-\\nessarily easy to exclude artificial entities—even patterns of data or com-\\nputer programs such as computer viruses. In this chapter, we examine\\nsystems that exhibit many properties of life, but we are not necessarily\\nclaiming that these systems are actually alive. The important thing is that\\nwe are building processes and systems modeled on the ways in which living\\norganisms behave and evolve. In much the same way that Artificial Intelli-\\ngence uses techniques modeled on the way in which the human brain\\nworks, so Artificial Life, a somewhat wider subject in some ways, uses tech-\\nniques modeled on the way in which life works.\\n13.3 Emergent Behavior\\nThe idea of emergent behavior is fundamental to the field of Artificial Life.\\nBy observing the ways in which patterns of sensible behavior emerge in real\\nlife, researchers have been able to develop systems that can produce their\\nown behavior. We have seen this idea already: CYC, the system that has\\nthousands of pieces of information, has been able to form its own analogies\\nabout the world by observing the patterns in the data that it sees.\\nMuch of Artificial Life is based around a simple idea: Evolution works. The\\nprocess or mechanism of evolution may not be fully understood, but the\\nfact remains that complex creatures have evolved in such a way that they\\nare able to survive, despite changing environments, lack of food or warmth,\\nand other complications that are presented by nature.\\nThe reason that evolution works is that creatures that are “successful” in\\nsome way survive and reproduce. If a creature survives and reproduces,\\nthen it will pass on its genetic structure to its offspring. Although this\\nprocess takes place over hundreds of thousands of years, it is possible to use\\nmethods that are based on the same principle that can take place on a com-\\nputer within hours, minutes, or even seconds.\\nOne of the early principles of Artificial Life is that complex behavior can be\\ngenerated (i.e., it emerges) from simple rules. An excellent example of this'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 392, 'page_label': '393'}, page_content='366 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nprinciple is the Boids system, developed by Craig Reynolds in 1987. The\\nidea of this system was that it would model the flocking behavior of birds.\\nRather than having an overall mechanism for controlling the flock, his sys-\\ntem had just a few simple rules that each individual bird obeyed.\\nOne rule ensured that each boid would stay near to other boids by having\\neach boid tend to move toward the center of gravity of the whole flock.\\nAnother rule ensured that boids did not collide with each other.\\nIn running his simulation, Reynolds found that the boids moved in a way\\nextremely similar to the way in which flocks of birds and shoals of fish\\nmove. This technique is now widely used in animation software and in pro-\\nducing computer graphics for movies.\\nOne of the most interesting aspects of the boids was the way in which their\\nbehavior emerged from the rules. For example, no one told the system how\\nto behave when the flock encountered obstacles. Reynolds found that when\\npresented with a series of pillar-shaped obstacles, his computer-simulated\\nboids split into two separate flocks to go around the pillars and then\\nrejoined on the other side of the pillars.\\nClearly, the boids knew that they could not fly through obstacles and that\\nthey should not collide with the obstacles, but the behavior that enabled\\nthe flock to navigate the obstacles was entirely emergent.\\nThis shows how complex behavior can emerge from simple rules. As we\\nwill see later in this chapter, the introduction of evolutionary methods can\\nproduce even more startling results.\\n13.4 Finite State Automata\\nA finite state automaton (FSA) is a simple device that has a finite set of\\nstates and an input string (often thought of as being on a tape, running\\nthrough a device that can read one symbol at a time). Each symbol that the\\nFSA reads in is compared with a rule that dictates which state to move to\\nfrom that state, with that input. After reading the entire input, the finite\\nstate machine is either in an accepting state, which means its answer is\\n“yes” to some question, or it is in some other state, in which case the answer\\nis “no. ” A finite state machine can be represented by a diagram such as the\\none in Figure 13.1.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 393, 'page_label': '394'}, page_content='13.4 Finite State Automata 367\\nbb\\na\\na\\n1 2\\nFigure 13.1\\nA finite state automaton\\nThe FSA in Figure 13.1 determines whether an input string has an even\\nnumber of a’s or not. The two circles represent the two states, 1 and 2,\\nwhich the FSA can be in. The possible input symbols are a and b. The arrow\\nat the top left of the diagram shows where the FSA starts: in state 1. When\\nthe FSA is in state 1, it will stay there until it receives an a, which sends it to\\nstate 2. Similarly, when it is in state 2, it will stay there until it receives an a,\\nwhich will send it back to state 1.\\nHence, if the FSA receives an input with an even number of a’s, it will finish\\nin state 1, otherwise it will finish in state 2.\\nState 1 is an accepting state, which is shown by its having a thicker outline\\nthan state 2.\\nFSAs provide an extremely useful tool for Artificial Intelligence, and com-\\nputer science in general. They also provide an interesting model for Artifi-\\ncial Life, as we see elsewhere in this chapter (Sections 13.5 and 13.10).\\nThe FSA in Figure 13.1 has just two states, but in theory an FSA could\\nhave an extremely large number of states and a much larger vocabulary of\\ninput symbols.\\nA rather simplistic view of living entities might be to consider that each one\\nis simply an FSA. In other words, place an entity in a particular situation\\nand provide it with certain inputs from its environment, and its response\\nwill be deterministically decided by a set of rules. Of course, this is not how\\nliving creatures work at all, but it is possible to mimic certain behaviors of\\nliving creatures using FSAs. For example, boids can be thought of as FSAs.\\nEach boid has a set of inputs (its own location and speeds, and information\\nabout where the other birds and obstacles are) and a state (which direction'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 394, 'page_label': '395'}, page_content='368 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nit is flying and how fast) and a set of rules that determine which state to\\nmove to from each state, according to the input data.\\nIn the next section, we see how much simpler automata can be built, which\\nwhen combined together can produce extremely interesting behavior.\\n13.5 Cellular Automata\\n13.5.1 Conway’ s Life\\nConway’s Life, also known as the Game of Life, is a system that uses a grid\\nof squares and a set of simple rules. Conway’s Life is an excellent illustra-\\ntion of the power of emergent behavior.\\nConway’s Life consists of a two-dimensional grid of squares (or cells), each\\nof which can be either alive or dead. This could be considered to model a\\nreal-world terrain, where each square represented a piece of land, and a\\nsquare would be considered alive if it had a living creature in it and dead\\n(or empty) if it did not.\\nAny given configuration is changed into a successive configuration, or gen-\\neration, by the application of a set of four rules. These rules determine\\nwhat will happen to each cell on the basis of its eight neighbors (assuming\\nan infinite grid). The rules can be defined as follows:\\n1. If a dead cell has exactly three living neighbors in one generation,\\nthen those neighbors will reproduce in the next generation, and\\nthe empty cell will “come to life. ”\\n2. If a living cell has two or three living neighbors, then that cell is\\n“happy, ” and remains alive in the following generation.\\n3. If a living cell has less than two living neighbors, then it dies of\\nloneliness in the next generation.\\n4. If a living cell has more than three living neighbors, then it dies of\\novercrowding in the next generation.\\nFigure 13.2 shows a set of configurations of Conway’s Life, where each cell\\nis either empty (dead) or contains an O (in which case it is alive). The first\\nconfiguration shown in Figure 13.2 is transformed by the rules to the sec-\\nond configuration in the next generation. The second is transformed into\\nthe third, and so on. Hence, the five illustrations in Figure 13.2 show five\\nsuccessive generations of Conway’s Life.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 395, 'page_label': '396'}, page_content='13.5 Cellular Automata 369\\nFigure 13.2\\nFive successive genera-\\ntions of Conway’ s Life\\nThe most interesting aspect of this particular sequence is that the final con-\\nfiguration is almost exactly the same as the first configuration, except that it\\nhas been shifted across and down by one cell. Clearly, by applying the same\\nrules again, the shape will continue to move in this way. This particular\\nconfiguration is known as a glider.\\nConway’s Life becomes more interesting when played over a larger grid (for\\nexample, using a computer monitor with each pixel representing a cell) and\\nwith the starting configuration selected randomly. In some cases, after a\\nnumber of generations, all the cells in the grid have died. In other cases, the\\nsystem reaches a stable state where each generation is the same as the previ-\\nous generation, or where the system oscillates between a few patterns.\\nOne very interesting pattern is known as a glider gun. This configuration\\nconstantly spews out gliders, which then glide away from it. In this way, we\\ncan see a system that in a very simple way can be said to reproduce. Rather\\nthan just changing, or stagnating, the system is able to constantly produce\\nnew “entities, ” if we can consider a glider to be an entity. We will see how this\\nconcept can be more reasonably applied in other areas of Artificial Life.\\nConway’s Life is an example of a cellular automaton. A cellular automaton\\nconsists of a set of cells, each of which contains data (in this case, “alive” or\\n“dead” or “empty” or “full” or 1 or 0). The system is an automaton, or com-\\nputer, in the sense that it acts on a set of input data to produce an output.\\nCellular automata can use more complex sets of rules, and cells can be\\nallowed many more possible values than the two used in Conway’s Life.\\nJohn Von Neumann and Stanislaw Ulam invented the concept of Cellular\\nautomata in the 1950s. Ulam and Von Neumann considered each cell in the\\ngrid of the cellular automata to be a finite state automaton where each cell’s\\nstate could be determined by applying a set of rules to its previous state. In\\ntheir system, each cell could be in one of 29 possible states.\\nBy applying the rules of the system to an initial configuration, cells would\\ntransform their neighbors into different kinds of cells.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 396, 'page_label': '397'}, page_content='370 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nFigure 13.3\\nFive generations of a one-\\ndimensional cellular\\nautomaton\\nVon Neumann’s idea was that in this way, a machine could be created that\\ncould reproduce itself. This is a profound idea and is something that is still\\nresearched today, as we discuss in Section 13.6.\\n13.5.2 One-Dimensional Cellular Automata\\nThe cellular automata that Von Neumann and Conway invented were two\\ndimensional, so that each cell has eight neighbors. Much interesting\\nresearch has been carried out on one-dimensional cellular automata, where\\ncells are arranged in a line, rather than a grid, and each cell has two direct\\nneighbors. It is usual in such systems for the rules to be based not just on\\nthe immediate neighbors, but on the cells one square away from those as\\nwell. So a cell is affected by a total of five values: its four neighbors (two on\\neach side), as well as its own value.\\nFor example, we could create a rule that says that if a living cell has at least\\ntwo living neighbors on either side of it, then it will live, but if it has less\\nthan two neighbors, then it will die. We will further say that if a dead cell\\nhas at least three living neighbors, then it will come to life.\\nThis kind of rule is known as a legal rule, in that if a cell is not alive, and has\\nno living neighbors, then it will stay dead. It is also known as a totalistic\\nrule, which means that the next state of a cell is determined solely by the\\ntotal number of living cells there are in its vicinity. A totalistic rule does not\\ntake into account which side the living cells are on, for example.\\nLegal and totalistic rules for cellular automata can be expressed as a single\\nfive-bit number. Our rule above would be expressed as follows:\\n12345\\n00111\\nFigure 13.3 shows a cellular automaton in which this rule has been applied to\\nproduce five successive generations. The first line of the diagram shows the\\nfirst generation. The second line shows the second generation, and so on.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 397, 'page_label': '398'}, page_content='13.6 Self-Reproducing Systems 371\\nClearly, this particular cellular automaton is not going to produce very inter-\\nesting behavior because it will eventually fill up the entire system with life\\nand will reach a stable (orstagnant) configuration that will never change.\\nBecause the rules consist of five bits, there are 32 possible rules for such cel-\\nlular automata, some of which will produce much more interesting pat-\\nterns than the one shown in Figure 13.3. Some sets of rules have been used\\nto produce patterns that quite closely resemble the patterns that grow nat-\\nurally on some sea shells.\\nAgain, we are seeing how complexity can emerge from a simple set of rules.\\nCellular automata have been applied in a number of fields, including\\npathology, where they are used to analyze blood smears. They have also\\nbeen applied in the field of image processing, and it has been suggested that\\ncellular automata rules resemble the manner in which the visual cortex is\\nstructured.\\n13.6 Self-Reproducing Systems\\nAs we have already seen, Von Neumann postulated the idea of a self-repro-\\nducing system based on cellular automata in the 1950s. Another form of\\nself-reproducing system was invented by Christopher Langton at the end of\\nthe 1970s. Langton’s aim was to develop the simplest system that could\\nreproduce itself.\\nHis creations were called loops. Each loop consisted of just 94 cells,\\narranged in a shape rather like a lower-case letter q. Each cell could take one\\nof eight possible values. Each loop contained all the information that was\\nneeded to produce another identical loop, which in turn could produce a\\nfurther loop, and so on.\\nThe loops’ reproduction was carried out through the tail of the q shape,\\nwhich contained cells that grew to produce a new loop, which then broke\\noff once it had fully formed.\\nOf course, the loops were not real “living” creatures in any way, but they did\\nexhibit one important property of life: reproduction. The loops only “existed”\\nas data in a computer, or as images on a screen, but they represented a step\\nforward—the first artificial system that was capable of self-reproducing.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 398, 'page_label': '399'}, page_content='372 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nVon Neumann’s work predicted that it would be feasible to have a self-\\nreproducing system in the real world: he imagined robots that could\\nvisit other planets, mine minerals from the planet, refine those miner-\\nals, and create new versions of themselves from the materials they\\nfound.\\n13.7 Evolution\\nEach change that occurs from one generation to the next in cellular\\nautomata such as Conway’s Life is simple. By running a large number of\\ngenerations of such a system, reasonably complex patterns can be observed.\\nIn his book,The Blind Watchmaker, Richard Dawkins (1991) describes such\\nchanges as single-step selection . By contrast, the process of evolution\\ninvolves cumulative selection.\\nCumulative selection means that at each step, existing entities or items of\\ndata “reproduce” to form new entities. Rather than each step being based\\non simple rules that define how one state will change to the next, the next\\nstate is based on the best features of the previous state and, in general,\\nimproves on that previous state.\\nIn nature, natural selection is the process that chooses which entities will\\nreproduce. Darwin’s idea of “survival of the fittest” means that the creatures\\nthat manage to reproduce are probably the strongest, in some way, and so sub-\\nsequent generations will tend to inherit stronger features from their parents.\\nIn many Artificial Life systems, natural selection is replaced by artificial\\nselection—for example, in some cases, a person chooses which entity\\nshould reproduce from a population of entities. In The Blind Watchmaker,\\nDawkins described biomorphs, a system of artificial selection that he orig-\\ninally designed to evolve tree-like shapes. The shape of any biomorph was\\ndetermined by just nine variables, or genes. Each gene represented a feature\\nof the biomorphs, such as the branching angle between branches or the\\nlength of branches.\\nThe system produced a set of slightly different biomorphs, and the user\\ncould select one to reproduce. The next generation would consist of a set of\\nbiomorphs that were very similar to the one chosen by the user, but each\\none would differ slightly in one gene. This process of modification is\\nknown as mutation, and we see how it is applied in genetic algorithms in\\nChapter 14.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 399, 'page_label': '400'}, page_content='13.8 Evolution Strategies 373\\nAlthough Dawkins had intended his biomorphs to resemble trees, after\\nrunning just a few generations of his system, he found that the biomorphs\\nwere evolving into shapes that looked like insects. His system had produced\\ncreatures that he had never imagined it would be capable of generating.\\nThis is another example of emergent behavior: complex changes emerging\\nfrom simple rules.\\nDawkins’ biomorphs exhibited artificial selection, where a human chose\\nwhich entities could reproduce in each generation. Systems that more closely\\nresemble natural selection are also possible. As we see in Chapter 14, genetic\\nalgorithms are evolved to solve particular problems by using a measure of\\nfitness based on how close each algorithm comes to solving the problem.\\n13.8 Evolution Strategies\\nEvolution strategies were first developed in the 1960s by Ingo Rechenberg\\nand Hans-Paul Schwefel as a way of solving engineering problems. The idea\\nis similar to hill climbing, which we see in Chapter 4. A possible solution to\\nthe problem is represented as a set of parameters. The initial generation is a\\nrandomly selected set of parameters, and each subsequent generation is pro-\\nduced by adding anormally distributed value to each of the parameters.\\n(The normally distributed mutation values have a mean of zero, and smaller\\nvalues are more likely than larger values. This is based on the fact that in\\nnature, mutations tend to be small changes, rather than large changes).\\nIf the new set of parameters (the offspring) gives a better solution than the\\nprevious set (theparent), then the process continues with the offspring. Oth-\\nerwise, the offspring is rejected, and a new offspring is generated for the parent.\\nNote that in evolving evolution strategies, each offspring is produced from\\njust one parent. In other words, the system uses asexual reproduction.W e\\nwill see how sexual reproduction can be used to develop artificial evolu-\\ntion systems where offspring are produced from more than one parent,\\noften combining good features of each parent to produce offspring that are\\n“better, ” by some metric.\\nThe idea of metrics is an important one, when applying artificial natural\\nselection. T o have a system that evolves entities without human interven-\\ntion, a metric is needed that can be used to determine fitness. A fitter entity\\nis one that is “better” by some criteria: better able to solve a particular prob-\\nlem, stronger, or more beautiful, for example.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 400, 'page_label': '401'}, page_content='374 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nlog\\n+\\n*\\n2xy\\nFigure 13.4\\nTree representation of \\n2x +  log y\\nSelecting a suitable metric is usually the first hurdle when developing an evo-\\nlutionary solution to a problem. For example, to evolve a solution to the prob-\\nlem of sorting a set of numbers, a metric could count how many numbers\\nwere in the correct locations. A more sophisticated metric might count how\\nmany numbers were in the correct order, even if not in the right locations.\\n13.9 Genetic Programming\\nGenetic programming was developed by John Koza in the early 1990s.\\nKoza used genetic programming to evolve solutions to problems in the\\nform of LISP programs, or S-expressions (symbolic expressions). LISP\\nprograms and the data manipulated by LISP programs are both S-expres-\\nsions, and so LISP programs can manipulate each other, or even themselves.\\nGenetic programming can be thought of as a way to search through the\\nspace of possible S-expressions for the one that best solves a given problem.\\nEach S-expression can be represented as a tree, with the operators and val-\\nues in the expression at nodes in the tree. For example, Figure 13.4 shows\\nthe tree for the expression 2x + log y, which in LISP would be expressed as\\n+(*(2x) (log (y))).\\nT o apply genetic programming, the following five steps must first be taken:\\n1. Select a set of terminals.\\nThe terminals are the variables to be used in expressions. In the\\nexample above, the terminals are x and y.\\n2. Select a set of primitive functions.\\nThe primitive functions are the functions that are allowed in our\\nexpressions. In the expression above, we have used the primitive\\nfunctions *, +, and log. We could allow other primitive functions,\\ndepending on the nature of the problem that is to be solved.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 401, 'page_label': '402'}, page_content='13.10 Evolutionary Programming 375\\n3. Select a fitness function.\\nThe fitness function is a way of determining how successful or fit\\nany given expression is. Typically, this will involve applying the S-\\nexpression as a program to a set of sample data and seeing how\\nclose to the correct solutions the results are.\\n4. Select parameters for the system.\\nThe parameters to be chosen include the population size (that is,\\nhow many entities will exist in each generation) and the number of\\ngenerations to run the system for.\\n5. Select a method for determining the result of a run.\\nEach run of the system will produce a new generation. A method\\nneeds to be chosen that will determine which program that has\\nbeen generated so far is the best. Similarly, a termination condition\\nis often chosen that enables the system to stop when it has found a\\nperfect solution.\\nT o produce a new generation, mutation and crossover are applied to the\\ncurrent generation. Mutation simply involves making small changes to an\\nS-expression (such as replacing the “+” operator with the “–” operator, or\\nincreasing the value of a constant from 2 to 2.1).\\nCrossover involves taking two entities from the population and combining\\nfeatures of each to produce a new offspring. In Chapter 14, we see how\\ncrossover is an important aspect of genetic algorithms.\\n13.10 Evolutionary Programming\\nEvolutionary programming (EP) was invented by Lawrence Fogel in 1966.\\nEP was used to evolve solutions to the problem of working out what the\\nnext symbol would be in a finite sequence of symbols: a\\n1, a2, a3, a4, a5,...,\\nan. The method works by evolving FSAs. In the first generation, a set of ran-\\ndom FSAs is generated. The next generation is evolved by producing one\\noffspring from each FSA in the previous generation. Reproduction involves\\napplying one of five mutation operators:\\n1. changing an output symbol\\n2. changing a state transition'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 402, 'page_label': '403'}, page_content='376 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\n3. adding a state\\n4. deleting a state\\n5. changing the initial state\\nT o determine the fitness of an FSA, it is run against each initial subset of the\\nlist of symbols that exists so far and its prediction compared with the actual\\nnext values.\\nHence, if the existing sequence is 1,2,3,4,5,6,7,8,9, then the FSA would first\\nbe run just with the number 1, and its output compared with 2. Next, it\\nwould be run with the sequence 1,2 and the output compared with 3.\\nFinally, it would be run with 1,2,3,4,5,6,7,8 and its output compared with 9.\\nA successful FSA would probably generate 10 as the next number in the\\ncomplete sequence.\\nEach generation contains the parents from the previous generation and\\neach parent’s offspring. Half of these FSAs are allowed to survive—the ones\\nthat make the most correct guesses on the subsequences. These FSAs are\\nthen allowed to reproduce to generate the next generation, and so on.\\n13.11 L-Systems\\nIn the late 1960s, a biologist, Aristid Lindenmayer, developed a set of rules\\nto describe the growth patterns of plants. His “plants” consisted of cells,\\neach of which could take on one of two values— a or b. These represented\\nthe two types of cells seen in the early growth stages of a particular type of\\nalgae. The rules Lindenmayer applied to the cells were as follows:\\nRule 1: a -> ab\\nRule 2: b -> a\\nHence, if we start out with a in the first step, then on the next step this will\\nbecome ab. On the next step, this will become aba, followed by abaab and\\nthen abaababa. This pattern of growth fairly closely matched the growth\\npatterns of the plants that Lindenmayer was studying.\\nThese sets of rules were called L-systems, and it turned out that L-systems\\ncould be used to produce images of remarkably lifelike artificial plants. By\\napplying the L-system rules, strings of thousands of cells could be gener-\\nated, and by interpreting the symbols in those strings as branching pat-\\nterns, images of plant-like structures could be created. By using graphic'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 403, 'page_label': '404'}, page_content='13.12 Classifier Systems 377\\nrendering techniques, images can be generated from L-systems that are\\nindistinguishable from real plants. These images are often used in com-\\nputer games and films.\\nPerhaps more usefully, L-systems can also be used to model biological sys-\\ntems, such as the development processes involved in the growth of plants,\\nthus making it possible to study the workings of life itself, by simulating it\\nin a virtual laboratory.\\n13.12 Classifier Systems\\nClassifier systems, based on the expert systems we saw in Chapter 9, were\\ninvented by John Holland in 1986. As with expert systems, a classifier sys-\\ntem consists of a set of rules that tell the system how to behave in particular\\ncircumstances—how to respond to features in its environment.\\nA classifier system, though, also has the ability to generate better responses\\nand to learn to respond to unfamiliar situations by treating its rules as a\\npopulation to be evolved.\\nThe classifier system consists of the following components:\\n■ detectors that receive inputs from the environment\\n■ effectors that send outputs to the environment, and carry out actions\\n■ a rule system, which consists of a population of classifiers; a vari-\\nable measure of fitness is associated with each rule\\n■ detectors that receive feedback from the environment concerning\\nhow well the system is performing\\n■ a bucket-brigade algorithm for assigning credit and blame to\\nclassifiers\\n■ a procedure for reproducing classifiers by application of a set of\\ngenetic operators\\n■ a set of message lists—for input, output, and internal messages\\nThe operation of the classifier system is as follows:\\nFirst, the environment sends the system an input message, which is received\\nby the input detector.\\nThis message tells the system about some feature of the environment, or about\\nsome event that has occurred (such as a move that has been made in a game).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 404, 'page_label': '405'}, page_content='378 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nThis message is placed on the input message list and translated into a set of\\ninternal messages that the system can interpret, using its classifier rules.\\nThese internal messages cause some of the classifiers to fire—the choice of\\nwhich classifier rules fire is based on the relative fitness of the rules, and\\nalso on how well their antecedents match the internal messages. This is\\nknown as a bidding system, where the classifiers that generate the highest\\nbid (based on fitness and closeness of match) get to fire.\\nThe effect of the classifiers firing is either to generate further internal mes-\\nsages, which may cause further classifiers to fire, or to generate output mes-\\nsages, which are passed back to the environment.\\nThe environment then evaluates the system’s actions and provides feedback\\non how successful they were.\\nAt this point, the system uses a bucket-brigade algorithm to assign credit\\nor blame to the various classifiers in the system. This involves increasing\\nthe fitness of the classifiers that contributed most to a successful outcome\\nand decreasing the fitness of those that contributed most to an unsuccess-\\nful outcome.\\nFinally, successful rules are allowed to reproduce using crossover and\\nmutation operators to produce new rules, whereas unsuccessful rules are\\ndropped from the system altogether.\\nEach classifier consists of three parts:\\n■ a condition (the antecedent of the rule)\\n■ a message (the action of the rule)\\n■ a fitness measure\\nWe can represent classifier rules in the form (c1, c2, c3, c4, c5) -> M, f.H e r e  c1\\nto c5 are the variables that make up the input to the system, and M is the\\noutput message that results from firing this classifier rule, which represents\\nan action or a classification. f is the fitness of the classifier rule.\\nFor example, we can assume that the inputs to the system are numeric vari-\\nables that can take on values from 1 to 10 and that the classification or\\naction that results from each classification is one of five possible actions:\\nA\\n1, A2, A3, A4,o r  A5. Classifier rules do not need to specify a value for each\\nvariable and can specify * to indicate that any value can match that vari-\\nable. Hence, possible classifier rules might be:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 405, 'page_label': '406'}, page_content='13.12 Classifier Systems 379\\n(1, 2, 3, 4, 5) -> A1, 0.7\\n(1, *, *, *, *) -> A3, 2.4\\n(4, 2, *, 1, *) -> A2, 9.1\\n(*, 9, *, 6, 2) -> A3, 7.2\\n(3, 4, 5, *, *) -> A4, 4.5\\n(1, 2, *, *, *) -> A5, 6.2\\nRule 1, for example, specifies that the string (1, 2, 3, 4, 5) is classified as clas-\\nsification A1, with a fitness of 0.7.\\nNow let us imagine that an input message arrives from the environment,\\nwhich is (1, 2, 3, 4, 5). This will match classifiers 1, 2, and 6. These three\\nclassifiers now bid. The value of a classifier’s bid is a function of that classi-\\nfier’s fitness and how closely its antecedent matches the input message. This\\nmeasure of closeness is determined by adding 1 for each exact match and\\n0.5 for each * (which matches any input symbol). These values are summed\\nand divided by the length of the message. This number is then multiplied\\nby the classifier’s fitness to produce its total bid.\\nHence, the bids for the three matching classifiers in our example are as follows:\\nFor classifier 1:\\nbid = ((1 + 1 + 1 + 1 + 1) / 5) * 0.7 = 0.7\\nFor classifier 2:\\nbid = ((1 + 0.5 + 0.5 + 0.5 + 0.5) / 5) * 2.4 = 0.96\\nFor classifier 6:\\nbid = ((1 + 1 + 0.5 + 0.5 + 0.5) / 5) * 6.2 = 4.34\\nThe classifier with the highest bid is successful, and fires, providing a classi-\\nfication of A5. This is fed back to the environment as an output message,\\nand the environment evaluates it to determine if this is correct or not.\\nIf the classifier has made a correct assessment, its fitness is increased, which\\nis determined by subtracting the bid value from a positive reward score. If\\nit made an incorrect assessment, the reward will be negative (or lower than\\nthe bid value) and so its fitness level will decrease.\\nIn fact, in most classifier systems, the bidding process is far more complex,\\nand more than one classifier can be successful by forming joint bids. This is\\nwhere the bucket-brigade algorithm becomes important for determining'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 406, 'page_label': '407'}, page_content='380 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nwhich classifiers to reward and to what extent, based on how much they\\ncontributed to the success (or failure) of the system as a whole. Holland\\nbased this bidding system on economic processes, with individual classi-\\nfiers acting like businesses bidding for contracts.\\nFinally, reproduction occurs. Let us examine how this happens, by assuming\\nthat the system has decided to reproduce from the two fittest classifiers, 3 and 4.\\nThese classifiers are defined as follows:\\n3. (4, 2, *, 1, *) -> A2, 9.1\\n4. (*, 9, *, 6, 2) -> A3, 7.2\\nFirst, a position is chosen randomly from within the antecedent. This point\\nis called the crossover position. For our example, we will assume that the\\nsystem has chosen the position between the third and fourth variables as its\\ncrossover position, as follows:\\n3. (4, 2, *,\\n| 1, *) -> A2, 9.1\\n4. (*, 9, *, | 6, 2) -> A3, 7.2\\nNow crossover is applied as follows: the first half of classifier 3, before the\\ncrossover position, is joined to the second half of classifier 4, after the\\ncrossover position. This produces an offspring classifier, which we will call\\nclassifier 7:\\n7. (4, 2, *, 6, 2) -> A2, 8.4\\nNote that the output message for this new classifier is chosen to be A2,\\nbecause this is the output classifier of the parent classifier that contributed\\nthe larger part to the offspring (3 variables).\\nThe fitness of the offspring is determined by taking proportionally from\\nthe parents—three-fifths of the fitness of classifier 3 (because it con-\\ntributed three of the five variables) and two-fifths of the fitness of classifier\\n4. Hence the fitness of classifier 7 is defined as\\n(3 / 5) * 9.1 + (2 / 5) * 7.2 = 8.4\\nSimilarly, crossover is applied in the other proportions by attaching the first\\npart of classifier 4 to the second part of classifier 3, to produce\\n8. (*, 9, *, 1, *) -> A3, 7.96\\nThe final part of the reproduction process involves the optional application\\nof a mutation operator. This simply involves changing one of the parts of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 407, 'page_label': '408'}, page_content='13.13 Artificial Immune Systems 381\\nthe offspring after it has been produced. For example, a variable value\\nmight change to another value, or to *. Typically, as we see in Chapter 14,\\nmutation is applied sparingly, so that not too much of the parents’ genetic\\ninformation is lost. Hence, we might apply mutation to one of the symbols\\nin offspring 7 to produce\\n7. (4, 2, *, 6, *) -> A2, 8.4\\nThe description of classifier systems so far has been rather abstract. We can\\nimagine classifier systems being used, for example, to play a game such as\\nchess, where a static evaluation function is used to determine whether a\\nmove was good or bad, and where the inputs are the positions of the pieces\\non the board.\\nIn the 1980s, Stewart Wilson, a researcher at Polaroid, used classifier sys-\\ntems to build an artificial creature he called “*” . * was placed in a world\\nconsisting of rocks and food. Over a period of time, * learned to deal with\\nits world more and more efficiently. For example, it learned that food was\\noften near a rock, but that banging into a rock was painful. Hence, when it\\nencountered a rock, it would stop and then walk around the rock to see if\\nany food was present. This artificial creature had learned to survive in its\\nown environment without anyone needing to teach it how. Its survival\\nemerged from a combination of its environment and its reasonably simple\\nclassifier system “brain.”\\nOf course, this did not take place in the real world but, like most Artificial\\nLife, took place inside a computer in the form of binary data.\\n13.13 Artificial Immune Systems\\nArtificial immune systems (AIS) are a relatively recent innovation. The idea\\nbehind AIS is to build systems based on the immune system in human beings\\nand other animals. The biological immune system is a massively parallel sys-\\ntem that is able to deal with changes in individual bodies, changes in envi-\\nronment, and even to adapt to rapidly evolving viruses and other attackers.\\nOne of the first uses for AIS was to build a system that could defend com-\\nputers against viruses. Early antivirus systems based on this technique\\nrelied on people reporting a new virus to the “immune system, ” which\\nwould then make attempts to analyze the virus to determine ways to iden-\\ntify and block it.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 408, 'page_label': '409'}, page_content='382 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nMore advanced methods are now applied using artificial immune systems\\nto solve combinatorial search problems and are also applied in computer\\nsecurity, machine learning, and fault diagnosis.\\n13.14 Chapter Summary\\n■ A definition of life is not easy to produce. Counterexamples can be\\nfound for most definitions.\\n■ Artificial Life is modeled on life in the same way that Artificial\\nIntelligence is modeled on the human brain.\\n■ Complex behavior tends to emerge from simple systems when\\nusing techniques modeled on life. Such behaviors are emergent.\\n■ Cellular automata such as Conway’s life show how life can be simu-\\nlated in an extremely simple system based on finite state automata.\\n■ Langton’s loops were an example of a simple self-reproducing sys-\\ntem. Von Neumann postulated an entity that could physically\\nreproduce itself.\\n■ Evolution strategies use asexual reproduction to search for solu-\\ntions to engineering problems.\\n■ Genetic programming methods evolve S-expressions or LISP pro-\\ngrams to solve problems.\\n■ Evolutionary programming involves evolving finite state automata\\nto predict the next item in a sequence of symbols.\\n■ L-systems use simple rules to build complex plant-like structures.\\n■ Classifier systems combine evolutionary methods (genetic algo-\\nrithm) with a production system to build a system that is able to\\nadapt to changes in its environment.\\n13.15 Review Questions\\n13.1 What is life?\\n13.2 What is Artificial Life? How does it relate to Artificial Intelligence?\\nIs one an alternative to the other, or are they complementary?\\n13.3 Explain what is meant by emergent behavior.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 409, 'page_label': '410'}, page_content='13.16 Further Reading 383\\n13.4 Explain how Conway’s Life is modeled on life. What interesting\\nproperties does it exhibit? Why do you think it has fascinated peo-\\nple for so long?\\n13.5 Explain how a system might be built that could reproduce itself.\\nWould such a system be alive?\\n13.6 Explain how genetic programming could be used to solve problems.\\n13.7 What is evolutionary programming? How does it differ from\\ngenetic programming?\\n13.8 Explain why L-systems are of interest to Artificial Life researchers.\\n13.9 Explain the relationship between classifier systems and production\\nsystems. How are classifier systems built? What advantages do they\\nhave over production systems?\\n13.10 Explain how systems modeled on the human immune system\\nmight provide a solution to the problem of computer viruses or of\\nunsolicited bulk e-mails (“spam”).\\n13.16 Further Reading\\nThere is a great deal of literature on the subject of Artificial Life. A good\\nintroduction to the subject from a relatively nontechnical point of view can\\nbe found in Levy (1993) and Kelly (1994). Adami (1997) is a more\\nadvanced text on the subject.\\nLangton (1995) provides a number of interesting articles on Artificial Life,\\nincluding philosophic and sociologic perspectives.\\nClassifier systems and genetic algorithms are covered by many of the main\\nArtificial Intelligence texts, but few of them cover any other aspects of Arti-\\nficial Life.\\nInformation on artificial immune systems can be found in de Castro and\\nTimmis (2002) and Dasgupta (1999).\\nDawkins (1991) gives an excellent view of the subject from the biologic\\nevolutionary perspective.\\nA fictional account of the potentials of Artificial Life can be found in Prey\\nby Michael Crichton.\\nIntroduction to Artificial Life, by Christoph Adami (1997 – T elos)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 410, 'page_label': '411'}, page_content='384 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nGenetic Programming: An Introduction: On the Automatic Evolution of Com-\\nputer Programs and Its Applications , by Wolfgang Banzhaf, Peter Nordin,\\nRobert E. Keller, and Frank D. Francone (1997 – Morgan Kaufmann)\\nDigital Biology, by Peter Bentley (2002 – Simon & Schuster)\\nThe Philosophy of Artificial Life, by Margaret A. Boden (1996 – Oxford Uni-\\nversity Press)\\nSwarm Intelligence: From Natural to Artificial Systems , by Eric Bonabeau,\\nMarco Dorigo, and Guy Theraulaz (1999 – Oxford University Press)\\nArtificial Immune Systems and Their Applications, edited by Dipankar Das-\\ngupta (1999 – Springer V erlag)\\nThe Blind Watchmaker, by Richard Dawkins (1996 – W. W. Norton & Com-\\npany)\\nArtificial Immune Systems: A New Computational Intelligence Paradigm ,b y\\nLeandro N. de Castro and Jonathan Timmis (2002 – Springer V erlag)\\nEvolutionary Computation in Bioinformatics , edited by Gary B. Fogel and\\nDavid W. Corne (2002 – Morgan Kaufmann)\\nCreation: Life and How to Make It, by Steve Grand (2001 – Harvard Univer-\\nsity Press)\\nFrom Animals to Animats 7: Proceedings of the Seventh International Confer-\\nence on Simulation of Adaptive Behavior , edited by Bridget Hallam, Dario\\nFloreano, John Hallam, Gillian Hayes, and Jean-Arcady Meyer (2002 – MIT\\nPress ; also available are the proceedings from the first to sixth conferences)\\nSilicon Second Nature: Culturing Artificial Life in a Digital World , by Stefan\\nHelmreich (2000 – University of California Press)\\nEmergence: The Connected Lives of Ants, Brains, Cities, and Software ,b y\\nSteven Johnson (2001 – Scribner)\\nOut of Control: The New Biology of Machines , by Kevin Kelly (1994 –\\nFourth Estate)\\nSwarm Intelligence by James Kennedy, Russell C. Eberhart, and Yuhui Shi\\n(2001 – Morgan Kaufmann)\\nGenetic Programming: On the Programming of Computers by Means of Nat-\\nural Selection, by John R. Koza (1992 – MIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 411, 'page_label': '412'}, page_content='13.16 Further Reading 385\\nGenetic Programming II: Automatic Discovery of Reusable Programs, by John\\nR. Koza (1994 – MIT Press)\\nArtificial Life: An Overview, edited by Christopher Langton (1995 – MIT Press)\\nArtificial Life: A Report from the Frontier Where Computers Meet Biology,b y\\nSteven Levy (1993 – Vintage Books)\\nEvolutionary Algorithms for Single and Multicriteria Design Optimization ,\\nby Andrzej Osyczka (2001 – Physica V erlag)\\nArtificial Life VIII: Proceedings of the Eighth International Conference on\\nArtificial Life, edited by Russell Standish, Mark A. Bedau, and Hussein A.\\nAbbass (2003 – MIT Press; also available are the proceedings from the first\\nthrough the seventh conferences)\\nEvolutionary Art and Computers , by Stephen T odd and William Latham\\n(1992 – Academic Press)\\nVirtual Organisms: The Startling World of Artificial Life ,b y  M a r k  W a r d\\n(2000 – St Martin’s Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 412, 'page_label': '413'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 413, 'page_label': '414'}, page_content='14CHAPTER\\nGenetic Algorithms\\nSome call it evolution,\\nAnd others call it God.\\n—William Herbert Carruth, Each in His Own Tongue\\nThe first technical descriptions and definitions of adaptation come from biol-\\nogy. In that context adaptation designates any process whereby a structure is\\nprogressively modified to give better performance in its environment. The\\nstructures may range from a protein molecule to a horse’s foot or a human\\nbrain or, even, to an interacting group of organisms such as the wildlife of the\\nAfrican veldt.\\n—John H. Holland, Adaptation in Natural and Artificial Systems\\n14.1 Introduction\\nThe idea of local search is introduced in Chapter 5. Local search methods\\ninvolve making small changes to potential solutions to a problem until an\\noptimal solution is identified. Genetic algorithms are a form of local search\\nthat use methods based on evolution to make small changes to a popula-\\ntion of chromosomes in an attempt to identify an optimal solution.\\nIn this chapter, the representations used for genetic algorithms are dis-\\ncussed, including the idea of schemata. The genetic operators, crossover\\nand mutation, are explained, as is the idea of fitness.\\nThe procedures used to run genetic algorithms are also discussed, and an\\nattempt is made to explain why genetic algorithms work.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 414, 'page_label': '415'}, page_content='388 CHAPTER 14 Genetic Algorithms\\nAn example is given of how genetic algorithms might be used to evolve a\\nstrategy for playing a simple game (Prisoner’s Dilemma), and the idea of\\nallowing humans to input into the process in order to evolve images of\\n“creatures” is also explored.\\n14.2 Representations\\nWe have seen a number of different representations that can be used in\\nevolutionary techniques like genetic algorithms. Genetic programming is\\nused to evolve S-expressions, which can be used as LISP programs to solve\\nproblems. Classifier systems use a string of numbers that represent proper-\\nties of the environment and symbols that represent responses to those\\nproperties.\\nThe simplest representation for genetic algorithms is the one that was used\\nby John Holland: a string of bits. A string of bits is known as a chromo-\\nsome, and each bit is known as a gene. Both of these terms are directly bor-\\nrowed from genetics and illustrate the close manner in which genetic\\nalgorithms mirror biological processes.\\nFor most of this chapter, we discuss genetic algorithms using this represen-\\ntation, but it is worth remembering that many other representations are\\npossible, and different representations will be more appropriate for partic-\\nular problems.\\nThe population consists of a set of chromosomes, each of which, as we\\nhave seen, is made up of genes. A chromosome is usually taken to represent\\na complete “individual” within the population—in other words, a complete\\nrepresentation of a solution, or a classification. It is also possible to com-\\nbine chromosomes together to form creatures, which more closely mirrors\\nreal genetics because each individual in the real world has a number of\\nchromosomes. For now, we will continue with Holland’s approach, where a\\nchromosome represents an entire individual.\\nEach gene in the chromosome represents some facet of that individual’s\\ngenetic makeup. For example, the genes could be entirely independent and\\nrepresent the presence or otherwise of certain body parts in an animal.\\nMore usually, the genes are combined together in a less transparent way.\\nFor example, we will see how genetic algorithms can be used to solve math-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 415, 'page_label': '416'}, page_content='14.3 The Algorithm 389\\nematical problems, where the bits of a chromosome are usually treated as\\nthe bits of a binary number that represents a solution to the problem.\\n14.3 The Algorithm\\nThe process for running a genetic algorithm is as follows. Note that this\\nprocess is largely independent of the representation that is being used.\\n1. Generate a random population of chromosomes (this is the first\\ngeneration).\\n2. If the termination criteria are satisfied, stop. Otherwise, continue\\nwith step 3.\\n3. Determine the fitness of each chromosome.\\n4. Apply crossover and mutation to selected chromosomes from the\\ncurrent generation to generate a new population of chromo-\\nsomes—the next generation.\\n5. Return to step 2.\\nNote that the evolutionary part of the classifier system process that we saw\\nin Chapter 13 is in fact a genetic algorithm.\\nThe size of the population should be determined in advance. Usually, the\\npopulation size remains constant from one generation to the next. In some\\nsituations, it can be useful to have a population that changes size.\\nThe size of each chromosome must remain the same for crossover to be\\napplied. It is possible to run genetic algorithms with variable chromosome\\nsizes, but this is unusual.\\nTypically, the fittest chromosomes are selected in each generation to mate\\nwith each other, and each pair of chromosomes is allowed to produce two\\noffspring. The resultant set of offspring chromosomes then replaces the\\nprevious generation.\\nIt is also possible to allow particularly fit parents to produce relatively more\\noffspring and to allow certain members of a generation to survive to the\\nnext generation. For most of this chapter, we will assume that each pair of\\nparents produces two offspring and that those offspring replace the parents.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 416, 'page_label': '417'}, page_content='390 CHAPTER 14 Genetic Algorithms\\n14.4 Fitness\\nRichard Dawkins’ biomorph world, which is discussed in Chapter 13, is a\\nform of genetic algorithm. Rather than applying an objective fitness level,\\nfitness was determined subjectively by a human operator. Additionally,\\neach generation was the offspring of just one parent, to which mutation\\nwas applied.\\nWith more traditional genetic algorithms, a metric is needed whereby the\\nfitness of a chromosome can be objectively determined. For example, in\\nusing genetic algorithms to sort numbers into numeric order, a suitable fit-\\nness measure might be determined by running the algorithm and counting\\nhow many numbers it places in the correct position. A more sophisticated\\nmeasure of fitness could be obtained by measuring how far from its correct\\nplace each incorrectly placed number is.\\nKarl Sims evolved “creatures” that were bred according to their abilities to\\nperform simple tasks, such as walking, jumping, and swimming (Sims\\n1994). Sims used a representation and a set of rules that determined how\\nthe various body parts of his creatures interacted with each other and with\\ntheir environment. In this case, then, the fitness measure was based on the\\nextent to which the physical form ( phenotype) represented by the genetic\\ninformation (genotype) met certain criteria.\\n14.5 Crossover\\nIn Chapter 13, we see how crossover is used in classifier systems. The crossover\\noperator is applied to two chromosomes of the same length as follows:\\n1. Select a random crossover point.\\n2. Break each chromosome into two parts, splitting at the crossover\\npoint.\\n3. Recombine the broken chromosomes by combining the front of\\none with the back of the other, and vice versa, to produce two new\\nchromosomes.\\nFor example, consider the following two chromosomes:\\n110100110001001\\n010101000111101'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 417, 'page_label': '418'}, page_content='14.5 Crossover 391\\n100110001\\n011100110\\n101100001\\n010110110\\nFigure 14.1\\nIllustrating two-point\\ncrossover\\nA crossover point might be chosen between the sixth and seventh genes:\\n110100 | 110001001\\n010101 | 000111101\\nNow the chromosome parts are recombined as follows:\\n110100 | 000111101 => 110100000111101\\n010101 | 110001001 => 010101110001001\\nThis process is based on the way in which DNA strands recombine with each\\nother in human reproduction to combine features of each parent in a child.\\nSingle-point crossover is the most commonly used form, but it is also pos-\\nsible to apply crossover with two or more crossover positions.\\nIn two-point crossover, two points are chosen that divide the chromosomes\\ninto two sections, with the outer sections considered to be joined together\\nto turn the chromosome into a ring. The two sections are swapped with\\neach other, as shown in Figure 14.1.\\nIn Figure 14.1, the genes from parent 1 are shaded in grey, while the genes\\nfrom parent 2 are not shaded.\\nAnother form of crossover is uniform crossover. Here, a probability, p,i s\\nused to determine whether a given bit from parent 1 will be used, or from\\nparent 2. In other words, a child can receive any random bits from each of\\nits parents. For example, let us assume we have the following two parents:\\nParent 1: 10001101\\nParent 2: 00110110\\nThe offspring of these two chromosomes might be determined as shown in\\nFigure 14.2.\\n100110001\\n011100110\\n110100101\\n001110010\\nFigure 14.2\\nIllustrating uniform\\ncrossover of two-parent\\nchromosomes to produce\\ntwo offspring'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 418, 'page_label': '419'}, page_content='392 CHAPTER 14 Genetic Algorithms\\nThe first bit of the first child is chosen to be from parent 1 with probability\\np and from parent 2 with probability 1 – p. If a bit from parent 1 is chosen\\nfor child 1, then the corresponding bit from parent 2 is chosen for child 2,\\nand vice versa. Uniform crossover is also often used to produce just one off-\\nspring from each pair of parents, unlike traditional one- or two-point\\ncrossover, which usually produces two offspring from each pair of parents.\\nUniform crossover does mix up the genes of the gene pool substantially,\\nand in some cases it can be sensible to use a very high (or very low) value of\\np to ensure that most of the genes come from one parent or the other.\\nIn some cases, cloning can be applied, whereby crossover is not applied at\\nall, and a new offspring is produced that is identical to its single parent.\\nDawkins’ biomorph system can be thought of as a genetic algorithm with\\ncloning and mutation, where fitness is determined subjectively.\\n14.6 Mutation\\nY ou may recognize genetic algorithms as being rather similar to the hill-\\nclimbing methods we see in Chapter 4. Hill-climbing involves generating a\\npossible solution to the problem and moving toward a better solution than\\nthe current one until a solution is found from which no better solution can\\nbe found. Hill climbing does not perform well with problems where there\\nare local maxima. T o enable genetic algorithms to avoid this problem, the\\nmutation operator was introduced.\\nMutation is a unary operator (i.e., it is applied to just one argument—a\\nsingle gene) that is usually applied with a low probability, such as 0.01 or\\n0.001. Mutation simply involves reversing the value of a bit in a chromo-\\nsome. For example, with a mutation rate of 0.01, it might be expected that\\none gene in a chromosome of 100 genes might be reversed. Here we see\\nmutation applied to one of the offspring from our example above:\\n010101110001001\\n⇓\\n010101110101001\\n14.7 Termination Criteria\\nThere are typically two ways in which a run of a genetic algorithm is termi-\\nnated. Usually, a limit is put on the number of generations, after which the\\nrun is considered to have finished.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 419, 'page_label': '420'}, page_content='14.8 Optimization of a Mathematic Function 393\\nWith some problems, the run can stop when a particular solution has been\\nreached, or when the highest fitness level in the population has reached a\\nparticular value. For example, we see in the following section how a genetic\\nalgorithm can be used to solve a mathematical function. In this case, it is\\nclear that the run can stop when the correct solution has been reached,\\nwhich can be easily tested for.\\nIn the case of Dawkins’ biomorph world, no such termination conditions\\nexist. It does not make sense to impose an artificial limit on the number of\\ngenerations in the run, and because no objective measure of fitness is\\ninvolved, the system cannot determine when to stop on that basis.\\nThis is an important distinction. In many cases, genetic algorithms are used\\nto solve problems that have an objective solution, in which case the algo-\\nrithm can stop when it reaches that solution. In other cases, they are used\\nfor more abstract purposes, such as to generate interesting pictures. In these\\ncases, human judgment must be used to determine when to terminate.\\n14.8 Optimization of a Mathematic Function\\nWe will see how a genetic algorithm can be used to find a maximum value\\nof a mathematic function.\\nWe will attempt to maximize the following function:\\nf(x) = sin(x)\\nover the range of x from 1 to 15, where x is in radians.\\nEach chromosome represents a possible value of x using four bits.\\nFigure 14.3 shows the discrete graph for this function.\\n1.50\\n1.00\\n0.50\\n0.00\\n–0.50\\n–1.00\\n–1.50\\nx\\nf(x)\\nFigure 14.3\\nDiscrete graph for the\\nfunction f(x) = sin(x),\\nwhere x ranges from \\n0 to 15.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 420, 'page_label': '421'}, page_content='394 CHAPTER 14 Genetic Algorithms\\nTable 14.1 Generation 1\\nChromosome Genes Integer value f(x) Fitness f/H11032(x) Fitness ratio\\nc1 1001 9 0.41 70.61 46.3%\\nc2 0011 3 0.14 57.06 37.4%\\nc3 1010 10 /H110020.54 22.80 14.9%\\nc4 0101 5 /H110020.96 2.05 1.34%\\nWe will use a population size of four chromosomes. The first step is to gen-\\nerate a random population, which is our first generation:\\nc1 = 1001\\nc2 = 0011\\nc3 = 1010\\nc4 = 0101\\nT o calculate the fitness of a chromosome, we need to first convert it to a\\ndecimal integer and then calculate f (x) for this integer.\\nWe will assign fitness as a numeric value from 0 to 100, where 0 is the least\\nfit and 100 is the most fit.\\nf(x) generates real numbers between /H110021 and 1. We will assign a fitness of\\n100 to f(x) = 1 and fitness of 0 to f (x) = /H110021. Fitness of 50 will be assigned\\nto f(x) = 0. Hence, fitness of x, f /H11032(x) is defined as follows:\\nf/H11032(x) = 50(f(x) + 1)\\n= 50(sin(x) + 1)\\nThe fitness ratio of a chromosome is that chromosome’s fitness as a per-\\ncentage of the total fitness of the population. We will see later why this is a\\nuseful calculation.\\nTable 14.1 shows the calculations that are used to calculate the fitness val-\\nues for our first generation.\\nNow we need to run a single step of our genetic algorithm to produce the\\nnext generation. The first step is to select which chromosomes will repro-\\nduce. Roulette-wheel selection involves using the fitness ratio to randomly\\nselect chromosomes to reproduce. This is done as follows:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 421, 'page_label': '422'}, page_content='14.8 Optimization of a Mathematic Function 395\\nThe range of real numbers from 0 to 100 is divided up between the chro-\\nmosomes proportionally to each chromosome’s fitness. Hence, in our first\\ngeneration, c1 will have 46.3% of the range (i.e., from 0 to 46.3), c2 will\\nhave 37.4% of the range (i.e., from 46.3 to 83.7), and so on.\\nA random number is now generated between 0 and 100. This number will\\nfall in the range of one of the chromosomes, and this chromosome has\\nbeen selected for reproduction. The next random number is used to select\\nthis chromosome’s mate. Hence, fitter chromosomes will tend to produce\\nmore offspring than less fit chromosomes.\\nIt is important that this method does not stop less fit chromosomes from\\nreproducing at all, though, because this helps to ensure that populations do\\nnot stagnate, by constantly breeding from the same parents.\\nIn our example, though, chromosome c4 will be very unlikely to reproduce\\nbecause this would only occur if the random number fell in the narrow\\nrange between 98.6 and 100.\\nWe will need to generate four random numbers to find the four parents\\nthat will produce the next generation. Our first random number is 56.7,\\nwhich means that c2 has been chosen as the first parent. Next, 38.2 is cho-\\nsen, so its mate is c1.\\nWe now need to combine c1 and c2 to produce two new offspring. First, we\\nneed to randomly select a crossover point. We will choose the point\\nbetween the second and third bits (genes):\\n10 | 01\\n00 | 11\\nCrossover is now applied to produce two offspring, c5 and c6:\\nc5 = 1011\\nc6 = 0001\\nIn a similar way, c1 and c3 are chosen to produce offspring c7 and c8, using\\na crossover point between the third and fourth bits:\\nc7 = 1000\\nc8 = 1011\\nThe population c1 to c4 is now replaced by the second generation, c5 to c8.\\nc4 did not have a chance to reproduce, and so its genes will be lost. c1,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 422, 'page_label': '423'}, page_content='396 CHAPTER 14 Genetic Algorithms\\nTable 14.2 Generation 2\\nChromosome Genes Integer value f(x) Fitness f/H11032(x) Fitness ratio\\nc5 1011 11 /H1100210 0 %\\nc6 0001 1 0.84 92.07 48.1%\\nc7 1000 8 0.99 99.47 51.9%\\nc8 1011 11 /H1100210 0 %\\nwhich was the fittest chromosome in the first generation, was able to repro-\\nduce twice, thus passing on its highly fit genes to all members of the next\\ngeneration.\\nThe fitness values for the second generation are shown in Table 14.2.\\nThis generation has produced two extremely fit chromosomes and two\\nvery unfit chromosomes. In fact, one of the chromosomes, c7, is the opti-\\nmal solution. At this point, the termination criteria would probably deter-\\nmine that the run could stop. Otherwise, the algorithm will continue to run\\nbut will not find any better solutions. It has taken just one step to get from\\na random configuration to the optimal solution.\\nClearly, this was a very simplistic example. Real problems are likely to be\\nmuch harder to solve. They are also likely to involve much larger popula-\\ntion sizes (typically population sizes of between 100 and 500 are used), and\\nchromosomes are likely to contain far greater numbers of bits.\\nIn many cases, genetic algorithms quickly produce optimal or near-optimal\\nsolutions to combinatorial problems that would otherwise be impractical\\nto solve. This raises an interesting question: Why do genetic algorithms\\nwork? We will now address this question.\\n14.9 Why Genetic Algorithms Work\\nGenetic algorithms are a local search method (see Chapter 5), in some ways\\nsimilar to simulated annealing and hill-climbing methods.\\nIt is possible to explain genetic algorithms by comparison with natural evo-\\nlution: small changes that occur on a selective basis combined with repro-\\nduction will tend to improve the fitness of the population over time. This'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 423, 'page_label': '424'}, page_content='14.9 Why Genetic Algorithms Work 397\\nargument is not very convincing, and John Holland (1975) invented\\nschemata (the plural of schema) to provide an explanation for genetic\\nalgorithms that is more rigorous.\\n14.9.1 Schemata\\nIn Chapter 13, we see how strings of numbers are used to represent input\\npatterns in classifier systems. In these patterns, * is used to represent “any\\nvalue” or “don’t care, ” so that the following string:\\n1011*001*0\\nmatches the following four strings:\\n1011000100\\n1011000110\\n1011100100\\n1011100110\\n(The bits which have matched * are shown in bold).\\nA schema is a string of bits that represents a possible chromosome, using *\\nto represent “any value. ” A schema is said to match a chromosome if the bit\\nstring that represents the chromosome matches the schema in the way\\nshown above. For example, the following schema:\\n*11*\\nmatches the following four chromosomes:\\n0110\\n0111\\n1110\\n1111\\nNote that a schema with n *’s will match a total of 2n chromosomes.\\nEach chromosome of r bits will match 2r different schemata. For example,\\nthe following chromosome:\\n101\\nmatches the following eight schemata:\\n101\\n10*\\n1*1\\n1**\\n*01\\n*0*'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 424, 'page_label': '425'}, page_content='398 CHAPTER 14 Genetic Algorithms\\n**1\\n***\\nBecause schemata are made up of three different values (0, 1, and *), there\\nare 3m different schemata of length m. For example, there are nine possible\\nschemata of just two bits:\\n00\\n01\\n0*\\n10\\n*0\\n11\\n*1\\n1*\\n**\\nThe defining length of a schema is defined as the distance between the first\\nand last defined bits (bits that are not *) in the schema. For example, the\\ndefining length of each of the following schemata is 4:\\n**10111*\\n1*0*1**\\n11111\\n1***1\\n***********10**1***************\\nNote that a schema’s defining length is not dependent on the number of\\nbits it has, except that its defining length must be less than or equal to its\\nlength. We write this as\\nd\\nL(S) ≤ L(S)\\nwhere dL(S) is the defining length of schema S, and L(S) is the length of\\nschema S.\\nThe order of a schema is defined as the number of defined bits (i.e., the\\nnumber of bits that are not *) in the schema. Hence, the following\\nschemata all have order 4:\\n**10*11*\\n1*0*1**1\\n1111\\n1***1***1***1\\n1***********10**1***************'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 425, 'page_label': '426'}, page_content='14.9 Why Genetic Algorithms Work 399\\nWe will write the order of a schema S as O(S). The order of a schema tells us\\nhow specific it is. A schema with a high order is more specific than one\\nwith a lower order.\\n14.9.2 How Reproduction Affects Schemata\\nWe can think of genetic algorithms as a way of manipulating schemata.\\nThis will help us to reason about why genetic algorithms work.\\nFirst of all, we consider what it means for a schema to be present in a pop-\\nulation. Let us consider the following population of 10 chromosomes, each\\nof length 32:\\nC1 = 01000100101010010001010100101010\\nC2 = 10100010100100001001010111010101\\nC3 = 01010101011110101010100101010101\\nC4 = 11010101010101001101111010100101\\nC5 = 11010010101010010010100100001010\\nC6 = 00101001010100101010010101111010\\nC7 = 00101010100101010010101001010011\\nC8 = 11111010010101010100101001010101\\nC9 = 01010101010111101010001010101011\\nC10 = 11010100100101010011110010100001\\nLet us consider the following schema:\\nS0 = 11010***************************\\nThis schema is matched by three chromosomes in our population: c 4,c 5,\\nand c10. We say that schema S0 matches three chromosomes in generation i\\nand write this as follows:\\nm(S0, i) = 3\\nIt is useful now to consider the concept of fitness as it applies to schemata.\\nThe fitness of a schema, S, in generation i is written as follows:\\nf(S, i)\\nThe fitness of a schema is defined as the average fitness of the chromo-\\nsomes in the population that match the schema. Hence if we define the fit-\\nness of c\\n4,c 5, and c10 as follows:\\nf(C4, i) = 10\\nf(C5, i) = 22\\nf(C10, i) = 40'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 426, 'page_label': '427'}, page_content='400 CHAPTER 14 Genetic Algorithms\\nhence, the fitness of the schemaS0 is defined as the average of these three values:\\nf(S0, i) = (10 + 22 + 40) / 3\\n= 24\\nWe will now investigate which factors affect the likelihood of a particular\\nschema surviving from one generation to the next. In other words, what\\nprobability is there that a given schema that is present in the parent gener-\\nation will be in the subsequent generation?\\nFirst, let us consider the process whereby chromosomes reproduce, without\\nintroducing crossover or mutation.\\nFirst, let us assume that there is a chromosome that matches a schema,S,i n\\nthe population at time i.\\nThe number of occurrences of S in the population at time i is\\nm(S, i)\\nand the number of occurrences of S in the population in the subsequent\\ngeneration is:\\nm(S, i+1)\\nThe fitness of S in generation i is\\nf(S, i)\\nNow we will calculate the probability that a given chromosome, c, which\\nmatches the schema S at time i, will reproduce and thus its genes will be\\npresent in the population at time i + 1. The probability that a chromosome\\nwill reproduce is proportional to its fitness, so the expected number of off-\\nspring of chromosome c is\\nwhere a(i) is the average fitness of the chromosomes in the population\\nat time i.\\nBecause chromosome c is an instance of schema S, we can thus deduce\\n(1)\\nwhere c\\n1 to cn are the chromosomes in the population at time i that match\\nthe schema S.\\nmS ,  i + 1 fc  i fc  i\\nai\\n1n( ) = ( ) ++ ( )\\n( )\\n,, K\\nm(c,  i +1) fc ,  i\\nai= ( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 427, 'page_label': '428'}, page_content='14.9 Why Genetic Algorithms Work 401\\nLet us compare this with the definition of the fitness of schema S, f (S, i),\\nwhich is defined as follows:\\n(2)\\nBy combining formula 1 with formula 2 above, we obtain:\\nThis tells us that the more fit a schema is compared with the average fitness\\nof the current population, the more likely it is that that schema will appear\\nin a subsequent population of chromosomes. A schema whose fitness is the\\nsame as the average fitness of the population will likely maintain the same\\nnumber of occurrences from one generation to the next. In contrast, there\\nwill be fewer occurrences of a given schema whose fitness is lower than the\\naverage fitness of the population and more occurrences of a given schema\\nwhose fitness is higher than the average.\\n14.9.3 How Mutation and Crossover Affect Schemata\\nThe above calculations have not taken into account mutation or crossover.\\nBoth mutation and crossover can destroy the presence of a schema. In\\nother words, mutation and crossover are both capable of reducing the\\nnumber of occurrences of a particular schema in a population of chromo-\\nsomes. They are also capable of increasing the number of occurrences of a\\ngiven schema.\\nA given schema can be said to have survived crossover, if the crossover\\noperation produces a new chromosome that matches the schema from a\\nparent that also matches the schema.\\nIf the crossover point is chosen so that it is within the defining length of a\\nschema, S, then that schema will be destroyed by the crossover operation.\\nFor a schema to survive crossover, the crossover point must be outside the\\ndefining length. Hence, the probability that a schema S of defining length\\nd\\nL(S) and of length L(S) will survive crossover is\\npS dS\\nLSs\\nL( ) =− ( )\\n( ) −1 1\\nm(S,  i +1) f S,  i m S,  i\\nai= ( ) ⋅ ( )\\n( )\\nfS ,  i fc  i fc  i\\nmS ,  i\\n1n( ) = ( ) ++ ( )\\n( )\\n,, K'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 428, 'page_label': '429'}, page_content='402 CHAPTER 14 Genetic Algorithms\\nHence, a shorter schema is more likely to survive from one generation to\\nthe next than a longer schema. In practical terms, this means that a feature\\nof the chromosomes that is expressed by relatively few bits is more likely to\\nbe passed on from a parent to its offspring than a feature that is expressed\\nby a large number of bits.\\nThe above formula assumes that crossover is applied to each pair of parents\\nthat reproduce. In fact, it is usually the case that some chromosomes are able\\nto reproduce asexually (by cloning). Hence, if the crossover operator is applied\\nwith probabilityp\\nc , then the above formula can be modified as follows:\\nHence, the less likely crossover is, the more likely any given schema is to\\nsurvive from one generation to the next.\\nIn fact, even if the crossover point is chosen within the defining length, it is\\nstill possible for a schemata to survive crossover, as in the following example.\\nLet us apply crossover to the following two chromosomes:\\n10111101\\n01001110\\nThe schema **0011** is matched by the second of these chromosomes. If a\\ncrossover point is chosen between the fourth and fifth bits, then the off-\\nspring will be\\n10111110\\n01001101\\nIn this generation, the second chromosome also matches the schema\\n**0011**, despite the fact that the crossover point was chosen within the\\ndefining length of the schema. Hence, we can modify our formula to allow\\nfor this (fairly unlikely) occurrence:\\nNow let us consider the effect of mutation on schemata. The probability\\nthat mutation will be applied to any given bit in a chromosome is p\\nm.\\nHence, a schema will survive mutation if mutation is not applied to any of\\npS p dS\\nLSsc\\nL( ) ≥− ⋅ ( )\\n( ) −1 1\\npS p dS\\nLSsc\\nL( ) =− ⋅ ( )\\n( ) −1 1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 429, 'page_label': '430'}, page_content='14.9 Why Genetic Algorithms Work 403\\nthe defined bits of the schema. Because a schema S has O(S) defined bits,\\nthe probability of survival can be defined as\\nps(S) = (1 /H11002pm)O(S)\\nHence, a schema is more likely to survive mutation if it has a lower order.\\nWe can combine the three equations we have to give one equation that\\ndefines the likelihood of a schema surviving reproduction using crossover\\nand mutation. This formula defines the expected number of chromosomes\\nthat match a schema, S, in a generation at time i + 1:\\nThis rather daunting formula represents the schema theorem, developed\\nby Holland, which can be stated as “Short, low order schemata which are\\nfitter than the average fitness of the population will appear with exponen-\\ntially increasing regularity in subsequent generations” .\\nThe above analysis provides a way to understand the behavior of genetic\\nalgorithms and goes some way toward explaining why they work, but it\\ndoes not really provide a full answer to that question.\\nAlthough genetic algorithms have been widely studied, and there is\\ngood empirical evidence that they work, there is yet no theoretical proof\\nthat use of crossover provides better solutions than other local search\\ntechniques.\\n14.9.4 The Building-Block Hypothesis\\nThe building-block hypothesis is a consequence of the schema theorem,\\nwhich can be stated as “Genetic algorithms manipulate short, low-order,\\nhigh fitness schemata in order to find optimal solutions to problems.” These\\nshort, low-order, high-fitness schemata are known as building blocks.\\nIn other words, genetic algorithms work well when a small group of genes\\nthat are close together represent a feature that contributes to the fitness of a\\nchromosome. Hence, the representation that is chosen for genetic algo-\\nrithms is very important. Randomly selecting bits to represent particular\\nfeatures of a solution is not good enough. Bits should be chosen in such a\\nmS ,  i + 1 f S,  i m S,  i\\nai p dS\\nLS pc\\nL\\nm\\nOS\\n( ) ≥ ( ) ⋅ ( )\\n( )\\n⋅− ⋅ ( )\\n( ) − ⋅−( )\\uf8eb\\uf8ed \\uf8f6\\uf8f8\\n\\uf8eb\\n\\uf8ed\\uf8ec\\n\\uf8f6\\n\\uf8f8\\uf8f7\\n()1 1 1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 430, 'page_label': '431'}, page_content='404 CHAPTER 14 Genetic Algorithms\\nway that they group naturally together into building blocks, which genetic\\nalgorithms are designed to manipulate.\\n14.9.5 Deception\\nOne problem faced by genetic algorithms is known as deception.L e t  u s\\nassume a population of chromosomes of 8 bits. We will consider four\\nschemata and their fitness levels:\\nS\\n1 = 11****** f(S 1) = 50\\nS2 = ******11 f(S 2) = 40\\nS3 = 11****11 f(S 3) = 5\\nS4 = 00****00 f(S 4) = 65\\nNote that S1 and S2 are two building blocks, which combine together to give\\nS3, but that S3 is much less fit than S1 or S2.\\nLet us now assume that the optimal solution in this problem is\\nS5 = 11111111 f(S 5) = 100\\nThe genetic algorithm will find it hard to reach this optimal solution\\nbecause it will prefer to match the most fit building blocks with chromo-\\nsomes such as\\n00111100\\nHence, genetic algorithms can be misled or deceived by some building\\nblocks into heading toward suboptimal solutions.\\nOne way to minimize the effects of deception is to use inversion, which is a\\nunary operator that reverses the order of a subset of the bits within a chro-\\nmosome. For example, inversion applied to the following chromosome:\\n1010011100\\nbetween the fourth and eighth bits would produce the following chromosome:\\n1011110000\\nLike mutation, inversion is applied with a low probability (such as one in a\\nthousand) and can help to avoid converging on incorrect solutions.\\nAnother way to avoid deception is to use messy genetic algorithms, which\\nare described in the next section.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 431, 'page_label': '432'}, page_content='14.10 Messy Genetic Algorithms 405\\n14.10 Messy Genetic Algorithms\\nMessy genetic algorithms were developed as an alternative to standard\\ngenetic algorithms.\\nWith messy genetic algorithms (mGAs), each bit is labeled with its position.\\nA chromosome does not have to contain a value for each position, and, in\\nfact, a given position in a chromosome can have more than one value.\\nEach bit in a chromosome is represented by a pair of numbers: the first\\nnumber represents the position within the chromosome, and the second\\nnumber is the bit value (0 or 1).\\nHence, the following chromosome:\\n((1,0), (2,1), (4,0))\\ncould be a chromosome with four bit positions, where the third bit posi-\\ntion is not specified. The following chromosome, in contrast, has two val-\\nues specified for the third position:\\n((1,0), (2,1), (3,1), (3,0), (4,0))\\nGoldberg (1989) specifies methods for dealing with chromosomes that are\\nunderspecified (i.e., where a bit position is not defined) or that are over-\\nspecified (where a bit position is defined twice).\\nUnderspecified bits are filled in by copying bits from a template chromo-\\nsome that is usually chosen as the best-performing chromosome from the\\nprevious generation.\\nA method is needed to deal with overspecified chromosomes: the most\\nusual method is simply to work on a left-to-right basis and use the first\\nvalue that is assigned to a given bit position. Hence, for example, the fol-\\nlowing chromosome:\\n((1, 0), (3, 0), (2, 1), (1,1))\\nwould be modified to\\n((1, 0), (3, 0), (2, 1))\\nBecause bit 1 is overspecified. The first occurrence, working from left to\\nright, is used, and any other occurrences are discarded.\\nmGAs use the mutation operator as with standard genetic algorithms.\\nInstead of crossover, mGAs use the splice and cut operators.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 432, 'page_label': '433'}, page_content='406 CHAPTER 14 Genetic Algorithms\\nTwo chromosomes can be spliced together by simply joining one to the end\\nof the other. Hence the following two chromosomes:\\n((1,0), (3,0), (4,1), (6,1))\\n((2,1), (3,1), (5,0), (7,0), (8,0))\\ncan be spliced to produce the following chromosome:\\n((1,0), (3,0), (4,1), (6,1), (2,1), (3,1), (5,0), (7,0), (8,0))\\nNote that the genes do not need to be in any particular order because each\\none has its position specified as part of its representation.\\nThe cut operator splits one chromosome into two smaller chromosomes.\\nHence, the result of the above splice operation could be cut to produce the\\nfollowing two chromosomes:\\n((1,0), (3,0), (4,1))\\n((6,1), (2,1), (3,1), (5,0), (7,0), (8,0))\\nMGAs are more immune to deception than standard genetic algorithms\\nand have been shown to converge on optimal solutions with extremely\\ndeceptive functions (Goldberg 1989).\\n14.11 Prisoner’ s Dilemma\\nWe will now see how genetic algorithms can be used to evolve strategies for\\nplaying a simple game: the Prisoner’s Dilemma.\\nThe background of the game is as follows:\\nTwo prisoners have been arrested on suspicion of committing a crime.\\nThey are kept in separate cells, and each is told that if he betrays his friend\\nhe will receive a reward. If his friend does not betray him, then he will go\\nfree, and receive a reward, while his friend is tortured. If both betray each\\nother, they will both be tortured, and if neither betrays the other, they will\\nbe set free.\\nThe dilemma for each prisoner is whether to defect and betray his friend,\\nor whether to cooperate with his friend and keep silent. Defection always\\nbrings a greater reward than cooperation, but the best overall result is\\nobtained if both prisoners cooperate.\\nThe game of Prisoner’s Dilemma is played over a number of turns, where\\non each turn each player can choose whether to defect or to cooperate, and\\npoints are awarded to each player according to the outcome, as defined in\\nTable 14.3.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 433, 'page_label': '434'}, page_content='14.11 Prisoner’ s Dilemma 407\\nTable 14.3 Point allocation in Prisoner’ s Dilemma\\nPlayer 1 Player 2 S1 S2 Notes\\ndefects defects 1 1 Penalty for mutual defection\\ndefects cooperates 5 0 Player 1 has been tempted to defect.\\nPlayer 2 is the “sucker.”\\ncooperates defects 0 5 Player 1 is the “sucker.” Player 2 has been\\ntempted to defect.\\ncooperates cooperates 3 3 Reward for mutual cooperation\\nIn Table 14.3, S1 and S2 are the number of points received by player 1 and\\nplayer 2, respectively, in each given situation.\\n14.11.1 Strategy Representation\\nWe will choose a representation that represents the strategy of a given\\n“player” or chromosome in the population. For our system, we will allow\\neach player to determine its move on a given turn in the game based on the\\nresults of the previous three turns.\\nEach turn in the game can have one of four outcomes, as shown in Table\\n14.3. We shall represent each of these outcomes by a number from 0 to 3:\\n0: reward (both players have cooperated)\\n1: sucker (the player cooperates, and the opponent defects)\\n2: penalty (both players defected)\\n3: temptation (the player defects and the opponent cooperates)\\nNow, by using 0 to represent defection and 1 to represent cooperation, a\\nthree-dimensional array of binary values can be used to represent a strat-\\negy. Because there are four possible choices for each turn, and our strategies\\nwill be based on the previous three turns, our array will need to be 4 /H110034 /H11003\\n4 = 64 bits. We will also include three bits that represent the player’s behav-\\nior on the first three turns, so each chromosome will be represented by 67\\nbits. (We will place the three bits that represent the first three turns at the\\nend of the chromosome.)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 434, 'page_label': '435'}, page_content='408 CHAPTER 14 Genetic Algorithms\\nHence, a chromosome that consists of 67 1s would cooperate on every go,\\nregardless of the behavior of its opponent. Similarly, 67 0s would mean that\\nthe chromosome defected on every turn.\\nHence, the following chromosome:\\n10000000000000000000000000000000000000000000000000000000000000000111\\nrepresents the following rather simple strategy:\\nCooperate on the first three turns, and thereafter, only cooperate in the\\nevent that both players have cooperated on the previous three turns. (The\\nfirst bit represents the [0,0,0] position in the array, which corresponds to\\nthree consecutive occurrences of “reward. ”)\\nThe following chromosome represents a slightly more sophisticated strategy:\\n1001000000001001000000000000000000000000000000001001000000001001111\\nThe last three chromosomes represent the first three decisions: this chromo-\\nsome will cooperate on the first three turns. Thereafter, this chromosome\\nwill cooperate only if the opponent has cooperated on the previous three\\nturns. If the opponent cooperates, then the possible results are either reward\\nor temptation. Hence, there are eight possible combinations of three moves\\nin which the opponent has cooperated. These are represented by the eight\\n1’s in the chromosome above at positions 0, 3, 12, 15, 48, 51, 60, and 63.\\nBit position 0 represents the decision when the previous three outcomes\\nhave all been value 0 - reward (i.e., both players have cooperated on the\\nprevious three turns). Position three represents (0, 0, 3) or (reward, reward,\\ntemptation)—in other words, the opponent has cooperated on each of the\\nthree turns, and the player cooperated on the first two turns and defected\\non the third turn.\\nEach chromosome represents a complete strategy for how to play Prisoner’s\\nDilemma over an arbitrary number of turns, basing each decision on the\\noutcome of the previous three turns, and with the first three decisions\\nhard-coded. Clearly, there are an astronomical number of possible chro-\\nmosomes (2\\n67 /H3336115 /H110031019) and therefore a correspondingly large number\\nof possible strategies.\\n14.11.2 Possible Strategies\\nThe simplest strategies for playing Prisoner’s Dilemma are “always defect”\\nand “always cooperate, ” whose chromosomes consist of 67 0s and 67 1s,\\nrespectively.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 435, 'page_label': '436'}, page_content='14.11 Prisoner’ s Dilemma 409\\nEach of these strategies is a reasonable way to play Prisoner’s Dilemma.\\n“Always defect” ensures that the opponent’s score is minimized, whereas\\n“always cooperate” ensures a maximum possible combined score, in the\\nevent that the opponent also always cooperates. Of course, “always cooper-\\nate” does not do well against “always defect. ” In fact, no strategy does well\\nagainst “always defect, ” being able to achieve a maximum score of only 1\\nout of a possible 5 for each turn. Conversely, the player using the “always\\ndefect” strategy can achieve the full 5 points if its opponent cooperates but\\ngets just 1 point if the opponent also defects.\\nOne very successful strategy is called tit-for-tat. Tit-for-tat involves coop-\\nerating to begin with and thereafter doing whatever the opponent did on\\nthe previous turn.\\nThis strategy only uses information about the previous turn, so much of\\nthe data of the chromosome that plays tit-for-tat is redundant. Our\\nhope is that chromosomes whose strategies are based on the previous\\nthree turns, rather than just one turn, can perform better overall than\\ntit-for-tat.\\nWe will assume that a game consists of 100 turns of Prisoner’s Dilemma,\\nand that the total score for a chromosome for a game is the sum of the\\npoints awarded to it in those 100 turns.\\nHence, playing tit-for-tat against tit-for-tat or against “always cooperate”\\nwould achieve a total of 300 points. Playing “always defect” can achieve a\\ntotal of 500 points when playing against “always cooperate. ” Table 14.4\\nshows the six possible total scores for these three strategies played against\\neach other over 100 turns.\\nTable 14.4 Total scores for three strategies played over 100 turns\\nPlayer1 Player2 S1 S2\\nAlways cooperate Always cooperate 300 300\\nAlways cooperate Always defect 0 500\\nAlways cooperate Tit-for-tat 300 300\\nAlways defect Always defect 100 100\\nAlways defect Tit-for-tat 104 99\\nTit-for-tat Tit-for-tat 300 300'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 436, 'page_label': '437'}, page_content='410 CHAPTER 14 Genetic Algorithms\\nClearly, the scores for a game can vary between 0 and 500, depending on\\nthe strategy chosen and the strategy of the opponent.\\n14.11.3 Evolution of Strategies\\nThe process for running this genetic algorithm is as follows:\\n1. Produce a random population of chromosomes. We will start with\\na population of just 100 chromosomes.\\n2. Determine a score for each chromosome by playing its strategy\\nagainst a number of opponents.\\n3. Select a number of chromosomes from the population to repro-\\nduce, applying crossover and mutation according to appropriate\\nprobabilities.\\n4. Replace the previous generation with the new population pro-\\nduced by reproduction.\\n5. Return to step 2.\\nA method must be applied to determine for how many generations to run\\nthe genetic algorithm. We will use 100 runs of the genetic algorithm. A ter-\\nmination condition could be applied that determined when an optimal\\nsolution has been reached, but as we will see, it is not necessarily clear how\\nto identify such a strategy.\\n14.11.4 Choice of Opponents\\nAs a simple example, we will start by considering playing the chromosomes\\nagainst a fixed strategy.\\nFirst, we will determine each chromosome’s fitness by playing its strategy\\nagainst “always defect” over 100 turns.\\nClearly, the best strategy against “always defect” is also “always defect. ” In\\nour experiments, a population of 100 chromosomes evolved such that the\\naverage score of the 100 chromosomes reached the maximum of 100 after\\njust two generations, as shown in Figure 14.4.\\nSimilar results were found when playing the chromosomes against “always\\ncooperate”: the best strategy here is to play “always defect. ” After just a few\\ngenerations, the average score of the population converged on the maxi-\\nmum of 500 points.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 437, 'page_label': '438'}, page_content='14.12 Diversity 411\\n95\\n85\\n75\\n65\\n55\\n45\\n1 5 9 1 31 72 12 52 93 33 74 14 54 9\\nGeneration\\nAverage Score\\naverage score over 50 generations\\nagainst \"always defect\"\\nFigure 14.4\\nAverage scores of a \\npopulation of 100 \\nchromosomes playing\\nagainst “always defect”\\nover 50 generations\\nWhen playing against tit-for-tat, the genetic algorithms converged after just\\na few generations to play “always cooperate, ” which is the best strategy to\\nplay against tit-for-tat. In fact, the best strategy to play against tit-for-tat is\\nto cooperate on every turn except the last turn, but our representation does\\nnot allow this strategy.\\nMore interesting results were obtained when the chromosomes were able to\\nplay against each other, rather than against fixed strategies. This genetic\\nalgorithm was found to evolve strategies that were as successful as the best\\nheuristic methods developed by humans (Axelrod 1987).\\nPlaying the chromosomes against each other is similar to the idea of intro-\\nducing predators into a population, which is discussed in Section 14.14.\\n14.12 Diversity\\nOne problem with the genetic algorithm we have described above for play-\\ning Prisoner’s Dilemma is that the populations tend to stagnate. That is,\\nonce a chromosome evolves that achieves a very high score, chromosomes\\nthat are different and score less well than this one will tend to die out, and\\nthe population will end up with all chromosomes playing the same strat-\\negy. In other words, the population lacks diversity.\\nDiversity is a useful measure that is often used in genetic algorithms to\\navoid stagnation. Like mutation, it also helps to avoid local maxima.\\nHence, it is often a good idea to incorporate a measure of diversity into a\\ngenetic algorithm’s fitness metric.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 438, 'page_label': '439'}, page_content='412 CHAPTER 14 Genetic Algorithms\\nFor example, a diversity score of a chromosome could be calculated by\\ncomparing that chromosome with the highest scoring chromosome and\\ncounting the number of bits that differ. This count could then be added to\\nthe fitness score (Winston 1992).\\n14.13 Evolving Pictures\\nIn many genetic algorithm systems, the fitness of an individual chromo-\\nsome is determined by treating the data that is contained in its genes as a\\nrepresentation of a possible solution to the problem or, in some cases, a\\nstrategy for dealing with particular situations.\\nIn the case of Dawkins’ biomorphs, the fitness was determined by user\\nchoice. In this case, the genes contained in each chromosome were not\\ninterpreted as a strategy or a solution to a problem, but as a visual repre-\\nsentation of an image that resembled a living creature—a biomorph.\\nSimilar work was done by T odd and Latham (1992), who used complex\\nchromosomes and human determination of fitness to evolve extremely\\ncomplex, rendered creatures. Their book includes pictures of surprisingly\\nbeautiful creatures that appear to have been designed by an extremely\\nimaginative artist.\\nIn biological terms, the chromosome is the genotype of the creature, and\\nthe physical (or visual) manifestation of the genotype is the phenotype.\\nUsing this model, in most genetic algorithm systems, the phenotype is a\\nkind of behavior, or a solution to a problem. It is possible to have the phe-\\nnotype of a chromosome be interpreted in more than one way. For exam-\\nple, a genetic algorithm could be used to evolve strategies for playing a\\ngame such as Prisoner’s Dilemma and where each chromosome is also\\ninterpreted as a visual representation of a creature. In this way, creatures\\nwould be evolved whose appearance was dependent in some nontranspar-\\nent way on their behavior. By adding human intervention in determining\\nfitness, a system can be developed that automatically evolves creatures, but\\nwhere humans can override a creature’s fitness in order to bias evolution\\ntoward particular kinds of images.\\nUsing evolutionary techniques for drawing pictures has other applications.\\nIn principle, a person who has an idea of what he or she wants to draw can\\nproduce a picture using crossover and mutation without having any artistic\\nability. For example, police are able to use this technique to allow a witness'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 439, 'page_label': '440'}, page_content='14.14 Predators and Co-evolution 413\\nto produce a picture of a suspect in a crime, by repeatedly selecting the face\\nthat looks most like the person they are thinking of.\\nEvolutionary techniques can also be used to evolve behaviors. Karl Sims\\nevolved creatures whose genes were used to represent not only a physical\\nbody but a set of rules to determine how those body parts would behave.\\nHis aim was to evolve creatures that could perform simple tasks such as\\nwalking, swimming, and jumping.\\n14.14 Predators and Co-evolution\\nIn Section 14.11 we see that when genetic algorithms were evolved to play\\nPrisoner’s Dilemma against each other, they developed much more com-\\nplex strategies than they did when they were evolved to play against fixed\\nopponents.\\nThere is strong empirical evidence to suggest that one of the key driving\\nfactors behind the evolution of most complex behaviors and abilities in\\nreal-world organisms is the existence of predators. In a world without\\npredators, there is less pressure to perform well and less need to develop\\nsophisticated behaviors in order to survive. This principle also applies in\\nartificial evolution.\\nIn the 1980s, Danny Hillis used evolutionary methods to evolve algorithms\\nfor sorting a sequence of numbers. He called the entities he was evolving\\n“ramps. ” He found that when he introduced “parasites, ” which generated\\nincreasingly complex sets of numbers to sort, his ramps were able to per-\\nform far better than they had without the parasites.\\nWhen the evolution of two or more species are intertwined in this way, the\\nprocess is known as co-evolution. This phenomenon was first observed by\\nCharles Darwin (1859). Coevolution can be thought of as an arms race:\\nwhen one country develops the bow and arrow, its enemies need to develop\\na similarly powerful weapon or an appropriate defense. In doing so, this\\ncountry might stumble upon explosives and thus force their enemies to\\ndevelop suitable defenses to this. In much the same way that the develop-\\nment of one country’s military might causes other countries to develop\\nsimilar capabilities, coevolution in animals means that if one predator\\ndevelops the ability to run faster, its prey must match its speed, or develop\\nanother defense, or it will be wiped out.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 440, 'page_label': '441'}, page_content='414 CHAPTER 14 Genetic Algorithms\\nOf course, this process does not happen quickly: at first, the prey will do\\nbadly, and a great many of them will be wiped out. This fact will ensure that\\nthe stronger, faster, better-defended individuals will be more likely to sur-\\nvive and produce offspring. In this way, over a period of many generations,\\nthe species will gradually become better able to survive.\\nAs a result, of course, the predator will need to become faster or develop\\nnew abilities that enable it to catch enough prey. In some cases the cycle is\\nbroken as one species dies out or a new species supersedes it.\\nCo-evolution is a vital element of biological evolutionary processes and can\\nalso be taken advantage of in genetic algorithms and other processes that\\ninvolve artificial evolution.\\n14.15 Other Problems\\nGenetic algorithms have been successfully applied to a number of problems\\nin computer science. Most combinatorial search problems (described in\\nmore detail in Chapter 5) can be successfully solved using genetic algo-\\nrithms. A great deal of work has been done on applying genetic algorithms\\nto the traveling salesman problem, for example, but also to a number of\\nother problems, including the following:\\n■ The Knight’s T our (moving a knight over a chess board using valid\\nmoves, such that it lands on each square exactly once)\\n■ The CNF-satisfiability problem\\n■ Robot navigation\\n■ The knapsack problem\\n■ The timetable problem (assigning teachers to pupils and classrooms)\\n14.16 Chapter Summary\\n■ A genetic algorithm consists of a population of chromosomes,\\neach of which contains a number of genes. The genetic algorithm\\nmanipulates these chromosomes using crossover and mutation to\\nproduce a new, superior generation of chromosomes. This opera-\\ntion is repeated, until an optimal solution is obtained.\\n■ A fitness metric is essential if a problem is to be solved using\\ngenetic algorithms.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 441, 'page_label': '442'}, page_content='14.17 Review Questions 415\\n■ Mathematical functions can be readily solved using genetic algorithms.\\n■ Genetic algorithms manipulate schemata. A schema consists of a\\nset of 1s, 0s, and *, where a * represents “don’t care” .\\n■ The schema theory states: Short, low order schemata which are fitter\\nthan the average fitness of the population will appear with exponen-\\ntially increasing regularity in subsequent generations.\\n■ The building-block hypothesis states that genetic algorithms solve\\nproblems using discrete building blocks.\\n■ Genetic algorithms are susceptible to deception—a problem\\nwhereby inadequate building blocks appear in highly fit entities.\\n■ Genetic algorithms can be used to evolve strategies for playing the\\nPrisoner’s Dilemma.\\n■ When playing against fixed strategies, the genetic algorithms\\nquickly converge on the optimal strategy. When playing against\\neach other, more complex strategies emerge.\\n■ Diversity can be as important as fitness in evaluating chromo-\\nsomes for genetic algorithms.\\n■ Pictures can be evolved using strategies similar to genetic algo-\\nrithms, but with a degree of human intervention.\\n■ Coevolution is the process whereby the development of one\\nspecies affects the evolutionary path taken by another species.\\nCoevolution has been used successfully to improve the perform-\\nance of systems developed using artificial evolution.\\n14.17 Review Questions\\n13.1 Explain the meaning of the following terms in the context of\\ngenetic algorithms:\\n■ fitness\\n■ chromosome\\n■ gene\\n■ population\\n■ generation\\n■ crossover'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 442, 'page_label': '443'}, page_content='416 CHAPTER 14 Genetic Algorithms\\n■ mutation\\n■ deception\\n13.2 Explain how crossover, mutation, and reproduction affect schemata.\\n13.3 Explain how schemata help us to understand why genetic algo-\\nrithms work. What does the schema theorem tell us?\\n13.4 Explain why genetic algorithms are susceptible to deception. Why\\ndo messy genetic algorithms help avoid this problem?\\n13.5 Explain why diversity is important when using genetic algorithms\\nto solve problems.\\n13.6 Explain why introducing predators can help in systems that use\\nartificial evolutionary techniques.\\n13.7 Describe three problems that might be solved using genetic algo-\\nrithms that were not described in this chapter.\\n13.8 Could genetic algorithms be used to play complex games such as\\nchess and checkers? Explain your answer.\\n13.9 “Once you’ve chosen a good representation for your problem you\\nmight as well solve the problem using traditional means—genetic\\nalgorithms are a waste of effort” . Discuss.\\n14.18 Exercises\\n13.1 Using pen and paper and a die as a random number generator,\\nwork through five generations of evolution starting from the fol-\\nlowing five chromosomes. Y ou will need to select a mutation rate,\\nand determine a strategy for crossover.\\n1100110011\\n0001111010\\n1010100001\\n0000101000\\n0111000101\\n13.2 Write a program in the programming language of your choice that\\nuses a genetic algorithm to evolve strategies for playing Prisoner’s\\nDilemma. Start out by having your chromosomes play against a'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 443, 'page_label': '444'}, page_content='14.19 Further Reading 417\\nfixed strategy, and observe the behavior of the system. Now have\\nthe chromosomes play against each other. How does this affect\\ntheir performance?\\n14.19 Further Reading\\nFor a detailed description of messy genetic algorithms, see Goldberg\\n(1989). The original text on genetic algorithms is Holland (1992). More\\nintroductory coverage is available in the standard Artificial Intelligence\\ntexts. For a discussion of the connection between computer viruses and\\nArtificial Life, see Spafford (1989). The best modern discussions of coevo-\\nlution can be found in Kelly (1994) and Dawkins (1996).\\nWork carried out by Forrest and Mitchell (1992) on Royal Road functions\\nhas shown that in fact the schema theory does not model the real behavior\\nof genetic algorithms as accurately as one would hope.\\nGenetic Algorithm for the Prisoner Dilemma Problem , by R. Axelrod (1987 -\\nin Genetic Algorithms and Simulated Annealing, edited by L. Davis – Hyper-\\nion Books)\\nEfficient and Accurate Parallel Genetic Algorithms, by Erick Cantu-Paz (2002\\n– Kluwer Academic Publishers)\\nPractical Handbook of Genetic Algorithms , by Lance Chambers (1995 –\\nCRC Press)\\nGenetic Algorithms and Genetic Programming in Computational Finance ,\\nedited by Shu-Heng Chen (2002 – Kluwer Academic Publishers)\\nAn Introduction to Genetic Algorithms for Scientists and Engineers, by David\\nA. Coley (1999 – World Scientific Publishing Company)\\nThe Origin of Species, by Charles Darwin (1859 – reprinted by Penguin)\\nAdaptive Learning by Genetic Algorithms: Analytical Results and Applications\\nto Economic Models, by Herbert Dawid (1999 – Springer V erlag)\\nThe Blind Watchmaker, by Richard Dawkins (1996 – W. W. Norton & Com-\\npany)\\nGenetic Algorithms and Engineering Optimization, by Mitsuo Gen and Run-\\nwei Cheng (1991 – Wiley Interscience)\\nGenetic Algorithms in Search, Optimization and Machine Learning, by David\\nE. Goldberg (1989 – Addison Wesley)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 444, 'page_label': '445'}, page_content='418 CHAPTER 14 Genetic Algorithms\\nMessy Genetic Algorithms: Motivation, Analysis and First Results by D.E.\\nGoldberg (1989 - in Complex Systems, Vol. 3, pp. 493–530)\\nRapid, Accurate Optimization of Difficult Problems Using Fast Messy Genetic\\nAlgorithms, by David E. Goldberg, Kalyanmoy Deb, Hillol Kargupta, and\\nGeorges Harik (1993 – in Proceedings of the Fifth International Conference\\non Genetic Algorithms, pp. 56–64)\\nPractical Genetic Algorithms, by Randy L. Haupt and Sue Ellen Haupt (1998\\n– Wiley Interscience)\\nAdaptation in Natural and Artificial Systems: An Introductory Analysis with\\nApplications to Biology, Control, and Artificial Intelligence , by John H. Hol-\\nland (1992 – MIT Press)\\nGenetic Algorithms + Data Structures = Evolution Programs , by Zbigniew\\nMichalewicz (1999 – Springer)\\nAn Introduction to Genetic Algorithms, by Melanie Mitchell (1998 – MIT Press)\\nThe Royal Road for Genetic Algorithms: Fitness Landscapes and GA Perfor-\\nmance, by Melanie Mitchell, Stephanie Forrest, and John H. Holland (1992\\n- In Towards a Practice of Autonomous Systems: Proceedings of the First Euro-\\npean Conference on Artificial Life , edited by Francisco J. Varela and Paul\\nBourgine, pp. 245–254, MIT Press)\\nPrisoner’s Dilemma: John Von Neumann, Game Theory and the Puzzle of the\\nBomb, by William Poundstone (1994 – MIT Press)\\nRepresentations for Genetic and Evolutionary Algorithms, by Franz Rothlauf\\nand David E. Goldberg (2002 – Springer V erlag)\\nArtificial Evolution for Computer Graphicsby Karl Sims (1991 –Siggraph ’91\\n- Annual Conference Proceedings, 1991, pp. 319–328).\\nEvolving Virtual Creatures, by Karl Sims (1994 - Siggraph ’94 - Annual Con-\\nference Proceedings, 1994, pp. 43–50)\\nComputer Viruses as Artificial Life, by Eugene Spafford (1989 – inArtificial Life,\\nAn Overview, edited by Christopher G. Langton, 1995, MIT Press, pp. 249–265)\\nThe Simple Genetic Algorithm: Foundations and Theory, by Michael D. Vose\\n(1999 – MIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 445, 'page_label': '446'}, page_content='Planning\\n5\\nIntroduction To Part 5\\nPart 5 is divided into two chapters.\\nIntroduction to Planning\\nThis chapter introduces the ideas behind planning. It starts\\nby explaining the relationship between search and plan-\\nning, and explains why in many real-world problems, plan-\\nning is preferable to search. It also explains situation\\ncalculus, which is an extension to first-order predicate cal-\\nculus used in planning systems, to represent the way the\\nworld changes over time.\\nThis chapter explains the frame problem and introduces\\nways to overcome the problem. It also explains means–ends\\nanalysis, which is covered in more detail in Chapter 16.\\nPlanning Methods\\nChapter 16 expands on the ideas introduced in Chapter 15.\\nIt presents a number of representations and methods that\\nare used in planning, starting with STRIPS. It presents a\\nnumber of examples to illustrate how STRIPS is able to\\nsolve planning problems and also discusses the kinds of\\nproblems that might be difficult to solve using STRIPS. A\\nnumber of other representations such as planning graphs\\nand ADL are also explored, as well as some more advanced\\nplanning methods such as probabilistic planning and\\ndynamic world planning.\\nPART\\n15\\nCHAPTER\\n16\\nCHAPTER'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 446, 'page_label': '447'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 447, 'page_label': '448'}, page_content='15CHAPTER\\nIntroduction to Planning\\nThe best laid schemes o’ mice an’ men gang aft a-gley.\\n—Robert Burns, To a Mouse\\nYou can never plan the future by the past.\\n—Edmunde Burke, Letter to a Member of the National Assembly\\nGrow old along with me!\\nThe best is yet to be,\\nThe last of life, for which the first was made:\\nOur times are in His hand\\nWho saith ‘A whole I planned,\\nYouth shows but half; trust God: see all nor be afraid!’\\n—Robert Browning, Rabbi Ben Ezra\\nIn preparing for battle I have always found that plans are useless, but that\\nplanning is indispensable.\\n—Dwight D. Eisenhower\\n15.1 Introduction\\nPlanning has recently become a very exciting area of research in Artificial\\nIntelligence. Much of the work of Artificial Intelligence is concerned with\\nproblem solving. As we will see, planning is no exception. Planning is a very\\nsimple concept: a planner starts in an initial state and has a particular goal'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 448, 'page_label': '449'}, page_content='422 CHAPTER 15 Introduction to Planning\\nit needs to achieve. T o reach the goal state, the planner develops a plan and\\nthen executes that plan.\\nThe planner might be a robot arm, or a mobile robot, but in many cases it\\nexists purely in software and is designed to plan solutions to virtual problems.\\nA planner has a set of possible actions it can take, and as we will see, these\\nactions are usually limited, depending on the current state of the planner.\\nFor example, a robot cannot open a door if the door is already open, or if it\\nis nowhere near the door, or if its hands are full.\\nThe actions that a planner can take are its atomic actions—these actions\\nare usually described in terms of the effect they have on the planner and on\\nthe world. For example, a planning system might use actions such as “pick\\nup object X” or “close door” or “go to supermarket. ”\\nNote that thus far we have assumed that the planner is the same entity as\\nthe entity that will execute the plan. This is not necessarily the case because\\na planner may be intended to develop plans for another entity. In this book,\\nthough, we tend to continue with the assumption that the system that\\ndevelops the plans is the same system that executes the plans.\\nA plan can usually be considered as a set of subgoals, much like the goal\\ntrees we see in Chapter 3. In this chapter and in Chapter 16, we see some of\\nthe representations that are used to describe the state of a planner and its\\nactions. We also examine the methods used by planners to efficiently design\\nand execute plans.\\nIn this chapter, the ideas behind planning are introduced, starting with\\nusing search as a way of identifying a plan of actions.\\nThis chapter also introduces situation calculus, which is a form of first-\\norder predicate calculus, which enables us to reason about the way in which\\npredicates change over time as the result of actions. Effect axioms and\\nframe axioms are introduced, and we discuss the frame problem and how\\nit can be solved by using successor state axioms.\\nThis chapter also introduces the idea of means–ends analysis, an idea that\\nis extensively illustrated in Chapter 16 in a discussion of STRIPS. T o illus-\\ntrate means–ends analysis, this chapter introduces the General Problem\\nSolver, or GPS.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 449, 'page_label': '450'}, page_content='15.2 Planning as Search 423\\n15.2 Planning as Search\\nOne approach to planning is to use search techniques, as described in Part\\n2 of this book. For example, a robotic planning agent might have a state\\nthat is described by the following variables:\\nRoom the robot is in\\nRoom the cheese is in\\nIs the robot holding the cheese?\\nLet us further suppose that there are just three rooms—room 1, room 2,\\nand room 3—and that these rooms are arranged such that there is a door\\nfrom each room to each other room. The robot starts out in room 1, and\\nthe cheese starts in room 3. The robot’s goal is to find the cheese.\\nThe actions the robot can take are as follows:\\nMove from room 1 to room 2\\nMove from room 1 to room 3\\nMove from room 2 to room 1\\nMove from room 2 to room 3\\nMove from room 3 to room 1\\nMove from room 3 to room 2\\nPick up cheese\\nNote that each of these rules has a set of dependencies: to move from room\\n1 to room 2, the robot must currently be in room 1, in order to pick up the\\ncheese, and the robot and the cheese must be in the same room, and so on.\\nIn this chapter, and in Chapter 16, we examine in more detail how these\\nrules are expressed, but for this example we will assume that the actions can\\nonly be carried out in a way that makes sense in our world.\\nWe will use a three-value vector to represent the current state:\\n(room robot is in, room cheese is in, is robot holding cheese?)\\nSo the initial state can be described as (1, 3, no), and the goal state can be\\ndescribed as ( x, x, yes) where x is a variable that indicates the room in\\nwhich both the cheese and the robot are located.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 450, 'page_label': '451'}, page_content='424 CHAPTER 15 Introduction to Planning\\n2,3,no 3,3,no\\n3,3,yes\\nGo to room 3Go to room 2\\nPick up cheese\\n1,3,no\\nFigure 15.1\\nA highly simplistic search\\ntree used to develop a plan\\nNote that there are 18 possible states that can be described by this vector,\\nbut that the following 6 states are not possible because they involve the\\nrobot holding the cheese, but with the cheese in a different room from the\\nrobot: (1, 2, yes), (1, 3, yes), (2, 1, yes), (2, 3, yes), (3, 1, yes), (3, 2, yes).\\nHence, there are actually only 12 valid states.\\nIn each state where the robot and the cheese are in different rooms, there\\nare two possible actions that can be taken. For example, in the state (1, 2,\\nno), the robot can either move from room 1 to room 2, or can move from\\nroom 1 to room 3.\\nT o develop a plan, the robot can simply produce a search tree, such as the\\none shown in Figure 15.1, which starts with the initial state and shows\\nevery state that can be achieved by applying each action from that state. A\\nsuitable plan is found when the goal state is reached. As with the search\\ntrees we built when examining the cannibals and missionaries problem in\\nChapter 3, we have excluded repeated states.\\nNote that in this simple problem, the search tree is very small, and finding a\\nsuitable plan is thus very easy.\\nIn fact, in most real-world problems, and, indeed, in most Artificial Intelli-\\ngence problems, there are many more possible states, many more actions\\nthat can be taken, and many more variables to consider.\\nFor a robotic agent to be of any use in the real world, it would need to have\\nhundreds or even thousands of possible actions it could take to be able to\\ndeal with the enormous complexities it would surely face. Similarly, for the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 451, 'page_label': '452'}, page_content='15.2 Planning as Search 425\\nrobot to understand the world in sufficient detail, it would need to have a\\nstate representation that consisted of a very large number of variables.\\nGiven these factors, the search tree that would be produced for even a very\\nsimple problem would become prohibitively large.\\nThe main problem with using search for planning is that it does not take\\nany account of the effects of actions when considering which action to take.\\nIf the agent’s goal is to find the cheese that is in the next room, then consid-\\nering paths in the search tree that start with actions such as “phone the doc-\\ntor, ” “look out of the window, ” “switch on the television set, ” and so on\\nwould be wasteful.\\nAdditionally, as we will see, it does not necessarily make sense for a plan to\\nbe built starting from the initial state. As we have seen in Chapters 3 and 4,\\nit often makes sense to approach a problem by starting from the goal state\\nand working back toward the initial state. Search does not necessarily func-\\ntion well in such situations.\\nAnother reason that search is not usually the best way to approach plan-\\nning is that it does not take into account the independence of multiple\\ngoals. When a planner is presented with a goal that consists of several parts,\\nsuch as “buy some eggs, feed the cat, and get little Johnny a haircut, ” it can\\nconsider each of these goals separately and solve each of them consecu-\\ntively, rather than trying to solve them as a whole. Because a search-based\\napproach would consider the three goals as a single problem, it would end\\nup enormously overcomplicating the problem.\\nOf course, in some situations, the order in which subgoals are achieved can\\nbe very important. For example, the planner might have as a goal “feed the\\ncat, and wash the cat’s bowl. ” In this case, the planner might need to con-\\nsider carefully the order in which it carries out its actions if it is to avoid\\nmaking a simple error, such as putting food into the cat’s bowl and then\\nimmediately washing it up, without waiting for the cat to eat.\\nSimilarly, when solving problems such as the 8-puzzle, described in Chap-\\nter 4, each of the subgoals very much depends on the others. When taking\\nactions to slide tile 5 into its square, it is very likely that all the other tiles\\nwill be moved about, and so independently placing each tile in its appro-\\npriate square will not do. Hence, search is a very good way to plan a solu-\\ntion to problems such as the 8-puzzle or the eight-queens problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 452, 'page_label': '453'}, page_content='426 CHAPTER 15 Introduction to Planning\\n15.3 Situation Calculus\\nIn the example used above, we have used a very simple notation to indicate\\nthe current state of the planner. For most real problems this notation\\nwould not make sense because the number of variables to consider in each\\nstate would be prohibitively large.\\nOne notation that is often used in discussing planning is situation calcu-\\nlus. Situation calculus is a form of first-order predicate calculus.\\nAs we see in Chapter 7, first-order predicate calculus allows us to make\\nassertions about objects but does not provide a very good way of expressing\\nchange, or temporal relationships. Situation calculus allows us to describe\\nan object in one state, or situation, and then describe how that object will\\nchange when a given action is taken.\\nFor example, the following situation calculus expression represents the\\nassertion that in situation S\\n1, the robot is in the same room as the cheese:\\nHere the predicateIn is used to indicate which room a given entity is in. The\\npredicate has been augmented with a third variable, which is the situation that\\nis being described. This third variable,S\\n1, is known as asituation variable.\\nOf course, not all objects change over time, and so we can continue to use\\nstandard predicates without a situation variable. For example, we could use\\nthe following expression to assert that the robot’s name is Robbie and that\\nthis will not change over time:\\nName (Robot, Robbie)\\nT o describe the effects that actions have on the world, we use the Result\\nfunction. The Result function takes as arguments an action and a situation\\nand returns the situation that occurs as a result of that action. For example,\\nResult (Move\\ni,j ,S 1) = S2\\nHere we are using the notation Movei,j to indicate the action of moving\\nfrom room i to room j. Hence, if the current situation is S1, and the robot\\ncarries out the action of moving from room 1 to room 2, this will result in\\nsituation S\\n2.\\n∃ ( ) ∧ ( )( )x In Robot x S In cheese x S,, ,, 11'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 453, 'page_label': '454'}, page_content='15.4 The Frame Problem 427\\nA number of rules are needed to describe the effects of actions. These rules\\nare known as effect axioms . For example, we might have the following\\neffect axiom:\\nThis effect axiom states the following rule:\\nIf the robot is in a room, y, and an object x is also in that room, then if the\\nrobot carries out a Ta keaction, this will result in a new situation in which\\nthe robot has object x. We might want to further refine this axiom, by\\nensuring that x is the kind of object that the robot can carry—for example,\\nby stating that it must be light enough and small enough to carry and that\\nit must not be fixed down.\\n15.4 The Frame Problem\\nAs we have seen, when we carry out an action, the world changes. In fact,\\nsome aspects of the world change, but others stay the same. Determining\\nwhich stay the same is known as the frame problem.\\nAn effect axiom states what changes when the robot carries out a particular\\naction in a particular situation. It does not make any statements about what\\ndoes not change. For example, when the robot carries out a Ta keaction, it\\ndoes not find itself in a different room. This kind of rule can be expressed\\nin a frame axiom, such as the following:\\nOf course, most actions that we take do not have any effect on the vast\\nmajority of objects in the real world. This is likely to be true in the world of\\na robot or software planner. As a result, many of frame axioms are needed\\nif we are to describe all of the effects that do not result from carrying out a\\nparticular action. The problem of having enormous numbers of frame\\naxioms is known as the representational frame problem.\\nThe representational frame problem can be solved by using successor state\\naxioms, which effectively combine the effect axioms with the frame\\n∀ ( ) ⇒ ( )( )y s In Robot y s In Robot y Result Take s,, , , , ,\\n∀ ( ) ∧ ( ) ⇒ ( )( )x y s In Robot y s In x y s Has Robot x Result Take s,, ,, ,, ,, ,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 454, 'page_label': '455'}, page_content='428 CHAPTER 15 Introduction to Planning\\naxioms. Successor state axioms describe the way in which a predicate\\nchanges over time. For example,\\nThe axiom states the following:\\nThere are only two ways in which an action,a, can result in the robot hold-\\ning object x. The first of these is if the action is Ta ke, and the robot is in the\\nsame room ( y) as the object. The second possibility (after the ∨ in the\\nexpression) is if the robot already has the object, and the action is not Drop.\\nNote that this axiom uses iff (⇔) rather than implies. A ⇔ B means that A\\nimplies B, but that if B is not true, then A can also not be true. In other\\nwords, it is stating that if either the robot takes the object or it already has it,\\nthen it will have it, but that if neither of those is true, then it will not have it.\\nIn this way, one successor state axiom is needed for each predicate whose\\nvalue can change. Although these axioms may become very complex, they\\navoid the enormous number of unhelpful rules that a system based on\\neffect axioms and frame axioms would have (such as “If I pick up the\\ncheese, then the room’s walls will not change color. ”)\\n15.5 Means–Ends Analysis\\nTypically, a planner needs to find a correct set of actions (a plan) that will\\ntake it from one state to another state—the goal state. One approach to\\nplanning is to consider the differences between the goal state and the cur-\\nrent state, and select actions that aim to lessen those differences: this is\\ncalled means–ends analysis.\\nUnlike search techniques, means–ends analysis can select an action even if it\\nis not possible in the current state. If a planner selects an action that results in\\nthe goal state, but is not currently possible, then it will set as a new goal the\\nconditions necessary for carrying out that action. For example, let us con-\\nsider the blocks world, which is often used to illustrate planning problems.\\nThe blocks world contains a number of blocks and a surface or table top.\\nBlocks can be placed on top of each other or can be placed onto the table.\\n∀ ( )( ) ⇔\\n=∧ ( ) ∧ ( )( )\\n( ) ∧≠( )\\na x y s Has Robot x Result a s\\na Take In Robot y s In x y s\\nHas Robot x s a Drop\\n,,, ,, ,\\n,, ,,\\n,,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 455, 'page_label': '456'}, page_content='15.5 Means–Ends Analysis 429\\na\\nb\\nFigure 15.2\\nA start state in the blocks\\nworld\\nThe planner is a robot arm that is able to pick blocks up and to move them\\naround. Let us suppose that the world consists of two blocks, a and b,a s\\nshown in Figure 15.2.\\nLet us suppose that our robot’s goal is to place block b on top of block a,\\nwith block a resting on the table. Using means–ends analysis, our planner\\nstarts by considering how the goal state differs from the current state. In\\nthis case, the differences are:\\nBlock b is not on top of block a.\\nBlock a is on top of block b.\\nOur planner could now consider the following two possible actions:\\n1. Place block b on top of block a.\\n2. Remove block a from on top of block b.\\nEach of these actions is interesting because it reduces the differences\\nbetween the current state and the goal state. Our planner might start by\\nselecting action 2 and removing block a from on top of block b. The differ-\\nences between this new state and the goal state are now as follows:\\nBlock b is not on top of block a.\\nBlock a is not on the table.\\nThe planner’s next action might therefore be to place block a on the table.\\nNow the difference between the current state and the goal state is as follows:\\nBlock b is not on top of block a.\\nThe next action to consider is thus\\n3. Place block b on top of block a.\\nUnfortunately, action 3 cannot be carried out because the robot arm is not\\ncurrently holding block b. So we have a new goal state, which is\\nRobot arm is holding block b.\\nHence, before carrying out action 3, the planner must achieve this goal,\\nwhich it does by carrying out the following action:\\n4. Pick up block b.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 456, 'page_label': '457'}, page_content='430 CHAPTER 15 Introduction to Planning\\nNote that all of this planning is carried out before the robot starts to move.\\nHence, when it has completed building the plan, it is able to carry out the\\nfollowing actions:\\nRemove block a from on top of block b.\\nPlace block a on the table.\\nPick up block b.\\nPlace block b on top of block a.\\nHence, the goal has been achieved. As we will see, this approach can be used in\\nmuch more complex planning systems to solve far more interesting problems.\\nThe General Problem Solver,o r  GPS, was developed by Newell, Shaw, and\\nSimon in the late 1950s (Newell et al. 1959, Newell and Simon 1963) as an\\nattempt to simulate human thought, with the intention of using this\\napproach to solve problems of a general nature.\\nGPS uses means–ends analysis to solve logic problems, such as showing the\\nequivalence of the following two logical expressions:\\n(R →\\n¬P) ∧ (R → Q)\\n¬(¬Q ∧ P)\\nIn their 1963 paper, Newell and Simon explain how GPS examines the dif-\\nferences between these two expressions and use a set of three simple meth-\\nods that can be used to transform expressions, based on the logical\\nequivalence rules shown in Chapter 7 of this book.\\nFor example, GPS might start by removing the → operators in the first\\nexpression, using the following rule:\\nA → B\\n⇔¬ A V B\\nAs we see in Chapter 16, STRIPS is a planning system that uses\\nmeans–ends analysis in a manner similar to that used by GPS to control the\\nactions of a robot through a simple environment.\\n15.6 Chapter Summary\\n■ One way to find a plan to solve a problem is to apply a search\\nmethod and search through the search space of all possible plans.\\nThis works well for very simple problems, but is not efficient for\\ncomplex problems or those involving many possible actions and\\nvariables.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 457, 'page_label': '458'}, page_content='15.8 Exercises 431\\n■ In solving problems where each action can undo the effects of\\nother actions, it is necessary to use search.\\n■ Situation calculus is an extension of first-order predicate calcu-\\nlus, which uses situation variables to express how objects change\\nover time.\\n■ Effect axioms for an action state what changes after that action\\ntakes place.\\n■ Frame axioms state what variables do not change after carrying out\\nan action.\\n■ The frame problem is the problem of determining what does not\\nchange when an action is carried out. Using successor state axioms,\\nwhich combine features of effect axioms and frame axioms, solves\\nthis problem.\\n■ Means–ends analysis, as used by GPS, involves determining the dif-\\nferences between the current state and the goal state, and choosing\\nactions that minimize those differences.\\n15.7 Review Questions\\n15.1 What is planning?\\n15.2 Explain why the search methods described in Chapter 4 can be\\nused for planning.\\n15.3 Why can first-order predicate calculus not be used for planning\\nwithout the addition of situation variables?\\n15.4 Explain what is meant by the frame problem. What is the represen-\\ntational frame problem? Why are these problems so important to\\nthe study of planning?\\n15.5 Explain the idea behind means–ends analysis. Compare it with\\nsearch as a planning method. Which more closely matches the\\nmethods people use when formulating plans in everyday life?\\n15.8 Exercises\\n15.1 Write a program in the language of your choice that uses search to\\nformulate plans for moving from one arrangement of three blocks\\nto another. Assume that the available actions that can be taken are to'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 458, 'page_label': '459'}, page_content='432 CHAPTER 15 Introduction to Planning\\nmove a block from one of three locations to another, and that if one\\nblock is placed into a location in which another block is already\\npresent, then the second block is placed on top of the first block.\\n15.2 Consider extending your program to work with larger numbers of\\nblocks. How well do you think it will work?\\n15.9 Further Reading\\nMost of the standard texts provide good coverage of planning. Russell and\\nNorvig provide a particularly thorough treatment, in the context of intelli-\\ngent agents. Newell, Shaw, and Simon’s papers on GPS and Fikes and Nilsson’s\\npaper on STRIPS provide good introductions to those systems. The Further\\nReading section of Chapter 16 contains more references on planning.\\nSTRIPS: A New Approach to the Application of Theorem Proving to Problem\\nSolving, by Richard E. Fikes and Nils J. Nilsson (1971 – in Computation &\\nIntelligence, edited by George F. Luger, 1995 – MIT Press)\\nThe Robots Dilemma Revisited: The Frame Problem in Artificial Intelligence ,\\nby Kenneth M. Ford and Zenon W. Pylyshyn (1996 – Ablex Publishing)\\nGPS, A Program That Simulates Human Thought, by Alan Newell and Her-\\nbert A. Simon (1963 – in Computation & Intelligence , edited by George F.\\nLuger, 1995 – MIT Press)\\nReport on a General Problem Solving Program , by Alan Newell, J. C. Shaw,\\nand Herbert A. Simon (1959 – in Proceedings of the International Conference\\non Information Processing, pp. 256–264)\\nThe Robots Dilemma: The Frame Problem in Artificial Intelligence, by Zenon\\nW. Pylyshyn (1987 – Ablex Publishing)\\nChoices, Values, and Frames, edited by Daniel Kahneman and Amos Tversky\\n(2000 – Cambridge University Press)\\nRecent Advances in AI Planning , by Daniel S. Weld (1998 – appeared in AI\\nMagazine, 1999)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 459, 'page_label': '460'}, page_content='16CHAPTER\\nPlanning Methods\\nI have a cunning plan. . .\\n—Baldrick from Blackadder\\nThis very remarkable man\\nCommends a most practical plan:\\nYou can do what you want\\nIf you don’t think you can’t,\\nSo don’t think you can’t – think you can.\\n—Charles Inge, ‘On Monsieur Coué’\\nAwake, my St John! Leave all meaner things\\nTo low ambition, and the pride of kings.\\nLet us (since Life can little more supply\\nThan just to look about us and to die)\\nExpatiate free o’er all this scene of man;\\nA mighty maze! but not without a plan.\\n—Alexander Pope, An Essay on Man\\n16.1 Introduction\\nPlanning methods are used to solve problems where a sequence of actions\\nmust be carried out to reach a goal. In this chapter, we often consider the\\nblocks world, in which a robot arm must reorganize a set of blocks from\\none arrangement to another. Planning is also used a great deal in industry,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 460, 'page_label': '461'}, page_content='434 CHAPTER 16 Planning Methods\\nfor routing transportation, organizing allocation of machines in factories,\\nand controlling robots and intelligent agents.\\nIn Chapter 19, we see how intelligent agents can base their actions on beliefs,\\ndesires, and intentions. Agents have beliefs about the world and desires that\\nmust be fulfilled. T o achieve these desires, an agent forms intentions, or\\nplans, which specify in advance what it will do. An agent that does not plan\\nis able only to respond to its environment as it encounters it and will often\\nfind itself falling into traps that a planning agent would have foreseen.\\nPlanning is an extremely important part of Artificial Intelligence research.\\nThis chapter explores a number of algorithms and representations that are\\nused in planning and introduces some of the ideas that have been\\nresearched in the past 10 years.\\nWe start by examining STRIPS, which was an early planning system based\\non the means–ends strategy discussed in Chapter 15. Although STRIPS has\\nbeen superseded by a number of more sophisticated methods, the language\\nit uses to represent planning problems is still widely used.\\nWe then briefly explore partial order planning, in which plans are specified\\nsuch that the order in which some actions are carried out is unimportant.\\nThis chapter explores the ways in which propositional calculus can be used\\nto represent and solve planning problems, including producing plans by\\nexamining the satisfiability of propositional sentences.\\nWe then explore some other representations for planning problems includ-\\ning planning graphs (used by the GraphPlan algorithm) and ADL, which is\\nan extension of the STRIPS language.\\nWe also examine ways in which planning can be carried out in an uncertain\\nworld and ways in which planners can learn from their past actions and\\nmistakes.\\nFinally, we briefly explore the relationship between planning and scheduling.\\n16.2 STRIPS\\nSTRIPS (Stanford Research Institute Problem Solver) is an operator-based\\nplanning approach that was developed by Fikes and Nilsson in the 1970s\\n(Fikes and Nilsson 1971). This is in contrast with the use of situation vari-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 461, 'page_label': '462'}, page_content='16.2 STRIPS 435\\nables and frame axioms that we see in Chapter 15, when using the logic of\\nsituation calculus.\\nSTRIPS uses a means–ends analysis strategy, which was described in Sec-\\ntion 15.5. Means–ends analysis simply involves identifying the differences\\nbetween the current state and the goal state, and selecting actions that\\nreduce those differences.\\nSTRIPS uses well-formed formulae (wffs) in first-order predicate calculus\\nto describe the world, in much the same way that we see in Chapter 15.\\nSTRIPS was designed to provide planning for robotic agents to enable\\nthem to navigate through a world of blocks, but the approach can also be\\nused in other planning problems.\\nFor example, the following wff can be used to state the rule that if an object\\nis in one location, then it cannot be in another:\\nThis wff states that if an object, o, is in location x, where x is not the same\\nlocation as y, then object o cannot be in location y.\\nNote that unlike the examples in Chapter 15, locations in STRIPS are\\nexpressed as vectors, rather than as entire rooms. In other words, in the\\nabove expression, x and y represent the physical coordinates of the robot,\\nmeasured in some units of distance from a point that is considered to be\\nthe origin: (0,0).\\n16.2.1 Planning and Executing\\nSTRIPS uses a set of operators, which represent the actions that can be\\ntaken, or the steps that can be included in a plan. For example, operator\\nPush (o, x, y) enables the robot to push object o from location x to location\\ny. Note that there is a distinct difference between considering the operator\\nPush and actually carrying out the act of pushing. This is the difference\\nbetween planning and executing. Most of this chapter is concerned with\\nplanning, which means selecting a suitable sequence of operators. Once the\\nsequence has been chosen, the plan can be executed, which means carrying\\nout the actions described. This has some important implications: if carry-\\ning out an action has an unexpected effect that was not planned for, the\\nplan may not succeed. Should the robot continue with the plan regardless,\\n∀∀∀( ) ( ) ∧≠( )( ) ⇒¬ ( )( )o x y A Tox x y A Toy,,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 462, 'page_label': '463'}, page_content='436 CHAPTER 16 Planning Methods\\nor should it stop and develop a new plan based on the unexpected state it\\nhas found the world in? We consider these issues in Section 16.11.\\n16.2.2 Operators\\nEach operator that STRIPS uses is defined by two components. The first is\\nthe effect that the operator will have on the world, and the second is the\\npreconditions that must be met for the action to be carried out.\\nThe preconditions are specified as a set of wffs that must be proven to hold\\nfor the current state, or world model. The world model contains a list of\\nwffs that are true of the world in the current state, such as AT(r, x), which\\nmeans that the robot is at position x,o r  AT(o, y), which means that object o\\nis at position y.\\nSTRIPS includes information on two different types of effect that an oper-\\nator can have: the statements (or wffs) that become true after carrying out\\nthe action and the statements that are no longer true. Each operator can\\nthus be defined by a list of wffs that must be added to the world model and\\na list of wffs that must be deleted. These lists are often called the add list\\nand the delete list.\\nHence, the Push (o, x, y) operator could be fully defined as in the following\\nexample:\\nPrecondition: AT(r, x)\\n∧ AT(o, x)\\nDelete: AT(r, x)\\nAT(o, x)\\nAdd: AT(r, y)\\nAT(o, y)\\nIn other words, to push object o from position x to position y, the robot\\nand the object must both start out in position x. As a result of this action,\\nneither the robot nor the object will still be in position x: both will be in\\nposition y.\\nThis definition defines an operator schema, which means that it does not\\ndefine an actual action, but rather a type of action. A real action is an\\ninstance of the schema, in which the variables are instantiated with actual\\nobjects. Hence, for example, we could describe pushing object o\\n1 from'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 463, 'page_label': '464'}, page_content='16.2 STRIPS 437\\nposition with coordinates (2,3) to position (1,4) by the following operator\\ninstance:\\nPush (o1, (2,3), (1,4))\\nWhen the world model includes statements that can be used to instantiate\\nthe preconditions of a particular operator, then we say that this operator is\\napplicable.\\nThe final element of STRIPS is the goal state, which is described by a wff, or\\na set of wffs, that define the state that the robot wants to reach. Once the\\nplanner finds a way to reach this goal state, it has successfully solved its\\nproblem and is ready to execute the plan.\\n16.2.3 Implementation of STRIPS\\nThe algorithm used by the original STRIPS program to develop plans was\\nas follows:\\nFirst, the current world model is compared with the wffs that define the\\ngoal state. If the goal can be satisfied by the current world model, then the\\nproblem is solved, and the planning can terminate.\\nIn fact, STRIPS used the method explained in Chapter 8 for proving theo-\\nrems using resolution. This method involves assuming the negation of the\\ngoal, and then showing that this is inconsistent with the current world\\nstate, by using the method of unification to instantiate variables in the\\nschemata with real-world objects. If this method successfully shows an\\ninconsistency, then the goal is consistent with the world state.\\nIf this is not the case, then a plan must be developed.\\nT o select a suitable operator (or action) to apply, STRIPS used the same\\nmethod as GPS (which is described in Chapter 15), which means determin-\\ning the differences between the current state and the goal state and select-\\ning an operator that lessens those differences.\\nHaving applied unification and resolution, the original STRIPS program\\nused the resulting partial proof as a representation of these differences.\\nHence, the running of STRIPS involved alternately applying resolution\\n(theorem proving) and means–ends analysis.\\nSTRIPS solves the frame problem by making what is known as the STRIPS\\nassumption: that any statement that is true before applying an operator is'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 464, 'page_label': '465'}, page_content='438 CHAPTER 16 Planning Methods\\nb\\nca\\nFigure 16.1\\nStart state for the blocks\\nworld problem\\nalso true after applying the operator, unless it is included in the operator’s\\ndelete list.\\n16.2.4 Example: STRIPS\\nWe will now examine a simple example of STRIPS in action, in the blocks\\nworld, which consists of a table, three blocks (a, b, and c) and a robot arm\\nthat can move blocks around.\\nThe initial state of the world is shown in Figure 16.1. Block a is on the table,\\nand block b is on top of block c, which is in turn placed directly on the\\ntable.\\nWe will use two predicates to describe the world:\\nOn (x, y) means that block x is on top of block y.\\nClear (x) means that block x has no block on top of it.\\nWe will also use t to represent the table. Hence,On (a, t) means that block a\\nis on the table.Clear (t) will always be true because we assume that the table\\nis large enough to hold at least three blocks at once.\\nOur goal is to place block c on top of block a, which can be stated as\\nOn (c, a)\\nOur start state can be described as\\nOn (a, t)\\nOn (b, c)\\nOn (c, t)\\nClear (b)\\nClear (a)\\nClear (t)\\nWe have one available operator schema: MoveOnto (x,y), which means\\n“move object x from wherever it is, and place it on top of object y.”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 465, 'page_label': '466'}, page_content='16.2 STRIPS 439\\nMoveOnto (x,y) is defined as\\nPreconditions: On (x, z) ∧ Clear (x) ∧ Clear (y)\\nDelete: On (x, z)\\nClear (y)\\nAdd: On (x, y)\\nClear (z)\\nIn fact, this is not quite correct because if we move object b from on top of\\nobject c and place it on the table,Clear (t) is still true. We could address this\\nby including an additional operator schema MoveOntoT able(x), which is\\ndefined as follows:\\nPreconditions: On (x, y) ∧ Clear (x)\\nDelete: On (x, y)\\nAdd: On (x, t)\\nClear (y)\\nA number of approaches can be used to build the plan. The first approach\\nwe will consider is to use forward chaining. In other words, we will simply\\nsearch through the space of possible plans until we find a suitable one. This\\nwill involve constructing a tree where the root node represents the start\\nstate, and other nodes represent other possible states that can be obtained\\nby applying operators.\\nFor example, from the initial state, there are three operators we could\\napply:\\nMoveOnto (a, b)\\nMoveOnto (b, a)\\nMoveOntoT able(b)\\nOther operators, such as MoveOntoT able(c) are not possible because their\\npreconditions are not met by the current world state.\\nLet us suppose that we choose to apply MoveOntoT able(b). This has pre-\\ncondition\\nOn (b, y) \\n∧ Clear (b)\\nwhich is matched by instantiating y with c. Hence, after using the operator,\\nwe will need to apply the following add and delete lists to our current state:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 466, 'page_label': '467'}, page_content='440 CHAPTER 16 Planning Methods\\nDelete: On (b, c)\\nAdd: On (b, t)\\nClear (c)\\nHence, our state description becomes\\nOn (a, t)\\nOn (b, t)\\nOn (c, t)\\nClear (b)\\nClear (a)\\nClear (c)\\nClear (t)\\nFrom this position we could apply any of the following operators:\\nMoveOnto (a, b)\\nMoveOnto (a, c)\\nMoveOnto (b, a)\\nMoveOnto (b, c)\\nMoveOnto (c, a)\\nMoveOnto (c, b)\\nUsing the blind search method, we would simply try each of these and add\\na new node to the tree for each resulting state. In fact, by applying\\nMoveOnto (c, a), we produce a state that matches the goal state, and so a\\nsuitable plan has been found.\\nThis method did not use means–ends analysis, and although it would be\\nfeasible for a problem of this scale, it would not work at all for real-world\\nproblems involving hundreds of operators and objects.\\nThe means–ends analysis approach would start by noticing the differences\\nbetween the start state and the goal state: block c is not on block a and is in\\nfact under block b. The fact that block c is not clear does not matter because\\nthis is not mentioned explicitly in the goal.\\nT o reduce this difference, we could apply the operator MoveOnto (c, a).\\nHowever, this operator’s preconditions are not currently met because it'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 467, 'page_label': '468'}, page_content='16.2 STRIPS 441\\nrequires that c be clear. We note that operator MoveOntoT able(b) has the\\ndesired effect of clearing c. Hence, we have arrived at a suitable plan by\\nusing a form of backward chaining—starting at the goal and identifying\\nsteps that could lead to the goal.\\nOf course, we were lucky in our second choice. We might equally have cho-\\nsen MoveOnto (b, a) because this also has the effect of clearing c. In this\\ncase, we would then find ourselves in a position where we had further diffi-\\nculties. At this point, the planner would likely backtrack and try a different\\nchoice because it clearly made the problem harder, rather than moving\\ncloser to a solution.\\n16.2.5 Example: STRIPS and Resolution\\nLet us now consider in more detail the mechanics of STRIPS and, in partic-\\nular, how the original program used resolution and unification. We will\\nconsider a slightly different version of the blocks world, as designed by\\nFikes and Nilsson in their original description of STRIPS. The robot\\n(named Shakey, due to his unstable gait) starts out in position x, and his\\ntask is to bring two objects, a and b, together. The two objects start out in\\npositions y and z. We will assume that two objects are together when they\\nare both at the same position, and we will further assume that the robot can\\npush two objects together, ignoring the difficulties that this would pose in\\nthe real world.\\nHence, the initial world state is described by\\nAT(r, x)\\nAT(a, y)\\nAT(b, z)\\nThe goal can be described by the following wff:\\nThe operators available to the planner are the following:\\nP\\nush (o, x, y)\\nPrecondition: AT(r, x) ∧ AT(o, x)\\nDelete: AT(r, x)\\nAT(o, x)\\n∃( ) ( ) ∧ ( )( )p A Tap A Tbp,,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 468, 'page_label': '469'}, page_content='442 CHAPTER 16 Planning Methods\\nAdd: AT(r, y)\\nAT(o, y)\\nGo (x, y)\\nPrecondition: AT(r, x)\\nDelete: AT(r, x)\\nAdd: AT(r, y)\\nThe first stage is to negate the wff that describes the goal:\\n(Note that we have used the equivalence between \\n¬(∃x)e and (∀x)¬e and\\nhave then applied DeMorgan’s law to obtain this expression.)\\nFor the purposes of resolution, we can consider this to be the set of clauses:\\n{(¬AT(a,p),¬AT(b, p))}\\nWe now attempt to prove by using resolution that this expression is incon-\\nsistent with the current world state. In fact, this will not be possible, and we\\nwill obtain only a partial proof, which can then be used to describe the dif-\\nference between the initial state and the goal state.\\nThe first stage of this resolution would be to unify the following sets of\\nclauses:\\n{(\\n¬AT(a,p),¬AT(b,p))}\\n{(AT(r,x)), (AT(a, y)), (AT(b,z))}\\nWe will apply the unifier {y/p} and obtain the following set of clauses:\\n{(¬AT(a,y),¬AT(b,y)), (AT(r,x)), (AT(a,y)), (AT(b,z))}\\nThis resolves to give the following set of clauses:\\n{(¬AT(b,y)), (AT(r,x)), (AT(b,z))}\\nClearly, a difference that needs to be rectified is that object b is not at loca-\\ntion y, but is at location z. Hence, STRIPS will see if it can apply operator\\nPush (b, z, y).\\nT o determine whether this operator’s preconditions are met, the precondi-\\ntions are negated and added to the set of clauses. The preconditions for the\\npush operator are\\n∀( ) ¬ ( ) ∨¬ ( )( )p A Tap A Tbp,,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 469, 'page_label': '470'}, page_content='16.3 The Sussman Anomaly 443\\nb\\nca\\na\\nb\\nc Figure 16.2\\nThe start and goal states\\nfor the Sussman anomaly\\nproblem\\nAT(r, z) ∧ AT(b, z)\\nWe negate this and apply DeMorgan’s law to give\\n¬AT (r,z) ∨¬ AT (b,z)\\nWe add these to the clauses and obtain\\n{(¬AT(b,y)), (AT(r,x)), (AT(b,z)), (¬AT(r,z),¬AT(b,z))}\\nThis resolves to give the following partial proof:\\n{(¬AT(b,y)), (AT(r,x)), (¬AT(r,z))}\\nAgain, a complete proof was not possible, and we are left with a partial res-\\nolution proof. This shows that a further difference that needs to be rectified\\nis that the robot is at position x, whereas it should be in position z, in order\\nto carry out the Push (b,z,y) operator. Hence, the Go (x,z) operator is\\nattempted.\\nIn this case, the precondition when negated is\\n¬AT(r, x)\\nThis is now added to the set of clauses that are to be resolved, and the\\nprocess continues.\\nEventually, a set of operators is found that enables the clauses to resolve to\\nfalsum, meaning that the goal state has been reached.\\n16.3 The Sussman Anomaly\\nWe return now to the blocks world of the example in Section 16.2.4.\\nConsider the start state shown in Figure 16.2. Our goal now is to place\\nblock c on top of block b, and block a on top of block b. This is the second\\nstate shown in Figure 16.2.\\nThe STRIPS approach to this problem would start by either moving b onto\\nthe table, and then placing c on a, or by moving a on top of b without first\\nremoving b from c. In either of these cases, the solution cannot be reached\\nwithout undoing this first move. Many early planning systems could not'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 470, 'page_label': '471'}, page_content='444 CHAPTER 16 Planning Methods\\nd\\nc\\nb\\na\\nc\\nd\\na\\nb\\nFigure 16.3\\nStart and goal states for a\\npartial order planning\\nproblem\\nsolve problems of this nature, in which the two aspects of the goal have\\ndependencies on each other. The correct solution, of course, is to first move\\nb onto the table, then place a on top of b, and finally move c on top of a,b u t\\nthis involves interleaving the solutions to the two components of the goal,\\nwhich is not easily achieved using STRIPS in its original form. Later in this\\nchapter, we see methods that are able to deal with Sussman’s anomaly more\\nelegantly.\\n16.4 Partial Order Planning\\nThe plans that we have considered so far are known as total order plans\\nbecause they dictate the order in which each action must be carried out. In\\nsome cases, a partial order plan can be used, in which actions that are\\ndependent on each other are ordered in relation to each other but not nec-\\nessarily in relation to other independent actions.\\nFor example, let us consider the blocks world problem with the start and\\ngoal states as shown in Figure 16.3.\\nA total order plan for this problem might be described as follows:\\nMoveOntoT able(b)\\nMoveOntoT able(d)\\nMoveOnto (a, b)\\nMoveOnto (c, d)\\nT o place a on top of b, it is important that b is moved onto the table first. It\\ndoes not matter whether c or d have moved yet.\\nHence, a partial order plan can be described as shown in Figure 16.4.\\nThis partial order plan shows that before MoveOnto (a,b) can be carried\\nout, MoveOntoT able(b) must be applied; similarly, it shows that MoveOn-\\ntoT able(c) must be applied before MoveOnto (a,b) can be carried out.\\nT o finish, or reach the goal state, the robot must have carried out all four\\nactions.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 471, 'page_label': '472'}, page_content='16.4 Partial Order Planning 445\\nMoveOnto Table (b)\\nMoveOnto Table (a, b)\\nMoveOnto Table (c)\\nMoveOnto Table (a, b)\\nStart\\nFinish Figure 16.4\\nA partial order plan\\nStart\\nGoal\\nFigure 16.5\\nThe initial stage in build-\\ning a partial order plan\\nThis representation for a plan enables the planner to consider a number of\\ndifferent plans, without worrying about ordering actions that are not\\ndependent on each other. At the time of execution, the robot can select any\\nordering that matches this partial order. Note that the partial plans can be\\ninterleaved. Hence, a suitable total plan might be\\nMoveOntoT able(b)\\nMoveOnto (a, b)\\nMoveOntoT able(d)\\nMoveOnto (c, d)\\nThe idea behind partial order planning is to develop a partial order plan\\nthat ends with the goal state and where each transition in the plan is legal,\\naccording to the definitions of the available operators.\\nT o see how this works, let us consider the blocks world problem shown in\\nFigure 16.3. We will examine how the planning system might build up a\\npartial order plan.\\nAt first, the system has a start state and a goal state, as shown in Figure 16.5.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 472, 'page_label': '473'}, page_content='446 CHAPTER 16 Planning Methods\\nMoveOnto (a, b) MoveOnto (c, d)\\nStart\\nGoal\\nFigure 16.6\\nThe next stage in building\\nthe partial order plan\\nOur task is now to build a plan that gets us from the start state to the goal\\nstate.\\nThe first step is to add in operators that achieve the conditions set in the\\ndefinition of the goal state. As we saw above, these operators are MoveOnto\\n(a,b) and MoveOnto (c,d). In Figure 16.6, we have added these operators to\\nthe partial plan (it is a partial order plan, but it is also a partial plan, in the\\nsense that it is not yet complete).\\nIn Figure 16.6, the solid arrows represent causal links or establishes links,\\nwhich show how an action causes or establishes a set of conditions that\\nmatch the criteria for the goal state. The dashed arrows are not yet causal\\nlinks because they do not explain how one gets from the start state to the\\nnext states. These dashed arrows simply show the order in which actions\\nmust occur, so in this case, the two operators shown must occur after the\\nstart state.\\nNext we must find operators that will enable us to satisfy the preconditions\\nof the operators we have just added. These preconditions are met by\\nMoveOntoT able(b) and MoveOntoT able(d).\\nAdding these operators to the partial plan shown in Figure 16.6 leads to the\\npartial order plan (which is no longer a partial plan because it gives all the\\nsteps necessary to get from the start to the goal) shown in Figure 16.4.\\nIn building a partial order plan, there is a potential problem to consider,\\nwhich is that one action might undo the effects of another action, in which\\ncase the order in which those actions are carried out can be important. A\\ncausal link is said to be protected when it is needed to establish the precon-\\nditions of an operator below it in the plan. If another operator has the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 473, 'page_label': '474'}, page_content='16.5 The Principle of Least Commitment 447\\nFinish\\nOp 1 Op 2\\nStart\\nOp 2\\nStart\\nOp 3\\nFinish\\nOp 1\\nOp 3\\nStart\\nFinish\\nOp 1\\nOp 3 Op 2\\nFigure 16.7\\nAn example of a causal link\\nbeing threatened by\\nanother operator and two\\nways in which this problem\\ncan be rectified\\neffect of deleting some necessary part of that precondition, then the pro-\\ntected link is said to be threatened by this second operator. This is shown\\nin the first part of Figure 16.7.\\nLet us suppose that one of the preconditions of Op 3 is x, and that Op 1 has\\nx in its add list. In other words,x is one effect of carrying out the action Op\\n1. Unfortunately, Op 2 has x in its delete list, meaning that one effect of car-\\nrying out Op 2 is to undo x or to set ¬x to be true. Hence, the partial order\\nplan shown in the first part of Figure 16.7 does not work because the nature\\nof the partial order is such that Op 2 might be carried out between Op 1\\nand Op 3, thus ruining the plan. Hence, the second and third parts of Fig-\\nure 16.7 show ways in which the plan can be rearranged to ensure that this\\nproblem is avoided.\\nIn the second part of Figure 16.7, Op 2 has been demoted, which means\\nthat the plan ensures that it must occur before Op 1. In the final part of\\nFigure 16.7, Op 2 has been promoted, meaning it must be carried out after\\nOp 3. The partial order already dictated that Op 3 must take place after Op\\n1, so this also ensures that Op 2 must take place after Op 1.\\n16.5 The Principle of Least Commitment\\nIn many planning problems, a number of additional variables exist that are\\nnot relevant to the problem nor need to be modified to solve it. For exam-\\nple, suppose that in solving the problem shown in Figure 16.3 there was\\nactually another block, block e, on the table. Because the goal does not state\\nanything about this block, it does not matter and can be left alone, unless it'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 474, 'page_label': '475'}, page_content='448 CHAPTER 16 Planning Methods\\nin some way gets in the way of solving the problem (e.g., if it is on top of\\nblock b).\\nHowever, the planning system may still want to use block e because in for-\\nward or backward chaining it will make use of the variables that are pres-\\nent.\\nIn such cases, planners use the principle of least commitment , which\\nmeans that as few variables as possible are instantiated in producing a plan.\\nHence, an operator schema such as MoveOnto (a, y) should be used in pref-\\nerence to one where y has been instantiated [e.g., MoveOnto (a, b)], wher-\\never that is possible. In that way, the planner works with an accurate and\\nworking plan but does so in as efficient a manner as possible.\\nFor example, if the solution involved moving block a onto block b via the\\ntable or another block, the planner could consider this without deciding\\nwhether to move it via a block or the table.\\n16.6 Propositional Planning\\nMany planning problems can be expressed purely in propositional logic\\nnotation. In fact, plans expressed in STRIPS notation can always be con-\\nverted to propositional notation, although this will often involve increasing\\nthe number of variables required enormously.\\nFor example, if we consider a blocks world problem in which there are two\\nblocks, A and B, we might represent the various possible states in STRIPS\\nnotation using the following predicates:\\nClear (x)\\nOn (x, y)\\nIn propositional notation, we will use one propositional variable for each\\npossible state variable; hence:\\nX\\n1 is equivalent to Clear (A)\\nX2 is equivalent to Clear (B)\\nX3 is equivalent to On (A, B)\\nX4 is equivalent to On (B, A)\\nIn this case, we can represent any state by the use of just four propositional\\nvariables. Of course, if there were four blocks instead of two, then we would'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 475, 'page_label': '476'}, page_content='16.6 Propositional Planning 449\\nrequire 4 variables to represent Clear and 12 variables to represent On, and,\\nin general, for n blocks, we will require n2 propositional variables. Of\\ncourse, in most planning problems we will have more than just blocks to\\nconsider, and so the number of propositional variables required will\\nincrease accordingly.\\nA state can be represented by an assignment of truth values to each of the\\navailable variables; hence, in our simple blocks world we could have a state\\nrepresented by the following sentence:\\nX\\n1 ∧¬ X2 ∧ X3 ∧¬ X4\\nThis state can be represented in STRIPS notation as\\nClear (A) ∧¬ Clear (B) ∧ On (A, B) ∧¬ On (B, A)\\nOf course, a propositional logic sentence can also represent a number of\\nstates. For example, the following sentence represents all states in which A\\nis clear and B is not clear:\\nX1 ∧¬ X2\\nIn fact, due to the simplicity of this example, there is only one such state,\\nbut if we allow additional blocks, then ¬X2 (B is not clear) could be caused\\nby a block other than A, and so X 1 ∧¬ X2 represents a set of states, rather\\nthan a single state.\\nActions can also be represented using propositional sentences. T o do this,\\nwe use a new notation to represent the state that results from an action. If\\nwe use X\\n1 to represent the fact that A is clear before an action is taken, then\\nwe use ¬X1/H11032to represent that A is no longer clear after the action. Hence,\\nthe action MoveOnto (A, B) can be represented by the following proposi-\\ntional sentence:\\nX1 ∧ X2 ∧¬ X3 ∧¬ X4 ∧ X1/H11032∧¬ X2/H11032∧ X3/H11032∧¬ X4/H11032\\nThis sentence states that the preconditions for the action are X1 ∧ X2 ∧¬ X3\\n∧¬ X4, and that after the action is taken the state can be described as X1/H11032∧\\n¬X2/H11032∧ X3/H11032∧¬ X4/H11032.\\nIn this case, there is only one state in which the MoveOnto (A, B) action can\\nbe carried out. In most real problems, there will be several states from\\nwhich a given action can be applied. If we assume that it is possible to move\\nblock A onto block B even if A is not clear (i.e., if some other object is on'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 476, 'page_label': '477'}, page_content='450 CHAPTER 16 Planning Methods\\ntop of A), then we would express the action MoveOnto (A, B) by the follow-\\ning sentence, in disjunctive normal form:\\n(X1 ∧ X2 ∧¬ X3 ∧¬ X4 ∧ X1/H11032∧¬ X2/H11032∧ X3/H11032∧¬ X4/H11032) ∨\\n(¬X1 ∧ X2 ∧¬ X3 ∧¬ X4 ∧¬ X1/H11032∧¬ X2/H11032∧ X3/H11032∧¬ X4/H11032)\\nThe reason that this propositional notation is useful for planning is that a\\nnumber of techniques exist for manipulating propositional expressions,\\nand these techniques can, in theory, be applied to planning, as we will see in\\nthe next section.\\n16.7 SAT Planning\\nOne way in which propositional notation can be used in planning is to\\ndetermine the satisfiability of a set of sentences that express the problem.\\nAs explained in Chapter 7, a sentence is satisfiable if some assignment of\\ntruth values to the variables in the sentence makes the sentence true. The\\nsatisfiability problem (also known as SAT) in general is NP-complete,\\nwhich means that in the worst case, solving a SAT problem for n variables\\nwill involve testing m\\nn possible assignments of variables, where n is the\\nnumber of variables in the expression, and m is the number of values each\\nvariable can take.\\nDuring the 1990s, a number of techniques were developed that improved\\nthe performance of systems designed to solve the satisfiability problem.\\nGSAT is an example of such a system, which is explained in detail in Sel-\\nman et al. (1992).\\nThere are two main approaches to SAT. One class of solutions uses a sys-\\ntematic approach, meaning that each possible assignment of truth values is\\ntested until a solution is found. These methods are guaranteed to find a\\nsolution if one exists, but in the worst case can be very inefficient.Stochas-\\ntic methods involve randomly testing assignments. One such method is\\nW alksat, which operates in a similar way to the exchanging heuristics seen\\nin Chapter 5. Walksat involves repeatedly changing the value of variables in\\nunsatisfied clauses until a solution is found (Selman et al. 1994).\\nSAT planning involves encoding the start state, goal state, and operators\\n(frame axioms and effect axioms) in conjunctive or disjunctive normal\\nform and then using a method such as GSAT to show whether the sen-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 477, 'page_label': '478'}, page_content='16.8 Planning Graphs 451\\npreconditions preconditionseffects effects\\nState 0 State 1 State 2\\nFigure 16.8\\nA stylized illustration of a\\nplanning graph\\ntences are satisfiable or not. If they are, then a suitable plan can be formu-\\nlated.\\n16.8 Planning Graphs\\nA planning graph can be used to develop plans for problems that can be\\nrepresented using propositional logic. GraphPlan is an example of an algo-\\nrithm that uses planning graphs to develop plans for problems that are\\nexpressed in STRIPS notation (and can, therefore, as we saw above, be con-\\nverted to propositional form).\\nA planning graph consists of a number of levels. This is illustrated in Figure\\n16.8. The first level (usually called the zeroth level) contains the proposi-\\ntions that are true in the start state for the problem. The next level of the\\ngraph contains the actions that can be carried out in this state. The level\\nafter that contains the states that can be led to by carrying out these actions.\\nHence, each even-numbered level in the plan represents a state, and each\\nodd-numbered level represents actions. The final state in the graph repre-\\nsents the goal state.\\nThe links between level 0 and level 1 show how the preconditions of the\\nactions in level 1 are met by the propositions in level 0. Similarly, the links\\nfrom level 1 to level 2 show how the actions in level 1 produce the state con-\\ntained in level 3 (state 1).\\nIt is useful to be able to show which propositions do not change as a result\\nof a given action. These are shown by persistence actions, which are equiv-\\nalent to the frame axioms discussed in Section 15.4. A persistence action is'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 478, 'page_label': '479'}, page_content='452 CHAPTER 16 Planning Methods\\nba\\na\\nb\\nFigure 16.9\\nA blocks world problem,\\nshowing start state and\\ngoal state\\nusually shown on a planning graph as an arrow with a clear box on it (see\\nFigure 16.10).\\nPlanning graphs form a very compact representation: because the graph\\nonly shows actions that are possible in each state, it reduces significantly\\nthe number of actions that must be considered in constructing a plan.\\nA final feature of the planning graphs is the inclusion of mutual exclusion\\ninformation, or mutexes. A mutex exists between two effects or actions\\nthat are mutually exclusive. For example, in our blocks work, Clear (B) is\\nmutually exclusive with On (A, B) because the two statements cannot be\\ntrue at the same time. At each level of the planning graph, lines are drawn\\nbetween actions or propositions that are mutually exclusive with each\\nother.\\nLet us examine the planning graph for the problem illustrated in Figure\\n16.9.\\nThe available actions are MoveOnto and MoveOntoT able, as defined above\\nin Section 16.2.4. We will continue to use the STRIPS notation predicates\\nOn (x, y) and Clear (x).\\nAn incomplete planning graph for this problem is shown in Figure 16.10.\\nThe planning graph in Figure 16.10 shows the actions that are possible\\nfrom the initial state (State 0). The actions in level 1 are connected by links\\nto their preconditions in Level 0. Similarly, the results of the actions in level\\n1 are shown by links to propositions in level 2 (which represents state 1).\\nMutexes between actions and between propositions are shown as heavy\\nblack lines. For example, Clear (A) is mutex with both \\n¬Clear (A) and On\\n(B, A).\\nNote that not all mutexes have been shown because to do so would involve\\nrendering the diagram hard to follow. Similarly, not all propositions are\\nincluded. For example, propositions such as \\n¬On (A, Table) and ¬On (B,\\nTable) have been excluded, again for the sake of clarity.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 479, 'page_label': '480'}, page_content='16.8 Planning Graphs 453\\nOn (B, Table)\\nOn (A, Table)\\nClear (A)\\nClear (B)\\nOn (A, B)\\nOn (B, A)\\n¬Clear (B)\\n¬Clear (A)\\nMoveOnto (B, A)\\nMoveOnto (A, B)\\nState 0 Action 0 State 1\\nOn (B, Table)\\nOn (A, Table)\\nClear (B)\\nClear (B)\\nFigure 16.10\\nPartial planning graph for\\nthe blocks world problem\\nshown in Figure 16.9\\nThe graph shows persistence actions as lines with squares on them. These\\nrepresent the possibility that a proposition might not change from one\\nstate to the next.\\nNote that for even an extremely simple problem, the planning graph can\\nappear very complex. In fact, planning graphs produce a much more com-\\npact representation than many other methods.\\nThe planning graph shows at each state every proposition that could possi-\\nbly be true in that state, as a result of the actions that are in the previous\\nlevel.\\nHaving produced the planning graph, it is possible to determine immedi-\\nately whether it is possible to formulate a plan that will solve the problem.\\nIf any of the literal propositions that are included in the goal state defini-\\ntion are not included in the final level of the planning graph, then it is not\\npossible to formulate a plan to reach the goal state. On the other hand, if all\\nthe goal propositions are included in the final level, then it may be possible\\nto formulate a suitable plan. This will depend on the mutexes in the final\\nlevel, which restrict which states can be achieved.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 480, 'page_label': '481'}, page_content='454 CHAPTER 16 Planning Methods\\nNote that each state level in the graph contains information about a num-\\nber of different possible states, which can be determined by examining the\\nmutex information at that level.\\nThe next stage in using planning graphs is to extract a plan from the plan-\\nning graph. This can be done using an algorithm such as GraphPlan, which\\nis explained in the next section.\\n16.8.1 GraphPlan\\nGraphPlan is a planning algorithm that was invented by Avrim Blum and\\nMerrick Furst (1997). It uses planning graphs to formulate plans to prob-\\nlems that are expressed in STRIPS notation.\\nThe GraphPlan algorithm runs by iteratively building a planning graph,\\nstarting from the initial state and working toward the goal state.\\nFirst, the propositions that describe the goal state are compared with the\\ncurrent state. If all of these propositions are present, and no two of them\\nare joined by a mutex link, then it is possible that a solution has already\\nbeen reached. At this stage, a second phase of the algorithm is run to try to\\nextract a plan from the current graph plan.\\nIf the current state does not contain all the necessary propositions, then the\\nnext level of the planning graph is produced by applying all applicable\\noperators, and determining all possible propositions that can be made true\\nby these operators.\\nThis algorithm repeats until a suitable plan is found, or until it can be\\nshown that no plan exists.\\nGraphPlan has the desirable property that if a plan exists, it is guaranteed\\nto find it, and it is guaranteed to find the shortest possible plan due to the\\niterative way in which it builds the planning graph. It is also guaranteed to\\nterminate in the case where no plan exists. In such cases, the planning\\ngraph will reach a state where each new level that is added is the same. At\\nthis stage, the graph is said to have leveled off. If the graph levels off, and\\nthe final level does not have all of the desired propositions or some of them\\nare connected to each other by mutex links, then no suitable plan exists.\\nWhen a state is found in which all goal propositions are present and are not\\nmutex, the method for finding a plan is applied, which works as follows:\\nStarting from the final level in the planning graph and working backward,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 481, 'page_label': '482'}, page_content='16.9 ADL and PDDL 455\\noperators are selected at each level that are not mutex and that provide all\\nof the conditions required at each level, either to meet the goal conditions\\nor to meet the preconditions of the actions in the next level.\\nThe plan that GraphPlan produces is a partial order plan, in which no\\nordering constraint is placed on actions that are at the same level.\\n16.8.2 Mutex Conditions\\nThere are a number of reasons that a pair of actions or propositions are\\nmutually exclusive, or mutex, to each other:\\n1. Two actions that have effects inconsistent with each other are\\nmutex. For example, MoveOnto (A, B) and MoveOntoT able(A) are\\nmutex because one has the effect of adding On (A, B) and the other\\nadds On (A, Table).\\n2. If the effect of one action interferes with the precondition of\\nanother, then the two actions are mutex. For example, MoveOnto\\n(A, B) has the effect of deleting Clear (B) and so is mutex with\\nMoveOnto (B, A), which has the precondition Clear (B).\\n3. If one action has proposition P as its precondition, and another\\naction has precondition \\n¬P, then the two actions are mutex.\\n4. If one proposition is inconsistent with, or the negation of, another\\nproposition, then the two are mutex. For example, in our simple\\nblocks world, On (A, B) is mutex with On (A, Table) and is also\\nmutex with \\n¬On (A, B).\\n16.9 ADL and PDDL\\nA number of alternative representations exist for expressing planning\\nproblems, in addition to STRIPS. ADL (Action Description Language) is a\\nmore expressive language than STRIPS, which can be used to represent a\\nnumber of problems that cannot be adequately represented in STRIPS.\\nUnlike STRIPS, which can only represent unquantified expressions such as\\nA\\n∧ B, goals in ADL can be quantified, allowing expressions such as ∃x.\\nP(x) ∧¬ Q(x).\\nPreconditions in STRIPS must be expressed as conjunctions (such as A ∧ B\\n∧¬ C), but preconditions in ADL can be expressed as disjunctions (such as\\nA ∨ B). Additionally, ADL allows for conditional effects, which state effects'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 482, 'page_label': '483'}, page_content='456 CHAPTER 16 Planning Methods\\nthat will occur as a result of carrying out a particular action depending on\\ncertain conditions.\\nFor example, in a more complex blocks world, it might be that block A is\\ntwice as big as blocks B and C, and so the action MoveOnto (B, A) might\\nonly have the effect of negating Clear (A) if On (C, A) is already true. This\\ntype of conditional effect would be hard to express in STRIPS notation.\\nAnother feature of ADL is that it enables types to be attached to variables.\\nThis means that in many situations, fewer rules need to be expressed than\\nwith STRIPS because rules can be set up that ensure that objects involved\\nin actions have the correct type.\\nPDDL (Planning Domain Definition Language) is a standardized syntax\\nfor expressing planning problems, which was developed for the AIPS (Arti-\\nficial Intelligence Planning Systems) planning competition. PDDL can be\\nused to represent STRIPS and ADL, and was introduced to provide a com-\\nmon notation that could be used by all planning systems.\\n16.10 Probabilistic Planning\\nIn all of our discussion of planning so far, we have assumed that actions are\\ndeterministic. That is, we have assumed that if you apply action A in state S,\\nthen we can state with certainty what the resulting state will be. Of course,\\nthis is unrealistic for many real-world planning situations, and probabilis-\\ntic planners have been developed that aim to deal with this uncertainty.\\nIn some systems, it is possible to consider nondeterministic actions, where\\nan action applied in a particular state will nondeterministically lead to one\\nof several possible states.\\nSituation calculus can be extended to express probabilistic relationships\\n(Mateus et al. 2001). This enables the language to express the various effects\\nthat can occur as a result of a particular action and how probable each of\\nthose effects are. Deterministic actions are a special case of probabilistic\\nactions in which the probability of the effect is 1.\\n16.11 Dynamic World Planning\\nIn addition to assuming that the actions our planner takes are determinis-\\ntic, we have also assumed in this discussion that the world itself is static. Of\\ncourse, the real world is dynamic, and in many situations there are other\\nagents that can also affect the world.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 483, 'page_label': '484'}, page_content='16.12 Case-Based Planning Systems 457\\nIt has been said that planning in a dynamic world is pointless because the\\nworld may change in such a way that the plan becomes useless. In spite of\\nthis difficulty, there are methods that can be applied to planning in\\ndynamic environments.\\nOne principle that is often applied is execution monitoring. Once a plan-\\nner has produced a plan, let us say for a robot, that robot is usually expected\\nto simply carry out, or execute, the plan. A planner that uses execution\\nmonitoring checks the preconditions of each action as it executes it. If the\\npreconditions are no longer met, because something has changed, then the\\nplanner may need to start again and devise a new plan.\\nThis process of devising a new plan when something has gone wrong is\\nknown as replanning.\\nSimilarly, the planner checks the goal conditions at each step, in case it has\\naccidentally solved the problem. For example, while executing a plan for a\\nblocks world problem, another robot may have arrived and solved the rest\\nof the problem, in which case our robot can stop executing its plan.\\nAn alternative method for dealing with dynamic environments, or uncer-\\ntainty, is to use conditional planning. Conditional planning assumes that\\nat each step of the plan, one of several different possible situations could\\nresult. In other words, the planner does not have complete information\\nabout the problem domain before it starts planning.\\nThe conditional planning approach involves developing a plan that covers\\nevery possible eventuality. This is a good way to guarantee that a plan will\\nnot fail, but in the real world, there may be far too many possibilities to\\nplan for.\\n16.12 Case-Based Planning Systems\\nA traditional planning system must reformulate its plan every time it is\\npresented with a new problem. Of course, in some situations it will be pre-\\nsented with a problem it has seen before, or a problem that shares elements\\nwith previous problems.\\nA case-based planning system stores each plan it formulates in memory\\nand is able to reuse these plans to help it solve new problems.\\nCHEF is an example of a case-based planning system, which was designed\\nto produce recipes for Chinese food based on a given set of ingredients.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 484, 'page_label': '485'}, page_content='458 CHAPTER 16 Planning Methods\\nWhen CHEF is presented with a set of ingredients that it has not encoun-\\ntered before, such as chicken and carrots, it is able to formulate a recipe\\nbased on an existing recipe, such as stir-fried beef and onions.\\nIn situations where CHEF’s plan has not been successful, it is able to learn\\nfrom its errors, in order to avoid making such errors again. If for example it\\novercooks the chicken, it will learn that in future plans it should not cook\\nchicken for as long as it might cook beef.\\nOne important aspect of case-based planning systems is the memory that is\\nused to store plans. Clearly it must be possible to look up a variety of items\\nin this memory to find the plan or plans that best suit the current situation.\\n16.13 Planning and Scheduling\\nThe planning techniques we have discussed in this chapter are extremely\\nuseful for solving a range of problems. We have mainly considered prob-\\nlems in the toy blocks world, which involve selecting the right sequence of\\nactions to rearrange a collection of blocks from one configuration to\\nanother. Planning can also be helpful in solving problems such as the trav-\\neling salesman problem, which was discussed in Chapter 3, along with a\\nrange of other similar problems that involve selecting a suitable course of\\ntravel that meets a set of constraints, and enable the traveler to move from\\none location to another in a desired time frame.\\nA rather different kind of problem is job shop scheduling, which is used to\\nplan a sensible allocation of machinery to a set of jobs. Each job consists of\\na set of tasks that must be carried out, usually specified as a partial order\\n(hence, some tasks must be done sequentially, but other tasks can be car-\\nried out in parallel). Each machine can perform a subset of the available\\ntasks, and the problem of job shop scheduling is to allocate tasks to\\nmachines such that no machine is being used for two tasks at the same time\\nand so that all the tasks get carried out. In some cases, it is desirable to find\\nthe most efficient such arrangement, so that the jobs are completed as\\nquickly as possible.\\nThe problem of scheduling a number of tasks among a set of machines is\\nvery similar in many ways to the planning problems we have examined\\nalready. The main difference is that a schedule must specify when each task\\nis carried out and how long it will take, whereas the plans we have exam-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 485, 'page_label': '486'}, page_content='16.14 Chapter Summary 459\\nined simply specify a sequence of actions, with no concern about how long\\neach action takes or when it should be done.\\nOne approach to scheduling is to treat it as a straightforward planning\\nproblem. This results in a plan that describes the order in which actions\\nshould be carried out. A human operator can then augment this plan with\\ninformation about when to perform each task.\\nAlternatively, scheduling can be seen as a constraint satisfaction problem\\n(see Chapter 5), where the constraints specify how long each task will take\\nand that one machine cannot be used for two tasks at a time.\\nIn practice, a combination of approaches is usually used. Planning tech-\\nniques such as the ones we have discussed in this chapter are applied in\\nconjunction with search methods suitable for solving constraint satisfac-\\ntion problems.\\n16.14 Chapter Summary\\n■ STRIPS is an operator-based planning approach based on\\nmeans–ends analysis.\\n■ An operator can be defined by an operator schema that describes a\\nnumber of possible operators, using variables that are instantiated\\nto provide an operator.\\n■ The Sussman anomaly occurs in problems in which a planner\\nneeds to be able to consider two aspects of the problem independ-\\nently. Such problems cannot be readily solved using the traditional\\nSTRIPS approach.\\n■ A total order plan specifies the order in which all actions must be\\ncarried out. A partial order plan allows some operators to be spec-\\nified in parallel, such that the order is determined at execution\\ntime.\\n■ The principle of least commitment states that it is a good idea at\\neach stage of planning to commit to as few decisions as possible.\\n■ Most plans (and in particular, all plans that can be represented\\nusing the STRIPS language) can be represented in propositional\\nlogic notation, meaning that plans can be developed using meth-\\nods that solve the satisfiability problem for a set of propositions.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 486, 'page_label': '487'}, page_content='460 CHAPTER 16 Planning Methods\\n■ A planning graph represents states and actions at alternate levels,\\nand shows all possible states and all possible actions at each point\\nby using mutex relationships to show which combinations are not\\nallowed.\\n■ GraphPlan is an algorithm that uses planning graphs to extract\\nplans.\\n■ ADL is an alternative planning representation that is more expres-\\nsive than the STRIPS language.\\n■ Probabilistic planning involves working with operators where the\\noutcome of a given operator is not certain. Similarly, planning in\\nmany situations needs to function in a dynamic environment in\\nwhich the world can change from one time-step to the next.\\nDynamic world planners often use replanning to cope when such\\nchanges interfere with their plans.\\n■ Case-based planning involves storing plans in a searchable mem-\\nory and reusing them to solve new problems.\\n■ Planning means selecting which operators to apply; scheduling is\\nused to determine at what time to carry out the actions in order to\\nmeet a set of constraints.\\n16.15 Review Questions\\n16.1 Explain the difference between the STRIPS language and the ADL\\nlanguage. Why is ADL described as being more expressive than\\nSTRIPS? What kinds of problems might ADL be used to solve for\\nwhich STRIPS might not be adequate?\\n16.2 Explain what is meant by the principle of least commitment. How\\ndo you think it might relate to the generation of partial order\\nplans?\\n16.3 Explain how the satisfiability problem relates to planning. How\\nefficient do you think this method might be compared with\\nSTRIPS planning or using GraphPlan?\\n16.4 Explain what is meant by dynamic world planning. What is meant\\nby probabilistic planning? What is the difference between proba-\\nbilistic planning and nondeterministic planning?\\n16.5 What is meant by replanning?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 487, 'page_label': '488'}, page_content='16.17 Further Reading 461\\nc\\nb\\nda ca\\nb\\nd\\nFigure 16.11\\nStart and goal state for\\nExercise 16.1\\n16.6 Explain why case-based planning can be used to produce a plan-\\nning system that is able to learn.\\n16.7 Compare and contrast planning and scheduling.\\n16.16 Exercises\\n16.1 Use the operators described in Section 16.2.4 and the STRIPS\\nmethod to solve the blocks world planning problem shown in Fig-\\nure 16.11. The first state shown is the start state, and the second\\nstate is the goal state.\\n16.2 Produce a planning graph for the blocks world problem shown in\\nFigure 16.11.\\n16.3 Use resolution and unification to solve the blocks world problem\\nshown in Figure 16.11. How does this plan compare with the one\\nyou generated in exercises 16.1 and 16.2?\\n16.17 Further Reading\\nPlanning has increased in prominence in the Artificial Intelligence world in\\nthe past decade, and as a result, better coverage can be found in the more\\nrecent textbooks. Russell and Norvig (1995) provide the fullest coverage of\\nthe standard texts.\\nReasoning About Plans, by James F. Allen, Henry A. Kautz, Josh T enenberg,\\nand Richard Pelavin (1991 – Morgan Kaufmann)\\nRecent Advances in AI Planning, by Susanne Biundo and Maria Fox (2000 –\\nSpringer V erlag)\\nFast Planning Through Planning Graph Analysis , by A. Blum and M. Furst\\n(1997 – in Artificial Intelligence, Vol. 90, pp. 281–300).\\nRobot Motion: Planning and Control, edited by Michael Brady, John Holler-\\nbach, Timothy Johnson, T omás Lozano-Pérez, and Matthew T. Mason\\n(1983 – MIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 488, 'page_label': '489'}, page_content='462 CHAPTER 16 Planning Methods\\nSTRIPS: A New Approach to the Application of Theorem Proving to Problem\\nSolving, by Richard E. Fikes and Nils J. Nilsson (1971 – in Computation &\\nIntelligence, edited by George F. Luger, 1995, MIT Press)\\nArtificial Intelligence & Manufacturing Research Planning Workshop , edited\\nby George F. Luger (1998 – AAAI)\\nProbabilistic Situation Calculus , by Paulo Mateus, António Pacheco, Javier\\nPinto, Amílear Sernadas, and Cristina Sernadas (2001 – in Annals of Math-\\nematics and Artificial Intelligence)\\nA New Method for Solving Hard Satisfiability Problems , by B. Selman, H.\\nLevesque, and D. Mitchell (1992 – in AAAI, Vol. 92, pp. 440–446)\\nNoise Strategies for Improving Local Search , by B. Selman, H. A. Kautz, and\\nB. Cohen (1994 – in AAAI, Vol. 94, pp. 337–343)\\nPlanning and Learning by Analogical Reasoning , by Manuela M. V eloso\\n(1994 – Springer V erlag T elos)\\nRecent Advances in AI Planning , by Daniel S. Weld (in AI Magazine,S u m -\\nmer 1999)\\nPractical Planning: Extending the Classical AI Planning Paradigm , by David\\nE. Wilkins (1989 – Morgan Kaufman)\\nIntelligent Scheduling, edited by Monte Zweben and Mark S. Fox (1998 –\\nMorgan Kaufmann)\\nIntelligent Planning: A Decomposition and Abstraction Based Approach ,b y\\nQiang Y ang (1998 – Springer V erlag)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 489, 'page_label': '490'}, page_content='Advanced Topics\\n6\\nIntroduction to Part 6\\nPart 6 is divided into five chapters.\\nAdvanced Knowledge Representation\\nThis chapter builds on the ideas presented in several of the\\nearlier chapters in this book, in particular Chapters 7, 9, 15,\\nand 16. It presents a number of more sophisticated knowl-\\nedge representation methods, including the blackboard\\narchitecture, scripts, and the Copycat architecture.\\nIt also presents more material on nonmonotonic reasoning\\nand reasoning about change. Finally, this chapter expands\\non topics introduced elsewhere in this book by discussing\\ncase-based reasoning and knowledge engineering.\\nFuzzy Reasoning\\nThis chapter introduces the subject of fuzzy logic. It dis-\\ncusses fuzzy sets and explains how they are used in fuzzy\\nsystems. It also explains how fuzzy logic provides an alter-\\nnative to the traditional logic presented in Chapters 7 and 8\\nof this book. It also discusses the ideas of fuzzy expert sys-\\ntems and neuro-fuzzy systems.\\nIntelligent Agents\\nChapter 19 introduces the concept of software agents and,\\nin particular, intelligent agents, which are able to independ-\\nently carry out tasks on behalf of a user. The chapter dis-\\ncusses a number of properties that agents can have such as\\nPART\\n17\\nCHAPTER\\n18\\nCHAPTER\\n19\\nCHAPTER'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 490, 'page_label': '491'}, page_content='intelligence, autonomy, benevolence, the ability to learn, and the ability to\\nmove about through a network, such as the Internet. The chapter intro-\\nduces a number of types of agents, such as interface agents, reactive agents,\\ncollaborative agents, and mobile agents. It also discusses architectures and\\nmethods that can be used to build agents. The chapter also discusses\\nrobotic agents, such as the Braitenberg vehicles.\\nUnderstanding Language\\nThis chapter discusses a number of techniques that are used by computer sys-\\ntems to understand written or spoken human language. In particular, it\\nfocuses on natural language processing (NLP) and information retrieval (IR).\\nIt presents the methods used to parse sentences and explains how semantic\\nand pragmatic analysis are used to derive meaning from sentences while\\navoiding being confused by the ambiguity inherent in human language.\\nMachine Vision\\nThis chapter presents a range of methods that are used to enable computers\\nto analyze visual data. It discusses edge detection and explains how convo-\\nlution can be used to detect edges in images. It also explains how images are\\nsegmented, and how the edges of three-dimensional line drawings can be\\nlabeled. It discusses texture and explains how important it is for computer\\nvision systems to use information derived from texture.\\nThis chapter also briefly discusses one method that is used for face recognition.\\n464 Part 6 Advanced Topics\\n20\\nCHAPTER\\n21\\nCHAPTER'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 491, 'page_label': '492'}, page_content='17CHAPTER\\nAdvanced Knowledge\\nRepresentation\\nLet knowledge grow from more to more,\\nBut more of reverence in us dwell;\\nThat mind and soul, according well,\\nMay make one music as before.\\n—Alfred Lord T ennyson,In Memoriam\\nWhether there be knowledge, it shall vanish away.\\n—The first epistle of Paul the apostle to the Corinthians, Chapter 13\\nWhat is all knowledge too but recorded experience,\\nand a product of history; of which therefore,\\nreasoning and belief, no less than action and passion,\\nare essential materials?\\n—Thomas Carlyle, Critical and Miscellaneous Essays\\nSo it is in travelling; a man must carry knowledge with him, if he would bring\\nhome knowledge.\\n—Samuel Johnson\\n17.1 Introduction\\nHuman beings use representations for the world around them all the time.\\nOne example is the use of language. Consider the following sentence:\\nThe cat sat on the mat.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 492, 'page_label': '493'}, page_content='466 CHAPTER 17 Advanced Knowledge Representation\\nThis sentence may seem trite, but it has real meaning to us. The word “cat”\\nrepresents a four-legged feline creature. The word “sat” represents an action\\nand tells us something about when that action took place. The word “mat”\\nrepresents another object, and the word “on” represents a relationship\\nbetween objects. What the word “the” represents is hard to define, but\\nclearly each word in a sentence, taken individually and grouped with other\\nwords, conveys meaning to a person who reads, hears, or speaks the words.\\nAnother representation we use regularly is that of images, or signs. Note\\nthat there is a significant difference between the audible representation of a\\nword when it is spoken compared with the visible representation when it is\\nwritten down. We use a vast number of signs, symbols, and images in our\\neveryday lives, including the following:\\n■ letters and numbers\\n■ mathematical equations\\n■ road signs\\n■ photographs of people, places, and things\\n■ caricatures and cartoons\\n■ alarms and other audible signals\\nThe list is endless.\\nThe human mind uses some form of representation for all concepts,\\nwhich enables us to understand such abstract ideas as “happiness, ” “late-\\nness,” and “common sense.” In this way, even a human baby is able to\\nunderstand the connection between the sound “woof!, ” the cartoon char-\\nacter Snoopy, and a dog. We use some kind of internal representation for\\na dog that allows us to associate those three different concepts together\\nin some way.\\nClearly this internal representation has a lot to do with our ability to think,\\nto understand, and to reason, and it is no surprise, therefore, that much of\\nArtificial Intelligence research is concerned with finding suitable represen-\\ntations for problems.\\nThroughout this book, we have considered representations and how they\\ncan be manipulated to solve problems. Representations we have consid-\\nered include:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 493, 'page_label': '494'}, page_content='17.1 Introduction 467\\n■ propositional and predicate calculus\\n■ semantic nets\\n■ search trees\\n■ frames\\nMost of the methods we have examined are dependent on a suitable repre-\\nsentation being chosen. It is impossible to solve a problem using genetic\\nalgorithms, planning, or classifier systems without first selecting an appro-\\npriate representation for the problem.\\nIn this chapter, we consider a number of methods of representing knowl-\\nedge, as well as exploring extensions to some of the representations we have\\nexplored elsewhere.\\nA number of the sections in this chapter build on ideas presented in earlier\\nchapters—in particular, Chapter 7 on logic, Chapter 9 on rules and expert\\nsystems, and Chapters 15 and 16 on planning.\\nThe chapter starts by discussing the ideas of representation, semantics, and\\ninterpretations and tries to explain why these are so important to Artificial\\nIntelligence.\\nIt then introduces a number of specific representational methods—the\\nblackboard architecture, scripts, and the Copycat architecture—and illus-\\ntrates how each of them is used.\\nThis chapter then concentrates on nonclassical logics, starting with a\\ndetailed discussion of nonmonotonic logics and nonmonotonic reasoning\\nmethods. The methods explained in this discussion include default reason-\\ning, truth maintenance systems, the closed world assumption, circumscrip-\\ntion, and abductive reasoning. This chapter also examines two methods for\\ndealing with uncertainty: the Dempster–Shafer theory and certainty factors.\\nThis chapter also expands on the discussion of situation calculus from\\nChapter 15 by explaining event calculus, temporal logic, and mental situa-\\ntion calculus, all of which are used to represent data in worlds that are sub-\\nject to change.\\nThis chapter has a brief discussion of the important steps in knowledge\\nengineering, an idea that was first introduced in Chapter 9, and also briefly\\nexplains why case-based reasoning (introduced in Chapter 16) is useful.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 494, 'page_label': '495'}, page_content='468 CHAPTER 17 Advanced Knowledge Representation\\n17.2 Representations and Semantics\\nMany representations involve some kind of language. We have seen, for\\nexample, propositional calculus and predicate calculus in Chapter 7, which\\nare languages used to represent and reason with logical statements; the lan-\\nguage of mathematics enables us to represent complex numeric relation-\\nships; programming languages such as Java and C++ use objects, arrays,\\nand other data structures to represent ideas, things, and numbers.\\nHuman beings use languages such as English to represent objects and more\\ncomplex notions. Human language is rather different from the languages\\nusually used in Artificial Intelligence, as we shall see in Chapter 20. In par-\\nticular, although human languages are able to express an extremely wide\\nrange of concepts, they tend to be ambiguous—a sentence can have more\\nthan one meaning, depending on the time and place it is spoken, who said\\nit, and what was said before it. Human languages are also very efficient: it is\\npossible to express in a few words ideas that took thousands of years for\\nhumans to develop (for example, the words existentialism, solipsism, and\\nmathematics).\\nWhen considering any representational language, it is vital to consider the\\nsemantics of the language (i.e., what expressions in the language mean or\\nwhat they represent).\\nIn some ways, despite its tendency for ambiguity, human language is very\\nexplicit—each sentence has a meaning that can be determined without any\\nexternal information. The sentence “the cat sat on the mat, ” for example,\\nhas a fairly specific meaning (although, it does not specify which cat or\\nwhich mat).\\nIn contrast, sentences in a language such as predicate calculus need to have\\nan interpretation provided. For example, we might write\\n∀xP (x) → Q(x)\\nThis sentence might have a number of interpretations, depending on our\\nchoice of meaning for P and Q. For example, we could interpret it as mean-\\ning “all men are mortal. ” An inference engine that manipulates such sen-\\ntences does not need to know the meanings of the sentences, but if the\\nsentences are being used to reason about the real world and to form plans,\\nthen of course the interpretations must be carefully chosen.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 495, 'page_label': '496'}, page_content='17.3 The Blackboard Architecture 469\\n17.3 The Blackboard Architecture\\nThe blackboard architecture is a method for structured knowledge repre-\\nsentation that was invented in the 1970s by H. Penny Nii (Nii 1986) for a\\nsystem called HEARSAY-II. HEARSAY-II contained an index of computer\\nscience papers, about which it was able to retrieve information in response\\nto spoken queries by users.\\nIn Chapters 3 and 9, we saw the difference between reasoning forward from\\na start state, applying rules or actions until a goal is reached, and working\\nbackward from a goal, seeing which rules or actions could lead to the goal\\nstate, and then selecting additional actions that satisfy the preconditions of\\nthose rules, and so on, until the start state is reached.\\nEach of these approaches has its advantages and is particularly useful when\\napplied to certain problems. In other situations, it is more appropriate to\\nuse an opportunistic reasoning model, where rules can be applied forward\\nor backward at different times, in whatever order most effectively solves the\\ncurrent problem. Opportunistic reasoning applies well to planning (which\\nwas discussed in Part 5 of this book), but in this section we are going to\\nexamine how it is used by blackboard systems to effectively represent and\\nuse specific domain knowledge.\\nIn Chapters 3 and 9, we examined a structured knowledge representation\\nsystem based on frames. Each frame contains information about an object,\\nand frames are linked to other frames by relations that express the ways in\\nwhich the objects relate to each other. As we saw, this representation uses\\nthe idea of inheritance to provide an efficient way to represent the ways in\\nwhich one object shares properties with another object.\\nAlso in Chapter 9, we examined production systems, which use rules to\\nrepresent expert knowledge about a domain. Similarly, blackboard systems\\nare also used to represent and manipulate expert domain knowledge. The\\nidea behind blackboard systems is that disparate knowledge from different\\nexpert sources can be combined by providing a central database—the\\nblackboard—on which the experts (known as knowledge sources ) can\\n“write” information. Because the blackboard is shared, one knowledge\\nsource can see facts appear as another knowledge source puts them there,\\nand it can thus deduce new facts and add them to the blackboard. In this'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 496, 'page_label': '497'}, page_content='470 CHAPTER 17 Advanced Knowledge Representation\\nKnowledge Source 1\\nKnowledge Source 2\\nKnowledge Source n\\nBlackboard\\nFigure 17.1\\nA simple blackboard\\narchitecture\\nway, a number of knowledge sources can be used together to solve a com-\\nplex problem, but each knowledge expert does not need to know from\\nwhere the data on the blackboard came.\\nA simple blackboard architecture is illustrated in Figure 17.1.\\nBecause the blackboard system uses opportunistic reasoning, the various\\nknowledge sources do not need to take turns to act. Each knowledge source\\ncan proactively examine the blackboard and add new data to it when it feels\\nthat it has something useful to contribute to the solution of the problem. In\\npractice, there is usually a central control mechanism that determines when\\neach knowledge source can interact with the blackboard, but it would be\\nequally possible to have each knowledge source an independent agent,\\nallowed to make its own decisions about when to act.\\nNii (1986) compared this approach with a group of people solving a jigsaw\\npuzzle on a large blackboard. Each person has a number of pieces of the\\npuzzle, and when a person notices an opportunity to place one of his or her\\npieces on the board, he or she does so. The people involved do not need to\\ncommunicate with each other, and no one needs to tell the individuals\\nwhen to place their pieces on the board—they can each act independently\\nand autonomously.\\nNii extends the analogy by supposing that the room has a monitor, who is\\na person able to control who is allowed to visit the blackboard and when.\\nNow only one person is allowed to place a piece on the blackboard at a\\ntime, and the monitor has complete authority to decide who can do so.\\nThe jigsaw puzzle analogy is helpful, but it does not quite describe the real\\nuse of blackboard systems. Each person solving the jigsaw puzzle has differ-\\nent pieces of the puzzle, but they all have the same kind of domain knowl-\\nedge. The idea behind blackboard systems is that experts with entirely'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 497, 'page_label': '498'}, page_content='17.3 The Blackboard Architecture 471\\ndifferent types of knowledge can work together to solve a single problem.\\nIn the next section, we explore in more detail the architecture of the black-\\nboard system, and in the section after that we see how the blackboard sys-\\ntem works in practice by considering the HEARSAY-II system.\\n17.3.1 Implementation\\nAs has already been suggested, the particular implementation of black-\\nboard system that is used can depend on the problem that is being solved,\\nand also on the computer systems that are available. We will now look at\\nsome of the key elements of real implementations of blackboard systems.\\nThe knowledge sources used in a blackboard system are entirely independ-\\nent. This means that by use of appropriate interfaces, a blackboard system\\ncan use a number of different representations for its knowledge sources.\\nTypically, knowledge sources are represented as rules or procedures. It is\\nalso possible to represent the information in a language such as first-order\\npredicate calculus.\\nThe only interaction that occurs between the different knowledge sources is\\nthrough the blackboard data structure. The blackboard can contain items\\nof data, partial solutions to the problem, and finally, a complete solution.\\nThese data are usually arranged hierarchically, so that each level in the hier-\\narchy represents a different level of abstraction of the problem. In\\nHEARSAY, for example, the levels represent aspects such as\\n1. the digital audio signal\\n2. the phonemes that make up the entire signal\\n3. the syllables that can be constructed from the phonemes\\n4. the words that can be constructed from the syllables\\n5. the complete sentence\\nEach knowledge source looks at data in the level(s) that are appropriate for\\nit and places data onto levels that are appropriate. For example, one knowl-\\nedge source might have the ability to extract phonemes from an audio sig-\\nnal, in which case it would need only to examine data at Level 1 and would\\nneed only to add data at Level 2.\\nTypically, the blackboard system has a control mechanism that determines\\nwhich knowledge source should act next, based on the most recent changes\\nthat have occurred in the database. Hence, if in the example given above a'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 498, 'page_label': '499'}, page_content='472 CHAPTER 17 Advanced Knowledge Representation\\nnew set of phonemes has been determined, the control module might\\nselect a knowledge source that has the ability to analyze phonemes to act\\nnext. In making this choice, the control module is said to be choosing the\\nfocus of attention of the system. At any one time, the system’s focus of\\nattention is directed at one knowledge source, or one piece of data, or a pair\\nthat consists of a knowledge source and a piece of data.\\nThe overall strategy of the blackboard system is determined by the control\\nmodule and which ordering it uses when choosing the point of focus.\\nHence, this choice is clearly of particular importance.\\n17.3.2 HEARSAY\\nHEARSAY was designed as a system that would combine phonology, syn-\\ntax, semantics, and a contextual understanding of words in order to under-\\nstand human speech. In Chapter 20, we learn more about systems that\\nunderstand spoken words, but in this section we will briefly examine how\\nthe blackboard architecture was applied to the problem.\\nIn the HEARSAY-II architecture, there were a number of knowledge\\nsources, each of which understood a different aspect of the sounds gener-\\nated when a human user would speak into the system’s microphone. The\\ncontext knowledge source has knowledge about the world, which it is able\\nto use to disambiguate words such as “their” and “there. ” This problem is\\ndiscussed in more detail in Chapter 20.\\nOne advantage of using the blackboard architecture for this problem is that\\na number of different knowledge sources could in fact be applied at each\\nstage—in particular, for example, in determining which word is being spo-\\nken, a number of different possible solutions might be generated by differ-\\nent modules at one level, and a higher level would later disambiguate\\n(using context, for example) and select the correct word. In this way, the\\nsound can be analyzed in a number of different ways in parallel to ensure\\nthat the best solution is obtained.\\n17.4 Scripts\\nA script (also known as a schema) is a data structure that is used as a struc-\\ntured representation for a situation that can be broken down into a\\nsequence of events. Scripts are often used in natural language processing,\\nwhich is discussed in more detail in Chapter 20.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 499, 'page_label': '500'}, page_content='17.4 Scripts 473\\nThe idea behind scripts is that for a given situation (such as buying food in\\na supermarket or attending a job interview) there is a finite set of knowl-\\nedge that is needed to understand what is said, and thus to determine how\\nto act and what to say. A script is a data structure that represents a very spe-\\ncific situation (such as buying apples from a fruit market).\\nThe script contains knowledge about the situation (such as the fact that in\\norder to buy an apple, one must pay the market seller, and that apples are\\ngood to eat unless they are rotten). A script has a set of entry conditions,\\nwhich state the preconditions necessary for a script to be used (e.g., to use\\nthe apples script, the story must start with someone near a fruit market),\\nand results, which occur as a result of running through the situation\\ndescribed by the script.\\nA script also encodes reasons: that is, why one engages in the situation\\ndescribed by the script. This enables a script-based system to understand\\nmotivations (e.g., why a person would want to buy an apple).\\nA story can be understood by matching elements in the story to appropri-\\nate parts of the script. In this way, the script-based system can answer ques-\\ntions whose answers are not explicitly stated in the story.\\nSchank (1975) proposes a script for understanding stories about restau-\\nrants. His script includes a number of roles, or types of people that might\\nbe involved, including customer, waitress, and chef. The script includes\\ninformation about reasons or why a customer might want to eat at a restau-\\nrant (clearly, hunger has something to do with this, as does money).\\nThe script is then broken down into a set of episodes, such as “entering, ”\\n“ordering, ” “eating, ” and “leaving. ”\\nEach episode is represented in the script data structure by a number of\\nrelated actions that the various people might perform.\\nLet us consider the following short story:\\nFred went to his favorite restaurant. The food was not as good as usual.\\nOn his way home, he realized he had left his wallet behind.\\nThe script system is able to match entities described in the story with its roles\\n(Fred is the customer, for example). Although the story does not mention a\\nwaitress or a chef, the script system knows that they would have been involved.\\nThe script system would also be able to answer questions such as “did Fred eat\\nat the restaurant?” even though the answer is not explicitly stated in the story.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 500, 'page_label': '501'}, page_content='474 CHAPTER 17 Advanced Knowledge Representation\\nA script is necessarily extremely specific. As we see in Chapter 20, more\\ngeneral systems for understanding language are extremely complex. How-\\never, in many situations it is possible to use scripts to understand natural\\nlanguage, provided the available scenarios are sufficiently restricted.\\n17.5 Copycat Architecture\\nThe Copycat architecture was invented by Melanie Mitchell in 1993. The\\nmotivation behind the Copycat system was an interest in solving problems\\nby analogy, such as the following problem:\\nhat is to head as glove is to what?\\nOf course, the answer to this problem is obvious, but for computer pro-\\ngrams to make such analogies is not easy. The Copycat system invented by\\nMitchell works on textual analogies, such as the following:\\nabc → abd\\nHence,\\ntuv → ?\\nIn fact, there are a number of possible answers to this problem, depending\\non the approach you choose to take. The answer most people will give\\nwould be “tuw” because they will have noted that in the first line, the third\\nletter in the group of three has been replaced by the letter that comes\\nimmediately after it in the alphabet (its successor, in other words). How-\\never, the following might also be a reasonable answer:\\ntud\\nT o solve such problems, the Copycat system uses a nondeterministic\\nmethod, such that when run repeatedly with the same problem it generates\\ndifferent answers.\\nThe architecture of the Copycat system consists of the following components:\\n■ the W orkspace\\n■ the Slipnet\\n■ the Coderack'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 501, 'page_label': '502'}, page_content='17.5 Copycat Architecture 475\\nF\\nE\\nD\\nC\\nB\\nA\\nGH I RSTU\\nV\\nW\\nX\\nY\\nZ\\nLetter category\\nopposite\\nfirst last\\nFigure 17.2\\nA simplified diagram of\\nthe slipnet in the Copycat\\nsystem\\nThe workspace is a data structure similar to a blackboard or to the message\\nlists used in classifier systems (see Chapter 13). It contains the input data\\n(such as “abc, ”“abd, ” and “tuv, ” for the problem given above) and is used as\\na working memory when solving problems. Eventually, it contains the\\nanswer that has been found.\\nThe slipnet is a network that contains a number of concepts. Each letter is\\nrepresented as a concept, as are ideas such as “opposite, ” “predecessor, ”\\n“sameness, ”“right, ” and “left. ” Each concept in the slipnet has an activation\\nlevel that indicates how relevant the concept is to the current problem. As a\\nproblem is being solved, the activation levels change.\\nThe slipnet can be thought of as the system’s long-term memory. It stores\\ninformation that the system has built up about the nature of objects and\\nconcepts, and the relationships between those concepts. The slipnet can\\nchange over time, as the system solves problems.\\nFigure 17.2 shows a simplified version of the slipnet used by the Copycat\\nsystem. Label nodes are included in the network that show, for example,\\nwhich concepts are “opposite” to each other. In other words, concepts are\\nused to show the relationship between concepts within the slipnet.\\nThe coderack contains a number of agents, or codelets. Each codelet\\nembodies a relationship between objects—such as “the b in abc is the suc-\\ncessor of a. ” The higher a concept’s activation level is, the more codelets will\\nbe assigned to working with that concept.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 502, 'page_label': '503'}, page_content='476 CHAPTER 17 Advanced Knowledge Representation\\nAn important concept in the Copycat system is slippage. Slippage is the\\nidea that allows Copycat to find analogies that are not necessarily directly\\napparent. For example, consider the following problem:\\nabc → abd\\niijjkk → ?\\nA solution to this problem would be iijjll, which is found by relating the\\nfirst letter in abc to the first group of identical letters ( ii) in iijjkk. This\\nmeans that the rule has changed from “replace the last letter with its succes-\\nsor” to “replace the last group of identical letters with their successor. ” This\\nkind of change is slippage and is vital to solving analogy problems.\\nThe final part of the Copycat architecture is the idea of temperature.A s\\nwith simulated annealing (Chapter 5), temperature represents the degree of\\ndisorder in the system. The greater the temperature, the further from a\\nsolution the system is, and the more random its codelets are allowed to be.\\nThe Copycat system starts with the problem representation in its work-\\nspace and with a reasonably high temperature.\\nAs it works, it builds up relationships in its workspace, such as “the letters\\nabc form a group where each letter is the successor of the letter to its left. ” It\\nthen tries to form correspondences between objects. For example, it might\\nform a correspondence between the entire group of letters abc and the\\ngroup ijk because they have a similar structure.\\nCopycat works by forming a rule that explains how to transform one object\\ninto another. For example, its rule might be “replace the rightmost letter by\\nits successor. ” This rule is adapted as Copycat works, in order to produce a\\nrule that will work with the problem object.\\nAs the system runs, the temperature lowers until it falls below a probabilis-\\ntic threshold, at which point it has reached a solution and can stop.\\nT o fully understand the Copycat system, it is well worth trying the online\\ndemonstration system.\\n17.6 Nonmonotonic Reasoning\\nAs was explained in Section 7.18, propositional logic and predicate logic\\nare monotonic reasoning systems. This means that if a conclusion C can be\\nderived from a set of expressions, S, then any number of additional expres-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 503, 'page_label': '504'}, page_content='17.6 Nonmonotonic Reasoning 477\\nsions being added to S cannot change the truth value of C, provided the\\nexpressions in S remain consistent.\\nIn other words, a monotonic reasoning system that stores facts about the\\nworld can deduce new facts from its existing facts but would never have\\ncause to delete or modify an existing fact (unless the world changed).\\nHence, the number of facts the system stores will increase monotonically.\\nIn real life, reasoning tends not to be so straightforward. For example, you\\nmight know that dogs like to eat meat, and that Fido is a dog. Hence, you\\nconclude that Fido will like to eat meat. If you are later informed that Fido\\nis a vegetarian dog, you will need to change your conclusion. This kind of\\nreasoning is called nonmonotonic reasoning.\\nA nonmonotonic reasoning system needs to be able to deal with the fact\\nthat conclusions change as new facts are introduced and, hence, that its\\nknowledge is not always certain because later facts may contradict it. In this\\ncontext, we often use the word “belief” rather than “fact” to describe items\\nof data that the system stores or deduces about the world.\\nIn this section, we introduce a number of systems and principles that are\\nused to deal with nonmonotonic reasoning situations.\\n17.6.1 Nonmonotonic Logic with the Modal Operator, M\\nOne way to reason in nonmonotonic situations is to use nonmonotonic\\nlogic. This involves augmenting the predicate calculus with a modal opera-\\ntor, M, which is used to represent the idea that a statement is consistent\\nwith all our beliefs. Hence, we might write\\n∀x bird (x) ∧ M flies (x) → flies (x)\\nThis can be read as follows:“for allx,i fx is a bird and it is consistent with our be-\\nliefs to believe thatx can fly, thenx can fly.” In other words, most birds can fly.\\nWe would consider M flies (x) to be false if we already knew that the bird\\nwas dead and, in addition, we knew that dead birds could not fly. Note that\\nwe have just described a nonmonotonic logic, which is used for nonmonot-\\nonic reasoning.\\n17.6.2 Default Reasoning\\nDefault reasoning is another form of nonmonotonic reasoning that\\ninvolves assuming certain statements to be true, unless there is some clear'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 504, 'page_label': '505'}, page_content='478 CHAPTER 17 Advanced Knowledge Representation\\nevidence to the contrary. This is a form of reasoning that people employ all\\nthe time. For example, a car might drive past you too fast for you to see\\nwhether it has a driver or not. It would be reasonable for you to assume that\\nthe car has a driver, unless you happen to know that it is a remote-con-\\ntrolled car, or you saw the driver jump out previously. This is default rea-\\nsoning, as it assumes certain facts by default, unless they are disproved by\\nsome other facts.\\nA notation similar to that described in Section 17.6.1 is used for default logic:\\nCar (x)\\n∧ :Has_Driver (x) → Has_Driver (x)\\nThis is a default rule, which states, in default logic notation, that if x is a\\ncar, and it is consistent with our beliefs to believe that x has a driver, then\\nwe can conclude that x does indeed have a driver.\\nSimilarly, the sentence above could be read as “if x is a car and there’s no\\nreason to suppose that x does not have a driver, then conclude that x does\\nhave a driver.”\\n17.6.3 Truth Maintenance Systems\\nA truth maintenance system (or TMS) stores information about how each\\nbelief was derived, as well as the beliefs themselves.\\nTruth maintenance systems are used in situations where belief revision is\\nimportant. In other words, situations in which the system’s beliefs need to\\nchange over time, as new facts come to light.\\nThe justification-based truth maintenance system (JTMS) was proposed by\\nJon Doyle in 1979.\\nThe JTMS stores reasons or justifications for beliefs, where a reason con-\\nsists of a pair of sets, such that the belief is true if the statements in the first\\nset (known as the in set) are all true, and the statements in the second set\\n(known as the out set) are all false. For example, the beliefQ might have the\\nfollowing reason:\\n({P, R}, {\\n¬S})\\nThis means that if P and R are both true, and ¬S is false, then we can\\ndeduce that Q is true. If we use this reason to conclude that Q is true, and\\nlater discover that ¬S is true, then we must retract our earlier conclusion.\\nThe JTMS uses a network of nodes, where each node represents a belief\\n(which can either be a simple statement such as “Fido is a dog” or a rule'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 505, 'page_label': '506'}, page_content='17.6 Nonmonotonic Reasoning 479\\nsuch as modus ponens, or “all dogs like to eat meat”). The JTMS also stores\\njustifications for each belief.\\nThe JTMS does not carry out logical operations on beliefs (such as ∧, ∨,\\nand →) because these operations can be carried out by a problem-solving\\nsystem external to the JTMS. Similarly, the JTMS does not need to under-\\nstand the meanings of its beliefs. This kind of logical interpretation is car-\\nried out by the problem-solving system. The JTMS simply ensures that as\\nnew beliefs are added to the system, the existing beliefs remain consistent.\\nThe JTMS is able to create new nodes, to add or retract justifications for\\nnodes, and can mark a node as a contradiction if it is informed by the prob-\\nlem solver that that is the case.\\nThe system either believes or does not believe in the statement represented\\nby each node, and so a node is described as being either in (the system\\nbelieves in it) or out (if the system does not believe in it). Of course, these\\nbeliefs can change, as new information is presented to the system and as it\\nmakes new arguments.\\nA node is considered to be contradictory if it represents a belief that is now\\nbelieved to be untrue. When such a contradiction is determined, the JTMS\\nmust use this information to retract beliefs that it had formed based (directly\\nor indirectly) on the incorrect belief. This retraction is done using depend-\\nency-directed backtracking(also called nonchronological backtracking—\\nsee Section 5.17). Dependency-directed backtracking in this case simply\\nmeans working back from the contradictory node to find assumptions that led\\nto the contradiction. These assumptions are retracted, until a minimal combi-\\nnation of retractions is found to ensure that the contradiction disappears.\\nAn alternative to the JTMS is the assumption-based truth maintenance sys-\\ntem, or ATMS. An ATMS is very similar to a JTMS, but rather than repre-\\nsenting a complete statement of the system’s beliefs at any given time, it\\nincludes information about all possible beliefs, or all possible worlds. Each\\nnode has associated with it a set of premises or assumptions that can be\\nused to make the node true. Hence, a node might have the following\\nassumptions associated with it:\\n({P}, {Q})\\nThis would mean that the node would be true if P is true, or it would be\\ntrue if Q is true. A node that has an empty set associated with it is neces-\\nsarily true, which means that it does not depend on other assumptions.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 506, 'page_label': '507'}, page_content='480 CHAPTER 17 Advanced Knowledge Representation\\n17.6.4 Closed-World Assumption\\nThe closed-world assumption (also known as negation by failure, partic-\\nularly as used by PROLOG) is an assumption used by systems that any fact\\nnot specifically known to be true is not true. For example, if a system uses a\\ndatabase of facts, and a particular fact is not included in the database, then\\nthat fact is assumed to be false.\\nThe open-world assumption is the inverse of this: that any fact not explic-\\nitly contained within the database is assumed to be true. Note that one sig-\\nnificant difference between STRIPS and ADL, two planning methods\\ndescribed in Chapter 16, is that STRIPS uses the closed-world assumption,\\nwhereas ADL uses the open-world assumption.\\nClearly, systems that use the closed-world assumption (or the open-world\\nassumption) must use nonmonotonic reasoning because they make\\nassumptions that may later prove to be false.\\nPROLOG uses the closed-world assumption, which means that if a fact is\\nnot contained within its database, then it is assumed to be false.\\n17.6.5 The Ramification Problem\\nThe ramification problem is similar to the frame problem, described in\\nChapter 15, which concerns the difficulty of needing to define all facts that\\ndo not change when an action is performed. The ramification problem\\nconcerns the additional consequences of actions that might not be imme-\\ndiately obvious. For example, if a robot picks up a block, and a fly has\\nlanded on the block, then the robot will also be picking up the fly. The ram-\\nification problem is the problem of determining how to deal with such\\npotentially highly complex consequences.\\n17.6.6 Circumscription\\nMcCarthy (1980) proposed a form of nonmonotonic reasoning, which he\\ncalled circumscription. Circumscription was designed to deal, like the\\nclosed-world assumption, with situations in which not every possible fact\\nis stated or denied.\\nMcCarthy imagined someone attempting to solve the missionaries and\\ncannibals problem (see Section 3.9.1), which involves having a group of\\nmissionaries and cannibals cross a river without the cannibals eating the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 507, 'page_label': '508'}, page_content='17.6 Nonmonotonic Reasoning 481\\nmissionaries. McCarthy imagined a person trying to solve this question by\\nasking questions such as “Does the boat leak?” or “Is there a bridge?”\\nCircumscription allows us to modify a first-order predicate calculus\\nexpression to show that no facts are true other than those stated in the\\nexpression.\\nBy applying circumscription in the problem of the cannibals and mission-\\naries, we can conclude that any facts not explicitly stated in the problem\\nspecification are not true.\\nThe circumscription of predicate P in an expression E is written\\nE(/H9278)\\n∧∀ x (/H9278(x) → P(x)) → ∀x (P(x) → /H9278(x))\\nwhere /H9278(x) is the result of substituting all occurrences of P with /H9278in E.\\nLet us consider a simple example from the blocks world:\\nE = IsBlock (A) ∧ IsBlock (B) ∧ IsBlock (C)\\nHere the predicate IsBlock is used to indicate that an object is a block.\\nWe can circumscribe the predicate IsBlock in E as follows:\\nFirst, we note that E(/H9278) is the following expression:\\n/H9278(A)\\n∧ /H9278(B) ∧ /H9278(C)\\nHence, the circumscription of IsBlock in E is\\n/H9278(A) ∧ /H9278(B) ∧ /H9278(C) ∧∀ x (/H9278(x) → IsBlock(x)) →\\n∀x (IsBlock (x) → /H9278(x))\\nNow to see what this really means, let us make the following substitution:\\n/H9278(x) ≡ (x = A ∨ x = B ∨ x = C)\\nClearly, /H9278(A) will become (A = A ∨ A = B ∨ A = C), which is true, and sim-\\nilarly for /H9278(B) and /H9278(C). Hence, these parts can be eliminated from the\\nexpression (since TRUE ∧ A = A).\\nThis results in the following expression:\\n∀x ((x = A ∨ x = B ∨ x = C) → IsBlock(x)) → ∀x (IsBlock (x) →\\n(x = A ∨ x = B ∨ x = c))\\nNow, we can use our original expression:\\nE = IsBlock (A) ∧ IsBlock (B) ∧ IsBlock (C)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 508, 'page_label': '509'}, page_content='482 CHAPTER 17 Advanced Knowledge Representation\\nHence, (x = A ∨ x = B ∨ x = C) → IsBlock (x) is clearly true. Since\\nTRUE → A = A\\nWe can thus eliminate the left-hand side of the first implication, to give the\\nfollowing expression:\\n∀x (IsBlock (x) → (x = A ∨ x = B ∨ x = C ))\\nIn other words, not only are A, B, and C blocks, but there is nothing else in\\nthe world that can be called a block.\\nNote that if we now add an additional expression to E,\\nIsBlock (D)\\nthe circumscribed expression we derived above is no longer true. We can\\ninstead, derive the following new circumscribed expression:\\n∀x (IsBlock (x) → (x = A ∨ x = B ∨ x = C ∨ x = D))\\nThis is a property of a nonmonotonic reasoning system: adding a new fact\\nnegates conclusions that have been logically deduced.\\n17.6.7 Abductive Reasoning\\nRecall the modus ponens rule from Section 7.11.4, which is written as follows:\\nThis tells us that if A is true, and we know that A implies B, then we can\\ndeduce B.\\nAbductive reasoning is based on a modified version of modus ponens,\\nwhich while not logically sound, is nevertheless extremely useful:\\nThis tells us that if we observe that B is true, and we know that A implies B,\\nthen it is sensible to see A as a possible explanation for B.\\nFor example, consider the case where B represents “Fred is not at work” and\\nA represents “Fred is sick. ” If we know that when Fred is sick he does not\\ncome to work, and we also know that Fred is not at work, then we use\\nabductive reasoning to conclude that Fred is sick. This might not be the\\ncase, as he may be on holiday or at a conference, but the point of abductive\\nBA B\\nA\\n→\\nAA B\\nB\\n→'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 509, 'page_label': '510'}, page_content='17.6 Nonmonotonic Reasoning 483\\nreasoning is that it provides a “good-enough” explanation for a phenome-\\nnon, which can be retracted later, if a preferable explanation is determined.\\nIn other words, abductive reasoning is nonmonotonic.\\n17.6.8 The Dempster–Shafer Theory\\nThe Dempster–Shafer theory of evidence is used to discuss the degree of\\nbelief in a statement. A degree of belief is subtly different from probability.\\nFor example, suppose that a barometer tells you that it is raining outside\\nand that you have no other way to determine whether this is the case or not\\nand no knowledge about the reliability of the barometer.\\nUsing probability theory, we might suppose that there is a 0.5 chance that the\\nbarometer is right, in which case the probability that it is raining would be 0.5.\\nHowever, using the Dempster–Shafer theory, we would start by stating that in\\nfact we have no knowledge about whether it is raining or not, and so we write\\nBel (Raining) = 0\\nSince we also have no knowledge about whether it is not raining, we can\\nalso write\\nBel (\\n¬Raining) = 0\\nNote that Bel (A) and Bel (¬A) do not need to sum to 1.\\nNow let us further suppose that we have determined that the barometer is\\n80% accurate.\\nHence, we can modify our belief as follows:\\nBel (Raining) = 0.8\\nThis tells us that because the barometer says it is raining, we have a belief of\\n0.8 that it is in fact raining. At this point, we still have the following:\\nBel (¬Raining) = 0\\nBecause the barometer is telling us that it is raining, we do not have any rea-\\nson to believe that it is not raining. Note again the difference between this\\nnotation and normal probabilistic notation, where P (Raining) and\\nP (\\n¬Raining) must sum to 1.\\nWe also define the plausibility of a statement, X, as follows:\\nPl (X) = 1 /H11002Bel (¬X)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 510, 'page_label': '511'}, page_content='484 CHAPTER 17 Advanced Knowledge Representation\\nHence, we can define a range for X, which is [ Bel (X), Pl (X)]. For the\\nexample above, our range is\\n[0.8, 1]\\nThe narrower this range is, the more evidence we have, and the more cer-\\ntain we are about our belief. That is to say, if we have a belief range of [0, 1],\\nthen we really do not know anything. If we have a belief range of [0.5, 0.5],\\nthen we are certain that the probability of the proposition is 0.5. Hence, if\\nwe have a wide range, then we know that we need to seek more evidence.\\nLet us now suppose that we have a second barometer, which is 75% accu-\\nrate, and which is also saying that it is raining outside. How does this affect\\nour belief? Dempster (1968) proposed a rule for combining beliefs of this\\nkind, which is applied as follows.\\nThe probability that both barometers are reliable is\\n0.75 /H110030.8\\n= 0.6\\nThe probability that both are unreliable is\\n0.25 /H110030.2\\n= 0.05\\nHence the probability that at least one of the barometers is reliable is\\n1 /H110020.05\\n= 0.95\\nThus, we can assign the following belief range to the belief that it is raining:\\n[0.95, 1]\\nOnce again, we have no reason to believe that it is not raining, and so the\\nplausibility of the statement “it is raining” is 1. If we receive some evidence\\nthat it is not raining (e.g., if we cannot hear any rain), then we might mod-\\nify this value.\\nLet us now suppose that the second barometer says that it is not raining,\\nwhile the first barometer continues to say that it is raining.\\nNow, it cannot be the case that both barometers are reliable because they\\ndisagree with each other. The probability that the first barometer is reliable\\nand that the second is unreliable is'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 511, 'page_label': '512'}, page_content='17.6 Nonmonotonic Reasoning 485\\n0.8 /H110030.25\\n= 0.2\\nSimilarly, the probability that the second is reliable and the first unreliable is\\n0.75 /H110030.2\\n= 0.15\\nThe probability that neither is reliable is\\n0.2 /H110030.25\\n= 0.05\\nDempster’s rule now lets us calculate the belief that it is raining. We can cal-\\nculate the posterior probability that it is raining, given that at least one of\\nthe barometers is unreliable as follows:\\nSimilarly, the probability that it is not raining, given that at least one of the\\nbarometers is unreliable is\\nHence, our belief that it is raining is Bel (Raining) = 0.5, and the plausibil-\\nity of this belief is 1 /H11002Bel(\\n¬Raining) = 1 /H110020.375 = 0.625. Hence, our belief\\ncan be expressed as the range\\n[0.5, 0.625]\\n17.6.9 MYCIN and Certainty Factors\\nIn Chapter 9, we introduced expert systems or production systems and\\nbriefly mentioned MYCIN, which was a system developed at Stanford Uni-\\n01 5\\n02 01 5 00 5\\n01 5\\n04\\n0 375\\n.\\n.. .\\n.\\n.\\n.\\n++\\n=\\n=\\n02\\n02 01 5 00 5\\n02\\n04\\n05\\n.\\n.. .\\n.\\n.\\n.\\n++\\n=\\n='),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 512, 'page_label': '513'}, page_content='486 CHAPTER 17 Advanced Knowledge Representation\\nversity in the 1980s for medical diagnosis. MYCIN was designed to help\\ndoctors select the correct antimicrobial agent to treat a patient, based on\\ninformation about the patient’s symptoms.\\nMYCIN uses abductive reasoning and backward chaining to estimate,\\nbased on a set of evidence concerning the patient’s symptoms, which bacte-\\nria is most likely to be causing the illness.\\nMYCIN uses certainty factors to represent degrees of belief: much as the\\nDempster–Shafer theory uses the Bel notation, certainty factors represent\\nthe degree of belief or disbelief, where the two do not necessarily sum to 1,\\nas they would in classical logic.\\nWe use M\\nB(H|E) to represent the measure of belief of hypothesis H,g i v e n\\nevidence E, and MD(H|E) to represent the measure of disbelief of hypothe-\\nsis H, given evidence E.\\nBecause a particular piece of evidence either supports or contradicts a\\nhypothesis, either MB(H|E) or MD(H|E) must be zero for any H and E.\\nWe now define the certainty factor,CF(H|E) as follows:\\nCF(H|E) = MB(H|E) /H11002MD(H|E)\\nThis value ranges from /H110021 to 1, where a high negative value indicates that\\nthe evidence gives a strong confidence that the hypothesis is false, and a\\nhigh positive value indicates that the evidence gives a strong confidence\\nthat the hypothesis is true.\\nEach production rule used by MYCIN has a certainty factor associated with\\nit. The following is a simplified example of one of MYCIN’s rules:\\nIF: The patient has meningitis\\nAND: The patient has no serious skin infection\\nAND: The infection is bacterial\\nTHEN: The infection could be caused by staphylococcus-coag-pos (0.75)\\nOR: streptococcus-group-a (0.5)\\nThis rule is of the form\\nIF A ∧ B ∧ C ∧ ... N THEN H1 (P1) ∨ H2 (P2) ∨ ... ∨ Hn (Pn)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 513, 'page_label': '514'}, page_content='17.7 Reasoning about Change 487\\nwhere A ... N are the observed evidence,H1 ... Hn are the possible hypothe-\\nses to explain the evidence, andPi is the certainty factor associated withHi.\\nCertainty factor algebra is used to combine the certainty factors of rules\\nwith the certainty factors of the evidence to determine how certain the\\nhypotheses are.\\nWhen a rule has a conjunction of premises, as in the example rule above,\\nthe minimum of the certainty factors of the premises is used as the cer-\\ntainty factor. If the rule has a disjunction of premises, then the maximum\\nof the certainty factors is used.\\n17.7 Reasoning about Change\\nAs we saw in Chapter 7, the classical propositional and predicate calculi\\nprovide us with a way to reason about an unchanging world. Most real-\\nworld problems involve a dynamic world, in which other people (or agents)\\neffect changes, where the world itself changes, and where robotic agents can\\nmove themselves and thus change their environment proactively.\\nIn Chapter 15, we briefly introduced the situation calculus that enables us\\nto use a notation such as the following:\\n∃x(In(Robot,x,S\\n1) ∧ In(cheese,x,S1))\\nThis sentence says that in situation S1, the robot is in the same room as\\nthe cheese.\\nIn this section we will explore two alternatives to the situation calculus:\\nevent calculus and temporal logic.\\n17.7.1 Temporal Logic\\nAn early system for dealing with change was temporal logic , a form of\\nmodal logic. T emporal logic extends predicate calculus with a set of modal\\noperators, which are usually defined as follows:\\nP means from now on, P will be true\\n/H17003P means that at some point in the future, P will be true'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 514, 'page_label': '515'}, page_content='488 CHAPTER 17 Advanced Knowledge Representation\\nCompare these with the modal operators presented in Chapter 7, where the\\nsame symbols were used to indicate “necessarily” and “possibly. ” In tempo-\\nral logic, the symbols are read as “henceforth” and “eventually. ”\\nLinear time temporal logic defines two other operators: “until” and “in the\\nnext time interval, ” which are usually written\\nQµP means that Q is true until P is true\\n/H11034P means that P will be true in the next time interval\\nA number of other operators are also sometimes used, including\\nP awaits Q means that Q is true until P is true, or if P is never true,\\nthen Q is true forever (this contrasts with “until, ” which\\nimplicitly assumes that P will at some point be true)\\nSofar P means that P has been true until now\\nOnce P means that P was true at some time in the past\\nP precedes Q means that P occurred before Q\\nThese temporal operators implicitly assume that there is a concept of time,\\nwhich is broken down into intervals. In particular, the /H11034operator indicates\\nthat some expression will be true in the next time interval. T emporal logic\\ndoes not require the lengths of these time intervals to be defined, although\\nclearly for it to be applied to real-world problems a mapping needs to be\\ndefined. In linear time temporal logic there is a finite set of states, such that\\neach state has a unique successor. Hence, the logic cannot reason about\\nmultiple possible futures. This is possible with an alternative form of tem-\\nporal logic: computation tree logic (CTL—also known as branching time\\ntemporal logic), which reasons about time in the form of a tree, with states\\nrepresented by nodes in the tree. Because each state can have more than one\\nsuccessor, it is possible in this logical system to reason about several possi-\\nble future outcomes.\\nCTL provides methods for reasoning about paths through the tree. For\\nexample, it is possible to create expressions such as “a path exists in whichP\\nis true” or “a path exists in which eventuallyP is true for all successor states.”\\nThere also exist modal operators similar to the “necessarily” and “possibly”\\noperators presented in Chapter 7, which state, for example, “P is true in all\\npossible future states” or “P is true in at least one possible future state. ”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 515, 'page_label': '516'}, page_content='17.7 Reasoning about Change 489\\n17.7.2 Using Temporal Logic\\nT emporal logic can be used in a number of applications. It is used, for\\nexample, in specification and verification of software programs and can\\nalso be used to verify the behavior of other systems, such as elevators. It can\\nalso be used to reason about problems that cannot otherwise be reasoned\\nabout using classical logics.\\nA system being defined by temporal logic has three main sets of conditions\\nthat define its behavior:\\n1. Safety conditions define behaviors that should never occur (such\\nas the elevator being on two floors at once).\\n2. Liveness conditions specify what the system should do—for exam-\\nple, if someone pushes the button on the first floor, then the eleva-\\ntor should move toward that floor.\\n3. Fairness conditions define the behavior of the system in nondeter-\\nministic situations. For example, if the elevator is stationary on the\\nsecond floor, and someone pushes the button on the first floor at\\nthe same time that someone else pushes the button on the third\\nfloor, the system must decide which direction to move the elevator.\\nWe will now examine an example of temporal logic being used to specify a\\nproblem. The dining philosophers problem is defined as follows:\\nA number of philosophers are sitting around a round table, eating\\nspaghetti and cogitating. There are six philosophers, six plates, and six\\nforks. Each philosopher has a plate in front of him or her, and there is a fork\\nbetween each pair of philosophers. For a philosopher to eat spaghetti, he or\\nshe must use two forks. Hence, only three philosophers can be eating at any\\none time. When a philosopher is not eating, he or she is thinking.\\nWe will use the notation eating(i) to indicate that philosopher i is eating\\nand thinking(i) to indicate that philosopher i is thinking.\\nThe safety properties for this problem are defined as follows:\\nEach philosopher cannot be eating and thinking at the same time:\\n¬(eating (i) ∧ thinking (i))\\nEach philosopher is always either eating or thinking:\\n(eating (i) ∨ thinking (i))'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 516, 'page_label': '517'}, page_content='490 CHAPTER 17 Advanced Knowledge Representation\\nParty\\nstarts\\nParty\\nendsspace\\ntime\\nFigure 17.3\\nThe space–time event that\\nis a party at Tom’ s house\\nIf one philosopher is eating, then the next philosopher cannot be eating:\\n¬(eating (i) ∧ eating (i + 1))\\nWe can also define the liveness properties of the system as follows:\\nIf a philosopher is eating now, then at some point in the future he or she\\nwill be thinking:\\n(eating (i) → /H17003thinking (i))\\nSimilarly, if a philosopher is thinking now, then at some point in the future\\nhe or she will be eating:\\n(thinking (i) → /H17003eating (i))\\nNote that in this notation, unlike situation calculus, there is no mention of\\nexplicit states. This is not necessary with temporal logic, which is one rea-\\nson for using it in preference to situation calculus.\\n17.7.3 Event Calculus\\nAn alternative method for reasoning about properties that vary over time is\\nthe event calculus. Event calculus is concerned mainly with fluents. A flu-\\nent is a function that varies with time. For example, if a ball is dropped\\nfrom a first-story window, then the ball’s speed and height are both fluents.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 517, 'page_label': '518'}, page_content='17.7 Reasoning about Change 491\\nThe event calculus also uses the notion of an event, which is a period of\\ntime bounded by a start and a finish point. Events can also be thought of as\\ntaking place in the real world and so have a space dimension as well as a\\ntime dimension. For example, the event called “the party at T om’s house”\\nhas a start and stop time, and takes place in a finite space, as shown in Fig-\\nure 17.3.\\nThe event calculus uses a number of predicates:\\nHappens (e, t)\\nStarts (e, f, t)\\nEnds (e, f, t)\\nwhere f is a fluent, e is an event, and t is a variable of time.\\nHappens (e, t) means that event e happens at time t. In fact, t can be a func-\\ntion of time, and thus this predicate can be used to express the fact that an\\nevent (e) takes place over a period of time, defined by the function t.\\nStarts (e, f, t) means that the evente causes fluent f to hold immediately after\\ntime t, and similarly,Ends (e, f, t) means that evente stops fluentf at time t.\\nA further predicate lets us state that fluent f was beginning at the start of\\nthe time period we are considering:\\nInitially (f)\\nFor example, let us consider the event in which a ball drops from a height of\\n10 meters to the ground. For this example, we will assume that time starts at\\nthe moment the ball is dropped, and we will consider the following fluents:\\nf\\n1 means the ball is motionless\\nf2 means the ball is falling\\nf3 means the ball is on the floor\\nWe will also consider the following events:\\ne1 is the event that the ball is dropped\\ne2 is the event that the ball hits the floor\\nHence, we can start with the following expression:\\nInitially (f1)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 518, 'page_label': '519'}, page_content='492 CHAPTER 17 Advanced Knowledge Representation\\nbecause the ball starts out motionless.\\nNext we can say\\nHappens (e1, t1)\\nwhich tells us that the ball is dropped at time t1.\\nWe can also define the causal relationships involved in this scenario, by say-\\ning the following:\\nStarts (e1, f2, t1)\\nEnds (e1, f1, t1)\\nFinally, we can define the consequences of the ball hitting the floor:\\nHappens (e2, t2)\\nEnds (e2, f2, t2)\\nStarts (e2, f3, t2)\\nStarts (e2, f1, t2)\\nAn additional predicate is used to express a period of time over which\\nsomething occurs:\\nT (e, i)\\nThis means that event e took place throughout the interval defined by i.F o r\\nexample, we might say\\nT (Dropping (Ball), T oday)\\nwhich would mean that the ball started dropping on or before the stroke of\\nmidnight this morning and continued to drop for the entire day.\\nIt might be more useful to express the idea that the ball was dropping at\\nsome time today, for which we use the E predicate:\\nE (Dropping (Ball), T oday)\\n17.7.4 Mental Situation Calculus\\nThe situation calculus and event calculus are used to describe events and\\ntheir effects on the world. It is also useful to consider the effects that events\\nhave on an agent’s beliefs about the world. For this, we use mental situa-\\ntion calculus.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 519, 'page_label': '520'}, page_content='17.7 Reasoning about Change 493\\nThe following functions are used:\\nHolds (P, S) means that proposition P holds in situation S\\nBelieves (P) means that the agent believes proposition P\\nHence, we might write:\\nHolds (Believes (Fly (Pigs)), S)\\nThis means that it is true in situationS that the agent believes that pigs can fly.\\nWe also use a number of functions based around the idea of knowledge. It\\nis convenient to write all of these using the same symbol:\\nKnows (P)\\nIn fact, this can have a number of different meanings depending on the\\nnature of P.\\nFor example,\\nHolds (Knows (¬Knows (P)), S)\\nmeans that it is true in situationS that the agent knows that it does not know\\nP,w h e r eP is some individual concept (such as the whereabouts of the piece\\nof cheese for which the robot is searching, or T om’s telephone number).\\nAdditionally, the Knows function can be used to refer to knowledge about\\npropositions:\\nHolds (Knows (Fly (Pigs)), S)\\nThis means that it is true in situation S that the agent knows that pigs can\\nfly. Note that in this notation we are treating Fly (Pigs) as a fluent, which\\nmay vary over time: It may be true at the moment that pigs can fly, but\\ntomorrow they may forget how.\\nWe can extend the Believes function to allow it to express the idea that a\\nbelief exists for an interval of time:\\nBelieves (P, i)\\nwhich means that the agent believes proposition P during the entirety of\\nthe interval defined by i.\\nWe can also treat knowledge and beliefs as fluents. For example, we might\\nwant to say\\nT(Believes (Fly (Pigs), Y esterday), T oday)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 520, 'page_label': '521'}, page_content='494 CHAPTER 17 Advanced Knowledge Representation\\nwhich means it is true (for the whole of) today that throughout yesterday\\nthe agent believed that pigs could fly.\\nEvents can occur that change an agent’s beliefs. For this purpose, we define\\na point fluent as defining the fact that an event takes place at some\\nmoment in time. We write\\nOccurs (e, S)\\nto state that event e occurs in situation S.\\nWe can then define a new function:\\nLearns (P)\\nwhich means that the agent learns proposition P.\\nHence,\\nOccurs (Learns (P), S) → Holds (F (Knows (P)), S)\\nF (P) means that P will be true in the future. Hence, this sentence means\\nthat if in situation S the agent learns proposition P, then it is true that the\\nagent will know proposition P at some future time.\\n17.8 Knowledge Engineering\\nKnowledge engineering was introduced in Chapter 9, in the context of\\nexpert systems. In fact, knowledge engineering is an essential part of many\\nArtificial Intelligence systems.\\nAll systems that are based on propositional calculus, predicate calculus, sit-\\nuation calculus, event calculus, temporal logic, and other such languages\\nare primarily designed to manipulate knowledge. For those systems to per-\\nform useful tasks, knowledge needs to be gathered that can be entered into\\nthe system. Of course, in some systems, knowledge is gathered by an\\nautonomous agent, and no knowledge engineering is necessary. In many\\nArtificial Intelligence systems today, this is not the case, and a knowledge\\nengineer is an essential component of the system.\\nThe knowledge engineer’s task is to gather knowledge (knowledge acquisi-\\ntion) about the problem space and to convert this knowledge into a form\\nusable by the system (e.g., into first-order predicate calculus). The knowl-\\nedge engineer must also consider the level of detail to use. For example, in\\ndefining the properties of a building, it might be considered sufficient to'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 521, 'page_label': '522'}, page_content='17.9 Case-Based Reasoning 495\\nsimply say Building (P) to define P as representing a building. It might also\\nbe more useful to include details such as HasWindows (P, 6), HasFloors (P,\\n2), and HasRoof (P). Alternatively, it might make more sense to define these\\nproperties for all buildings:\\n∀x Building (x) → HasWindows (x) ∧ HasFloors (x) ∧ HasRoof (x)\\nThe knowledge engineer might then choose to include detail about the\\nnature of buildings in terms of bricks, wood, and steel, and might further\\ninclude details about the physical nature of these materials. In some cases,\\nthis detail might be superfluous. In other words, it is important for the\\nknowledge engineer to select the correct level of detail to include in the\\nknowledge base that is being built.\\nThe important principle is to select predicates, functions, and constants\\nthat match the problem to be solved. If a system is being designed to deter-\\nmine the best layout of windows in a building, where the desired answer is\\nsimply the number of windows to include on each wall, then having a con-\\nstant to represent each brick in the building would be unnecessary.\\n17.9 Case-Based Reasoning\\nCase-based reasoning was briefly introduced in Chapter 16, where it was\\ndiscussed in the context of planning. Case-based reasoning involves reusing\\npreviously identified solutions to solve new problems and is often used in\\nexpert systems, as well as in other types of systems, such as the checkers-\\nplaying system developed by Samuel (see Chapter 6).\\nA case-based reasoning system uses a memory that can store solutions to past\\nproblems, along with information about whether each solution was successful\\nor not. Such a system must therefore have the ability to look up a new prob-\\nlem, in order to find a previous case that was similar or identical to the current\\nproblem. Once such a case is found, the solution that was applied is modified\\nin order to apply it directly to the current problem. This solution is then stored\\nin the memory, along with information about whether it succeeded or failed.\\nFor a case-based system to function adequately, the representation of cases\\nmust be carefully considered. The details that are used to index each case\\nneed to be relevant and also must be able to distinguish the case from other,\\ndissimilar cases. The notion of similarity is important: what features mark\\ntwo cases as similar? This is not always obvious, and the features that are\\nused to define each case must be carefully selected.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 522, 'page_label': '523'}, page_content='496 CHAPTER 17 Advanced Knowledge Representation\\nCase-based systems make the task of knowledge acquisition relatively straight-\\nforward: the knowledge engineer simply needs to obtain examples of prob-\\nlems and their solutions (cases), which are entered into the system’s memory.\\nCases can be stored in a number of formats. For example, each case can be\\ndefined simply as a vector of the features that define the case and its solu-\\ntion. Alternatively, each case can be stored as a set of situated action rules (as\\nused in Brooks’ subsumption architecture, which is described in Chapter\\n19), each of which represents a solution to a particular situation (problem).\\nCase-based reasoning can be a very efficient way for a system to learn to solve\\nproblems, by examining its performance at solving past problems. As the sys-\\ntem encounters more cases, it becomes better able to solve new problems. Of\\ncourse, as the database of cases becomes larger, it becomes slower at retrieving\\ncases from the database, and so there is a trade-off between performance and\\nefficiency. It is possible to avoid this problem by only storing the most suc-\\ncessful solutions and “forgetting” solutions that were less successful. Samuel’s\\ncheckers program used this idea to remember only the “best” positions.\\n17.10 Chapter Summary\\n■ Knowledge representation is vital to Artificial Intelligence and has\\nbeen used extensively throughout this book.\\n■ The blackboard architecture is a structured knowledge representa-\\ntion that uses opportunistic reasoning to combine inputs from a\\nnumber of knowledge sources.\\n■ Scripts are used to represent situations (such as going to a restau-\\nrant) that often conform to a particular pattern.\\n■ The Copycat architecture is used to solve analogy problems of the\\nform “A is to B as C is to what?”\\n■ Classical logic is monotonic, which means that as new facts are\\nadded to a database, old conclusions are never contradicted. Many\\nreal-life situations require nonmonotonic reasoning.\\n■ The modal operator M is used to represent the idea that a proposi-\\ntion is consistent with our current beliefs.\\n■ Default reasoning uses assumptions about default values for cer-\\ntain variables, unless evidence to the contrary is found.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 523, 'page_label': '524'}, page_content='17.11 Review Questions 497\\n■ Truth maintenance systems are used to ensure that the facts con-\\ntained in a system’s database are consistent, in a nonmonotonic\\nreasoning environment.\\n■ The closed-world assumption is the assumption that any statement\\nnot explicitly known to be true is false.\\n■ The ramification problem is an extension of the frame problem.\\nThe ramification problem is the problem of dealing with small but\\npotentially significant side effects of actions.\\n■ Circumscription is a form of nonmonotonic reasoning that enables\\nus to deduce which facts are false, based on a limited set of statements.\\n■ Abductive reasoning involves determining a possible explanation\\nfor an observed phenomenon and is widely used by people and\\nArtificial Intelligence systems.\\n■ The Dempster–Shafer theory provides a way to reason about\\ndegrees of belief.\\n■ MYCIN uses certainty factors to reason about degrees of certainty.\\n■ T emporal logic is an extension of first-order predicate calculus,\\nwhich uses a set of modal operators to reason about change.\\n■ Event calculus is similar to situation calculus, but reasons about\\nfinite events.\\n■ Mental situation calculus allows us to reason about beliefs and\\nknowledge, and how they change over time.\\n■ Knowledge engineering is a vital element of many Artificial Intelli-\\ngence systems.\\n■ Case-based reasoning allows a system to learn from previous solu-\\ntions to problems, in order to solve new problems.\\n17.11 Review Questions\\n17.1 Explain why the blackboard architecture is an effective way to\\ncombine information from a number of knowledge sources.\\nDescribe the main components of the blackboard architecture.\\n17.2 Explain what kinds of problems the Copycat architecture can solve.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 524, 'page_label': '525'}, page_content='498 CHAPTER 17 Advanced Knowledge Representation\\n17.3 Explain what is meant by nonmonotonic reasoning, and explain\\nwhy it is so important in Artificial Intelligence. Explain the differ-\\nence between the terms nonmonotonic reasoning and nonmonotonic\\nlogic.\\n17.4 Explain the purpose of a truth maintenance system.\\n17.5 Explain what is meant by abductive reasoning. Explain your views\\non its usefulness in solving the following types of problems:\\na. solving logical puzzles\\nb. medical diagnosis\\nc. controlling the behavior of a robotic agent\\nd. understanding spoken human language\\n17.6 Compare and contrast the Dempster–Shafer theory and certainty\\nfactors.\\n17.7 Explain the idea behind temporal logic. What kinds of problems is\\nit useful for solving? Give three examples.\\n17.8 Can semantic networks be used to represent anything that can be\\nrepresented using temporal logic? Explain your answer.\\n17.9 Explain what is meant by knowledge engineering, and why it is\\nuseful for systems other than expert systems.\\n17.10 What is case-based reasoning? From which attributes of human\\nintelligence do you think it is derived? Describe the last time you\\nused case-based reasoning in your normal life.\\n17.12 Exercises\\n17.1 Download the Copycat demonstration applet, the details for which\\ncan be found on the Internet using any search engine. Examine the\\nfollowing problems, and observe how the Copycat system solves\\nthem. In each case, produce three suitable solutions yourself before\\nyou see what solutions Copycat comes up with. How often does it\\nfind the same solutions as you do?\\na. AABB is to AACC as JJKK is to what?\\nb. ABB is to ABCC as JKK is to what?\\nc. AABC is to AABD as IJKK is to what?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 525, 'page_label': '526'}, page_content='17.12 Exercises 499\\nd. BCD is to BCE as JFFWWW is to what?\\ne. A is to Z as EFG is to what?\\nf. FSF is to SFS as ABBBC is to what?\\n17.2 Use temporal logic to describe the following situation:\\nThere are three barbers in the shop. Each barber can shave either of\\nthe other two barbers but cannot shave himself. If a barber is not\\nshaving, then he sits and reads the newspaper. If a customer arrives\\nand a barber is free, then he will shave that customer. If a customer\\narrives and no barber is free, then the customer will sit and read the\\npaper until a barber is free. Each barber needs to be shaved once a\\nday.\\n17.3 Devise a representation for the following statement:\\nYesterday, Bob went to the cinema, and he saw the filmTitanic.A f t e r -\\nward, he went straight home, with thoughts of the film going through\\nhis head. Angela went to the cinema at the same time and saw the film\\nThe Lord of the Rings. After the film, Angela went for a swim.\\nNow add sufficient facts to the knowledge base you have created to\\nenable an Artificial Intelligence system to answer the following\\nquestions:\\nDid Bob meet Angela yesterday?\\nDid Bob and Angela leave the cinema at the same time?\\nDid Bob and Angela spend time together after the films?\\nDid Bob enjoy the film?\\nY ou will need to add basic facts to the knowledge base such as:\\nLord of the Rings is longer than Titanic.\\nLord of the Rings started at the same time as Titanic.\\nSome of these facts will be common sense, and others you will\\nneed to invent to give reasonable answers to the questions.\\nHow do you use the facts in the knowledge base to derive answers\\nto the questions?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 526, 'page_label': '527'}, page_content='500 CHAPTER 17 Advanced Knowledge Representation\\n17.13 Further Reading\\nLuger (1995) provides an excellent range of papers on the subject of Artifi-\\ncial Intelligence in general, and in particular it has a number of papers that\\nare relevant to this chapter. Russell and Norvig (1995) have a great deal of\\ncoverage of knowledge representation and knowledge engineering, prima-\\nrily in the context of intelligent agents.\\nAdditional references for MYCIN are contained in the Further Reading sec-\\ntion of Chapter 9 of this book.\\nNonmonotonic Reasoning, by Grigoris Antoniou (1997 – MIT Press)\\nA Logical Theory of Nonmonotonic Inference and Belief Change , by Alexan-\\nder Bochman (2001 – Springer V erlag)\\nNonmonotonic Reasoning : An Overview , by Gerhard Brewka, Jürgen Dix,\\nand Kurt Konolige (1995 – Cambridge University Press)\\nNonmonotonic Reasoning : From Theoretical Foundation to Efficient Compu-\\ntation, by G. Brewka (1991 – Cambridge University Press)\\nA Generalization of Bayesian Inference, by A. P . Dempster (1968 - in Journal\\nof the Royal Statistical Society)\\nA Truth Maintenance System, by Jon Doyle (1979 – in Computation & Intel-\\nligence – Collected Readings, edited by George F. Luger, The MIT Press)\\nProbabilistic Interpretations for Mycin’s Certainty Factors, by O. Heckerman\\n(1986 – in Uncertainty in Artificial Intelligence , edited by L. N. Kanal and\\nJ. F. Lemmer, Elsevier Science Ltd., pp. 167–196)\\nHandbook of Logic in Artificial Intelligence and Logic Programming: Non-\\nmonotonic Reasoning and Uncertain Reasoning , edited by Dov M. Gab-\\nbay, J. A. Robinson, and Christopher John Hogger (1994 – Oxford\\nUniversity Press)\\nCase-Based Reasoning, by Janet Kolodner (1993 – Morgan Kaufmann)\\nCase-Based Reasoning: Experiences, Lessons, and Future Directions, edited by\\nDavid B. Leake (1996 –AAAI Press)\\nFor the Sake of the Argument : Ramsey Test Conditionals, Inductive Infer-\\nence and Nonmonotonic Reasoning , by Isaac Levi (1996 – Cambridge Uni-\\nversity Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 527, 'page_label': '528'}, page_content='17.13 Further Reading 501\\nNonmonotonic Logic: Context-Dependent Reasoning, by V . W. Marek and M.\\nTruszczynski (1993 – Springer V erlag)\\nCircumscription: A Form of Non-Monotonic Reasoning , by John McCarthy\\n(1980 – in Computation & Intelligence – Collected Readings , edited by\\nGeorge F. Luger, The MIT Press)\\nA Production System Version of the Hearsay-II Speech Understanding System,\\nby Donald McCracken (1981 - UMI Research)\\nBlackboard Systems: The Blackboard Model of Problem Solving and the Evolu-\\ntion of Blackboard Architectures, by H. Penny Nii (1986 – inComputation &\\nIntelligence – Collected Readings, edited by George F. Luger, The MIT Press)\\nSoft Computing in Case Based Reasoning, edited by Sankar K. Pal, Tharam S.\\nDillon, and Daniel S. Y eung (2000 – Springer V erlag)\\nInside Case-Based Reasoning , by Christopher K. Riesbeck and Roger C.\\nSchank (1989 – Lawrence Erlbaum)\\nChange, Choice and Inference: A Study of Belief Revision and Nonmonotonic\\nReasoning, by Hans Rott (2002 – Oxford University Press)\\nThe Structure of Episodes in Memory , by Roger C. Schank (1975 – in Com-\\nputation & Intelligence – Collected Readings, edited by George F. Luger, The\\nMIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 528, 'page_label': '529'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 529, 'page_label': '530'}, page_content='18CHAPTER\\nFuzzy Reasoning\\nAnd new philosophy calls all in doubt,\\nThe element of fire is quite put out;\\nThe sun is lost, and th’earth, and no man’s wit\\nCan well direct him, where to look for it.\\n—John Donne, An Anatomy of the World\\nTo be, or not to be: that is the question.\\n—William Shakespeare, Hamlet\\nI used to love mathematics for its own sake, and I still do, because it allows for\\nno hypocrisy and no vagueness, my two bêtes noires.\\n—Henri Beyle Stendahl, La Vie d’Henri Brulard\\n18.1 Introduction\\nThis chapter introduces the idea of fuzzy sets and fuzzy logic. The chapter\\nexplains how fuzzy sets are defined and explains how linguistic variables,\\nfuzzy operators, and hedges are applied. It also explains the concepts of\\nfuzzy logic and how they can be applied in solving real-world problems.\\nThis chapter explains how fuzzy expert systems can be built, as well as neuro-\\nfuzzy systems, which are a cross between neural networks and fuzzy systems.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 530, 'page_label': '531'}, page_content='504 CHAPTER 18 Fuzzy Reasoning\\n18.2 Bivalent and Multivalent Logics\\nIn classical logic, which is often described as Aristotelian logic, there are\\ntwo possible truth values: propositions are either true or false. Such systems\\nare known as bivalent logics because they involve two logical values.\\nThe logic employed in Bayesian reasoning and other probabilistic models is\\nalso bivalent: each fact is either true or false, but it is often unclear whether\\na given fact is true or false. Probability is used to express the likelihood that\\na particular proposition will turn out to be true.\\nOne early multivalent logic was used to reason about the Uncertainty Prin-\\nciple, used in quantum physics. This logic had three values: true, false, and\\nundetermined.\\nAn extension of this three-valued logic is to consider 0 to represent false, 1\\nto represent true, and to use real numbers between 0 and 1 to represent\\ndegrees of truth.\\nNote that this is not the same as probability: if a fact has a probability value\\nof 0.5, then it is as likely to be true as it is to be false, but in fact it will only\\nbe either true or false. If in a multivalent logic we have a proposition that\\nhas a logical value of 0.5, we are saying something about the degree to\\nwhich that statement is true. In probability theory we are dealing with\\nuncertainty (at the moment we don’t know whether the proposition will be\\ntrue or false, but it will definitely either be true or false—not both, not nei-\\nther, and not something in between), but with multivalent logic we are cer-\\ntain of the truth value of the proposition; it is just vague—it is neither true\\nnor false, or it is both true and false.\\nAlthough this kind of logic may sound absurd, in this chapter we will see\\nhow it can be put to practical use and indeed how multivalent logics, and in\\nparticular fuzzy logic, have become an extremely important part of Artifi-\\ncial Intelligence.\\n18.3 Linguistic Variables\\nIn fuzzy set theory and fuzzy logic, we make great use of linguistic vari-\\nables. A linguistic variable is a concept such as “height, ” which can have a\\nvalue from a range of fuzzy values including “tall,” “short,” and “medium.”\\nThe linguistic variable “height” may be defined over the universe of dis-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 531, 'page_label': '532'}, page_content='18.4 Fuzzy Sets 505\\n1\\n0\\n4 ft 8 ft Height\\nDegree of\\nmembersip\\nof the fuzzy\\nset of tall\\npeople\\nFigure 18.1\\nChart showing the mem-\\nbership function for the\\nfuzzy set of tall people\\ncourse from 2 feet up to 8 feet. As we will see, the values “tall, ” “short, ” and\\n“medium” define subsets of this universe of discourse.\\n18.4 Fuzzy Sets\\nFuzzy logic is used to reason about fuzzy sets. Fuzzy sets contrast with the\\nsets used in traditional set theory, which are sometimes known as crisp\\nsets. A crisp set can be defined by the values that are contained within it. A\\nvalue is either within the crisp set, or it is not. For example, the set of natu-\\nral numbers is a crisp set: 1, 2, 3, 4, and so on are natural numbers and so\\nare definitely members of the set of natural numbers. Numbers such as 0.2,\\n101.101, and /H9266are definitely not members of the set of natural numbers.\\nOn the other hand, let us consider the set of tall people. Bill is 7 feet tall,\\nand so it is pretty clear that he is included in the set of tall people. John is\\nonly 4 feet tall, and so most would say that he is not included in the set.\\nWhat about Jane, who is 5 feet 10 inches tall? Some would certainly say she\\nis tall, but others would say she is not.\\nThe fuzzy set of tall people contains Bill, and it also contains Jane, and it\\neven contains John. Each is a member of the set to some degree and is not a\\nmember of the set to some degree. This can be seen in the chart in Figure\\n18.1, which shows the degree of membership that a person of a given height\\nhas in the fuzzy set of tall people.\\nThis definition of a fuzzy set is extremely natural and fits much better with\\nthe way people really talk about things. It is very common to say of some-\\none that she is “fairly tall” or “not very tall” but actually quite unusual to use\\nthe unqualified descriptions “tall” or “not tall. ” This is because each person'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 532, 'page_label': '533'}, page_content='506 CHAPTER 18 Fuzzy Reasoning\\nBaby Child Adult Teenager\\nDegree of\\nmembership\\n1\\n0\\n0 100 Age\\nFigure 18.2\\nGraph showing member-\\nship of the fuzzy sets baby,\\nchild, teenager, and adult\\nhas his or her own idea of what tall means, and in fact our definitions of tall\\nare not precise—if we were asked to define a group of people as either tall\\nor not tall, and then asked to repeat the exercise, we might well classify one\\nperson as tall on the first occasion and as not tall on the second occasion.\\nThis is modeled very clearly in the fuzzy set, which defines each person as\\nbeing both tall and not tall, to some extent.\\nY ou may recall from Section 7.20 that we defined the law of the excluded\\nmiddle, which is a fundamental rule of classical logic, and which states that\\na proposition must either be true or false: it cannot be both true and false,\\nand it is not possible for a statement to be neither true nor false. This is the\\nbasis of Aristotelian logic, but as we will see, in fuzzy logic, a statement can\\nbe both true and false, and also can be neither true nor false. Whereas in\\nclassical logic we can state axioms such as\\nA\\n∨¬ A = TRUE\\nA ∧¬ A = FALSE\\nin fuzzy logic these do not hold—A ∨¬ A can be, to some extent, false, and\\nA ∧¬ A can to some extent be true: the law of the excluded middle does not\\nhold in fuzzy logic.\\nThe idea of the intersection between crisp sets is easy to understand: if an\\nitem is in set A and is also in set B, then it is in the intersection of sets A and\\nB. Similarly, we can define an intersection between fuzzy sets. Consider the\\nfuzzy sets whose membership functions are shown in Figure 18.2.\\nFigure 18.2 shows the membership functions for the fuzzy sets baby, child,\\nteenager, and adult. Note that there are intersections between baby and'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 533, 'page_label': '534'}, page_content='18.4 Fuzzy Sets 507\\nchild, between child and teenager, and between teenager and adult. Note\\nthat at some age, let us say 12, a person might be defined as all of the fol-\\nlowing: a child, not a child, a teenager, and not a teenager. Our definitions\\nof the sets do not allow a person to be a child and an adult at the same time,\\nbut we could easily redefine the sets such that a person could be to some\\nextent a child and at the same time to some extent an adult.\\n18.4.1 Fuzzy Set Membership Functions\\nA fuzzy set A is defined by its membership function, MA.\\nFor example, we might define the membership functions for the fuzzy sets\\nB and C (baby and child) as follows:\\nSimilarly, we could define membership functions for fuzzy setsT (teenager)\\nand A (adult). Note that there is nothing special about these functions—\\nthey have been chosen entirely arbitrarily and reflect a subjective view on\\nthe part of the author. Different functions could very well be chosen for\\nM\\nB(x) and MC(x), which would equally reasonably define those sets.\\nT o represent a fuzzy set in a computer, we use a list of pairs, where each pair\\nrepresents a value and the fuzzy membership value for that value. Hence,\\nwe write the fuzzy set A as\\nA = {(x\\n1, MA(x1) ) ,...,( xn, MA(xn))}\\nFor example, we might define B, the fuzzy set of babies as follows:\\nB = {(0, 1), (2, 0)}\\nThis can also be thought of as representing the x and y coordinates of two\\npoints on the line, which represents the set membership function, as shown in\\nFigure 18.2. Similarly, we could define the fuzzy set of children,C, as follows:\\nC = {(1, 0), (7, 1), (8, 1), (14, 0)}\\nMx\\nx for x\\nfor x\\nMx\\nx for x\\nfor x and x\\nx for x\\nB\\nC\\n( ) = −≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n( ) =\\n− ≤\\n>≤\\n− >\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n1 2 2\\n02\\n1\\n6 7\\n17 8\\n14\\n6 8'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 534, 'page_label': '535'}, page_content='508 CHAPTER 18 Fuzzy Reasoning\\n18.4.2 Fuzzy Set Operators\\nTraditional set theory (developed by Georg Cantor in the 19th century)\\nuses a number of operators that can be applied to sets A and B:\\nNot A the complement of A, which contains the elements that are\\nnot contained in A\\nA ∩ B the intersection of A and B, which contains those elements\\nthat are contained in both A and B\\nA ∪ B the union of A and B, which contains all the elements of A\\nand all the elements of B\\nWe can think of these as being related to the logical operators, ¬, ∧, and ∨.\\nNaturally, the set “Not A” is the same as ¬A. The intersection of A and B is\\nthe same as the conjunction of A and B: A ∧ B. Similarly, the union of A\\nand B is the same as the disjunction of A and B: A ∨ B.\\nAs a result, the set operators are commutative, associative, and distributive,\\nas we would expect, and they obey DeMorgan’s laws:\\n¬(A ∪ B) = ¬A ∩ ¬B\\n¬(A ∩ B) = ¬A ∪ ¬B\\nWe can define similar operators for fuzzy sets. The complement of fuzzy set\\nA, whose membership function is MA is defined as\\nM¬A(x) = 1 /H11002MA(x)\\nThus, we could define the set of not-babies, ¬B, as follows:\\nM¬B(x) = 1 /H11002MB(x)\\nSo,\\n¬B = {(0, 0), (2, 1)\\nSimilarly, we can define ¬C:\\n¬C = {{(1, 1), (7, 0), (8, 0), (14, 1)}\\nFor each x, we have defined M¬C(x) as being 1 /H11002MC(x).\\nWe can now define fuzzy intersection of two sets as being the minimum of\\nthe fuzzy membership functions for the sets. That is,\\nMA ∩ B (x) = MIN (MA (x), MB (x))\\nSo, for example, let us determine the intersection of B and C, babies and\\nchildren:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 535, 'page_label': '536'}, page_content='18.4 Fuzzy Sets 509\\nRecall that we define B and C as follows:\\nB = {(0, 1), (2, 0)}\\nC = {(1, 0), (7, 1), (8, 1), (14, 0)}\\nT o determine the intersection, we need to have the sets defined over the\\nsame values; hence, we augment set B:\\nB = {(0, 1), (1, 0.5), (2, 0), (7, 0), (8, 0), (14, 0)}\\nSimilarly, we augment C:\\nC = {(0, 0), (1, 0), (2, 0.166), (7, 1), (8, 1), (14, 0)}\\nNow we can find the intersection, by using\\nMB ∩ C (x) = MIN (MB (x), MC (x))\\n∴ B ∩ C = {(0, 0), (1, 0), (2, 0), (7, 0), (8, 0), (14, 0)}\\nBut this has not worked! Clearly we need to define the set using values that\\nwill correctly define the ranges. In other words, we can correctly define B ∩\\nC as follows:\\nB ∩ C = {(1, 0), (1.75, 0.125), (2, 0)}\\nwhere 1.75 was used as the value for x. This was determined by calculating\\nthe value of x for which MB(x) = MC(x).\\nLet us consider for a moment what the intersection of two fuzzy sets actually\\nmeans. As we said previously,B ∩ C can be thought of as being similar to B\\n∧ C. If a person is in the setB ∩ C, then she isboth a baby And a child. So the\\nintersection of two sets is the set of elements that belong to both those two\\nsets, or the elements that belong to the conjunction of the two sets.\\nSimilarly, we can define the union of two fuzzy sets A and B as follows:\\nM\\nA ∪ B (x) = MAX (MA (x), MB (x))\\nHence, the union of the fuzzy sets of babies and children is as follows:\\nB ∪ C = {(0, 1), (1.75, 0.25), (7, 1), (8, 1), (14, 0)}\\nAgain, recall that the union B ∪ C is similar to the disjunction B ∨ C.A  p e r -\\nson who belongs to the set B ∪ C is either a baby Or a child.\\nLet us consider one final fuzzy set operator—containment.\\nIn traditional set theory, if crisp set A contains crisp set B, then this means\\nthat all elements of set B are also elements of set A. In other words, the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 536, 'page_label': '537'}, page_content='510 CHAPTER 18 Fuzzy Reasoning\\nA\\nDegree of\\nmembership\\n1\\n0\\n0 100 Age\\nP\\nFigure 18.3\\nMembership functions for\\nthe fuzzy sets adults (A)\\nand pensioners (P)\\nunion A ∪ B = A and the intersection A ∩ B = B. In this case,B is said to be\\na subset of A, which is written A ⊂ B.\\nT o see how fuzzy subsets work, let us consider a new fuzzy set, P, which is\\nthe fuzzy set of pensioners. We will define this set by the following mem-\\nbership function:\\nLet us suppose in this case that we are considering the universe of people to\\nrange over the ages between 0 and 100 (not to exclude people over the age\\nof 100, but simply to make the mathematics a little simpler).\\nIn Figure 18.3 we can see the membership functions for A and P.\\nThe intersection of A and P, A ∩ P, can be seen clearly from this diagram to\\nbe P.H e n c e ,P is a subset of A,o r  A ⊂ P.\\nThe definition of fuzzy containment is as follows:\\nB ⊂ A iff\\n∀x (MB (x) ≤ MA (x))\\nIn other words,B is a fuzzy subset ofA if B’s membership function is always\\nsmaller than (or equal to) the membership function for A.\\n18.4.3 Hedges\\nA hedge is a fuzzy set qualifier, such as “very,” “quite,” “extremely,” or\\n“somewhat. ” When one of these qualifiers is applied to a fuzzy set, such as\\n“tall people, ” we produce a new set. For example, by applying the “very”\\nMx\\nfor x\\nx for xp ( ) =\\n≤\\n− >\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n05 5\\n55\\n45 55'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 537, 'page_label': '538'}, page_content='18.5 Fuzzy Logic 511\\nhedge to “tall people, ” we produce a subset of “tall people” called “very tall\\npeople. ” Similarly we can produce a new subset of “quite tall people” or\\n“somewhat tall people. ”\\nThe meanings of these hedges are fairly subjective, as are the meanings of\\nfuzzy sets themselves. However, it is usual to use a systematic mathematic\\ndefinition for the hedges so that they can be applied logically.\\nOften a hedge is applied by raising the set’s membership function to an\\nappropriate power. For example, it is common to consider the “very” hedge\\nto square the value of the membership function. For example, if M\\nA is the\\nmembership function for fuzzy set A of tall people, then the membership\\nfunction for VA, the fuzzy set of very tall people is\\nMVA (x) = (MA (x))2\\nSimilarly, we can define hedges such as “quite, ”“somewhat, ” and “extremely, ”\\nas raising the membership function to powers of 1.3, 0.5, and 4, respectively.\\nHence, if Jane has a fuzzy membership value of the “tall people” set of 0.6,\\nthen she has a membership value of “very tall people” of 0.6 2 = 0.36; a\\nmembership value of “quite tall people” of 0.6 1.3 = 0.515; a membership\\nvalue of “somewhat tall people” of 0.60.5 = 0.775; and a membership value\\nof “extremely tall people” of 0.64 = 0.1296.\\nNote that while hedges such as “very, ”“extremely, ” and “quite” define a sub-\\nset of a fuzzy set, hedges such as “somewhat” or “more or less” expand the\\nset to which they are applied. A person who is not at all tall, for example,\\nmight be defined as being, to some extent, “somewhat tall. ”\\n18.5 Fuzzy Logic\\nFuzzy logic is a form of logic that applies to fuzzy variables. Fuzzy logic is non-\\nmonotonic, in the sense that if a new fuzzy fact is added to a database, this fact\\nmay contradict conclusions that were previously derived from the database.\\nWe have already seen that the functions MAX and MIN can be used with\\nfuzzy sets to calculate the intersection and union of two fuzzy sets. Simi-\\nlarly, the same functions can be used in fuzzy logic to calculate the disjunc-\\ntion or conjunction of two fuzzy variables.\\nEach fuzzy variable can take a value from 0 (not at all true) to 1 (entirely\\ntrue) but can also take on real values in between. Hence, 0.5 might indicate\\n“somewhat true, ” or “about as true as it is false. ”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 538, 'page_label': '539'}, page_content='512 CHAPTER 18 Fuzzy Reasoning\\nIf A and B are fuzzy logical values, then we can define the logical connec-\\ntives ∧ and ∨ as follows:\\nA ∨ B ≡ MAX (A, B)\\nA ∧ B ≡ MIN (A, B)\\nSimilarly, we can define negation as follows:\\n¬A ≡ 1 /H11002A\\nRecall from Chapter 7 that we can define any binary logical connective\\nusing just ¬ and ∧ . Hence, we can define any fuzzy logic connective using\\njust MIN and the function f (x) = 1 /H11002x.\\nClearly, we cannot write a complete truth table for a fuzzy logical connec-\\ntive because it would have an infinite number of entries. We can, however,\\nproduce a fuzzy truth table for a finite set of input values. For example, we\\ncould consider the set {0, 0.5, 1}, which would be used in a multivalent logic\\nthat had three logical values. Hence,\\nABA  ∨ B\\n000\\n0 0.5 0.5\\n011\\n0.5 0 0.5\\n0.5 0.5 0.5\\n0.5 1 1\\n101\\n1 0.5 1\\n111\\nWe could similarly draw up truth tables for \\n∧ and the other logical connec-\\ntives. Consider the following, which is the three-valued truth table for ¬:\\nA ¬A\\n01\\n0.5 0.5\\n10'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 539, 'page_label': '540'}, page_content='18.5 Fuzzy Logic 513\\nNote in particular that ifA = 0.5, then A = ¬A. The extent to which A is true\\nis the same as the extent to which it is false. This is a fundamental aspect of\\nfuzzy logic and is a feature that would be entirely anathema to the thinking\\nof most classical logicians.\\nNow let us look at defining fuzzy logical implication, or →. Recall from\\nChapter 7 that in classical logic → is defined by the following:\\nA → B\\n≡¬ A ∨ B\\nHence, it would seem natural to define fuzzy implication as follows:\\nA → B ≡ MAX ((1 /H11002A), B)\\nLet us now examine the truth table for this function:\\nABA → B\\n001\\n0 0.5 1\\n011\\n0.5 0 0.5\\n0.5 0.5 0.5\\n0.5 1 1\\n100\\n1 0.5 0.5\\n111\\nIt is interesting to note that using this definition of implication, 0.5 → 0 =\\n0.5. This is somewhat counterintuitive because we would expect 0.5 → 0 =\\n0. Also, we have the counterintuitive statement that 0.5 → 0.5 = 0.5,\\nwhereas we would expect 0.5 → 0.5 = 1.\\nAs a result of this, a number of alternative definitions for fuzzy implication\\nhave been proposed. One such definition is known as Gödel implication,\\nwhich is defined as follows:\\nA → B\\n≡ (A ≤ B) ∨ B\\nUsing this definition, we can draw up an alternative fuzzy truth table for →\\nover three logical values as follows:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 540, 'page_label': '541'}, page_content='514 CHAPTER 18 Fuzzy Reasoning\\nABA → B\\n001\\n0 0.5 1\\n011\\n0.5 0 0\\n0.5 0.5 1\\n0.5 1 1\\n100\\n1 0.5 0.5\\n111\\nThis table seems more intuitive.\\nNow let us consider modus ponens, the logical rule we saw in Section 7.11.4:\\nIn fuzzy logic this rule also holds. We will now examine, by drawing up a\\ntruth table, whether it also holds for three-valued fuzzy logic.\\nABA → B( A ∧ (A → B)) → B\\n001 1\\n0 0.5 1 1\\n011 1\\n0.5 0 0 1\\n0.5 0.5 1 0.5\\n0.5 1 1 1\\n100 1\\n1 0.5 0.5 1\\n111 1\\nAA B\\nB\\n→'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 541, 'page_label': '542'}, page_content='18.6 Fuzzy Logic as Applied to Traditional Logical Paradoxes 515\\nWe have drawn up this truth table using our original, less satisfactory defi-\\nnition of →, and as a result, we have found that modus ponens does not\\nquite hold. If A = 0.5 and B = 0.5, then we have\\n(A ∧ (A → B)) → B = 0.5\\nAssuming we want modus ponens to hold, then this is not satisfactory\\nbecause we would want to obtain\\n(A ∧ (A → B)) → B = 1\\nIf we draw up the equivalent truth table but use Gödel implication, then we\\nfind that each row in the truth table has a final value of 1, as we would\\nexpect, and thus modus ponens holds.\\n18.6 Fuzzy Logic as Applied to Traditional Logical Paradoxes\\nThere are a number of well-known paradoxes in classical logic: problems\\nthat cannot be solved using propositional logic because they lead to a con-\\nclusion that contradicts one or more of the premises. For example, Rus-\\nsell’s paradox can be stated as follows:\\nA barber, who himself has a beard, shaves all men who do not shave\\nthemselves. He does not shave men who shave themselves.\\nWe now ask the following question: Who shaves the barber? If he shaves\\nhimself, then according to the second sentence in the statement above, he\\ncannot shave himself. But if he does not shave himself, then the first sen-\\ntence above tells us that he does shave himself.\\nThis paradox exemplifies the law of the excluded middle—the problem\\narises due to the fact that we cannot have A\\n∧¬ A. In fuzzy logic, this prob-\\nlem does not exist, and Russell’s paradox is not a paradox: the barber both\\nshaves himself and does not shave himself.\\nSimilarly, consider another commonly discussed paradox:\\n“All Cretans are liars, ” said the Cretan.\\nIf the Cretan is a liar, as his claim would suggest, then his claim cannot be\\nbelieved, and so he is not a liar. But if he is not a liar, then he is telling the\\ntruth, and all Cretans are liars. But because he is a Cretan, he must therefore\\nbe a liar. Again, this is a paradox that can be resolved by using fuzzy logical'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 542, 'page_label': '543'}, page_content='516 CHAPTER 18 Fuzzy Reasoning\\nvalues, instead of the two logical values “true” and “false. ” The Cretan’s\\nstatement is true and false, to some extent, at the same time.\\nThis makes perfect sense: when the Cretan says that all Cretans are liars, it\\nis unlikely that he is really speaking of every single Cretan. It is also unlikely\\nthat he really means that every Cretan lies every time he opens his mouth.\\nHence, his statement has a fuzzy truth value somewhere below 1, but some-\\nwhere above 0.\\n18.7 Fuzzy Rules\\nWe will now consider fuzzy rules, which are the fuzzy equivalent of the\\nrules we used in Chapter 9, when we considered expert systems.\\nThe rules we saw in Chapter 9 had the following form:\\nIF A THEN B\\nA fuzzy rule has the form\\nIF A = x then B = y\\nIn fact, to be more precise, a fuzzy rule can take the following form:\\nIF A op x then B = y\\nWhere op is some mathematical operator (such as =, >, or <)\\nHence, we might have fuzzy rules such as the following:\\nIF temperature > 50 then fan speed = fast\\nIF height = tall then trouser length = long\\nIF study time = short then grades = poor\\nBy using fuzzy inference, which is explained in the next section, an expert\\nsystem can be built based around fuzzy rules such as these.\\n18.8 Fuzzy Inference\\nAn alternative to Gödel implication called Mamdani implication (or Mam-\\ndani inference) is often used in fuzzy systems. Mamdani inference allows a\\nsystem to take in a set of crisp input values (from a set of sensors or inputs\\nfrom a human operator, for example) and apply a set of fuzzy rules to those\\nvalues, in order to derive a single, crisp, output value or action recommen-\\ndation. Mamdani inference was invented by Professor Ebrahim Mamdani\\nin the 1970s and was used by him to control a steam engine and boiler.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 543, 'page_label': '544'}, page_content='18.8 Fuzzy Inference 517\\nWe will now examine a simple example to see how this form of reason-\\ning works.\\nLet us suppose that we are designing a simple braking system for a car,\\nwhich is designed to cope when the roads are icy and the wheels lock.\\nThe rules for our system might be as follows:\\nRule 1 IF pressure on brake pedal is medium\\nTHEN apply the brake\\nRule 2 IF pressure on brake pedal is high\\nAND car speed is fast\\nAND wheel speed is fast\\nTHEN apply the brake\\nRule 3 IF pressure on brake pedal is high\\nAND car speed is fast\\nAND wheel speed is slow\\nTHEN release the brake\\nRule 4 IF pressure on brake pedal is low\\nTHEN release the brake\\nT o apply these rules, using Mamdani inference, the first step is to fuzzify\\nthe crisp input values.\\nT o do this, we need first to define the fuzzy sets for the various linguistic\\nvariables we are using.\\nFor this simple example, we will assume that brake pressure is measured\\nfrom 0 (no pressure) to 100 (brake fully applied). We will define brake pres-\\nsure as having three linguistic values: high ( H), medium (M), and low (L),\\nwhich we will define as follows:\\nH = {(50, 0), (100, 1)}\\nM = {(30, 0), (50, 1), (70, 0)}\\nL = {(0, 1), (50, 0)}\\nFigure 18.4 shows the membership functions for these three fuzzy sets.\\nLet us suppose that the pressure value in a given situation is in fact 60. This\\ncorresponds to fuzzy membership values for the three sets of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 544, 'page_label': '545'}, page_content='518 CHAPTER 18 Fuzzy Reasoning\\n1\\n0 100\\nPressure\\nM\\nHL\\nFigure 18.4\\nGraph showing member-\\nship functions for fuzzy\\nvariable pressure\\nML(60) = 0\\nMM(60) = 0.5\\nMH(60) = 0.2\\nSimilarly, we must consider the wheel speed. We will define the wheel speed\\nas also having three linguistic values: slow, medium, and fast. We will define\\nthe membership functions for these values for a universe of discourse of\\nvalues from 0 to 100:\\nS = {(0, 1), (60, 0)}\\nM = {(20, 0), (50, 1), (80, 0)}\\nF = {(40, 0), (100, 1)}\\nIf the wheel speed is in fact 55, then this gives us membership values as follows:\\nM\\nS(55) = 0.083\\nMM(55) = 0.833\\nMF(55) = 0.25\\nFor the sake of simplicity, we will define the linguistic variable car speed\\nusing the same linguistic values ( S, M, and F for slow, medium, and fast),\\nusing the same membership functions. Clearly, in a real system, the two\\nwould be entirely independent of each other.\\nLet us suppose now that the car speed is 80, which gives us the following\\nmembership values:\\nM\\nS(80) = 0'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 545, 'page_label': '546'}, page_content='18.8 Fuzzy Inference 519\\nMM(80) = 0\\nMF(80) = 0.667\\nWe now need to apply these fuzzy values to the antecedents of the sys-\\ntem’s rules.\\nRule 1, taken on its own, tells us that the degree to which we should apply\\nthe brake is the same as the degree to which the pressure on the brake pedal\\ncan be described as “medium. ”\\nWe saw above that the pressure is 60 and that M\\nM(60) = 0.5. Hence, Rule 1\\ngives us a value of 0.5 for the instruction “Apply the brake. ”\\nRule 2 uses an AND:\\nIF pressure on brake pedal is high\\nAND car speed is fast\\nAND wheel speed is fast\\nTHEN apply the brake\\nThe membership functions for the three parts of the antecedent are\\nM\\nH(60) = 0.2\\nMF(80) = 0.667\\nMF(55) = 0.25\\nUsually, the conjunction of two or more fuzzy variables is taken to be the\\nminimum of the various membership values. Hence, the antecedent for\\nRule 2 in this case has the value 0.2. Thus, Rule 2 is giving us a fuzzy value\\nof 0.2 for “Apply the brake.”\\nSimilarly, we evaluate Rules 3 and 4:\\nRule 3 M\\nH(60) = 0.2\\nMF(80) = 0.667\\nMS(55) = 0.083\\nHence, Rule 3 gives a value of 0.083 for “Release the brake. ”\\nRule 4 ML(60) = 0\\nHence, Rule 4 gives us a fuzzy value of 0 for “Release the brake. ”\\nNow we have four fuzzy values: 0.5 and 0.2 for “Apply the brake” and 0.083\\nand 0 for “Release the brake. ”'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 546, 'page_label': '547'}, page_content='520 CHAPTER 18 Fuzzy Reasoning\\n1\\n0 100\\nPressure\\nAR\\nFigure 18.5\\nMembership functions for\\n“Apply the brake” (A) and\\n“Release the brake” (R)\\nWe now need to combine these values together.\\nFirst, let us see what we mean by “Apply the brake” and “Release the brake. ”\\nFigure 18.5 shows fuzzy membership functions for “Apply the brake” ( A)\\nand “Release the brake” (R), which show the degree of pressure the brake\\nshould apply to the wheel for each value of these variables.\\nT o put that another way, the x-axis of the graph in Figure 18.5 shows the\\npressure applied by the brake to the wheel, and they-axis shows the degree to\\nwhich “Apply the brake” and “Release the brake” are true (M[A] and M[R]).\\nT o apply the rules, we first need to decide how to combine the differing val-\\nues for each of the two fuzzy variables. We have 0.2 and 0.5 for “Apply the\\nbrake” and 0.083 and 0 for “Release the brake. ” We could sum the values or\\ntake the minimum or take the maximum. The appropriate combination\\nwill depend on the nature of the problem being solved. In this case it makes\\nsense to sum the values because the separate rules are giving different rea-\\nsons for applying or releasing the brakes, and those reasons should com-\\nbine together cumulatively.\\nHence, we end up with a value of 0.7 for “Apply the brake” and 0.083 for\\n“Release the brake. ”\\nThe next step is to clip the membership functions of the two variables to\\nthese values, as is shown in Figure 18.6.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 547, 'page_label': '548'}, page_content='18.8 Fuzzy Inference 521\\n1\\n0.7\\n0.083\\n0 100\\nPressure\\nAR\\nFigure 18.6\\nShowing how the fuzzy\\nvalues for the antecedents\\nof the rules are applied to\\nthe consequents\\nIn Figure 18.6, the membership function for A has been clipped at 0.7, and\\nthe membership function for R has been clipped at 0.083. The resulting\\nshape is the shaded area under the two clipped lines and shows the com-\\nbined fuzzy output of the four rules.\\nT o use this fuzzy output, a crisp output value must now be determined\\nfrom the fuzzy values. This process of obtaining a crisp value from a set of\\nfuzzy variables is known as defuzzification. This can be done by obtaining\\nthe center of gravity (or centroid) of the shaded shape shown in Figure\\n18.6.\\nThe formula for the center of gravity, C, is as follows:\\nwhere M\\nA(x) is the membership function illustrated by the shaded area in\\nFigure 18.6.\\nIn fact, the center of gravity should really be calculated as a continuous\\nintegral, but if we use a discrete sum over a reasonable selection of values,\\nwe can obtain an answer that is close enough. In fuzzy systems it is not usu-\\nally necessary to be accurate to several decimal places, but rather to obtain\\na value in the right range.\\nC Mx x\\nMx\\nA\\nA\\n= ( )\\n( )\\n∑\\n∑'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 548, 'page_label': '549'}, page_content='522 CHAPTER 18 Fuzzy Reasoning\\nHence, we can calculate the center of gravity of the shaded shape in Figure\\n18.6 as follows:\\nHence, the crisp output value for this system is 68.13, which can be trans-\\nlated into the pressure applied by the brake to the wheel in the car.\\n18.9 Fuzzy Expert Systems\\nExpert systems, or production systems, are described in more detail in\\nChapter 9. An expert system consists of a set of rules that are developed in\\ncollaboration with an expert. Expert systems are used, for example, for\\nmedical diagnosis. Traditional expert systems use crisp logical values to\\ndetermine a diagnosis or a recommendation based on a set of evidence. In\\nmany ways, this is a fine way to apply the expert’s knowledge. On the other\\nhand, most expert decisions are not black and white. An expert who is pre-\\nsented with a patient with one set of symptoms will not usually be able to\\nprovide a diagnosis with absolute certainty but will have a strong feeling\\nabout a diagnosis based on the weight of evidence.\\nHence, applying fuzzy logic to these rules seems like a natural way to progress.\\nThe fuzzy expert system can be built by choosing a set of linguistic vari-\\nables appropriate to the problem and defining membership functions for\\nthose variables. Rules are then generated based on the expert’s knowledge\\nand using the linguistic variables. The fuzzy rules can then be applied as\\ndescribed above using Mamdani inference.\\nLet us now look at a simple example of how a fuzzy expert system can be\\nbuilt from an expert’s knowledge.\\nWe will consider an imaginary medical system designed to recommend a\\ndose of quinine to a patient or doctor based on the likelihood that that\\npatient might catch malaria while on vacation.\\nCreating the fuzzy expert system will involve the following steps:\\nC = ×( ) +×( ) +×( ) +×( ) ++ × ( )\\n++ ++ +\\n==\\n5 0 83 10 0 1 15 0 15 20 0 2 100 1\\n00 8 3 01 01 5 02 1\\n717 666\\n10 533 68 13\\n.. ..\\n.. . .\\n.\\n. .\\nK\\nK'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 549, 'page_label': '550'}, page_content='18.9 Fuzzy Expert Systems 523\\n1. Obtain information from one or more experts.\\n2. Define the fuzzy sets.\\n3. Define the fuzzy rules.\\nT o use the fuzzy expert system, we will use the following steps:\\n1. Relate observations to the fuzzy sets.\\n2. Evaluate each case for all fuzzy rules.\\n3. Combine information from the rules.\\n4. Defuzzify the results.\\n18.9.1 Defining the Fuzzy Sets\\nHaving obtained suitable information from our experts, we must start by\\ndefining the fuzzy sets.\\nIn this case, we will use the following fuzzy sets, or linguistic variables:\\n■ average temperature of destination (T)\\n■ average humidity of destination (H)\\n■ proximity to large bodies of water (P)\\n■ industrialization of destination (I)\\nNote that this is a purely imaginary example for the purposes of illustration\\nand explanation and has no real bearing on the way that malaria is pre-\\nvented or treated!\\nAs well as defining the linguistic variables, we need to give each one a range\\nof possible values. For this example, we will assume that each has just two\\nvalues: T emperature, humidity, and industrialization can be high ( H) or\\nlow (L), and proximity to water can be near (N) or far (F).\\nT o represent the fuzzy membership functions, we will use the notation M\\nAB\\n(x), where A is the variable (T, H, P,o r  I) and B is the value (H, L, N,o r  F).\\nFor example, MHL is the membership function for the fuzzy subset\\ndescribed as “humidity low. ”\\nThe crisp values that we will allow as inputs will range from 0 to 100 for\\ntemperature, humidity, and industrialization, and from 0 to 50 for proxim-\\nity to water.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 550, 'page_label': '551'}, page_content='524 CHAPTER 18 Fuzzy Reasoning\\nWe will define the membership functions for each of these fuzzy subsets\\nusing the following equations:\\nMx\\nfor x\\nx for x\\nfor x\\nIL ( ) =\\n<\\n− ≤<\\n≥\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n11 0\\n20\\n10 20\\n02 0\\n 10\\nMx\\nfor x\\nx for x\\nfor x\\nIH ( ) =\\n<\\n− ≤<\\n≥\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n01 0\\n10\\n10 20\\n12 0\\n 10\\nMx\\nfor x\\nx for x\\nfor x\\nPF ( ) =\\n<\\n− ≤<\\n≥\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n01 0\\n10\\n30 40\\n14 0\\n 10\\nMx\\nfor x\\nx for x\\nfor x\\nPN ( ) =\\n<\\n− ≤<\\n≥\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n11 0\\n40\\n30 40\\n04 0\\n 10\\nMx x\\nMx x\\nHH\\nHL\\n( ) =\\n( ) =−\\n100\\n1 100\\nMx\\nx for x\\nfor x\\nMx\\nx for x\\nfor x\\nTH\\nTL\\n( ) =\\n− ≥\\n<\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n( ) = −≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n25\\n75 25\\n02 5\\n1 75 75\\n07 5'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 551, 'page_label': '552'}, page_content='18.9 Fuzzy Expert Systems 525\\n1\\n0 1007525\\nTemperature\\nHL\\nFigure 18.7\\nGraph showing member-\\nship function for tempera-\\nture (high and low)\\n1\\n0 10050\\nHumidity\\nHL\\nFigure 18.8\\nGraph showing member-\\nship function for humidity\\n(high and low)\\nFigures 18.7 to 18.10 show graphs for all of these fuzzy membership functions.\\nWe need to define one more fuzzy set, which is the set used to describe the\\noutput of the system. In this case, our system will prescribe a dose of qui-\\nnine which can take on one of three values:\\nvery low dose (V)\\nlow dose (L)\\nhigh dose (H)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 552, 'page_label': '553'}, page_content='526 CHAPTER 18 Fuzzy Reasoning\\n1\\n0 504010\\nProximity to Water\\nNF\\nFigure 18.9\\nGraph showing member-\\nship function for proximity\\nto water (near and far)\\n1\\n0 10010 20\\nIndustrialization\\nLH\\nFigure 18.10\\nGraph showing member-\\nship function for industri-\\nalization (high and low)\\nWe will define three membership functions for these three fuzzy sets as follows:\\nMx\\nx for x\\nfor x\\nQL ( ) =\\n− ≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n50\\n50 50\\n05 0\\nMx\\nx for x\\nfor x\\nQV ( ) =\\n− ≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n10\\n10 10\\n01 0'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 553, 'page_label': '554'}, page_content='18.9 Fuzzy Expert Systems 527\\n1\\n0 10010 40 50\\nQuinine dose\\nL\\nV\\nH\\nFigure 18.11\\nMembership functions for\\nquinine dose values; V\\n(very low), L (low), and H\\n(high).\\nGraphs for these three membership functions are shown in Figure 18.11.\\n18.9.2 Defining Fuzzy Rules\\nThe second step in creating our fuzzy expert system is to define a set of\\nfuzzy rules.\\nThese rules, unlike those used by traditional expert systems, are expressed in\\nvague English terms and do not define cut-off points or thresholds, but rather\\nuse subjective terms such as “high” and “low. ” This maps more naturally to the\\nway an expert would express his or her knowledge and makes the process of\\nconverting that knowledge into rules far simpler and less prone to error.\\nOur rules are defined as follows:\\nRule 1 IF temperature is high\\nAND humidity is high\\nAND proximity to water is near\\nAND industrialization is low\\nTHEN quinine dose is high\\nMx\\nfor x\\nx for xQH ( ) =\\n≤\\n− >\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n04 0\\n40\\n60 40'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 554, 'page_label': '555'}, page_content='528 CHAPTER 18 Fuzzy Reasoning\\nRule 2: IF industrialization is high\\nTHEN quinine dose is low\\nRule 3: IF humidity is high\\nAND temperature is high\\nAND industrialization is low\\nOR proximity to water is near\\nTHEN quinine dose is high\\nRule 4: IF temperature is low\\nAND humidity is low\\nTHEN quinine dose is very low\\nThese rules may not be the best way to express this information, but they\\nwill suffice for this example.\\n18.9.3 Relating Observations to Fuzzy Sets\\nWe are now ready to make use of our fuzzy expert system.\\nWe will examine five sets of data, for five individuals, each of whom is trav-\\neling to a country that is at risk from malaria.\\nThe crisp data are as follows:\\ntemperature = {80, 40, 30, 90, 85}\\nhumidity = {10, 90, 40, 80, 75}\\nproximity to water = {15, 45, 20, 5, 45}\\nindustrialization = {90, 10, 15, 20, 10}\\nHence, for example, person three is traveling to an area where the average\\ntemperature is 30, the humidity is 40, the distance to water is 20, and the\\nlevel of industrialization is 25.\\nWe must now convert these crisp values into fuzzy membership values.\\nThis can be done by simply applying the relevant fuzzy membership func-\\ntions to each of the values. For example, let us look at some of the calcula-\\ntions for the first person:\\nT emperature = 80.\\nThese membership functions were defined as:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 555, 'page_label': '556'}, page_content='18.9 Fuzzy Expert Systems 529\\nSo,\\nMTH(80) = (80 – 25) / 75 = 0.733\\nMTL(80) = 0\\nSimilarly, we obtain the following membership function values:\\nMHH(10) = 10 / 100 = 0.1\\nMHL(10) = 1 /H11002(10 / 100) = 0.9\\nMPN(15) = (40 /H1100215) / 30 = 0.833\\nMPF(15) = (15 /H1100210) / 30 = 0.167\\nMIH(90) = 1\\nMIL(90) = 0\\nIn a similar fashion, we can obtain membership values for the other four\\ntravelers, which results in the following:\\nMTH = {0.733, 0.2, 0.067, 0.867, 0.8}\\nMTL = {0, 0.467, 0.6, 0, 0}\\nMHH = {0.1, 0.9, 0.4, 0.8, 0.75}\\nMHL = {0.9, 0.1, 0.6, 0.2, 0.25}\\nMPN = {0.833, 0, 0.667, 1, 0}\\nMPF = {0.167, 1, 0.333, 0, 1}\\nMIH = {1, 0, 0.5, 1, 0}\\nMIL = {0, 1, 0.5, 0, 1}\\nNote that for all of the fuzzy sets apart from temperature, the two possible\\nvalues sum to 1 in every case. For example, for the third person, member-\\nship of “high humidity” is 0.4, and membership of “low humidity” is 0.6.\\nThis relationship does not always have to hold for fuzzy sets, and in this\\ncase it does not hold for temperature.\\nMx\\nx for x\\nfor x\\nTL ( ) = −≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n1 75 75\\n07 5\\nMx\\nx for x\\nfor x\\nTH ( ) =\\n− ≥\\n<\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n25\\n75 25\\n02 5'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 556, 'page_label': '557'}, page_content='530 CHAPTER 18 Fuzzy Reasoning\\n18.9.4 Evaluating Each Case for the Fuzzy Rules\\nWe now have a set of fuzzy inputs that can be applied to the antecedents of\\nthe rules.\\nFor example, let us examine traveler number 1. The values are as follows:\\nMTH = 0.733\\nMTL = 0\\nMHH = 0.1\\nMHL = 0.9\\nMPN = 0.833\\nMPF = 0.167\\nMIH = 1\\nMIL = 0\\nRule 1, written with the appropriate fuzzy membership values for person 1,\\nis as follows:\\nIF temperature is high (0.733)\\nAND humidity is high (0.1)\\nAND proximity to water is near (0.833)\\nAND industrialization is low (0)\\nTHEN quinine dose is high (0)\\nRecall that to apply the fuzzy AND operator, we take the minimum value of\\nthe antecedents. In this case, therefore, the rule fires with value 0, which\\nmeans it does not fire at all.\\nWe will now apply the remaining rules in the same way:\\nRule 2 IF industrialization is high (1)\\nTHEN quinine dose is low (1)\\nIn this case, the rule fires with fuzzy strength of 1.\\nRule 3 IF humidity is high (0.1)\\nAND temperature is high (0.733)\\nAND industrialization is low (0)\\nOR proximity to water is near (0.833)\\nTHEN quinine dose is high (0.1)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 557, 'page_label': '558'}, page_content='18.9 Fuzzy Expert Systems 531\\nNote that in this case, the rule has an OR clause. This is calculated by taking\\nthe maximum value of its arguments, which in this case is 0.833. Hence, the\\noverall result of the rule is the minimum of 0.1, 0.733, and 0.833, which is 0.1.\\nRule 4 IF temperature is low (0)\\nAND humidity is low (0.9)\\nTHEN quinine dose is very low (0)\\nWe can use this method for all of the five sets of input data and obtain\\nresults as follows:\\nRule 1 (high dose): {0, 0, 0.067, 0, 0}\\nRule 2 (low dose): {1, 0, 0.5, 1, 0}\\nRule 3 (high dose): {0.1, 0.2, 0.067, 0.8, 0.75}\\nRule 4 (very low dose): {0, 0.1, 0.6, 0, 0}\\nIn this case, to combine Rules 1 and 3, which each give values for the “high\\ndose” fuzzy set, we will take the maximum value, thus obtaining the follow-\\ning values for “high dose” from these two rules:\\nhigh dose: {0.1, 0.2, 0.067, 0.8, 0.75}\\n18.9.5 Defuzzification\\nWe now need to defuzzify the outputs to obtain a crisp dosage recommen-\\ndation for each traveler.\\nLet us examine this process for traveler 1:\\nTraveler 1 obtained the following three fuzzy outputs:\\nvery low dose (V): 0\\nlow dose (L): 1\\nhigh dose (H): 0.1\\nT o defuzzify this output, we use the clipping operation described in Section\\n18.8. This clipping is shown graphically in Figure 18.12.\\nIn this case, we clip the V set to value 0, which means it is effectively not\\nused at all. We clip the L set to value 1, which means it is not clipped, and\\nwe clip the H set to value 0.1.The shaded area in Figure 18.12 is the com-\\nbined result of the three fuzzy sets, and obtaining the centroid of this\\nshaded shape will give us the crisp output value, which is the recommenda-\\ntion for dosage for this traveler.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 558, 'page_label': '559'}, page_content='532 CHAPTER 18 Fuzzy Reasoning\\n1\\n0 10010 40 50\\nQuinine dose\\nL\\nV\\nH\\nFigure 18.12\\nClipped fuzzy set for the\\noutput for traveler 1\\nC = (0.9 /H110035) + (0.8 /H1100310) + (0.7 /H1100315) + (0.6 /H1100320) + (0.5 /H1100325) + (0.4 /H1100330) + \\n(0.3 /H1100335) + (0.2 /H1100340) + (0.1 /H1100345) + (0.1 /H1100350) + (0.1 /H1100355) + (0.1 /H1100360) + \\n(0.1 /H1100365) + (0.1 /H1100370) + (0.1 /H1100375) + (0.1 /H1100380) + (0.1 /H1100385) + (0.1 /H1100390) + \\n(0.1 /H1100395) + (0.1 /H11003100)\\n0.9 + 0.8 + 0.7 + 0.6 + 0.5 + 0.4 + 0.3 + 0.2 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 \\n+ 0.1 + 0.1 + 0.1 + 0.1 + 0.1\\n= 165 / 5.6\\n= 29.46\\nRecall that we define the center of gravity as follows:\\nWe will sum over values of x, which increase in increments of 5. A more\\naccurate result could be obtained using smaller increments, but for this\\nexample, increments of 5 will give sufficient accuracy.\\nHence,\\nC Mx x\\nMx\\nA\\nA\\n= ( )\\n( )\\n∑\\n∑\\nThus, the recommended dose for traveler 1 is 29.46.\\nWe will now defuzzify the results for traveler 3.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 559, 'page_label': '560'}, page_content='18.9 Fuzzy Expert Systems 533\\n1\\n0.6\\n0.5\\n0.067\\n0 10010 40 50\\nQuinine dose\\nL\\nV\\nH\\nFigure 18.13\\nClipped fuzzy set for the\\noutput for traveler 3\\nC = (0.6 /H110035) + (0.5 /H1100310) + (0.5 /H1100315) + (0.5 /H1100320) + (0.5 /H1100325) + (0.4 /H1100330) + \\n(0.3 /H1100335) + (0.2 /H1100340) + (0.1 /H1100345) + (0.067 /H1100350) + (0.067 /H1100355) + (0.067 /H1100360) +\\n(0.067 /H1100365) + (0.067 /H1100370) + (0.067 /H1100375) + (0.067 /H1100380) + (0.067 /H1100385) + \\n(0.067 /H1100390) + (0.067 /H1100395) + (0.067 /H11003100)\\n0.6 + 0.5 + 0.5 + 0.5 + 0.5 + 0.4 + 0.3 + 0.2 + 0.1 + 0.067 + 0.067 + 0.067 + 0.067 + 0.067 + \\n0.067 + 0.067 + 0.067 + 0.067 + 0.067 + 0.067\\n= 128 / 4.3\\n= 29.58\\nTraveler 3 had the following results:\\nvery low dose (V): 0.6\\nlow dose (L): 0.5\\nhigh dose (H): 0.067\\nFigure 18.13 shows the result of using these values to clip the three fuzzy\\nsets, H, L, and V.\\nThe centroid of the shaded area shown in Figure 18.13 is calculated as fol-\\nlows:\\nUsing this same method, dosages can also be calculated for the other four\\ntravelers. This is left as an exercise for the reader.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 560, 'page_label': '561'}, page_content='534 CHAPTER 18 Fuzzy Reasoning\\n18.10 Fuzzy Systems That Learn\\nThe fuzzy systems we have seen so far are static: once the fuzzy sets and\\nrules are set up, they do not change. As new inputs are presented to them,\\nthey do not learn from those inputs. This makes sense because the rules\\nthat we have given to the systems are designed by experts and, so, should\\nnot need to change. On the other hand, we have already said that the rules\\nfrom the experts are subjective and vague. When one expert says that a dial\\nshould be set to “high, ” another expert might say it should be set to “very\\nhigh, ” but mean the same crisp setting.\\nFuzzy systems are designed to be able to cope with this kind of vagueness\\nand inaccuracy, and tend to produce good results regardless. However, in\\nsome situations it makes more sense to allow the fuzzy system to adapt. In\\nChapter 11, we saw how neural networks use a system based on the neural\\nstructures in human brains to learn how to deal with new problems. These\\nsystems are able to adapt—to learn how to deal with situations that they\\nhave not previously encountered and, in extreme cases, are able to learn to\\nsurvive when the environment in which they operate changes.\\nWe will now look at how fuzzy logic can be used in combination with neu-\\nral networks to produce fuzzy systems that are able to adapt and learn.\\n18.10.1 Neuro-fuzzy Systems\\nA neuro-fuzzy system is a neural network that learns to classify data using\\nfuzzy rules and fuzzy classifications (fuzzy sets). A neuro-fuzzy system has\\nadvantages over fuzzy systems and traditional neural networks: A tradi-\\ntional neural network is often described as being like a “black box, ” in the\\nsense that once it is trained, it is very hard to see why it gives a particular\\nresponse to a set of inputs. This can be a disadvantage when neural net-\\nworks are used in mission-critical tasks where it is important to know why\\na component fails.\\nFuzzy systems and neuro-fuzzy systems do not have this disadvantage.\\nOnce a fuzzy system has been set up, it is very easy to see which rules fired\\nand, thus, why it gave a particular answer to a set of inputs. Similarly, it is\\npossible with a neuro-fuzzy system to see which rules have been developed\\nby the system, and these rules can be examined by experts to ensure that\\nthey correctly address the problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 561, 'page_label': '562'}, page_content='18.10 Fuzzy Systems That Learn 535\\nT\\nH\\nQ\\nLayer 1 Layer 2 Layer 3 Layer 4 Layer 5\\nFigure 18.14\\nTypical layout of a five-\\nlayer neuro-fuzzy network\\nTypically, a fuzzy neural network is a five-layer feed-forward network. The\\nfive layers are as follows:\\n1 input layer—receives crisp inputs\\n2 fuzzy input membership functions\\n3 fuzzy rules\\n4 fuzzy output membership functions\\n5 output layer—outputs crisp values\\nFigure 18.14 shows the typical layout of such a network.\\nThe network in Figure 18.14 has two crisp inputs, T and H, and produces\\none crisp output, Q. It has five layers, whose functions are as follows:\\nThe first layer, the input layer, simply passes its crisp input values to the\\nnext layer in the network.\\nThe second layer contains information about the various fuzzy sets that are\\nbeing used to map the crisp inputs. In other words, it fuzzifies the inputs in\\nthe same way that we fuzzified our inputs in the examples in Section 18.9.3.\\nTypically, the neurons used in this second layer have triangular activation\\nfunctions, which represent the triangular membership functions of the\\nfuzzy sets, although any functions can be used.\\nThe third layer represents the fuzzy rules of the system. Each neuron in this\\nlayer represents a single fuzzy rule.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 562, 'page_label': '563'}, page_content='536 CHAPTER 18 Fuzzy Reasoning\\nTypically, the system would be set up with initial fuzzy rules built in, and the\\nnetwork would develop suitable weightings to give the best possible responses.\\nIn some cases, it is possible to start the system with no built-in rules, in which\\ncase the system learns its own rules and develops weights for them.\\nThe fourth layer in the network contains the neurons that represent the\\nmembership functions of the various possible outputs of the fuzzy rules (in\\nthis case there are three possible outputs, and so three neurons in the\\nfourth layer).\\nThe fifth and final layer is the layer that combines and defuzzifies the various\\no u t p u t st op r o d u c eo n es i n g l ec r i s po u t p u tf o rt h en e t w o r ki nr e s p o n s et oa\\ngiven set of inputs. In this case, the network has just one final output, but it is\\npossible to have a fuzzy neural network that produces a number of outputs.\\nThe connections between the layers have weights associated with them, and\\nusing the methods such as back-propagation, which are described in Chap-\\nter 11, the system is able to learn.\\nLet us now examine in detail the behavior of each of the levels of neurons\\nin the network to see how the entire network behaves.\\nWe will use the network shown in Figure 18.14 to learn a simple version of\\nthe fuzzy rules used in Section 18.9.2 for prescribing quinine. We will use\\njust two input variables: temperature (T) and humidity (H).\\n18.10.2 Layer 1: The Input Layer\\nThis layer simply passes the input values it receives (T and H in our exam-\\nple) to each of the neurons in the second layer. In fact, in our example this\\nlayer is set up so that input T is passed to the top two neurons of Layer 2,\\nand input H is passed to the bottom two neurons. This is because in this\\nexample each of the inputs has two possible values, which have different\\nmembership functions.\\n18.10.3 Layer 2: The Fuzzification Layer\\nThe neurons in Layer 2 represent the fuzzy membership functions for the\\ntwo inputs to the system. The top two neurons in this layer represent the\\nmembership functions for “high temperature” and “low temperature, ”\\nwhereas the bottom two represent “high humidity” and “low humidity. ” If\\nthe membership functions for “high humidity” and “high temperature”\\nwere the same, then these could be combined into one neuron.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 563, 'page_label': '564'}, page_content='18.10 Fuzzy Systems That Learn 537\\nEach neuron in this layer has an activation function (defined in Chapter\\n11) that is identical to the membership function it is representing. In this\\ncase, the activation functions will be the membership functions shown in\\nFigures 18.7 and 18.8.\\nHence, the output of each neuron (or the extent to which it fires) in this layer\\nis determined by applying the appropriate membership function to its inputs.\\n18.10.4 Layer 3: The Fuzzy Rule Layer\\nThe outputs of Layer 2 are the values that represent the extent to which\\neach of the inputs belongs to each of the fuzzy sets “high humidity, ” “low\\nhumidity, ” “high temperature,” and “low temperature. ”\\nThe way in which this layer usually works is that the various input values\\nare multiplied together to give the fuzzy intersection of the inputs. This\\nsingle input is then used as the antecedent of the rule, which determines the\\nextent to which the neuron fires.\\nThe network in this example will be using rules such as these:\\nIF T\\nH THEN QH AND QL\\nIF TH AND TL AND HH THEN QH AND QL\\nIF TL AND HH AND HL THEN QL AND QV\\nIF HL THEN QL AND QV\\n(These can be seen by examining the network shown in Figure 18.14 and\\nassuming that the neurons in Layer 2 are ordered from the top—TH, TL, HH,\\nHL—and that the neurons in Layer 4 are ordered from the top—QH, QL, QV).\\nThese rules will also be modified by the weights of the network. In cases\\nwhere the system is initially set up so that each rule uses each of the inputs, the\\nweights adapt so that inputs that are not relevant to a given rule fade to zero.\\n18.10.5 Layer 4: The Output Membership Function Layer\\nEach neuron in Layer 4 represents the membership function of one of the\\nfuzzy outputs of the rules in Layer 3. In our example, the neurons thus rep-\\nresent the membership functions for “high quinine dose, ” “low quinine\\ndose, ” and “very low quinine dose. ” The activation functions for these neu-\\nrons thus match the membership functions shown in Figure 18.11.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 564, 'page_label': '565'}, page_content='538 CHAPTER 18 Fuzzy Reasoning\\n18.10.6 Layer 5: The Defuzzification Layer\\nEach output of the system has a neuron in the fifth layer. In our case, the\\nsystem outputs just one value—the dose of quinine to prescribe to the trav-\\neler. The single node in Layer 5 takes inputs from each of the output nodes\\nin Layer 4 and combines them together to form one crisp output. This is\\ncalculated, as explained in Section 18.8, by combining the clipped fuzzy\\nmembership sets and determining the centroid of the shape that this com-\\nbined function describes. This centroid is the output value of the system.\\n18.10.7 How the System Learns\\nThe neuro-fuzzy system learns using the same techniques used by tradi-\\ntional neural networks. Learning is done by adjusting the weights of the\\nconnections between neurons in the network.\\nFor example, using back-propagation, a set of input training data is applied\\nto the system and the outputs compared with the correct outputs. The\\nerror between the outputs and the correct outputs is then fed back through\\nthe network to adjust the weights to improve the network’s performance\\nwith that set of training data. When this process is repeated, the network\\neventually converges on an optimal set of weights.\\nThe system can start with a set of rules that are a “blank canvas”—where all\\nnodes in one layer are connected to all nodes in the next layer. In this case,\\nthe system learns its own rules from the training data and eliminates\\nunnecessary inputs and outputs from nodes by setting their weights to zero.\\nAlternatively, the system can be set up using input from an expert or\\nexperts. This information can be used to create suitable rules in much the\\nsame way as for a traditional fuzzy system, and the network will determine\\nthe optimal weights to use with those rules. Such systems are very robust\\nand can usually detect rules that have been entered erroneously. For exam-\\nple, if one expert gave the following rule:\\nIF T\\nH and HH then QH\\nand another expert gave the following rule:\\nIF TH and HH then QV\\nclearly, one of these experts is incorrect. The system would show this by set-\\nting all the weights for the wrong rule to zero because the training data\\nwould match the correct rule but would not match the incorrect rule\\n(assuming the training data are correct).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 565, 'page_label': '566'}, page_content='18.12 Review Questions 539\\n18.11 Chapter Summary\\n■ Bivalent logics are based on two truth values (true and false, usually).\\n■ Multivalent logics allow a range of possible values.\\n■ A linguistic variable is a word such as “height” that can be used to\\nrepresent a variable that can take a number of possible fuzzy values.\\n■ A fuzzy set is defined by its membership function.\\n■ A number of fuzzy operators can be applied to fuzzy sets, including\\nfuzzy intersection, fuzzy union, and fuzzy inverse.\\n■ A hedge such as “very” or “extremely” can be applied to linguistic\\nvariables.\\n■ Fuzzy logic defines how we reason about fuzzy variables.\\n■ Fuzzy rules can be defined that tell a fuzzy system how to behave\\nbased on the value of certain fuzzy inputs.\\n■ Fuzzy inference (such as Mamdani inference) allows a fuzzy system\\nto convert crisp input values into fuzzy variables and then to rea-\\nson about those variables, resulting in a single crisp output.\\n■ Fuzzy expert systems have several advantages over traditional,\\nnonfuzzy expert systems.\\n■ Neuro-fuzzy systems are fuzzy systems that use techniques from\\nneural networks in order to learn.\\n18.12 Review Questions\\n18.1 Explain the difference between bivalent and multivalent logics.\\nWhich type of logic are you more familiar with?\\n18.2 What is the law of the excluded middle? Argue against the need for\\nthis law.\\n18.3 What is a linguistic variable? Give 10 examples of linguistic vari-\\nables that you might use to describe a building.\\n18.4 What are hedges? Give five hedges that apply to the linguistic vari-\\nables you gave in answer to question 18.3.\\n18.5 How do fuzzy sets differ from traditional sets? What is the connec-\\ntion between linguistic variables and fuzzy sets?\\n18.6 What is the connection between fuzzy sets and fuzzy logic?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 566, 'page_label': '567'}, page_content='540 CHAPTER 18 Fuzzy Reasoning\\n18.7 Explain carefully how fuzzy logic differs from traditional Aris-\\ntotelian logic.\\n18.8 Explain how Mamdani inference works.\\n18.9 Explain what is meant by Defuzzification. How is it performed?\\n18.10 What advantages would fuzzy expert systems have over traditional\\nexpert systems. Would they have any disadvantages?\\n18.11 What is a neuro-fuzzy system? How does it learn? Compare and\\ncontrast neuro-fuzzy systems with traditional neural networks.\\n18.13 Exercises\\n18.1 Develop fuzzy rules to control a set of traffic lights at a four-way\\njunction. Assume that there are sensors at each junction that\\ndetermine how many cars are waiting and how long they have\\nbeen waiting. The fuzzy rules should control the lights to mini-\\nmize delay to all cars. Each junction has a traffic light that can be\\nred (stop) or green (go). Y ou can add additional lights to each\\njunction to control traffic moving in different directions: in other\\nwords, you could allow traffic turning right to go, while traffic\\ngoing straight or turning left is required to stop. Y ou can allow\\nmore than one light to be green, as long as it cannot cause any\\naccidents. Implement the fuzzy rules in a fuzzy system in the pro-\\ngramming language of your choice.\\n18.2 Prove the following expressions using fuzzy logic:\\nA\\n∧ B → A\\nA ∧ B → ¬A\\nA ∧ B → A ∨ B\\n18.14 Further Reading\\nKosko (1993) provides a nontechnical introduction to the subjects covered\\nin this chapter. Of the main texts, Negnevitsky (2002) provides the greatest\\ncoverage of fuzzy logic and fuzzy systems, providing some excellent con-\\ncrete examples of how fuzzy systems work and how fuzzy techniques can be\\ncombined with other Artificial Intelligence methods, such as neural net-\\nworks and expert systems.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 567, 'page_label': '568'}, page_content='18.14 Further Reading 541\\nThe Fuzzy Systems Handbook: A Practitioner’s Guide to Building, Using, &\\nMaintaining Fuzzy Systems, by Earl Cox (1999 – Morgan Kaufmann)\\nFuzzy Logic for Business and Industry, by Earl Cox (2000 – Charles River Media)\\nAn Introduction to Fuzzy Control, by Dimiter Driankov, Hans Hellendoorn,\\nand M. Reinfrank (1996 – Springer V erlag)\\nComputational Intelligence: An Introduction , by Andries P . Engelbrecht\\n(2003 – John Wiley & Sons)\\nFuzzy Control: Synthesis and Analysis, edited by Shehu S. Farinwata, Dimi-\\ntar P . Filev, and Reza Langari (2000 – John Wiley & Sons)\\nFuzzy and Neural Approaches in Engineering , by J. Wesley Hines (1997 –\\nWiley Interscience)\\nApplications of Fuzzy Logic: Towards High Machine Intelligence Quotient\\nSystems, edited by Mohammad Jamshidi, Andre Titli, Lotfi Zadeh, and\\nSerge Boverie (1997 – Prentice Hall)\\nNeuro-Fuzzy and Soft Computing: A Computational Approach to Learning\\nand Machine Intelligence , by Jyh-Shing Roger Jang, Chuen-Tsai Sun, and\\nEiji Mizutani (1996 – Prentice Hall)\\nMultistage Fuzzy Control: A Model-Based Approach to Fuzzy Control and\\nDecision Making, by Janusz Kacprzyk (1997 – John Wiley & Sons)\\nFuzzy Thinking: The New Science of Fuzzy Logic , by Bart Kosko (1994 –\\nHyperion)\\nFuzzy Logic: The Revolutionary Computer Technology That Is Changing Our\\nWorld, by Daniel Mcneill (1994 – Simon & Schuster)\\nUncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions,\\nby Jerry M. Mendel (2000 – Prentice Hall)\\nAn Introduction to Fuzzy Sets: Analysis and Design , by Witold Pedrycz and\\nFernando Gomide (1998 – MIT Press)\\nThe Importance of Being Fuzzy , by Arturo Sangalli (1998 – Princeton Uni-\\nversity Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 568, 'page_label': '569'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 569, 'page_label': '570'}, page_content='19CHAPTER\\nIntelligent Agents\\nAn active line on a walk, moving freely without a goal. A walk for a walk’s\\nsake. The agent is a point that shifts position.\\n—Paul Klee, Pedagogical Sketchbook\\nMy team had written a number of programs to control swarms of agents.\\nThese programs were modeled on behavior of bees. The programs had many\\nuseful characteristics. Because swarms were composed of many agents, the\\nswarm could respond to the environment in a robust way. Faced with new and\\nunexpected conditions, the swarm programs didn’t crash; they just sort of\\nflowed around the obstacles, and kept going.\\n—Michael Crichton, Prey\\nFor I also am a man set under authority, having under me soldiers, and I say\\nunto one, Go, and he goeth; and to another, Come, and he cometh; and to my\\nservant, Do this, and he doeth it.\\n—The Gospel according to St Luke, Chapter 7, Verse 8\\n19.1 Introduction\\nAn agent is an entity that is able to carry out some task, usually to help a\\nhuman user. Agents can be biologic (people or animals, for example),\\nrobotic, or computational. This chapter is primarily concerned with the\\nlatter type, in particular with software agents. A software agent is a com-\\nputer program designed to carry out some task on behalf of a user.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 570, 'page_label': '571'}, page_content='544 CHAPTER 19 Intelligent Agents\\nAs we will see, there are a number of ways in which software agents can be\\nbuilt and a number of properties that they can have. One property with\\nwhich we are particularly concerned is intelligence. We will discuss in\\nmore detail what is meant by intelligence, in the context of agents, in Sec-\\ntion 19.2.1.\\nThis chapter also introduces other important properties that agents may or\\nmay not have, including autonomy, benevolence, the ability to collaborate\\n(with other agents, for example), and the ability to learn.\\nA number of architectures that can be used to build agents are discussed.\\nThis chapter also introduces a number of types of agents, such as reactive\\nagents, interface agents , information agents , and multiagent systems ,\\nwhich use a number of agents together to solve a single problem.\\nFinally, the chapter briefly introduces the ideas behind robotic agents and\\ndiscusses a particular type of robot, known as a Braitenberg vehicle, which is\\nused to discuss the nature of intelligence and our interpretation of behavior.\\nIn many ways, the field of Artificial Intelligence as a whole can be seen as\\nthe study of methods that can be used to build intelligent agents. For exam-\\nple, the techniques discussed in Chapters 3 through 6 can be thought of as\\nmethods that intelligent agents can use to enable them to search or to play\\ngames. Each of the methods explained in this book can be used by an intel-\\nligent agent or to build intelligent agent systems.\\n19.2 Properties of Agents\\n19.2.1 Intelligence\\nAn agent is a tool that carries out some task or tasks on behalf of a human. For\\nexample, a simple agent might be set up to buy a particular stock when its\\nprice fell below a particular level. A simple Internet search agent might be\\ndesigned to send queries to a number of search engines and collate the results.\\nIntelligent agents have additional domain knowledge that enables them to\\ncarry out their tasks even when the parameters of the task change or when\\nunexpected situations arise. For example, an intelligent agent might be\\ndesigned to buy books for a user on the Internet at the lowest possible\\nprice. The agent would need to be able to interact with a set of online book-\\nstores but would also need to be able to learn how to deal with new book-\\nstores or with individuals who were offering secondhand books. These'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 571, 'page_label': '572'}, page_content='19.2 Properties of Agents 545\\nkinds of agents that perform tasks on behalf of people are called interface\\nagents, which are discussed in Section 19.5.\\nMany intelligent agents are able to learn, from their own performance,\\nfrom other agents, from the user, or from the environment in which they\\nare situated. The ways in which agents can learn have been covered in some\\ndetail in Part 4 of this book, and the way in which some of these ideas can\\nbe applied by intelligent agents are introduced in Section 19.12.\\n19.2.2 Autonomy\\nIn addition to intelligence, an important feature of many intelligent agents\\nis autonomy—the ability to act and make decisions independently of the\\nprogrammer or user of the agent. For example, an intelligent buying agent\\nthat is designed to buy goods on behalf of a user needs to be able to make\\ndecisions about what items to purchase without checking back with the\\nuser. This autonomy is what sets intelligent agents aside from many other\\nArtificial Intelligence techniques.\\n19.2.3 Ability to Learn\\nMany agents have an ability to learn. In other words, when presented with\\nnew information, such an agent is able to store that new information in a\\nuseful form. For example, agents can learn from a user by observing actions\\nor by being given instruction. We see how interface agents use these kinds\\nof learning in Section 19.5. Agents can also learn from other agents in mul-\\ntiagent systems, which are described in Section 19.8.\\nLearning allows agents to improve their performance at carrying out a par-\\nticular task over time. If a human user tells an agent that it has carried out\\na task poorly, it is useful for that agent to be able to learn from this experi-\\nence to avoid making the same mistakes in the future.\\n19.2.4 Cooperation\\nIn multiagent systems, agents usuallycooperate with each other. This coop-\\neration implies some form of social interaction between agents. For exam-\\nple, a buying agent may negotiate with selling agents to make purchases. As\\nhas been mentioned, agents can also learn from each other. T o use the buy-\\ning agent example again, a buying agent may be informed by another buy-\\ning agent of a new shopping portal that the agent may find useful.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 572, 'page_label': '573'}, page_content='546 CHAPTER 19 Intelligent Agents\\nOf course, it is also useful for agents to cooperate with the humans who use\\nthem. Although in most agent systems, this cooperation is in the form of\\nsimple inputs and instructions, the manner in which agents cooperate with\\npeople can be very important, as we see in Section 19.5 when we discuss\\ninterface agents.\\n19.2.5 Other Agent Properties\\nAgents can have a number of other properties. A versatile agent is one that\\nis able to carry out many different tasks. Most agents are benevolent,b u t\\nsome can be competitive or nonhelpful. Similarly, agents may be altruistic\\nor antagonistic. Some agents can have the ability to lie to other agents, or\\nto users, whereas other agents are always truthful (this property is known as\\nveracity).\\nOther properties of agents include the extent to which they can be trusted\\nwith delegated tasks and whether or not they degrade gracefully (i.e.,\\nwhen the agent encounters a new problem that it is unable to solve, does it\\nfail completely, or is it able to make some progress?).\\nAn agent’s mobility is defined by its ability to move about on the Internet\\nor another network.\\n19.3 Agent Classifications\\nAs has been discussed in Section 19.2, agents can be classified according to\\na number of parameters. We will now discuss a variety of types of agents\\nthat are classified according to these, and other, parameters.\\nThe types of agents that we will look at are not mutually exclusive: an interface\\nagent can be reactive or utility based. It can also be versatile or nonversatile.\\nThe main classes of agents are defined as follows:\\n■ reactive agents\\n■ collaborative agents\\n■ interface agents\\n■ mobile agents\\n■ information-gathering agents'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 573, 'page_label': '574'}, page_content='19.4 Reactive Agents 547\\nWe also look at the difference between reactive agents and goal-based and\\nutility-based agents, which are defined by the ways in which they are moti-\\nvated. Reactive agents simply respond to inputs they receive, whereas goal-\\nbased and utility-based agents have an ability to reason about their\\npositions and make decisions on the basis of that reasoning.\\nSome agents are hybrids, which exhibit properties of more than one of the\\ncategories listed above. The eventual aim of most intelligent agent research\\nis to develop smart agents, which would be fully autonomous and able to\\nlearn and cooperate with other agents. Smart agents do not yet exist and are\\nnot covered by this book.\\n19.4 Reactive Agents\\nA simple reactive agent (also known as a reflex agent) is a production sys-\\ntem where inputs from the environment are compared with rules to deter-\\nmine which actions to carry out. In other words, reactive agents simply\\nreact to events in their environment according to predetermined rules.\\nA simple example of a reactive agent is the automatic mail filter that many\\ne-mail systems now possess. This mail filter examines each e-mail as it\\narrives and compares it against a set of rules, or templates, and classifies it\\naccordingly. A common use for such systems is to reject so-called “junk\\nmail” or “spam. ” More complex systems are used to route e-mails within an\\norganization, so that a consumer can send an e-mail to a central mail\\naddress, and the system will determine to which department within the\\ncompany to send the mail, based on its contents.\\nIn the case of the e-mail–filtering agent, the environment is simply an e-\\nmail inbox and the contents of that inbox.\\nA reactive agent does not tend to perform well when its environment\\nchanges or when something happens that it has not been told about. For\\nexample, an e-mail–filtering system might have problems when it receives\\nan e-mail that is entirely in Chinese. New rules can of course be written to\\ndeal with such situations, but it might be more desirable to have an agent\\nthat can learn to adapt to new situations.\\nA more complex reactive agent can be developed that combines inputs\\nfrom its environment with information about the state of the world and\\ninformation about how its actions affect the world.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 574, 'page_label': '575'}, page_content='548 CHAPTER 19 Intelligent Agents\\nHence, a scheduling system might be based on the e-mail–filtering agent\\nsystem, which assigns tasks to employees based on the content of e-mails as\\nthey arrive.\\nFor example, when an e-mail arrives from a customer, reporting a bug in\\nthe company’s software system, the agent might assign a task to the engi-\\nneering department to fix the bug. The agent would then wait for further\\ninformation from the engineering department. If it did not receive assur-\\nance that the bug had been fixed within a reasonable amount of time, it\\nmight contact the engineering department again. The agent’s ability to do\\nthis derives from the fact that it is able to store information about the state\\nof the world (such as “engineering department working to fix bug number\\n36,234,120”) and about how its actions affect the state of the world (such as\\n“when I send this e-mail to engineering, they will start to work on fixing\\nthe bug”).\\nIf a subsequent e-mail arrives from a different customer, reporting the\\nsame bug, the agent would not need to report the bug again because it\\nknows that it has already reported it. Instead, it might reply to the customer\\nsaying something like\\nThank you for your email—we are already aware of this problem, and\\nour engineers are working to fix it now.\\n19.4.1 Goal-based Agents\\nGoal-based agents are more complex than reactive agents. Rather than fol-\\nlowing a predetermined set of rules, a goal-based agent acts to try to\\nachieve a goal. This is often done by using search (see Part 2) or planning\\n(see Part 5).\\nA goal-based agent might, for example, be given the goal of finding pages\\non the Internet that are of interest to an Artificial Intelligence researcher.\\nThe agent will be designed so that it is capable of carrying out actions (such\\nas loading a web page, examining it, and following links from one web page\\nto another). It is also able to identify when it has reached a goal (for exam-\\nple, by matching the pages it finds against a set of keywords whose presence\\nindicates relevance to Artificial Intelligence).\\nThis goal based agent would search the Internet looking for pages that\\nmatched its criteria and would presumably report those pages to its owner\\nor to a client. This kind of agent does not take into account how efficiently'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 575, 'page_label': '576'}, page_content='19.4 Reactive Agents 549\\nit is searching or how relevant the pages are that it is finding. In other\\nwords, its aim is simply to satisfy its goal; it does not take into account how\\nwell it has satisfied the goal or how efficiently. Utility-based agents, which\\nare described in the next section, use these concepts to attempt to provide\\nbetter results and in a more efficient manner.\\n19.4.2 Utility-based Agents\\nA utility-based agent is similar to a goal-based agent, but in addition to\\nattempting to achieve a set of goals, the utility-based agent is also trying to\\nmaximize some utility value . The utility value can be thought of as the\\nhappiness of the agent, or how successful it is being. It may also take into\\naccount how much work the agent needs to do to achieve its goals.\\nLet us return to our example from the previous section of an agent that\\nsearches for pages on the Internet that are of interest to Artificial Intelli-\\ngence researchers.\\nThe utility-based agent can use knowledge about the Internet to follow the\\nmost worthwhile paths from one page to another. In other words, it can use\\nheuristic-based search techniques to minimize the amount of time it\\nspends examining pages that are not of interest and to maximize the likeli-\\nhood that if an interesting page exists, it will be found (this combines\\nsearch concepts from Chapters 4 and 5 with information retrieval tech-\\nniques, which are discussed in Chapter 20).\\nThe techniques we saw in Chapter 6 for game-playing systems can also be\\nused as part of a utility-based agent. In this case, the agent’s utility function\\nis based on how successful it is at playing the game, and its goal is to maxi-\\nmize this utility function by winning the game.\\n19.4.3 Utility Functions\\nA utility function maps a set of states to the set of real numbers. In other\\nwords, given a particular state of the world, an agent is able to use its utility\\nfunction to derive a score, or utility value, that tells it how “happy” it is in\\nthat state or how successful it has been if it reaches that state.\\nThe static board evaluators that we saw in Chapter 6 are an example of\\na utility function that is used to evaluate a single position in a board\\ngame.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 576, 'page_label': '577'}, page_content='550 CHAPTER 19 Intelligent Agents\\nBy searching through a tree of possible future states, based on available\\nactions, and selecting a path that maximizes the utility function through-\\nout the tree, a utility-based agent is able to achieve its goals effectively and\\nefficiently.\\nFor example, our Artificial Intelligence research agent might assign a high\\nutility value to pages that are written in English and that appear to be writ-\\nten by a reliable source.\\nThe idea of utility is closely related to the idea of rationality. An agent that\\nbehaves rationally is one that attempts to maximize its utility function. This\\nutility function may not seem rational to all observers, although a rational\\nagent might be programmed to lose at chess as spectacularly as possible. By\\nlosing a game, this agent maximizes its utility function and so, contrary to\\nappearance, it is behaving rationally.\\nThis model of utility is based on economics theory. One utility function for\\npeople is money. In general, people tend to prefer to have more money\\nrather than less money. It is not as simple as this though. We might assume\\nthat the utility function for a human relating to money (ignoring other\\naspects of life) is simply based on the amount of money that that person\\nhad. This is contradicted by an experiment carried out in 1982 by psychol-\\nogists, Tversky and Kahneman. In their experiment, they offered subjects\\ntwo consecutive choices:\\n1. A or B:\\nA = 80% chance of winning $4000\\nB = 100% chance of winning $3000\\n2. C or D:\\nC = 20% chance of winning $4000\\nD = 25% chance of winning $3000\\nMost subjects choose A, rather than B; and C, rather than D. Let us consider\\nthe utility of these choices. In the choice between A and B, we have an 80%\\nchance of winning $4000 or a 100% chance of winning $3000. The\\nexpected values of these two choices are\\nE(A) = 0.8 /H110034000 = 3200\\nE(B) = 1.0 /H110033000 = 3000'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 577, 'page_label': '578'}, page_content='19.5 Interface Agents 551\\nHence, the most rational choice, using a simple utility function, would be to\\nselectA rather thanB. For the choice betweenC and D, the expected values are\\nE(C) = 0.2 /H110034000 = 800\\nE(D) = 0.25 /H110033000 = 750\\nSo in this choice, most people make the more rational decision on the basis\\nof the simple utility function. What this experiment tells us is that people\\nhave much more complex utility functions than we might assume.\\nSimilarly, utility-based intelligent agents usually need sophisticated utility\\nfunctions. In the case of a chess playing agent, for example, a utility func-\\ntion based solely on the number of pieces each player has would not be suf-\\nficient. A utility function based on which player wins is fine, but as we saw\\nin Chapter 6, this does not help the agent to play the game because the\\nsearch tree is usually too large for the agent to reach a position where one\\nplayer has won.\\n19.5 Interface Agents\\nAn interface agent can be thought of as a personal assistant. Interface\\nagents are typically autonomous agents, capable of learning in order to\\ncarry out tasks on behalf of a human user. Typically, interface agents col-\\nlaborate with the user, but do not need to collaborate with other agents;\\nalthough in some cases, interface agents can learn by seeking advice from\\nother agents.\\nA typical example of an interface agent is a tool that is used to help a user\\nlearn to use a new software package. Such an agent has the ability to observe\\nwhat the user does and make suggestions for better ways to perform those\\ntasks. It is also able to assist the user in carrying out complex tasks, possibly\\nlearning as it does so. Interface agents can thus take instructions from users\\nand can also learn from feedback from users about whether they are doing a\\ngood job or not, in order to perform better in future.\\nIt is often useful for repetitive tasks to be delegated to an interface agent.\\nThe interface agent can learn how to carry out the task by observing the\\nuser and then is able to repeat the task as required.\\nKozierok and Maes (1993) describe an interface agent that is able to assist a\\nuser with scheduling meetings on a calendar. The agent is able to arrange\\nmeetings with other people and is also able to accept, reject, and rearrange'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 578, 'page_label': '579'}, page_content='552 CHAPTER 19 Intelligent Agents\\nmeetings on behalf of the user. By observing the user’s behavior, it is able to\\nlearn, for example, that the user does not like to book meetings on Friday\\nafternoons and so is able to avoid such meetings.\\nA number of tools exist that filter Usenet postings and new articles for a\\nuser. These tools can typically be trained by example: a user can show\\nexamples of interesting articles, and examples of uninteresting articles and\\nthe agent can learn to identify interesting articles and present those to the\\nuser, while avoiding uninteresting ones.\\n19.6 Mobile Agents\\nMobile agents are those capable of “moving” from one place to another. In\\nthe case of mobile robots, this literally means moving in physical space. In\\nthe case of mobile software agents, this mobility usually refers to the Inter-\\nnet or other network. An agent that is not mobile is static.\\nMobile agents travel from one computer to another, gathering information\\nand performing actions as needed on the basis of that information. A com-\\nputer virus can be thought of as a form of mobile agent, although most\\nviruses are not intelligent, merely autonomous. That is, they are able to act\\nwithout being given direct instruction from a human, but they do not\\nadapt intelligently to their surroundings—they simply follow a fixed set of\\nrules that tells them how to infect a computer and how to reproduce.\\nFor mobile agents to run on remote computers, a suitable environment\\nmust of course be provided that allows the agent to run on that machine.\\nAn example of a system that provides such an environment is T elescript,\\ndeveloped by General Magic. The Java programming language, developed\\nby Sun, can also be used for developing mobile agents.\\nThe idea that a mobile agent can be sent from one computer across the\\nInternet to run on another computer raises many security questions.\\nThe main advantages of mobile agents are in efficiency. An agent that has to\\ncommunicate with a number of remote servers and request large quantities\\nof information in order to make a decision uses a large amount of band-\\nwidth, which can be avoided if the agent is able to physically move to the\\nremote server and query it locally.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 579, 'page_label': '580'}, page_content='19.7 Information Agents 553\\nSimilarly, the mobile agent may be able to take advantage of superior com-\\nputing power or the existence of particular functional abilities at the\\nremote machine that are not present locally.\\nIn this way, mobile agents can be used to generate a distributed computing\\narchitecture, where computation takes place on multiple computers at\\narbitrary locations.\\nA further advantage of mobile agents is that they can carry out their tasks\\nasynchronously: the user can set a mobile agent off on a particular task and\\ncan then get on with other work, or maybe even switch the computer off.\\nWhen the user is ready to receive the results, the agent can be recalled.\\n19.7 Information Agents\\nInformation agents , also known as information-gathering agents ,a r e\\nusually used on the Internet and so are also sometimes called Internet\\nagents. An information agent is used to help a user find, filter, and classify\\ninformation from the vast array of sources available on the Internet.\\nInformation agents may be static or mobile. Some information agents are\\ncapable of learning, whereas the behavior of others is fixed. Additionally,\\ninformation agents can be collaborative or can work independently of\\nother agents. The distinctive feature of an information agent is the function\\nthat it provides, rather than the way it works.\\nThere is an overlap between information agents and other kinds of agents\\ndescribed in this chapter. The interface agents described in Section 19.5,\\nwhich monitor Usenet postings or online news articles, are examples of\\ninformation agents.\\nInformation agents know how to search the Internet, usually using a num-\\nber of search tools. In this way, they are able to cover as much content as\\npossible and thus maximize their recall (see Chapter 20). The real chal-\\nlenge is usually precision. This is heavily dependent on the ability of the\\nagent to receive input instructions from the user. Some agents learn by\\nexample: the user shows the agent examples of pages that are relevant and\\npages that are not relevant, and the system learns to differentiate the two\\ngroups. Other agents are directed by keywords or more sophisticated infor-\\nmation retrieval techniques (see Chapter 20) to identify relevant material\\nfor the user.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 580, 'page_label': '581'}, page_content='554 CHAPTER 19 Intelligent Agents\\nThe Internet provides some unique challenges to these agents. Internet data\\nis very dirty: most of the information on the Internet is not organized in\\nany way; much of it includes misspellings, incorrect grammar, and incor-\\nrect facts. Additionally, the Internet is global in nature, and so material is\\navailable in almost every language.\\nThe sheer quantity of the data and the dirty nature of the data make it very\\ndifficult for many information agent systems to provide adequate precision\\nin identifying relevant documents.\\nOf course, this is one of the reasons that information agents are so useful. It\\nis even harder for humans to locate the data they want than it is for the\\nagents. Agents have the advantage of speed and of being able to examine\\npages asynchronously, delivering results to a user, perhaps by e-mail, once\\nthey are available.\\nMore sophisticated information agents are able to monitor the browsing\\nhabits of users to identify the kinds of material they are interested in and to\\nuse that information to improve the performance of future searches.\\n19.8 Multiagent Systems\\nIn many situations, simple reactive agents are sufficient. The fact that they\\ndo not have the ability to learn means that they are not suited to operating\\nin complex, dynamic environments. Also, because such an agent is based\\non a set of rules, the number of tasks and situations that it can deal with is\\nlimited by the number of rules it has. In fact, most agents do not exist in\\nisolation.\\nMultiagent systemsare a common way of exploiting the potential power of\\nagents by combining many agents in one system. Each agent in a multiagent\\nsystem has incomplete information and is incapable of solving the entire\\nproblem on its own, but combined together, the agents form a system that\\nhas sufficient information and ability to solve the problem. The system does\\nnot have a centralized control mechanism for solving the problem.\\nAn example of how many simple agents can combine together to produce\\ncomplex behavior can be seen by examining the way that ant colonies func-\\ntion. Each ant has very little intelligence and very little ability to learn.\\nTaken as a whole, however, the ant colony is able to deal with complex situ-\\nations and in some ways behaves as a single living entity.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 581, 'page_label': '582'}, page_content='19.8 Multiagent Systems 555\\nIn much the same way, many “dumb” agents can be combined together to\\nproduce a more intelligent system. For example, the legs of a robot might\\nbe controlled by a set of agents. Each leg is controlled by a simple reactive\\nrobot that has instructions for how to move the leg according to what the\\nleg encounters.\\nCommunication and collaboration are desirable properties of multiagent\\nsystems. Communication means, for example, that agents can inform each\\nother of changes in the environment or of new discoveries they have made.\\nCollaboration means that agents can work together to solve a common goal.\\nIn fact, multiagent systems often involve relatively simple interactions\\nbetween agents, and as we have seen with systems like Reynolds’ Boids (Chap-\\nter 13), the system as a whole is able to solve complex problems without the\\nindividual agents necessarily knowing anything about the overall problem.\\nSuch emergent behavior is a valuable property of multiagent systems.\\nMultiagent systems can be given the ability to learn to solve new problems\\nusing genetic algorithms (see Chapter 14). In this way, robots have been\\nsuccessfully developed whose limbs are controlled by individual agents,\\neach of which has been developed using a genetic algorithm. The robots are\\nable to walk in a way that mimics the locomotion of insects (Gary Parker\\n1997, 1998).\\nAgents in a multiagent system can be collaborative or competitive. Agents\\ndesigned to play chess against other agents would clearly be competitive,\\nwhereas agents that traverse the Internet searching for specific material\\nmay find it advantageous to cooperate with other similar agents.\\nAn agent team is a group of agents that collaborate together to achieve\\nsome common goal. It is often the case that an agent team consists of\\nagents that operate in different ways and have different goals to accomplish.\\nFor example, a team of agents might be used to arrange travel for a busi-\\nnessman: one agent might book flights, another agent arranges hotel\\naccommodation, a third agent arranges meetings with business associates,\\nwhile a fourth agent arranges meals and entertainments.\\nIn some situations, these agents will be competing with other agents, bid-\\nding for purchases, but the agents within the team will cooperate with each\\nother (e.g., the meal-booking agent will inform the meeting booking agent\\nif it changes its restaurant bookings, which might affect a meeting that has\\nbeen arranged in that restaurant).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 582, 'page_label': '583'}, page_content='556 CHAPTER 19 Intelligent Agents\\n19.9 Collaborative Agents\\nCollaborative agent systems are multiagent systems in which the agents\\ncollaborate with each other to accomplish goals. This property, of cooper-\\nating to achieve a common goal, is known as benevolence.\\nCollaborative agents typically do not have the ability to learn, although\\nsome have simple learning abilities. As with multiagent systems, the idea is\\nthat a combination of many simple agents can solve a problem that each\\nagent individually would not be able to solve.\\nCollaborative agent systems are able to take advantage of their parallel\\nnature in order to solve problems faster than would otherwise be possible.\\nThey are also more reliable than traditional systems because additional\\nagents can be added to provide redundancy: if one agent fails, or provides\\nincorrect information, this will not affect the overall performance of the\\nsystem because other agents will provide corrective information.\\n19.10 Agent Architectures\\nIn this section, we will look at a number ofarchitecturesthat can be used to\\nbuild intelligent agents. The architecture of an agent is the way in which its\\nvarious processing modules are connected together and the way in which\\nthose modules are connected to the environment in which the agent operates.\\n19.10.1 Subsumption Architecture\\nThere are a number of architectures suitable for reactive agents. One of the\\nmost commonly used is Brooks’subsumption architecture (Brooks 1985).\\nThe subsumption architecture is a layered architecture that was designed\\nfor implementing physical robots, which does not involve any centralized\\nintelligence or control mechanism.\\nThe agent in this architecture has a set of inputs, a possible set of actions,\\nand a layered set of modules, each of which is designed to control some\\naspect of the agent’s behavior. Each layer is able to inhibit the behavior of\\nlayers below it.\\nThe modules are augmented finite state machines (AFSMs), which are\\nsimilar to the finite state automata we saw in Chapter 13. AFSMs are often\\nbased on production rules, as used by expert systems, which take the form\\ninput → action'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 583, 'page_label': '584'}, page_content='19.10 Agent Architectures 557\\nInputs\\nInputs\\nInputs Actions\\nActions\\nActions\\nEXPLORE\\nWANDER\\nAVOID OBSTACLES Figure 19.1\\nA three-layer subsumption\\narchitecture\\nThese rules are called situated action rules or situation action rules\\nbecause they map situations to actions. An agent that uses such rules is said\\nto be situated, in that it is affected by where it is in its environment.\\nAn AFSM is triggered when its inputs exceed a threshold. Each AFSM also\\nhas inhibitor inputs that can prevent it from triggering.\\nRather than having a centralized representation, the subsumption architec-\\nture relies on lower-level modules that combine together. From these com-\\nbined modules emerges intelligent behavior.\\nA simple subsumption architecture is shown in Figure 19.1.\\nThis architecture was proposed by Brooks as a control mechanism for a\\nrobot. Each layer in the architecture is designed to handle one type of behav-\\nior: exploring, wandering, or avoiding obstacles. The modules actasynchro-\\nnously, but each module can affect the behavior of the other modules.\\nThe WANDER module will take into account the instructions generated by\\nthe A VOID OBSTACLES module, but it is also able to suppress the instruc-\\ntions generated by the A VOID OBSTACLES module, in order to ensure that\\nwhile avoiding collisions, the robot still wanders around. This is to ensure\\nthat the robot does not simply focus on avoiding obstacles to the exclusion\\nof everything else.\\nMore important than wandering, for this robot, is exploration. Hence, the\\nEXPLORE module is able to suppress instructions from the WANDER\\nmodule to ensure that the robot continues to explore new territory, rather\\nthan simply wandering aimlessly.\\nFurther layers can be added to the architecture to generate more sophisti-\\ncated behavior—for example, Brooks describes a system that is able to\\nwander around among desks in an office, looking for empty drink cans.\\nThis system has an architecture with additional layers for identifying drink\\ncans, identifying desks, and so on (Brooks 1993).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 584, 'page_label': '585'}, page_content='558 CHAPTER 19 Intelligent Agents\\n19.10.2 BDI Architectures\\nBDI architectures, or Belief Desire Intention architectures, are based on\\nthe three concepts of belief, desire, and intention. A belief is a statement\\nabout the environment that the agent considers to be true. BDI agents have\\na set of beliefs that are similar to the set of facts contained in a rule-based\\nproduction system. A desire is a goal state that the agent would like to\\nreach, and the agent’s intentions are the plans it has for how to behave in\\norder to achieve its desires.\\nAn agent can have an intention to carry out a particular action, in which\\ncase it will probably do so. Alternatively, an agent can have an intention to\\nbring about a particular state.\\nWhen an agent commits to carrying out a particular action, or achieving a\\nparticular goal, it ‘promises’ that it will do so. Hence, a BDI agent has a set\\nof beliefs that lead it to establish a set of desires. T o achieve its desires, the\\nBDI agent considers a number of options and commits to one or more of\\nthem. These options now become the agent’s intentions.\\nIntentions persist until the goals are achieved, or until it becomes unrea-\\nsonable to continue to attempt to achieve them (e.g., if it becomes obvious\\nthat the goals can never be achieved or if new beliefs are developed that\\nlead the agent to change its desires).\\nA bold agent is one that establishes a set of intentions and then aims to\\ncarry them out without ever stopping to consider whether it should change\\nits intentions. A cautious agent is one that considers its intentions continu-\\nally. Kinny and Georgeff (1991) found that bold agents perform better than\\ncautious agents in worlds where the environment does not change very fre-\\nquently and that cautious agents perform better than bold agents in worlds\\nthat change quickly.\\n19.10.3 Other Architectures\\nA number of other agent architectures exist. Logic-based agents apply rules\\nof logical deduction to a symbolic representation of their environment.\\nThe state of such an agent is usually represented using first-order predi-\\ncates, and its behavior is determined by a set of deduction rules, usually\\nexpressed in first-order predicate logic.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 585, 'page_label': '586'}, page_content='19.10 Agent Architectures 559\\nLayer n\\nLayer 2\\nLayer 1\\nInputs\\nInputs\\nOutputs and\\nactions\\nOutputs and\\nactions\\nHorizontal Architecture Vertical Architecture\\n. . .\\nLayer n\\nLayer 2\\nLayer 1\\n. . .\\nFigure 19.2\\nHorizontal and vertical\\nagent architectures\\ncompared\\nIn contrast to logic-based architectures, purely reactive agents do not per-\\nform any symbol manipulation and rely on a simple mapping from inputs\\nto actions.\\nA number of layered architectures exist other than the subsumption archi-\\ntecture. The subsumption architecture is an example of a horizontal lay-\\nered architecture, where each layer receives inputs and contributes to the\\nactions and outputs of the agent. In a vertical layered architecture, input is\\npassed to one layer, which then passes information on to a further layer.\\nActions and outputs are eventually produced by the final layer. These two\\narchitecture types are illustrated in Figure 19.2.\\nT ouringMachines is an example of a horizontal architecture, which is\\nbased on three layers:\\n■ Reactive layer: This layer uses situation rules to react to changes in\\nthe agent’s environment.\\n■ Planning layer: This layer uses a library of plans (called schemas)\\nto determine the behavior of the agent, in order to achieve particu-\\nlar goals. In most situations, this is the layer that decides the main\\nbehavior of the agent.\\n■ Modeling layer:This layer contains a model of the agent and any other\\nagents in the world, in order to avoid conflicts with other agents.\\nInteRRaP is an example of a vertical layered architecture, which has three\\nlayers with very similar functions to the layers of the T ouringMachines\\narchitecture. Each layer in the InteRRap architecture has a database of rele-\\nvant knowledge: the reactive layer has a database of knowledge about the\\nworld the agent inhabits; the planning layer has a database of planning'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 586, 'page_label': '587'}, page_content='560 CHAPTER 19 Intelligent Agents\\nknowledge that contains information about the agent’s plans; the coopera-\\ntion layer (similar to the modeling layer in T ouringMachines) has social\\nknowledge about the other agents and their interactions.\\nIn the T ouringMachines architecture, each layer interacts with the environ-\\nment, directly receiving inputs and producing actions and outputs. In the\\nInteRRap architecture, only the bottom layer (the reactive, behavior layer)\\ninteracts directly with the world. If it is unable to deal with a particular sit-\\nuation, it passes the information on to the next layer, the planning layer.\\nSimilarly, if this layer cannot deal with the current situation, it passes the\\ninformation on to the final layer, the cooperation layer. Outputs are passed\\nback to the behavior layer, which turns them into actions or outputs.\\n19.11 Accessibility\\nWhen playing a game such as chess, each player knows what position he\\nwill be in after making any given move. What he does not usually know is\\nwhat move his opponent will make and, thus, what position he will reach\\nafter his opponent’s move.\\nIn some cases an agent’s state after carrying out a particular action can be\\ndeterministically predicted. In many situations, however, this is not the\\ncase, and the outcome is unpredictable, or stochastic. Given that an agent\\nusually has a certain degree of knowledge about the world and the way its\\nactions affect its state, we can make certain predictions. For example, an\\nagent can say that if it is in state S\\n1 and it takes action A, then it will move\\ninto state S2 with probability p. These probabilities are contained within a\\ntransition model, which enables the agent to make predictions about what\\neffect its actions will have on it and its environment.\\nIf an agent is able to determine all relevant facts about the environment in\\nwhich it operates, then that environment is described as being accessible.I f\\nit is inaccessible, then certain facts are hidden from the agent, although it\\nmay be able to deduce them by maintaining internal information about the\\nstate of the environment. For example, if an agent is in an environment in\\nwhich it is unable to determine the temperature, it may have a rule that says\\n“if you turn up the heating, the temperature will increase. ”\\nWe could consider two types of agents that play chess. One agent might have\\nthe ability to examine the board at each move of the game and make deci-\\nsions about what move to make from that point. The agent does not have the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 587, 'page_label': '588'}, page_content='19.12 Learning Agents 561\\nability to remember moves that have been made in the past, and thus the\\nonly way it can determine the current position is by examining the board.\\nThis agent acts in an accessible environment because, at any given point, it\\nhas access to all the information it needs to be able to play the game. If we\\nimagine that this agent is playing a game where half of the board is covered\\nup, and it is unable to see what happens there, then we can see that the\\nagent would have great difficulties because it would have no way of deter-\\nmining what was happening on that side of the board apart from a few lim-\\nited facts it could deduce, such as “my king is on this side of the board, so I\\nknow I do not have a king on the other side of the board. ”\\nA different type of agent might play the game without any direct access to\\nthe board at all. This agent stores information about the moves that have\\nbeen made in the past and is able to use this information to determine the\\ncurrent position of the board. This agent would play equally well whether\\nthe board were entirely visible or entirely covered up.\\nThis agent operates in an inaccessible environment, but, in fact, because the\\nenvironment it operates in is entirely deterministic, it is able to derive com-\\nplete knowledge about the board at all times.\\nAn agent that played a game such as poker would need to be able to act in\\nan inaccessible, stochastic environment because the cards the opponent has\\nare neither visible nor deterministically allocated.\\nIn an accessible, stochastic environment, agents use Markov decision\\nprocesses (MDPs) to determine the best course of action. In an inaccessi-\\nble, stochastic environment, agents use partially observable Markov deci-\\nsion processes (POMDPs). Clearly, POMDPs must operate with far less\\ninformation and so tend to be more complex than MDPs.\\n19.12 Learning Agents\\nMachine learning is covered in more detail in Part 4 of this book. An agent\\nthat is capable of learning (a learning agent) is able to acquire new knowl-\\nedge and skills and is able to use the new knowledge and skills to improve\\nits performance.\\nOne common way to provide agents with the ability to learn is to use neu-\\nral networks, which are covered in more detail in Chapter 11. A neural net-\\nwork is designed to learn in a similar manner to the way a human brain'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 588, 'page_label': '589'}, page_content='562 CHAPTER 19 Intelligent Agents\\nlearns. Another method for enabling agents to learn is to use genetic algo-\\nrithms. One way to use genetic algorithms in this way is to have the genetic\\nalgorithm breed populations of agents, with the aim of breeding a highly\\nsuccessful agent. Another way is to have each agent use a genetic algorithm\\nto develop suitable strategies for dealing with particular problems.\\n19.12.1 Multiagent Learning\\nMultiagent systems are often required to solve problems in dynamic and\\nunpredictable environments. In these circumstances, a learning ability is\\nparticularly important because the environment can change too quickly for\\npredetermined behaviors to be effective.\\nMultiagent learning can in many ways be more impressive than the learn-\\ning carried out by individual agents. Each agent in a learning multiagent\\nsystem can learn independently of the other agents and can also learn from\\nthe other agents.\\nIn this way, the agents can explore multiple potential strategies in parallel,\\nand when one agent discovers a particularly effective strategy, it can pass\\nthis knowledge on to other agents. For this reason, when the environment\\nchanges, multiagent learning systems are able to adapt much more quickly\\nthan nonlearning systems, or even individual learning agents.\\nIn centralized learning , the agents learn on an individual and distinct\\nbasis, whereas in decentralized learning , the actions of the individual\\nagents lead to the whole system learning. The classifier systems described in\\nChapter 13 are an example of a decentralized multiagent learning system,\\nwhere each rule can be thought of as a separate agent, and where the whole\\nsystem learns by experience how best to solve a problem.\\n19.13 Robotic Agents\\nThe agents described in this chapter so far have been software agents—they\\nexist only in a virtual world. Robotic agents,o r  robots, are artificial agents\\nthat exist physically in the real world.\\nMobile robotic agents controlled by Brooks’ subsumption architecture\\nhave been briefly described in Section 19.10.1.\\nRobotic agents operate in an inaccessible, stochastic environment. The real\\nworld has many properties that make the tasks of robotic agents much'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 589, 'page_label': '590'}, page_content='19.14 Braitenberg Vehicles 563\\nharder than those of many software agents. An ability to deal with uncer-\\ntainty is clearly important, as is robustness in the face of extremely unpre-\\ndictable and potentially dangerous environments.\\nRobots have been designed that build cars, using robotic arms and con-\\nveyer belts.\\nMore sophisticated are the robots that are designed to explore other planets\\nand collect samples for scientific analysis. Such robots, of course, require\\nautonomy: they cannot be controlled directly by human input because they\\nwould be too far away from the earth. One important aspect of such robots\\nis their ability to walk: this involves not just knowing how to move legs in\\nsuch a way as to move forward, but also how to navigate over hills and\\nrocks, around pot-holes and through valleys. Agents such as Atilla and\\nGenghis, designed by the MIT Mobot Lab (Mobot means “mobile robot”),\\nhave these abilities and are modeled on insects.\\nGenghis has six legs and a number of sensors that enable it to determine\\ncertain facts about its inaccessible environment. The interesting thing\\nabout Genghis is that nobody ever told it how to walk or steer around\\nobstacles. Its brain consists of 57 augmented finite state machines, each of\\nwhich is responsible for a simple piece of behavior, such as lifting a leg or\\nwandering. Using these AFSMs and feedback from its sensors, Genghis was\\nable to learn to walk from the experience of trying and failing to do so.\\n19.14 Braitenberg Vehicles\\nBraitenberg vehicles were invented by a neuroscientist, Valentino Braiten-\\nberg, in the 1980s. Braitenberg vehicles are imaginary robots used by Brait-\\nenberg in thought experiments on the nature of intelligence. There are 14\\ndifferent classes of vehicles, ranging from extremely simple to fairly com-\\nplex. We will consider just the six simplest types.\\nEven the simplest of his vehicles can exhibit interesting behaviors and tell\\nus a great deal about our assumptions concerning intelligence and thought.\\nThe simplest type of Braitenberg vehicle, known as vehicle 1, simply has\\none motor and a sensor. The sensor is wired directly to the motor, such that\\nthe more of whatever the sensor is designed to sense there is, the faster the\\nmotor turns. For example, if the sensor were a light sensor, then the motor\\nwould turn faster when the sensor could detect more light.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 590, 'page_label': '591'}, page_content='564 CHAPTER 19 Intelligent Agents\\nFigure 19.3\\nTwo varieties of Braiten-\\nberg vehicles type 2, seen\\nfrom above\\nThe behavior of this vehicle is very simple: the more light there is, the faster\\nit moves. It would normally move in a straight line, although imperfections\\nin its environment (such as friction and obstacles) might cause it to deviate.\\nThe second type of Braitenberg vehicle has two sensors and two motors.\\nThe motors and sensors are placed symmetrically around the vehicle, as\\nshown in Figure 19.3.\\nIn the first vehicle shown in Figure 19.3, the left-hand sensor (the sensors\\nare on the front of the vehicle) is connected to the left-hand motor, and the\\nright-hand sensor to the right-hand motor. In the second vehicle shown,\\nthe sensors and motors are connected the other way around. The first vehi-\\ncle will tend to move away from the source that its sensors detect, whereas\\nthe second vehicle will move toward it.\\nThese vehicles can be thought of as timid (the one that moves away from\\nthe source) and bold (the one that moves toward the source).\\nLet us now consider a type of the timid vehicle, which has a sensor for\\nproximity and where its motors have a built-in tendency to move even\\nwithout any stimulation to the sensors. When placed in a simple maze, this\\nvehicle will navigate through the maze without bumping into the walls.\\nClearly, apparently complex behavior can emerge from very simple con-\\ncepts. This timid vehicle was certainly not designed to traverse a maze, and\\nit does not have any knowledge of mazes or the world. An observer who did\\nnot know how the vehicle worked might conclude that it relied on a very\\nsophisticated form of Artificial Intelligence.\\nIt is interesting to note at this point some of the words that we have been\\nusing to describe agents: timid, bold, cautious, and so on. There is a ten-\\ndency to anthropomorphize the behaviors of agents, which is at least partly\\ndue to the impression that agents can give of having almost human-like\\nintelligence.\\nThe third type of vehicle is similar to the second type except that the sen-\\nsors are wired in such a way that they inhibit the motors: the more stimula-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 591, 'page_label': '592'}, page_content='19.15 Chapter Summary 565\\ntion they receive, the slower the motors turn. These types of vehicles will\\ntend to move toward a source of stimulation but will end up near the\\nsource, either facing it or turned away from it, depending on which way its\\nsensors are wired to the motors.\\nBraitenberg vehicles can have more than one type of sensor—for example,\\na vehicle might have light sensors and proximity detectors for objects.\\nThese sensors can be connected to motors in different ways, producing\\nmore and more complex behaviors.\\nThe fourth type of Braitenberg vehicle has a nonlinear relationship\\nbetween input to the sensors and the speed of the motors. For example,\\none of these vehicles might move slowly toward a light source and\\nspeed up as it gets closer, then slow down again as it gets very close to\\nthe source.\\nThe fifth type of vehicle has a primitive memory that can be used to store\\ninformation about events that happened in the past.\\nThe sixth type of Braitenberg vehicle is evolved using artificial evolution, as\\ndescribed in Chapters 13 and 14.\\nBraitenberg vehicles teach us the following principle, which Braitenberg\\ncalled the principle of “Uphill Analysis and Downhill Invention”: It is easier\\nto invent something than to analyze it. Fully functioning Braitenberg vehicles\\ncan be built using easily available components, and yet their behavior can be\\nextremely complex and, in some cases, impossible to analyze or explain.\\n19.15 Chapter Summary\\n■ An agent is an entity that carries out a task on behalf of a human user.\\n■ A software agent is an agent that exists solely as a computer program.\\n■ Intelligent agents have more knowledge or understanding of their\\nenvironment than simple agents and are able to use this intelli-\\ngence to carry out their tasks more effectively.\\n■ Autonomous agents are able to carry out their tasks without direct\\ninput from a human.\\n■ Some agents are able to learn from their user, from other agents,\\nfrom the environment, or by observing the consequences of their\\nown actions.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 592, 'page_label': '593'}, page_content='566 CHAPTER 19 Intelligent Agents\\n■ Reactive agents simply react to the environment they are in, using\\nsituated action rules, which provide an action for each situation.\\n■ Goal-based agents seek to achieve some goal, whereas utility-based\\nagents seek to maximize some utility function.\\n■ Interface agents are automated personal assistants.\\n■ Mobile agents are able to travel over a network, such as the Internet.\\n■ An information agent collects information (often from the Inter-\\nnet) on behalf of its owner.\\n■ Multiagent systems use a number of agents that usually collaborate\\ntogether to achieve some common goal.\\n■ The subsumption architecture is an example of a vertically layered\\narchitecture for controlling robots.\\n■ BDI architectures use beliefs, desires, and intentions to control agents.\\n■ An accessible environment is one in which all necessary facts are\\navailable to the agent. Many agents must be able to operate in inac-\\ncessible environments and often in stochastic ones, where the\\nchanges in the environment are unpredictable.\\n■ Robotic agents operate in the real world.\\n19.16 Review Questions\\n19.1 “A computer virus is a kind of intelligent agent. ” Discuss this state-\\nment. Consider the various agent properties that have been dis-\\ncussed in this chapter. Which of these properties do computer\\nviruses have?\\n19.2 Explain what is meant by the following terms in the context of agents:\\n■ intelligence\\n■ autonomy\\n■ learning\\n■ collaboration\\n■ utility\\n19.3 Explain the idea behind the BDI architecture. Why do you think\\nthis architecture is particularly appealing to human researchers?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 593, 'page_label': '594'}, page_content='19.18 Further Reading 567\\n19.4 Explain the nature of the first six types of Braitenberg vehicles.\\nDiscuss how these vehicles can help us to understand the nature of\\nintelligence.\\n19.5 Think of a real-world interface agent. Discuss to what extent this\\nagent has autonomy, learning abilities, and intelligence.\\n19.6 What do Braitenberg vehicles teach us about intelligence? Do you\\nthink the intelligence given to Braitenberg vehicles could be put to\\nsome practical use?\\n19.7 In Michael Crichton’s novel, Prey, he postulates a multiagent system\\nconsisting of millions of tiny robotic agents. The system evolves over\\na period of days to develop human-like intelligence, and a belligerent\\ndesire to destroy life. Discuss how plausible you think this idea is, in\\nthe context of the subjects introduced in this chapter.\\n19.17 Exercises\\n19.1 Implement an intelligent agent system to carry out a simple task\\nfor you in the programming language of your choice.\\n19.2 Investigate a software agent that comes with your computer, or\\nfind one that you can download for free. Explore its limitations\\nand its capabilities. T o what extent would you describe it as “intel-\\nligent”? What simple improvements would you suggest for the\\nagent? Which of the following properties does the agent exhibit:\\n■ intelligence\\n■ autonomy\\n■ ability to learn\\n■ cooperation\\n■ benevolence\\n■ veracity\\nT o what extent would it still be useful if it did not have the proper-\\nties that it does have? Which of the above properties might be given\\nto the agent to improve it? How would it be improved?\\n19.18 Further Reading\\nSeveral texts cover the subject of Artificial Intelligence from the perspective\\nof Artificial Agents—in particular, Russell and Norvig (1995) and Pfeifer'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 594, 'page_label': '595'}, page_content='568 CHAPTER 19 Intelligent Agents\\nand Scheier (1999). Weiss (1999) provides an excellent exploration of mul-\\ntiagent systems.\\nBrooks’ subsumption architecture was introduced in A Robust Layered\\nControl System For a Mobile Robot (from IEEE Journal of Robotics and\\nAutomation, RA-2, April, pp. 14–23), and was also published as MIT AI\\nMemo 864 (1985).\\nBraitenberg (1986) provides a fascinating description of his vehicles, as well\\nas providing an absorbing philosophical argument. A good practical expla-\\nnation of Braitenberg’s vehicles is also found in Pfeifer and Scheier (2000)\\nBehavior-Based Robotics, by Ronald C. Arkin (1998 – MIT Press)\\nSoftware Agents, edited by Jeffrey M. Bradshaw (1997 – AAAI Press)\\nVehicles: Experiments in Synthetic Psychology , by Valentino Braitenberg\\n(1986 – MIT Press)\\nIntelligent Agents for Mobile and Virtual Media , edited by Rae Earnshaw,\\nJohn Vince, and Margaret A. Arden (2002 – Springer V erlag)\\nCommitment and Effectiveness of Situated Agents , by D. Kinny and M.\\nGeorgeff (1991 – in Proceedings of the Twelfth International Joint Conference\\non Artificial Intelligence, pp. 82–88)\\nBraitenberg Creatures, by David W. Hogg, Fred Martin, and Mitchel Resnick\\n(1991 – originally published as Epistemology and Learning Memo #13)\\nA Learning Interface Agent for Scheduling Meetings , by R. Kozierok and P .\\nMaes (1993 – in Proceedings of the ACM-SIGCHI International Workshop on\\nIntelligent User Interfaces)\\nEvolutionary Robotics: The Biology, Intelligence, and Technology of Self-Orga-\\nnizing Machines, by Stefano Nolfi and Dario Floreano (2000 – MIT Press)\\nEvolving Hexapod Gaits Using a Cyclic Genetic Algorithm , by Gary Parker\\n(1997 – in Proceedings of the IASTED International Conference on Artificial\\nIntelligence and Soft Computing, pp. 141–144)\\nGenerating Arachnid Robot Gaits with Cyclic Genetic Algorithms , by Gary\\nParker (1998 - in Genetic Programming III, pp. 576–583)\\nMetachronal Wave Gait Generation for Hexapod Robots , by Gary Parker\\n(1998 – in Proceedings of the Seventh International Symposium on Robotics\\nwith Applications)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 595, 'page_label': '596'}, page_content='19.18 Further Reading 569\\nUnderstanding Intelligence, by Rolf Pfeifer and Christian Scheier (2000 –\\nMIT Press)\\nLayered Learning in Multiagent Systems: A Winning Approach to Robotic\\nSoccer, by Peter Stone (2000 – MIT Press)\\nMultiagent Systems: A Modern Approach to Distributed Artificial Intelligence,\\nedited by Gerhard Weiss (1999 – MIT Press)\\nIntroduction to MultiAgent Systems , by Michael Wooldridge (2002 – John\\nWiley & Sons)\\nStrategic Negotiation in Multiagent Environments , by Sarit Kraus (2001 –\\nMIT Press)\\nIntelligent Information Agents: The Agentlink Perspective (Lecture Notes in\\nComputer Science, 2586) , edited by Matthias Klusch, Sonia Bergamaschi,\\nand Pete Edwards (2003 – Springer V erlag)\\nAn Introduction to AI Robotics, by Robin R. Murphy (2000 – MIT Press)\\nReal-Time and Multi-Agent Systems, by Ammar Attoui (2000 – Springer V erlag)\\nUnderstanding Agent Systems, edited by Mark D’Inverno and Michael Luck\\n(2001 – Springer V erlag)\\nAgent Technology: Foundations, Applications, and Markets , edited by\\nNicholas R. Jennings and Michael J. Wooldridge (1998 – Springer V erlag)\\nSocially Intelligent Agents - Creating Relationships with Computers and\\nRobots, edited by Kerstin Dautenhahn, Alan H. Bond, Lola Canamero, and\\nBruce Edmonds (2002 – Kluwer Academic Publishers)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 596, 'page_label': '597'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 597, 'page_label': '598'}, page_content='20CHAPTER\\nUnderstanding Language\\nPhilosophy is a battle against the bewitchment of our intelligence by means\\nof language.\\n—Ludwig Wittgenstein,Philosophische Untersuchungen\\nLanguage is a form of human reason, and has its reasons which are\\nunknown to man.\\n—Claude Lévi-Strauss, La Pensée Sauvage\\nI linger yet with nature, for the night\\nHath been to me a more familiar face\\nThan that of man; and in her starry shade\\nOf dim and solitary loveliness\\nI learned the language of another world.\\n—Lord Byron, Manfred\\n20.1 Introduction\\nThis chapter explores several techniques that are used to enable humans to\\ninteract with computers via natural human languages.\\nNatural languages are the languages used by humans for communication\\n(among other functions). They are distinctly different from formal lan-\\nguages, such as C++, Java, and PROLOG. One of the main differences,\\nwhich we will examine in some detail in this chapter, is that natural lan-\\nguages are ambiguous, meaning that a given sentence can have more than'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 598, 'page_label': '599'}, page_content='572 CHAPTER 20 Understanding Language\\none possible meaning, and in some cases the correct meaning can be very\\nhard to determine. Formal languages are almost always designed to ensure\\nthat ambiguity cannot occur. Hence, a given program written in C++ can\\nhave only one interpretation. This is clearly desirable because otherwise the\\ncomputer would have to make an arbitrary decision as to which interpreta-\\ntion to work with.\\nIt is becoming increasingly important for computers to be able to under-\\nstand natural languages. T elephone systems are now widespread that are\\nable to understand a narrow range of commands and questions to assist\\ncallers to large call centers, without needing to use human resources.\\nAdditionally, the quantity of unstructured textual data that exists in the\\nworld (and in particular, on the Internet) has reached unmanageable pro-\\nportions. For humans to search through these data using traditional tech-\\nniques such as Boolean queries or the database query language SQL is\\nimpractical. The idea that people should be able to pose questions in their\\nown language, or something similar to it, is an increasingly popular one.\\nOf course, English is not the only natural language. A great deal of research\\nin natural language processing and information retrieval is carried out in\\nEnglish, but many human languages differ enormously from English. Lan-\\nguages such as Chinese, Finnish, and Navajo have almost nothing in com-\\nmon with English (although of course Finnish uses the same alphabet).\\nHence, a system that can work with one human language cannot necessar-\\nily deal with any other human language.\\nIn this section we will explore two main topics. First, we will examine natu-\\nral language processing, which is a collection of techniques used to enable\\ncomputers to “understand” human language. In general, they are con-\\ncerned with extracting grammatical information as well as meaning from\\nhuman utterances but they are also concerned with understanding those\\nutterances, and performing useful tasks as a result.\\nTwo of the earliest goals of natural language processing were automated trans-\\nlation (which is explored in this chapter) and database access. The idea here\\nwas that if a user wanted to find some information from a database, it would\\nmake much more sense if he or she could query the database in her language,\\nrather than needing to learn a new formal language such as SQL.\\nInformation retrieval is a collection of techniques used to try to match a\\nquery (or a command) to a set of documents from an existing corpus of'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 599, 'page_label': '600'}, page_content='20.2 Natural Language Processing 573\\ndocuments. Systems such as the search engines that we use to find data on\\nthe Internet use information retrieval (albeit of a fairly simple nature).\\n20.2 Natural Language Processing\\nIn dealing with natural language, a computer system needs to be able to\\nprocess and manipulate language at a number of levels.\\n1. Phonology. This is needed only if the computer is required to\\nunderstand spoken language. Phonology is the study of the sounds\\nthat make up words and is used to identify words from sounds. We\\nwill explore this in a little more detail later, when we look at the\\nways in which computers can understand speech.\\n2. Morphology. This is the first stage of analysis that is applied to\\nwords, once they have been identified from speech, or input into\\nthe system. Morphology looks at the ways in which words break\\ndown into components and how that affects their grammatical sta-\\ntus. For example, the letter “s” on the end of a word can often either\\nindicate that it is a plural noun or a third-person present-tense\\nverb.\\n3. Syntax. This stage involves applying the rules of the grammar from\\nthe language being used. Syntax determines the role of each word in\\na sentence and, thus, enables a computer system to convert sen-\\ntences into a structure that can be more easily manipulated.\\n4. Semantics.This involves the examination of the meaning of words and\\nsentences. As we will see, it is possible for a sentence to be syntactically\\ncorrect but to be semantically meaningless. Conversely, it is desirable\\nthat a computer system be able to understand sentences with incorrect\\nsyntax but that still convey useful information semantically.\\n5. Pragmatics. This is the application of human-like understanding to\\nsentences and discourse to determine meanings that are not imme-\\ndiately clear from the semantics. For example, if someone says,\\n“Can you tell me the time?” , most people know that “yes” is not a\\nsuitable answer. Pragmatics enables a computer system to give a\\nsensible answer to questions like this.\\nIn addition to these levels of analysis, natural language processing systems\\nmust apply some kind of world knowledge. In most real-world systems, this'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 600, 'page_label': '601'}, page_content='574 CHAPTER 20 Understanding Language\\nworld knowledge is limited to a specific domain (e.g., a system might have\\ndetailed knowledge about the Blocks World and be able to answer questions\\nabout this world). The ultimate goal of natural language processing would\\nbe to have a system with enough world knowledge to be able to engage a\\nhuman in discussion on any subject. This goal is still a long way off.\\nWe will now look at the individual stages of analysis that are involved in\\nnatural language processing.\\n20.2.1 Morphological Analysis\\nIn studying the English language, morphology is relatively simple. We have\\nendings such as -ing, -s, and -ed, which are applied to verbs; endings such as\\n-s and -es, which are applied to nouns; we also have the ending -ly, which\\nusually indicates that a word is an adverb. We also have prefixes such as\\nanti-, non-, un-, and in-, which tend to indicate negation, or opposition. We\\nalso have a number of other prefixes and suffixes that provide a variety of\\nsemantic and syntactic information.\\nIn practice, however, morphologic analysis for the English language is not\\nterribly complex, particularly when compared with agglutinative languages\\nsuch as German, which tend to combine words together into single words\\nto indicate combinations of meaning.\\nMorphologic analysis is mainly useful in natural language processing for\\nidentifying parts of speech (nouns, verbs, etc.) and for identifying which\\nwords belong together. In English, word order tends to provide more of this\\ninformation than morphology, however. In languages such as Latin, word\\norder was almost entirely superficial, and the morphology was extremely\\nimportant. Languages such as French, Italian, and Spanish lie somewhere\\nbetween these two extremes.\\nAs we will see in the following sections, being able to identify the part of\\nspeech for each word is essential to understanding a sentence. This can\\npartly be achieved by simply looking up each word in a dictionary, which\\nmight contain for example the following entries:\\n(swims, verb, present, singular, third person)\\n(swimmer, noun, singular)\\n(swim, verb, present, singular, first and second persons)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 601, 'page_label': '602'}, page_content='20.2 Natural Language Processing 575\\n(swim, verb, present plural, first, second, and third persons)\\n(swimming, participle)\\n(swimmingly, adverb)\\n(swam, verb, past)\\nClearly, a complete dictionary of this kind would be unfeasibly large. A more\\npractical approach is to include information about standard endings, such as:\\n(-ly,a d v e r b )\\n(-ed, verb, past)\\n(-s, noun, plural)\\nThis works fine for regular verbs, such as walk, but for all natural languages\\n(except Esperanto, the human-invented language) there are large numbers\\nof irregular verbs, which do not follow these rules. V erbs such as to be and\\nto do are particularly difficult in English as they do not seem to follow any\\nmorphologic rules.\\nThe most sensible approach to morphologic analysis is thus to include a set\\nof rules that work for most regular words and then a list of irregular words.\\nFor a system that was designed to converse on any subject, this second list\\nwould be extremely long. Most natural language systems currently are\\ndesigned to discuss fairly limited domains and so do not need to include\\nover-large look-up tables.\\nIn most natural languages, as well as the problem posed by the fact that\\nword order tends to have more importance than morphology, there is also\\nthe difficulty of ambiguity at a word level. This kind of ambiguity can be\\nseen in particular in words such as trains, which could be a plural noun or\\na singular verb, and set, which can be a noun, verb, or adjective. We will see\\nlater how parsers are designed to overcome these difficulties.\\n20.2.2 BNF\\nIn Section 20.2.4, we look at the methods that are available forparsing a piece\\nof text. Parsing involves mapping a linear piece of text onto a hierarchy that\\nrepresents the way the various words interact with each other syntactically.\\nFirst, we will look at grammars, which are used to represent the rules that\\ndefine how a specific language is built up.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 602, 'page_label': '603'}, page_content='576 CHAPTER 20 Understanding Language\\nMost natural languages are made up of a number of parts of speech, mainly\\nthe following:\\n■ verb\\n■ noun\\n■ adjective\\n■ adverb\\n■ conjunction\\n■ pronoun\\n■ article\\nIn fact it is useful when parsing to combine words together to form syntac-\\ntic groups. Hence, the words,a dog, which consist of an article and a noun,\\ncan also be described as a noun phrase . A noun phrase is one or more\\nwords that combine together to represent an object or thing (material or\\notherwise) that can be described by a noun. Hence, the following are valid\\nnoun phrases:\\n■ Christmas\\n■ the dog\\n■ that packet of chips\\n■ the boy who had measles last year and nearly died\\n■ my favorite color\\nNote that a noun phrase is not a sentence—it is part of a sentence.\\nSimilarly, we have verb phrases. A verb phrase is one or more words that\\nrepresent an action. The following are valid verb phrases:\\n■ swim\\n■ eat that packet of chips\\n■ walking\\nA simple way to describe a sentence is to say that it consists of a noun\\nphrase and a verb phrase. Hence, for example:\\nThat dog is eating my packet of chips.\\nIn this sentence,that dog is a noun phrase, and is eating my packet of chips is\\na verb phrase. Note that the verb phrase is in fact made up of a verb phrase,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 603, 'page_label': '604'}, page_content='20.2 Natural Language Processing 577\\nis eating, and a noun phrase, my packet of chips. In the next section, we will\\nexplore this idea in more detail and see how it enables us to build a parse\\ntree to identify the syntactic structure of a sentence.\\nA language is defined partly by its grammar. The rules of grammar for a\\nlanguage such as English can be written out in full, although it would be a\\ncomplex process to do so. T o allow a natural language processing system to\\nparse sentences, it needs to have knowledge of the rules that describe how a\\nvalid sentence can be constructed.\\nThese rules are often written in what is known as Backus–Naur form (also\\nknown as Backus normal form—both names are abbreviated as BNF).\\nBNF is widely used by computer scientists to define formal languages such as\\nC++ and Java. We can also use it to define the grammar of a natural language.\\nA grammar specified in BNF consists of the following components:\\n1. T erminal symbols.Each terminal symbol is a symbol or word that\\nappears in the language itself. In English, for example, the terminal\\nsymbols are our dictionary words such as the, cat, dog, and so on.\\nIn formal languages, the terminal symbols include variable names\\nsuch as x, y, and so on, but for our purposes we will consider the\\nterminal symbols to be the words in the language.\\n2. Nonterminal symbols. These are the symbols such as noun, verb\\nphrase, and conjunction that are used to define words and phrases\\nof the language. A nonterminal symbol is so-named because it is\\nused to represent one or more terminal symbols.\\n3. The start symbol. The start symbol is used to represent a complete\\nsentence in the language. In our case, the start symbol is simply\\nsentence, but in first-order predicate logic, for example, the start\\nsymbol would be expression.\\n4. Rewrite rules. The rewrite rules define the structure of the gram-\\nmar. Each rewrite rule details what symbols (terminal or nonter-\\nminal) can be used to make up each nonterminal symbol.\\nLet us now look at rewrite rules in more detail.\\nWe saw above that a sentence could take the following form:\\nnoun phrase verb phrase'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 604, 'page_label': '605'}, page_content='578 CHAPTER 20 Understanding Language\\nWe thus write the following rewrite rule:\\nSentence → NounPhrase VerbPhrase\\nThis does not mean that every sentence must be of this form, but simply\\nthat a string of symbols that takes on the form of the right-hand side can be\\nrewritten in the form of the left-hand side. Hence, if we see the words\\nThe cat sat on the mat\\nwe might identify that the cat is a noun phrase and that sat on the mat is a\\nverb phrase. We can thus conclude that this string forms a sentence.\\nWe can also use BNF to define a number of possible noun phrases. Note\\nhow we use the “|” symbol to separate the possible right-hand sides in BNF:\\nNounPhrase → Noun\\n| Article Noun\\n| Adjective Noun\\n| Article Adjective Noun\\nSimilarly, we can define a verb phrase:\\nVerbPhrase → Verb\\n| Verb NounPhrase\\n| Adverb Verb NounPhrase\\nThe structure of human languages varies considerably. Hence, a set of rules\\nlike this will be valid for one language, but not necessarily for any other lan-\\nguage. For example, in English it is usual to place the adjective before the\\nnoun (black cat, stale bread), whereas in French, it is often the case that the\\nadjective comes after the noun (moulin rouge).\\nThus far, the rewrite rules we have written consist solely of nonterminal\\nsymbols. Rewrite rules are also used to describe the parts of speech of indi-\\nvidual words (or terminal symbols):\\nNoun → cat\\n| dog\\n| Mount Rushmore\\n| chickens'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 605, 'page_label': '606'}, page_content='20.2 Natural Language Processing 579\\nVerb → swims\\n| eats\\n| climbs\\nArticle → the\\n| a\\nAdjective → black\\n| brown\\n| green\\n| stale\\nThese rules form a lexicon of the language, which details which words are\\navailable and which parts of speech they are.\\n20.2.3 Grammars\\nWe have briefly looked at the ways in which grammars can be described.\\nLet us now examine the types of grammars that exist.\\nNoam Chomsky invented a hierarchy of grammars. The hierarchy consists\\nof four main types of grammars.\\nThe simplest grammars are used to define regular languages . A regular\\nlanguage is one that can be described or understood by a finite state\\nautomaton. Such languages are very simplistic and allow sentences such as\\n“aaaaabbbbbb.” Recall that a finite state automaton consists of a finite\\nnumber of states, and rules that define how the automaton can transition\\nfrom one state to another.\\nA finite state automaton could be designed that defined the language that\\nconsisted of a string of one or more occurrences of the letter a.H e n c e ,t h e\\nfollowing strings would be valid strings in this language:\\naaa\\na\\naaaaaaaaaaaaaaaaa\\nRegular languages are of interest to computer scientists, but are not of great\\ninterest to the field of natural language processing because they are not\\npowerful enough to represent even simple formal languages, let alone the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 606, 'page_label': '607'}, page_content='580 CHAPTER 20 Understanding Language\\nmore complex natural languages. Sentences defined by a regular grammar\\nare often known as regular expressions.\\nThe grammar that we defined above using rewrite rules is a context-free\\ngrammar. It is context free because it defines the grammar simply in terms\\nof which word types can go together—it does not specify the way that\\nwords should agree with each. For example, the grammar defined in Sec-\\ntion 20.2.2 allows the following sentence, which is grammatically correct\\n(although not necessarily semantically):\\nA stale dog climbs Mount Rushmore.\\nIt also, however, allows the following sentence, which is not grammati-\\ncally correct:\\nChickens eats.\\nA context-free grammar can have only at most one terminal symbol on the\\nright-hand side of its rewrite rules. Rewrite rules for a context-sensitive\\ngrammar, in contrast, can have more than one terminal symbol on the\\nright-hand side. This enables the grammar to specify number, case, tense,\\nand gender agreement. Each context-sensitive rewrite rule must have at least\\nas many symbols on the right-hand side as it does on the left-hand side.\\nRewrite rules for context-sensitive grammars have the following form:\\nA X B → A Y B\\nwhich means that in the context of A and B, X can be rewritten as Y.E a c h\\nof A, B, X, and Y can be either a terminal or a nonterminal symbol.\\nContext-sensitive grammars are most usually used for natural language\\nprocessing because they are powerful enough to define the kinds of gram-\\nmars that natural languages use. Unfortunately, they tend to involve a\\nmuch larger number of rules and are a much less natural way to describe\\nlanguage, making them harder for human developers to design than con-\\ntext-free grammars.\\nThe final class of grammars in Chomsky’s hierarchy consists of recursively\\nenumerable grammars (also known as unrestricted grammars). A recur-\\nsively enumerable grammar can define any language and has no restric-\\ntions on the structure of its rewrite rules. Such grammars are of interest to\\ncomputer scientists but are not of great use in the study of natural language\\nprocessing.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 607, 'page_label': '608'}, page_content='20.2 Natural Language Processing 581\\nSentence\\nadjective nounarticle verb article noun\\nNoun phrase\\nVerb phraseNoun phrase\\nThe black cat the road crossed\\nFigure 20.1\\nParse tree for the sentence\\n“the black cat crossed the\\nroad”\\n20.2.4 Parsing: Syntactic Analysis\\nAs we have seen, morphologic analysis can be used to determine to which\\npart of speech each word in a sentence belongs. We will now examine how\\nthis information is used to determine the syntactic structure of a sentence.\\nThis process, in which we convert a sentence into a tree that represents the\\nsentence’s syntactic structure, is known as parsing.\\nParsing a sentence tells us whether it is a valid sentence, as defined by our\\ngrammar (for this section, we will assume that we are working with the\\nEnglish language and that the grammar we are using is English grammar).\\nIf a sentence is not a valid sentence, then it cannot be parsed.\\nParsing a sentence involves producing a tree, such as that shown in Figure\\n20.1, which shows the parse tree for the following sentence:\\nThe black cat crossed the road.\\nThis tree shows how the sentence is made up of a noun phrase and a verb\\nphrase. The noun phrase consists of an article, an adjective, and a noun.\\nThe verb phrase consists of a verb and a further noun phrase, which in turn\\nconsists of an article and a noun.\\nParse trees can be built in a bottom-up fashion or in a top-down fashion.\\nBuilding a parse tree from the top down involves starting from a sentence\\nand determining which of the possible rewrites for Sentence can be applied\\nto the sentence that is being parsed. Hence, in this case, Sentence would be\\nrewritten using the following rule:\\nSentence → NounPhrase VerbPhrase'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 608, 'page_label': '609'}, page_content='582 CHAPTER 20 Understanding Language\\nThen the verb phrase and noun phrase would be broken down recursively\\nin the same way, until only terminal symbols were left.\\nWhen a parse tree is built from the top down, it is known as aderivation tree.\\nT o build a parse tree from the bottom up, the terminal symbols of the sen-\\ntence are first replaced by their corresponding nonterminals (e.g., cat is\\nreplaced by noun), and then these nonterminals are combined to match the\\nright-hand sides of rewrite rules. For example, the and road would be com-\\nbined using the following rewrite rule:\\nNounPhrase → Article Noun\\nIn the next section we examine a practical example of a parser and see\\nh o wi tw o r k s .\\n20.2.5 Transition Networks\\nA transition network is a finite state automaton that is used to represent a\\npart of a grammar. A transition network parser uses a number of these\\ntransition networks to represent its entire grammar. Each network repre-\\nsents one nonterminal symbol in the grammar. Hence, in the grammar for\\nthe English language, we would have one transition network for Sentence,\\none for Noun Phrase, one for Verb Phrase, one for Verb, and so on.\\nFigure 20.2 shows the transition network equivalents for three produc-\\ntion rules.\\nIn each transition network, S1 is the start state, and the accepting state, or\\nfinal state, is denoted by a heavy border. When a phrase is applied to a tran-\\nsition network, the first word is compared against one of the arcs leading\\nfrom the first state. If this word matches one of those arcs, the network\\nmoves into the state to which that arc points. Hence, the first network\\nshown in Figure 20.2, when presented with a Noun Phrase, will move from\\nstate S1 to state S2.\\nIf a phrase is presented to a transition network and no match is found from\\nthe current state, then that network cannot be used and another network\\nmust be tried. Hence, when starting with the phrase the cat sat on the mat ,\\nnone of the networks shown in Figure 20.2 will be used because they all\\nhave only nonterminal symbols, whereas all the symbols in the cat sat on the\\nmat are terminal. Hence, we need further networks, such as the ones shown\\nin Figure 20.3, which deal with terminal symbols.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 609, 'page_label': '610'}, page_content='20.2 Natural Language Processing 583\\nVerb\\nVerb\\nNoun\\nNoun\\nNoun\\nNoun\\nNounPhrase\\nSentence → NounPhrase VerbPhrase\\nVerbPhrase → Verb\\n | Verb Noun\\nNounPhrase → Noun\\n| Article Noun\\n| Article Adjective Noun\\nVerbPhrase\\nS2\\nS2\\nS3\\nS1\\nS2S1 S3\\nS3\\nS4\\nS1\\nVerbPhrase\\nNounPhrase\\nSentence\\nTransition NetworkProduction Rule\\nArticle\\nAdjective\\nFigure 20.2\\nTransition network equiva-\\nlents for three rewrite\\nrules\\ncat\\nmat\\nthe\\na\\nsat\\nNoun → cat\\n | mat\\nArticle → the\\nVerb → sat\\n | a\\nS1\\nS2S1\\nS2\\nS2\\nS1\\nNoun\\nArticle\\nVerb\\nTransition NetworkProduction Rule\\nFigure 20.3\\nTransition network equiva-\\nlents for three rewrite\\nrules that represent termi-\\nnal symbols'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 610, 'page_label': '611'}, page_content='584 CHAPTER 20 Understanding Language\\nTransition networks can be used to determine whether a sentence is gram-\\nmatically correct, at least according to the rules of the grammar the net-\\nworks represent.\\nParsing using transition networks involves exploring a search space of pos-\\nsible parses in a depth-first fashion.\\nLet us examine the parse of the following simple sentence:\\nA cat sat.\\nWe begin in state S1 in the Sentence transition network. T o proceed, we\\nmust follow the arc that is labeled NounPhrase. We thus move out of the\\nSentence network and into the NounPhrase network.\\nThe first arc of the NounPhrase network is labeled Noun. We thus move\\ninto the Noun network. We now follow each of the arcs in the Noun net-\\nwork and discover that our first word, A, does not match any of them.\\nHence, we backtrack to the next arc in the NounPhrase network. This arc is\\nlabeled Article, so we move on to the Article transition network. Here, on\\nexamining the second label, we find that the first word is matched by the\\nterminal symbol on this arc. We therefore consume the word, A, and move\\non to state S2 in the Article network. Because this is a success node, we are\\nable to return to the NounPhrase network and move on to state S2 in this\\nnetwork. We now have an arc labeled Noun.\\nAs before, we move into the Noun network and find that our next word,cat,\\nmatches. We thus move to state S4 in the NounPhrase network. This is a\\nsuccess node, and so we move back to the Sentence network and repeat the\\nprocess for the VerbPhrase arc.\\nIt is possible for a system to use transition networks to generate a deriva-\\ntion tree for a sentence, so that as well as determining whether the sentence\\nis grammatically valid, it parses it fully to obtain further information by\\nsemantic analysis from the sentence. This can be done by simply having the\\nsystem build up the tree by noting which arcs it successfully followed.\\nWhen, for example, it successfully follows the NounPhrase arc in the Sen-\\ntence network, the system generates a root node labeled Sentence and an arc\\nleading from that node to a new node labeled NounPhrase. When the sys-\\ntem follows the NounPhrase network and identifies an article and a noun,\\nthese are similarly added to the tree. In this way, the full parse tree for the\\nsentence can be generated using transition networks.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 611, 'page_label': '612'}, page_content='20.2 Natural Language Processing 585\\nParsing using transition networks is simple to understand, but is not neces-\\nsarily as efficient or as effective as we might hope for. In particular, it does\\nnot pay any attention to potential ambiguities or the need for words to\\nagree with each other in case, gender, or number. In the next section, we\\nexamine augmented transition networks, which are a more sophisticated\\nparsing tool.\\n20.2.6 Augmented Transition Networks\\nAn augmented transition network , or ATN, is an extended version of a\\ntransition network. ATNs have the ability to apply tests to arcs, for example,\\nto ensure agreement with number. Thus, an ATN for Sentence would be as\\nshown in Figure 20.2, but the arc from node S2 to S3 would be conditional\\non the number of the verb being the same as the number for the noun.\\nHence, if the noun phrase were three dogs and the verb phrase were is blue,\\nthe ATN would not be able to follow the arc from node S2 to S3 because the\\nnumber of the noun phrase (plural) does not match the number of the\\nverb phrase (singular). In languages such as French, checks for gender\\nwould also be necessary.\\nThe conditions on the arcs are calculated by procedures that are attached to\\nthe arcs. The procedure attached to an arc is called when the network\\nreaches that arc. These procedures, as well as carrying out checks on agree-\\nment, are able to form a parse tree from the sentence that is being analyzed.\\n20.2.7 Chart Parsing\\nParsing using transition networks is effective, but not the most efficient\\nway to parse natural language. One problem can be seen in examining the\\nfollowing two sentences:\\n1. Have all the fish been fed?\\n2. Have all the fish.\\nClearly these are very different sentences—the first is a question, and the sec-\\nond is an instruction. In spite of this, the first three words of each sentence are\\nthe same. When a parser is examining one of these sentences, it is quite likely\\nto have to backtrack to the beginning if it makes the wrong choice in the first\\ncase for the structure of the sentence. In longer sentences, this can be a much\\ngreater problem, particularly as it involves examining the same words more\\nthan once, without using the fact that the words have already been analyzed.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 612, 'page_label': '613'}, page_content='586 CHAPTER 20 Understanding Language\\n0 1 2 3 4The cat eats big 5 6fisha\\nFigure 20.4\\nThe initial chart for the\\nsentence The cat eats a big\\nfish\\nAnother method that is sometimes used for parsing natural language is\\nchart parsing . In the worst case, chart parsing will parse a sentence of n\\nwords in O(n3) time. In many cases it will perform better than this and will\\nparse most sentences in O(n2) or even O(n) time.\\nIn examining sentence 1 above, the chart parser would note that the wordstwo\\nchildren form a noun phrase. It would note this on its first pass through the\\nsentence and would store this information in achart, meaning it would not\\nneed to examine those words again on a subsequent pass, after backtracking.\\nThe initial chart for the sentenceThe cat eats a big fishis shown in Figure 20.4.\\nFigure 20.4 shows the chart that the chart parse algorithm would start with\\nfor parsing the sentence. The chart consists of seven vertices, which will\\nbecome connected to each other by edges. The edges will show how the\\nconstituents of the sentence combine together.\\nThe chart parser starts by adding the following edge to the chart:\\n[0, 0, Ta r g e t→ • Sentence]\\nThis notation means that the edge connects vertex 0 to itself (the first two\\nnumbers in the square brackets show which vertices the edge connects).\\nTa r g e tis the target that we want to find, which is really just a placeholder to\\nenable us to have an edge that requires us to find a whole sentence. The\\narrow indicates that in order to make what is on its left-hand side ( Ta r g e t)\\nwe need to find what is on its right-hand side (Sentence). The dot (•) shows\\nwhat has been found already, on its left-hand side, and what is yet to be\\nfound, on its right-hand side. This is perhaps best explained by examining\\nan example.\\nConsider the following edge, which is shown in the chart in Figure 20.5:\\n[0, 2, Sentence → NounPhrase • VerbPhrase]\\nThis means that an edge exists connecting nodes 0 and 2. The dot shows us\\nthat we have already found a NounPhrase (the cat) and that we are looking'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 613, 'page_label': '614'}, page_content='20.2 Natural Language Processing 587\\n0 1 2 3 4The cat eats big 5 6fisha\\n[0, 2, Sentence → NounPhrase • VerbPhrase]\\nFigure 20.5\\nPartial chart for the sen-\\ntence The cat eats a big\\nfish, showing the edge [0,\\n2, Sentence→ NounPhrase\\n• VerbPhrase]\\nfor a VerbPhrase. Once we have found the VerbPhrase, we will have what is\\non the left-hand side of the arrow—that is, a Sentence.\\nThe chart parser can add edges to the chart using the following three rules:\\n1. If we have an edge [ x, y, A → B • C], which needs to find a C, then\\nan edge can be added that supplies that C (i.e., the edge [x, y, C →\\n• E]), where E is some sequence of terminals or nonterminals\\nwhich can be replaced by a C).\\n2. If we have two edges, [ x, y, A → B • C D] and [y, z, C → E •}, then\\nthese two edges can be combined together to form a new edge: [ x,\\nz, A → B C • D].\\n3. If we have an edge [ x, y, A → B • C], and the word at vertex y is of\\ntype C, then we have found a suitable word for this edge, and so we\\nextend the edge along to the next vertex by adding the following\\nedge: [y, y + 1, A → B C •].\\nLet us now see how this works, by examining the example of the sentence\\nshown in Figure 20.4: The cat eats a big fish.\\nWe start with the edge [0, 0,Ta r g e t→ • Sentence], which means that to find\\nour target, we must first find a sentence.\\nUsing rule 1 above, we can add the following edge to the chart:\\n[0, 0, Sentence → • NounPhrase VerbPhrase]\\nThis means we must now find a NounPhrase and a VerbPhrase.\\nWe now apply rule 1 again, to try to find a suitable NounPhrase, which\\ninvolves adding the following edge:\\n[0, 0, NounPhrase → • Article NounPhrase]'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 614, 'page_label': '615'}, page_content='588 CHAPTER 20 Understanding Language\\nblack\\ncat matSat on\\nFigure 20.6\\nA semantic net representa-\\ntion for the sentence The\\nblack cat sat on the mat\\nNow we are able to apply rule 3 because the word at the end of this edge\\n(from vertex 0 to vertex 0) is the, which is an Article. (This would be deter-\\nmined by looking the word up in a lexicon.) Hence, we can now add the\\nfollowing edge:\\n[0, 1, NounPhrase → Article • NounPhrase]\\nNow we are looking for another NounPhrase, so we use rule 1 again to add\\nthe following edge:\\n[0, 1, Noun Phrase → • Noun]\\nWe can now use rule 3 again because the next word is indeed a Noun,t o  a d d\\nthe following edge to the chart:\\n[0, 2, NounPhrase → Noun •}\\nThis process now continues, until we have reached an edge in which we\\nhave found everything we need. In this example, the final edge will be\\n[0, 6, Sentence → NounPhrase VerbPhrase•}\\nT o build a parse tree from the chart, we modify rule 2 so that when it com-\\nbines two edges together, it stores in the new edge information about the\\ntwo edges that were combined to form it (the children edges). Then when\\nthe parse has completed, we can obtain the parse tree directly from the\\nedges of the tree by starting from the first edge and recursively examining\\nthe children edges of each node.\\n20.2.8 Semantic Analysis\\nHaving determined the syntactic structure of a sentence, the next task of\\nnatural language processing is to determine the meaning of the sentence.\\nSemantics is the study of the meaning of words, and semantic analysis is\\nthe analysis we use to extract meaning from utterances.\\nSemantic analysis involves building up a representation of the objects and\\nactions that a sentence is describing, including details provided by adjectives,\\nadverbs, and prepositions. Hence, after analyzing the sentenceThe black cat\\nsat on the mat, the system would use a semantic net such as the one shown in\\nFigure 20.6 to represent the objects and the relationships between them.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 615, 'page_label': '616'}, page_content='20.2 Natural Language Processing 589\\nA more sophisticated semantic network is likely to be formed, which\\nincludes information about the nature of a cat (a cat is an object, an ani-\\nmal, a quadruped, etc.) that can be used to deduce facts about the cat (e.g.,\\nthat it likes to drink milk).\\nIn fact, semantic analysis is most useful in disambiguating sentences, as we\\nsee in the next section.\\n20.2.9 Ambiguity and Pragmatic Analysis\\nOne of the main differences between natural languages and formal lan-\\nguages like C++ is that a sentence in a natural language can have more than\\none meaning. This is ambiguity—the fact that a sentence can be inter-\\npreted in different ways depending on who is speaking, the context in\\nwhich it is spoken, and a number of other factors.\\nWe will briefly examine some of the more common forms of ambiguity\\nand look at ways in which a natural language processing system can make\\nsensible decisions about how to disambiguate them.\\nLexical ambiguity occurs when a word has more than one possible mean-\\ning. For example, a bat can be a flying mammal or a piece of sporting\\nequipment. The word set is an interesting example of this because it can be\\nused as a verb, a noun, an adjective, or an adverb. Determining which part\\nof speech is intended can often be achieved by a parser in cases where only\\none analysis is possible, but in other cases semantic disambiguation is\\nneeded to determine which meaning is intended.\\nSyntactic ambiguity occurs when there is more than one possible parse of a\\nsentence. The sentence Jane carried the girl with the spade could be inter-\\npreted in two different ways, as is shown in the two parse trees in Figure 20.7.\\nIn the first of the two parse trees in Figure 20.7, the prepositional phrase\\nwith the spade is applied to the noun phrase the girl, indicating that it was\\nthe girl who had a spade that Jane carried. In the second sentence, the\\nprepositional phrase has been attached to the verb phrase carried the girl ,\\nindicating that Jane somehow used the spade to carry the girl.\\nSemantic ambiguity occurs when a sentence has more than one possible\\nmeaning—often as a result of a syntactic ambiguity. In the example shown in\\nFigure 20.7 for example, the sentenceJane carried the girl with the spade, the\\nsentence has two different parses, which correspond to two possible mean-\\nings for the sentence. The significance of this becomes clearer for practical\\nsystems if we imagine a robot that receives vocal instructions from a human.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 616, 'page_label': '617'}, page_content='590 CHAPTER 20 Understanding Language\\nSentence\\nVerbNoun\\nNounPhrase NounPhrase\\nNoun\\nPhrase\\nPrepositional\\nPhrase\\nVerbPhrase\\ncarried the girlJane with the spade\\nSentence\\nVerbNoun\\nNounPhrase VerbPhrase\\nNoun\\nPhrase\\nPrepositional\\nPhrase\\nVerbPhrase\\ncarried the girlJane with the spade\\nFigure 20.7\\nTwo possible parse trees for the sentence Jane carried the girl with the spade\\nReferential ambiguity occurs when we use anaphoric expressions, or pro-\\nnouns to refer to objects that have already been discussed. An anaphora\\noccurs when a word or phrase is used to refer to something without naming\\nit. The problem of ambiguity occurs where it is not immediately clear which\\nobject is being referred to. For example, consider the following sentences:\\nJohn gave Bob the sandwich. He smiled.\\nIt is not at all clear from this who smiled—it could have been John or Bob.\\nIn general, English speakers or writers avoid constructions such as this to\\navoid humans becoming confused by the ambiguity. In spite of this, ambi-\\nguity can also occur in a similar way where a human would not have a\\nproblem, such as\\nJohn gave the dog the sandwich. It wagged its tail.\\nIn this case, a human listener would know very well that it was the dog that\\nwagged its tail, and not the sandwich. Without specific world knowledge,\\nthe natural language processing system might not find it so obvious.\\nA local ambiguity occurs when a part of a sentence is ambiguous; however,\\nwhen the whole sentence is examined, the ambiguity is resolved. For exam-\\nple, in the sentenceThere are longer rivers than the Thames, the phraselonger\\nrivers is ambiguous until we read the rest of the sentence,than the Thames.\\nAnother cause of ambiguity in human language is vagueness. As we saw in\\nChapter 18, when we examined fuzzy logic, words such as tall, high, and fast'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 617, 'page_label': '618'}, page_content='20.2 Natural Language Processing 591\\nare vague and do not have precise numeric meanings. A natural language\\nprocessing system may have no problem syntactically analyzing the sen-\\ntence The car is very fast , but it needs a good deal of world knowledge to\\nunderstand exactly what this sentence means. Of course, it will have differ-\\nent meanings to different people and in different circumstances: a normal\\nAmerican driver might interpret it as meaning that the car is traveling (or\\ncan travel) faster than 70 miles per hour. A German, used to traveling on\\nthe Autobahn, might consider 70 miles per hour to be very slow and might\\ninterpret the sentence as meaning that the car could travel over 130 mph.\\nHumans use a number of other constructions, such as metaphor (as in he\\nran like the wind) and metonymy (using a part of an object to describe the\\nwhole, as in the suit sat next to me ). We tend to take these forms of speech\\nfor granted and do not need to carry out much additional thought to\\nunderstand what is meant by them. Clearly, for a computer system this is\\nnot so easy.\\nThe process by which a natural language processing system determines which\\nmeaning is intended by an ambiguous utterance is known as disambigua-\\ntion. Disambiguation can be done in a number of ways. One of the most\\neffective ways to overcome many forms of ambiguity is to use probability.\\nThis can be done using prior probabilities or conditional probabilities. Prior\\nprobability might be used to tell the system that the word bat nearly always\\nmeans a piece of sporting equipment. Conditional probability would tell it\\nthat when the wordbat is used by a sports fan, this is likely to be the case, but\\nthat when it is spoken by a naturalist it is more likely to be a winged mammal.\\nContext is also an extremely important tool in disambiguation. Consider\\nthe following sentences:\\nI went into the cave. It was full of bats.\\nI looked in the locker. It was full of bats.\\nIn each case, the second sentence is the same, but the context provided by\\nthe first sentence helps us to choose the correct meaning of the word “bat”\\nin each case.\\nDisambiguation thus requires a good world model, which contains knowl-\\nedge about the world that can be used to determine the most likely meaning\\nof a given word or sentence. The world model would help the system to\\nunderstand that the sentenceJane carried the girl with the spadeis unlikely to'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 618, 'page_label': '619'}, page_content='592 CHAPTER 20 Understanding Language\\nmean that Jane used the spade to carry the girl because spades are usually used\\nto carry smaller things than girls. The challenge, of course, is to encode this\\nknowledge in a way that can be used effectively and efficiently by the system.\\nThe world model needs to be as broad as the sentences the system is likely\\nto hear. For example, a natural language processing system devoted to\\nanswering sports questions might not need to know how to disambiguate\\nthe sporting bat from the winged mammal, but a system designed to\\nanswer any type of question would.\\n20.3 Machine Translation\\nOne of the early goals of natural language processing was to build a system\\nthat could translate text from one human language to another. Behind this\\nattempt is an implicit assumption that human languages are like codes: in\\nother words, a word in one language is simply a code for a real-world\\nobject, emotion, action, place, etc., and can therefore be exchanged for the\\ncode in another language for the same thing. Clearly this works to some\\nextent: translating the world cheval from French into English can be\\nachieved by simply looking it up in a dictionary.\\nIt is much harder to translate entire sentences, for many of the reasons that\\nhave been given above for the difficulty of natural language processing in\\ngeneral. In particular, machine translation is not possible simply using syn-\\ntactic and lexical analysis: a knowledge of the world that is being discussed\\nis also essential, in order to disambiguate the text that is being translated. It\\nmay be, in some cases, that the text can be translated directly, ignoring the\\nambiguity, and creating a similarly ambiguous sentence in the target lan-\\nguage. This does not always work, however: the word bat in English has (at\\nleast) two meanings, but there is no single word in French that has both of\\nthose meanings. Hence, for a system to translate that word from English to\\nFrench, it must first determine which of the meanings is intended.\\nMachine translation systems have been developed, but at present the best\\nresults they can achieve are inadequate for most uses. One way in which they\\ncan be used is in combination with a human translator. The machine is able\\nto provide a rough translation, and the human then tidies up the resultant\\ntext, ensuring that ambiguities have been handled correctly and that the\\ntranslated text sounds natural, as well as being grammatically correct.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 619, 'page_label': '620'}, page_content='20.3 Machine Translation 593\\n20.3.1 Language Identification\\nA similar, but easier problem to machine translation is that of language\\nidentification. There are many thousands of human languages in the world,\\nand several hundred that are widely used today. Many of these are related to\\neach other, and so can be easily confused. For an English speaker who\\nknows no Italian or Spanish, those two languages can sometimes appear\\nsimilar, for example. A system that can identify which language is being\\nused in a piece of text is thus very useful. It is also particularly useful in\\napplying textual analysis of all kinds to documents that appear on the Inter-\\nnet. Because pages on the Internet often have no indication of which lan-\\nguage is being used, an automated system that is analyzing such documents\\nneeds to have the ability first to determine which language is being used.\\nOne way to determine the language of a piece of text would be to have a\\ncomplete lexicon of all words in all languages. This would clearly provide\\naccurate results, but is likely to be impractical to develop for a number of\\nreasons. The lexicon would be enormous, of course, and it would be very\\ndifficult to ensure that all words were really included.\\nThe acquaintance algorithm is a commonly used method for language\\nidentification that uses n-grams.A n  n-gram is simply a collection of n let-\\nters, but detailed statistics exist that indicate the likelihood of a particular\\nset of letters occurring in any given language. Hence, for example, the tri-\\ngrams ing, and, the, ent, and ant probably indicate that a document is in\\nEnglish. When the acquaintance algorithm is presented with sufficient text\\n(usually a few hundred to a thousand words is sufficient), it is able to iden-\\ntify the language with a surprisingly high degree of accuracy.\\nThe acquaintance algorithm is trained by being presented with text in each\\nlanguage that it is expected to identify. The system then calculates a vector\\nfor each language based on the training data. This vector stores informa-\\ntion about how many times each n-gram occurs in that language. When a\\ndocument in an unknown language is presented to the algorithm, it calcu-\\nlates a similar vector for this document and compares it with the vectors it\\nhas calculated for the training data. The vector that is closest indicates\\nwhich language is being used in the document.\\nOne advantage of this approach is that it is easy to tell how certain the algo-\\nrithm is about a particular document. A score is calculated for a document'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 620, 'page_label': '621'}, page_content='594 CHAPTER 20 Understanding Language\\nfor each possible language, and the language with the highest score is\\nselected. If the highest score is very much higher than the second highest\\nscore, this indicates a high degree of certainty. Conversely, if the top two or\\nthree scores are similar, then the algorithm is less certain, and there are one\\nor more other possibilities that might need to be examined.\\n20.4 Information Retrieval\\nInformation retrieval involves matching the text contained in a query or a\\ndocument to a set of other documents. Often, the task involves finding the\\ndocuments from a corpus of documents that are relevant to a user’s query.\\nInformation retrieval was briefly introduced in Chapter 12, where we saw\\nhow Bayes’ theorem can be used to produce a system that is effective at\\nmatching documents to a query and thus retrieving relevant documents\\nfrom a corpus in response to a user request.\\nThe idea behind information retrieval is that if a user enters a query such as\\nwhat is the capital of Sri Lanka?, then a good approach to finding the answer\\nis to find a document that contains all (or some) of the words contained in\\nthe query. In fact, words such as what, is, the, and of would normally be\\nstripped from the query (using a stop list, which contains words that are to\\nbe stripped from all queries) before processing, and the information\\nretrieval system would locate the documents that contained the words cap-\\nital, Sri, and Lanka.\\nThe corpus of documents is clearly very important. As has already been dis-\\ncussed, ambiguities in the query text can be avoided if the corpus is a very\\nspecific one. Information retrieval systems tend not to deal well with ambi-\\nguity because they are usually not given any world knowledge but are sim-\\nply designed to perform statistical analysis of words in order to pick out\\nsuitable responses to a query.\\nAs well as providing responses to a query, information retrieval can be used\\nto find other documents that are similar to a given document. This provides\\na “more like this” function that many search engines use, which enables a\\nuser to say “I like this web site—find me other ones that are similar. ”\\nThe main concept used in information retrieval is known as TF-IDF,\\n(T erm Frequency – Inverse Document Frequency).'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 621, 'page_label': '622'}, page_content='20.4 Information Retrieval 595\\nUsually, a TF-IDF value is calculated for each of a set of words, and the\\nresultant values are placed in a vector, which represents a document or\\npiece of text (such as a query).\\nThe inverse document frequency (IDF) of a wordW is calculated as follows:\\nWhere |D| is the number of documents in the corpus; DF (W) is the docu-\\nment frequency of W, which is the number of documents in the corpus\\nthat contain the word W.\\nThe term frequency of word W in document D is written TF ( W, D) and\\nrepresents the number of times the word W occurs in document D.\\nThe TF-IDF vector is the product of the TF and IDF values for a set of\\nwords for a particular document:\\nTF-IDF (D, W\\ni) = TF(Wi, D) /H11003IDF (Wi)\\nLet us now consider why this calculation makes sense. The inverse docu-\\nment frequency is designed to give a large value for infrequent words and a\\nlow value for frequent words. It is important that this calculation is done\\nusing the number of occurrences in the appropriate corpus. In some cases,\\nthe corpus can be representative of the English language as a whole, in\\nwhich case no assumptions are being made about the nature of the subjects\\nbeing searched for.\\nIn many other cases, however, the corpus should be representative of a par-\\nticular subject area. Hence, if the corpus were a set of documents about\\nNew Y ork City, then the word elephant would be relatively infrequent and\\nwould thus produce a relatively high IDF value. Conversely, words such as\\ntaxi, building, New York, and streets would be relatively common and so\\nwould receive relatively low IDF values.\\nLet us consider the following query that is put to an information retrieval\\nsystem using a corpus of documents about New Y ork City:\\nWhen did an elephant walk through the streets of New York?\\nFirst, the stop words would be stripped, leaving the following words:\\nElephant walk through streets New York\\nIDF W D\\nDF W( ) =\\n( )\\nlog'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 622, 'page_label': '623'}, page_content='596 CHAPTER 20 Understanding Language\\nAn IDF value would now be calculated for each word in the query. The\\nword elephant would certainly receive the highest score, and the words\\nthrough, streets, New, and Yor kwould all obtain very low scores.\\nNow, an index of the corpus of documents is consulted to obtain all docu-\\nments that contain all (or some) of the words contained in the query. One\\ntechnique here would be to require the least common word (elephant) to be\\nin all documents, but to allow documents to have some combination of one\\nor more of the other query words. Hence, the user would effectively be\\nmaking the following Boolean query:\\n“Elephant” and (“walk” or “through” or “streets” or “New” or “Y ork”)\\nAt this point, the TF part of TF-TDF comes into play. For each document\\nthat is retrieved, a TF-IDF value is calculated for the words in the query,\\nproducing a vector of six values for each document.\\nThe idea behind the TF calculation is that if a document mentions the\\nword elephant 10 times, then it is much more likely to be relevant to this\\nquery than a document that mentions it just once. On the other hand, a\\ndocument that mentions the word elephant just once is still more likely to\\nbe relevant than a document that does not mention the word elephant at\\nall, even if it has all the other words in the query several times.\\nThe most common behavior for an information retrieval system in response\\nto a query such as this is to return one or more of the most relevant docu-\\nments that were obtained from the corpus. The relevance of a document can\\nbe obtained by obtaining the magnitude of its TF-IDF vector.\\nIt is also possible to show a document to a corpus and ask it to find the most\\nsimilar documents in the corpus. In some cases, queries are considered to\\nbe documents and are treated in this way. In this case, a TF-IDF vector is\\ncalculated for the query document and is also calculated for each document\\nin the corpus. The most relevant documents are deemed to be those whose\\nTF-IDF vectors are closest to the vector of the query document.\\n20.4.1 Stemming\\nIf a user enters the query “where are elephants?” , it would clearly be foolish\\nfor the system to reject a document that contains several occurrences of the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 623, 'page_label': '624'}, page_content='20.4 Information Retrieval 597\\nword elephant simply because it does not contain the word elephants\\nexactly as used in the query.\\nStemming is often applied in information retrieval systems to avoid this\\nproblem. Stemming simply involves removing common stems such as -ing,\\n-s, and -ed from words. In this way, the word swimming will be stemmed to\\nswim and will match swims, swimmers, and so on. It will not usually be able\\nto match swam or swum because these are irregular forms.\\nThe most commonly used stemmer is Porter’s stemmer , which is an\\nextremely simple algorithm that has in some cases been shown to improve\\nthe performance of information retrieval systems.\\nPorter’s stemmer is explained in detail in Spärck Jones and Willett (1997).\\nThe following is a brief description of the algorithm. Each step is carried\\nout in turn on each word. The algorithm also includes conditions relating\\nto word length so that words such as sing are not stemmed, whereas words\\nsuch as hissing are. The algorithm is also careful to differentiate between\\nsingle and double letters, ensuring that hopping is stemmed to hop, and\\nhissing is stemmed to hiss.\\n1. -s is removed, and -sses is converted to ss (hence, caresses is\\nstemmed to caress).\\n2. -ed, -ing are removed. After -ed is removed, an -e is added if the\\nword now ends in -at, -bl,o r  -iz, ensuring that grated, disabled, and\\nrealized are correctly stemmed to grate, disable, and realize rather\\nthan grat, disabl, and realiz.\\n3. -y is converted to -i. This seems like a strange step, but ensures that\\nfly and flies are considered to be the same word because they are\\nboth stemmed to fli.\\n4. A number of specific rules are now applied such as:\\n-ATIONAL → ATE\\n-IVENESS → IVE\\n-BILITI → BLE\\n5. Endings such as -ative, -ful, -ness, -able, and -er are removed, and\\nendings such as -icative, -iciti, and -ical are converted to -ic.\\n6. -e is removed.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 624, 'page_label': '625'}, page_content='598 CHAPTER 20 Understanding Language\\n7. Double letters at the end of some words (based on length) are con-\\nverted to single letters—for example, controll is converted to con-\\ntrol. This ensures that the following words are all considered to be\\nthe same: controlling, control, controlled, controllable.\\nThe aim of stemming is to ensure that a query word will match other words\\nwith the same meaning that differ only in endings in the corpus. Hence, it\\nis desirable not necessarily that the stemmed words are real words, but that\\nwhen two words are stemmed they become the same if they really are the\\nsame word. Hence, the words flying, fly, and flies all stem to the nonword fli.\\nThe fact that fli is not a word does not matter because the aim is to match\\nthese words together in query and documents, not to show the stemmed\\nwords to the user.\\n20.4.2 Precision and Recall\\nThe success of an information retrieval system can be measured using two\\nmetrics: precision and recall. If a system has 100% precision, it means that\\nwhen it says that a particular document is relevant, then it is guaranteed to\\nbe correct. Lower precision means that it will wrongly classify some docu-\\nments as being relevant (false positives).\\nFor a system to have 100% recall, it must be guaranteed to find all relevant\\ndocuments within a corpus in response to a particular query. Lower recall\\nmeans that the system will fail to identify some documents as being rele-\\nvant (false negatives).\\nIn general, for most information retrieval techniques, precision and recall\\nare in opposition to each other, meaning that when the system’s precision\\nincreases it does so at the expense of recall, and vice-versa. This is intuitive:\\nthe only way to get 100% recall in most real-world situations is to be very\\nrelaxed about which documents are classified. In other words, a great deal\\nof documents must be classified as being relevant to ensure that all relevant\\ndocuments are found. Inevitably, this will mean that some irrelevant docu-\\nments will be found as well.\\nSimilarly, to obtain 100% precision it is necessary to return very few doc-\\numents, meaning that some documents that are in fact relevant will not\\nbe returned.\\nA perfect information retrieval system would be one that achieved 100%\\nrecall and 100% precision over a given corpus and a given set of queries.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 625, 'page_label': '626'}, page_content='20.5 Chapter Summary 599\\nSuch an information retrieval system is highly unlikely to ever be devel-\\noped for most reasonable problems. One of the main causes of this diffi-\\nculty is the complexity of human language, and the ambiguities it presents,\\nas discussed above.\\nInformation retrieval systems, unlike natural language processing systems,\\ndo not tend to take into account grammatical structures and thus are poor\\nat, for example, noticing that “ the city that used to be the capital of Sri\\nLanka” is no longer the capital of Sri Lanka.\\n20.5 Chapter Summary\\n■ Morphologic analysis involves examining the structure of individ-\\nual words.\\n■ BNF (Backus–Naur form or Backus normal form) is used to define\\nthe rules that make up a grammar for a language.\\n■ Grammars define the syntactic rules and structures of a language.\\n■ Parsing (or syntactic analysis) uses the grammar of a language to\\ndetermine the structure of a sentence or utterance, in order to\\nderive further information (such as meaning) from the words.\\n■ Semantic analysis involves examining the meaning of words\\nand phrases.\\n■ Ambiguity is a common problem with natural language processing\\nsystems. It can be dealt with to some extent by pragmatic analysis.\\n■ Machine translation involves presenting a piece of text in one\\nhuman language, which a computer program is then expected to\\ntranslate into another human language. Although a great deal of\\nwork has been carried out in this field, the success predicted in the\\n1950s has yet to be achieved.\\n■ Determining the language of a piece of text can be done by exam-\\nining the occurrence of particular trigrams within the text.\\n■ Information retrieval (IR) involves producing a response to a user\\nquery by selecting relevant documents from a corpus of docu-\\nments (such as the Internet).\\n■ An IR system that achieves 100% precision can guarantee that any\\ndocument it returns is relevant to the query.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 626, 'page_label': '627'}, page_content='600 CHAPTER 20 Understanding Language\\n■ An IR system that achieves 100% recall can guarantee that if a rele-\\nvant document exists for a query, then it will find it.\\n■ Achieving 100% recall and 100% precision is the goal of most infor-\\nmation retrieval systems and one that has not yet been achieved.\\n20.6 Review Questions\\n20.1 Explain what is meant by Natural Language Processing . Why is it\\nsuch a difficult subject?\\n20.2 Explain the role of each of the following in Natural Language Pro-\\ncessing:\\nmorphology\\nsyntax\\nsemantics\\npragmatics\\ngrammars\\n20.3 What is BNF? Why is it used to describe grammars?\\n20.4 How are transition networks used to represent a grammar?\\n20.5 Explain the difficulties involved in machine translation.\\n20.6 What is information retrieval? How does it differ from natural lan-\\nguage processing?\\n20.7 What are precision and recall? How are they related? Explain why it\\nis not usually possible to have 100% precision and 100% recall in\\nthe same system. Can you imagine a scenario in which it would be\\npossible to achieve 100% precision and 100% recall?\\n20.7 Exercises\\n20.1 Examine the BNF definition of the syntax of a programming lan-\\nguage such as C++, BASIC, or Java. What differences are immedi-\\nately obvious compared with the BNF for a human language\\ngrammar? Are there any similarities?\\n20.2 Implement Porter’s stemming algorithm in the programming lan-\\nguage of your choice. Y ou will need to find a full description of the\\nalgorithm. Y ou can find this in Porter (1980), Spärck Jones (1997),'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 627, 'page_label': '628'}, page_content='20.8 Further Reading 601\\nor online. Apply the algorithm to a dictionary of words, such as the\\none that comes with most UNIX implementations. Then allow a\\nuser to enter a word, and have the system look this word up and say\\n“yes” if it is present in its stemmed form in the dictionary or “no” if\\nit is not. For example, if the dictionary contains swim, fish, and\\ncheese, then it should say “yes” to swimming, fishing, and cheeses but\\nno to chocolate and swam.\\n20.3 Find a book or web site that defines a set of rules for English gram-\\nmar or the grammar of another human language. Express all of the\\nrules in BNF and as transition networks. What problems do you\\nencounter? Are there any rules that you cannot represent in either\\nsystem or in both systems?\\n20.4 Find a machine translation service online. Have it translate a piece\\nof text in a language with which you are not familiar into English.\\nWhat errors does the translator introduce? Can you determine any\\nsophisticated features based on the translation it produces?\\n20.5 Implement a language identification system in the programming\\nlanguage of your choice. Y ou should start by selecting a number of\\nlanguages (four or five should do). Y ou should have a suitable\\nquantity of typical material in each language—about 1000 words\\nin each language would be plenty. First, write an algorithm that\\ndetermines the most common 100 trigrams in each language. Now\\nbuild these data into a program that uses it to determine the lan-\\nguage of unseen text. Produce an alternative version of the soft-\\nware that calculates a frequency vector using all (26 * 26 * 26)\\ntrigrams. How does this system perform compared with the first\\none you produced in terms of accuracy and efficiency?\\n20.8 Further Reading\\nNatural language processing is briefly covered by most of the standard\\ntexts; information retrieval is less well covered. Spärck Jones and Willett\\n(1997) provide an excellent coverage of the topics of information retrieval,\\nwith papers from a number of researchers in the field.\\nNatural Language Understanding, by James Allen (1995 – Addison Wesley)\\nModern Information Retrieval , by Ricardo Baeza-Y ates and Berthier\\nRibeiro-Neto (1999 – Addison Wesley)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 628, 'page_label': '629'}, page_content='602 CHAPTER 20 Understanding Language\\nPlan Recognition in Natural Language Dialogue , by Sandra Carberry (1990\\n– MIT Press)\\nCross-Language Information Retrieval , edited by Gregory Grefenstette\\n(1998 – Kluwer Academic Publishing)\\nFoundations of Computational Linguistics: Human-Computer Communica-\\ntion in Natural Language, by Roland R. Hausser (2001 – Springer V erlag)\\nInformation Retrieval, by William R. Hersh (2002 – Springer V erlag)\\nSpoken Language Processing: A Guide to Theory, Algorithm and System\\nDevelopment, by Xuedong Huang, Alex Acero, Hsiao-Wuen Hon, and Raj\\nReddy (2001 – Prentice Hall)\\nNatural Language Processing and Knowledge Representation: Language for\\nKnowledge and Knowledge for Language , edited by Lucja M. Iwanska and\\nStuart C. Shapiro (2000 – AAAI Press)\\nText-Based Intelligent Systems: Current Research and Practice in Information\\nExtraction and Retrieval , edited by Paul Schafran Jacobs (1992 – Lawrence\\nErlbaum Assoc.)\\nSpeech and Language Processing: An Introduction to Natural Language Process-\\ning, Computational Linguistics and Speech Recognition, by Dan Jurafsky, James\\nH. Martin, Keith Vander Linden, and Nigel Ward (2000 – Prentice Hall)\\nIntelligent Multimedia Information Retrieval , edited by Mark T. Maybury\\n(1997 – AAAI Press)\\nComputational Linguistics , by T ony McEnery (1992 – Coronet Books –\\nout of print)\\nText Information Retrieval Systems , by Charles T. Meadow, Bert R. Boyce,\\nand Donald H. Kraft (2000 – Academic Press)\\nNatural Language Processing for Online Applications: Text Retrieval, Extrac-\\ntion, and Categorization , by Peter Jackson and Isabelle Moulinier (2002 –\\nJohn Benjamins Publishing Company)\\nSpotting and Discovering Terms through Natural Language Processing ,b y\\nChristian Jacquemin (2001 – MIT Press)\\nFoundations of Statistical Natural Language Processing , by Christopher D.\\nManning and Hinrich Schütze (1999 – MIT Press)\\nReadings in Machine Translation , edited by Sergei Nirenburg, Harold L.\\nSomers, and Y orick A. Wilks (2002 – MIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 629, 'page_label': '630'}, page_content='20.8 Further Reading 603\\nNatural Language Processing , by Fernando C. N. Pereira and Barbara J.\\nGrosz (1994 – MIT Press)\\nAn Algorithm for Suffix Stripping , by M. F. Porter (1980 – in Spärck Jones\\nand Willett 1997)\\nFundamentals of Speech Recognition , by Lawrence Rabiner and Biing-\\nHwang Juang (1993 – Pearson Education)\\nEvolutionary Language Understanding , by Geoffrey Sampson (1996 –\\nContinuum)\\nEvaluating Natural Language Processing Systems: An Analysis and Review,b y\\nKaren Spärck Jones, Julia R. Galliers (1996 – Springer V erlag)\\nReadings in Information Retrieval , edited by Karen Spärck Jones and Peter\\nWillett (1997 – Morgan Kaufmann)\\nTranslation Engines: Techniques for Machine Translation, by Arturo Trujillo\\n(1999 – Springer V erlag)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 630, 'page_label': '631'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 631, 'page_label': '632'}, page_content='21CHAPTER\\nMachine Vision\\nThe Lord looseth men out of prison: the Lord giveth sight to the blind.\\n—Psalm 146, V erse 7\\nYou see, but you do not observe.\\n—Sir Arthur Conan Doyle, The Adventures of Sherlock Holmes\\nAll the mighty world\\nOf eye and ear, both what they half create,\\nAnd what they perceive.\\n—Sir William Wordsworth\\n21.1 Introduction\\nThe vision system in mammals (such as human beings) is one of the most\\nremarkable systems in the natural world. Without vision, it can be argued\\nthat human beings would not have reached their current levels of technical\\nachievement, and indeed that none of the creatures alive today would have\\nbeen able to evolve successfully without vision.\\nProviding the ability for computer systems, agents, or robots to perceive the\\nworld visually is clearly highly desirable.\\nIn this chapter, we look at the techniques that are used to enable computers\\nto “see” the real world, in much the same way that we do.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 632, 'page_label': '633'}, page_content='606 CHAPTER 21 Machine Vision\\nThis chapter explains how the Canny method uses convolution to detect\\nedges in images. It also explains how an image recognition system can then go\\non to segment the image and thus determine what objects are being viewed.\\nThis chapter presents a popular method that is used for face recognition and\\nalso discusses the importance of textures in computer vision systems.\\n21.2 Human Vision\\nIn this section, we briefly describe the structure and function of the compo-\\nnents that make up the mammalian visual system and, in particular, the\\nhuman visual system. Understanding how humans see is vital to understand-\\ning how it can be possible to enable computers to perceive in a similar way.\\nFigure 21.1 shows a simplified diagram of the human vision system.\\nThe most important parts of the human visual system are the eyes and the\\nbrain—in particular, the part of the brain that is associated with vision is\\nthe visual cortex.\\nThe eye is the device that captures light that has bounced off nearby\\nobjects. This is achieved by a lens that focuses the light onto the retina,\\nwhich is a screen at the back of the eye containing millions of photorecep-\\ntors. Photoreceptors are cells that are sensitive to light. There are two types\\nof photoreceptors: rods and cones.\\nRod cells are highly sensitive and so respond well in situations where there\\nis little light, but they have a low level of acuity, meaning that the images\\nthey transmit to the brain are less detailed and “fuzzier” than those trans-\\nmitted by the cones. Additionally, rods do not have the ability to recognize\\ndifferences in color.\\nCones, on the other hand, are relatively insensitive and so only respond well\\nwhen presented with high levels of light, but they have a high level of acu-\\nity and are able to recognize differences in colors. The cone cells are mainly\\nsituated in the center of the retina, whereas the cones are mainly situated\\naround the edges. This explains why most of our vision in normal, well-lit\\ncircumstances takes place in the center of our field of vision (the corre-\\nsponding area of the retina is called the fovea), whereas at night, our\\nperipheral vision is more important. Y ou will notice, for example, that on a\\ndark night, you can often see stars out of the corner of your eye, but if you\\nturn your eye to look at those stars, they seem to disappear.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 633, 'page_label': '634'}, page_content='21.2 Human Vision 607\\nRIGHT EYE\\nOPTIC  NERVE\\nRETINA\\nOPTIC CHIASM\\nOPTIC TRACT\\nLATERAL\\nGENICULATE\\nNUCLEUS\\nOPTIC RADIATIONS\\nVISUAL CORTEX\\nLEFT EYE\\nLEFT OF\\nVISUAL FIELD\\nRIGHT OF\\nVISUAL FIELD\\nCENTER OF\\nVISUAL FIELD\\nFigure 21.1\\nA diagram of the human\\nbrain, showing the mam-\\nmalian system\\nSignals from the photoreceptors in the retina are passed via the optic nerve\\nto the lateral geniculate nucleus (LGN) and also to the superior collicu-\\nlus. The main pathway is the one to the LGN.\\nThe nerves that travel from the right eye go to the left-hand side of the\\nbrain, and the nerves from the left eye go to the right-hand side of the brain.\\nThe point where the optic nerves cross over each other is theoptic chiasm.\\nFrom the LGN, the signals are carried to the visual cortex by the optic radi-\\nations. This is done in such a way that if both eyes can see a point in the\\nfield of view, then the signals corresponding to this point from the two eyes\\nwill arrive at the same part of the brain. It is as a result of this that we are\\nable to perceive a three-dimensional depth to the world that we see. If you'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 634, 'page_label': '635'}, page_content='608 CHAPTER 21 Machine Vision\\nFigure 21.2\\nA picture of a plant and a\\nmagnified view of a rec-\\ntangular region of this\\nsame photograph. The\\nmagnified region has been\\ntaken from the area high-\\nlighted in the lower right-\\nhand side of the\\nphotograph.\\nshut one eye you will find that it is much harder to accurately perceive\\ndepth. For example, if you hold out a pen in one hand, shut one eye, and\\nthen try to place the cap on the pen with the other hand, you will find it\\nmuch harder to do than if you have both eyes open. This is due to the fact\\nthat we have binocular (“two eyes”),stereoscopic vision.\\n21.3 Image Processing\\nIn this section, we introduce the main techniques that are used in computer\\nvision systems to process images. The process of image recognition can be\\nbroken down into the following main stages:\\n■ image capture\\n■ edge detection\\n■ segmentation\\n■ three-dimensional segmentation\\n■ recognition and analysis\\nImage capture can be performed by a simple camera (or pair of cameras, to\\ngive stereoscopic vision), which converts light signals from a scene to elec-\\ntrical signals, much as the human visual system does.\\nHaving obtained these light signals, which are simply a set of 1s and 0s\\n(assuming a black and white system—if color is being used then each pixel,\\nor picture element, would be represented by a number indicating that\\npixel’s color).\\nFor example, look at the images shown in Figure 21.2.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 635, 'page_label': '636'}, page_content='21.3 Image Processing 609\\nFigure 21.3\\nA photograph of a hand\\nIn the second image in Figure 21.2, you can see the individual grey-scale\\npixels that made up a portion of the original photograph. Each pixel takes\\non one of a number of possible grey-scale values, often from 0 to 255. Color\\nimages are broken down in the same way, but with varying colors instead of\\ngrey scales. When a computer receives an image from an image sensor\\n(such as a camera), this is the form it receives—a set of pixels. We see in this\\nchapter how these pixels can be interpreted to give the computer an under-\\nstanding of what it is perceiving.\\n21.3.1 Edge Detection\\nThe first stage of analysis, once an image has been obtained, is to determine\\nwhere the edges are in the image. This idea has a sound biological basis, and\\nthere is evidence that edge detection is an important part of the mam-\\nmalian visual system. Because objects in the real world almost all have solid\\nedges of one kind or another, detecting those images is the first stage in the\\nprocess of determining which objects are present in a scene.\\nConsider the photograph shown in Figure 21.3 and the image shown in\\nFigure 21.4, which shows the edges detected from this photograph.\\nNote that in Figure 21.4, the edges of the hand have been clearly picked out\\nbecause these are the highest contrast edges in the photograph. Less clear are\\nthe edges of the fencing from the background, although these are also visible.\\nThe reason that edge detection is useful in mammalian vision is that in\\nmost situations a predator (or prey) can be seen to be contrasted sharply\\nwith its background. Hence, noting the edges in the field of vision will\\nenable an animal to quickly recognize other important animals near it.\\nThis, of course, explains why camouflage is such a popular technique in the\\nanimal kingdom. A photo of a brown moth sitting on the brown bark of a'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 636, 'page_label': '637'}, page_content='610 CHAPTER 21 Machine Vision\\nFigure 21.5\\nAn object illustrating sur-\\nface orientation disconti-\\nnuities (edges between\\nfaces of an object) (These\\nedges are shown in bold.)\\nFigure 21.4\\nThe edges from the photo-\\ngraph in Figure 21.2\\ntree will have very few edges, and they will not be easy to detect, compared\\nwith the edges in an image such as the photograph in Figure 21.3.\\nThere are a number of types of edges that can appear in a visual scene. The\\nedges we can see in Figure 21.4 are mostly depth discontinuities, which are\\nedges that represent the differences in depths between parts of the image.\\nIn this case, most of the edges represent the difference in depth between the\\nhand and the fencing behind it.\\nWhen viewing a three-dimension object such as a block on a table (as\\nshown in Figure 21.5) there are surface orientation discontinuities\\n(marked in bold lines in Figure 21.5), which represent edges between faces\\nof the same object—in other words, such an edge appears because the\\nobjects on either side of the edge are facing different directions.\\nThere are two other types of edges, which are caused by differences in color\\n(or texture) on a single surface ( surface reflectance discontinuities ) and\\nby shadows cast by objects (illumination discontinuities).\\nAll of these types of edges can be (and are) used in image recognition sys-\\ntems to determine the position and nature of objects within a visual field.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 637, 'page_label': '638'}, page_content='21.3 Image Processing 611\\n21.3.2 Convolution and the Canny Edge Detector\\nThe simplest way to find edges in an image is to differentiate the image.\\nAreas of consistent color will produce low differentials, and edges that are\\nareas of greatest change will produce greater differentials.\\nUnfortunately, because real images contain a great deal of noise, differenti-\\nation does not work well as an edge detection method because the noise\\nproduces extremely high differentials in areas where there is really no edge.\\nA more effective method of edge detection is to use convolution.\\nThe convolution of two discrete functions f(a, b) and g(a, b) is defined\\nas follows:\\nThe convolution of continuous functions f(a, b) and g( a, b) is defined\\nas follows:\\nThe idea of using convolution is to eliminate the effects of noise by\\nsmoothing the image. One way to smooth an image is to convolve it with\\nthe following Gaussian function:\\nAfter convolving the image with the Gaussian function, the resultant can be\\ndifferentiated to determine where the edges are. In fact, it is possible to\\neliminate a step from this process because it can be shown that convolution\\nwith G\\n/H9268(x) and then differentiating the result is the same as convolution\\nwith the differential of G/H9268(x), which is defined as follows:\\nHence, to detect edges in an image, we can convolve the image with G/H11032/H9268(x)\\nand obtain the peaks in the resultant. The peaks will correspond to the\\n′ ( ) = − −\\nGx x e\\nx\\nσ\\nπσ\\nσ\\n2\\n3\\n2\\n2 2\\nGx e\\nx\\nσ\\nπσ\\nσ( ) =\\n−1\\n2\\n2\\n2 2\\nfab g ab fab g a u b v d u d v,, , ,( ) ∗ ( ) = ( ) −−( )\\n−∞\\n∞\\n−∞\\n∞\\n∫∫\\nfab g ab fab g a u b v\\nvu\\n,, , ,( ) ∗ ( ) = ( ) −−( )\\n=−∞\\n∞\\n=−∞\\n∞\\n∑∑'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 638, 'page_label': '639'}, page_content='612 CHAPTER 21 Machine Vision\\nedges in the image. In doing so, we are using G/H11032/H9268(x) as a filter because we\\nare filtering out everything except the edges in the image.\\nUnfortunately, this method only works for one-dimensional strips of an\\nimage. It will detect an edge in a single line of pixels taken from an image,\\nwhich is useful, but not enough for detecting edges in real images.\\nT o detect edges that might be at any angle in an image, we need to convolve\\nthe image with two filters:\\nFilter 1: G/H11032\\n/H9268(x) G/H9268(y)\\nFilter 2: G/H11032/H9268(y) G/H9268(x)\\nThe image is convolved with each of these filters, and the results are\\nsquared and added together:\\n(I(x, y) * G/H11032/H9268(x) G/H9268(y))2 + (I(x, y) * G/H11032/H9268(y) G/H9268(x))2\\nwhere I(x, y) is the value of the pixel at location (x, y) in the image.\\nPeaks in the resultant then correspond to edges in the image. Pixels that are\\nconsidered to be edges are joined with adjacent pixels that are also edges in\\norder to determine the shape and location of the entire edge. This method,\\nknown as the Canny edge detector, produces edges such as the ones shown\\nin Figure 21.4.\\n21.3.3 Segmentation\\nOnce the edges have been detected in an image, this information can be\\nused to segment the image into homogeneous areas. In this case, when we\\nsay that an area of an image is homogeneous, we mean that its color or\\nintensity of shading does not vary dramatically—in other words, there are\\nno edges within the area.\\nThere are other methods available for segmenting an image, apart from\\nusing edge detection. One simple method is thresholding. Thresholding\\ninvolves finding the color of each pixel in an image and then considering\\nadjacent pixels to be in the same area as long as their color is similar\\nenough. This is very similar to edge detection but is used to segment the\\nimage, rather than just to find the edges in the image. This is different\\nbecause edge detection will not necessarily produce continuous edges and\\nwill, therefore, not necessarily divide the image into more than one area. In\\nthe photograph shown in Figure 21.3, there are clearly a number of distinct'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 639, 'page_label': '640'}, page_content='21.3 Image Processing 613\\nFigure 21.6\\nA line drawing of a simple\\nblocks world\\nsegments, but the edges detected in Figure 21.4 divide the image into only\\none or two segments.\\nA similar method for segmenting images is splitting and merging . Split-\\nting involves taking an area that is not homogeneous and splitting it into\\ntwo or more smaller areas, each of which is homogeneous. Merging\\ninvolves taking two areas (e.g., two individual pixels) that are the same as\\neach other, and adjacent to each other, and combining them together into a\\nlarger area. This provides a sophisticated iterative approach to segmenting\\nan image that is often far more reliable than simple thresholding.\\n21.3.4 Classifying Edges in Line Drawings\\nOnce a computer vision system has extracted the edges from an image, it\\nhas something that is rather similar to a line drawing. The line drawings in\\nFigure 21.6 are illustrative of the kind of representations a system might\\nhave in observing a simple blocks world.\\nSuch illustrations are easy for us to interpret. For a computer system to\\nunderstand what it is observing, it needs to first classify the edges in the\\ndiagram it has produced.\\nThere are three types of edges:\\n■ A convex edge is an edge between two faces that are at an angle of\\nmore than 180/H11034from each other.\\n■ A concave edge is an edge between two faces that are at an angle of\\nless than 180/H11034from each other.\\n■ Where only one of the two faces that are joined by an edge is visible\\nin the image, the edge is an occluding edge. (An occluding edge is\\na depth discontinuity.)\\nIn Figure 21.7, the edges have been labeled as convex, concave, or occluding\\nusing the traditional notation of + for a convex edge,/H11002for a concave edge,\\nand an arrow for an occluding edge. The direction of the arrow on an'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 640, 'page_label': '641'}, page_content='614 CHAPTER 21 Machine Vision\\n+\\n+ + ++\\n+\\n+\\n+\\n–\\n–\\n–\\n–\\n–\\n–\\n+\\n+\\n–\\n–\\nFigure 21.7\\nSimple blocks world line\\ndrawing with edges\\nlabeled as convex (+),\\nconcave (/H11002), and\\noccluding (arrow)\\noccluding edge is such that the visible surface is on the right of the direc-\\ntion of the arrow.\\nHaving determined which type each edge in the image is, the system can\\nmake further assessments about the nature, shape, and relative position of\\nthe objects in the picture. Now we need a method for determining which\\ntype each edge is.\\nFirst, if we assume that all objects in our image are polyhedral (i.e., all the\\nedges are straight and all surfaces are flat), then we can make the following\\nassumption: A single line will have the same type (convex, concave, or\\noccluding) for its entire length. If you look carefully at Figure 21.7, you will\\nsee that this is the case. No line starts out as concave and ends up convex, or\\nany other combination. This is because the type of an edge is determined\\nby the angle of the faces that the edge joins, and if all lines are straight and\\nall faces flat, then this angle cannot change. (T o see that this is true, try to\\nimagine a polyhedral object where the angle between two faces varies over\\nthe edge that joins them).\\nIn the 1970s, Huffman (1971) showed that further assumptions could be\\nmade that would help in the analysis of line drawings of polyhedral shapes.\\nIf one considers a vertex at which a number of edges meet, it can be shown\\nthat there are only a few possible combinations of edges that can make up\\nthat vertex.\\nMost vertices form a point of connection for three flat faces. Such vertices\\nare called trihedral vertices. There are only 16 possible arrangements of\\nedges that can make up trihedral vertices in the real world, and these are\\nshown in Figure 21.8.\\nAs you can see from Figure 21.8, there are a number of labelings of a trihe-\\ndral vertex that are simply not possible.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 641, 'page_label': '642'}, page_content='21.4 Using Texture 615\\n+\\n++\\n+\\n+\\n++\\n++\\n–\\n––\\n–\\n–\\n––\\n–+\\n––\\nFigure 21.8\\nThe 16 possible ways to\\nlabel trihedral vertices\\nAs a result, in analyzing a scene such as the one shown in Figure 21.6, a\\ncomputer vision system can use Huffman’s 16 trihedral vertices as con-\\nstraints to limit the possible labelings for the diagram.\\nThe Waltz algorithm does so by selecting a possible label for one junction,\\nfrom the list shown in Figure 21.8, and then moving onto an adjacent junc-\\ntion and attempting to apply a labeling to this junction. If none is possible,\\nthe algorithm backtracks and tries a different labeling for a previous junc-\\ntion. Hence, the method applies depth-first search to the structure until a\\nlabeling is found that ensures that all junctions have valid labelings.\\nIn some cases, there will be more than one possible labeling. In other words,\\nthe image is ambiguous. In such cases, additional information must be used.\\nOften shading information can be used to provide additional constraints.\\n21.4 Using Texture\\nT exture is a vital aspect of the visual world. It helps us to identify a wide\\nrange of facets of what we see—it does not just tell us the materials that\\nthings are made of; it also gives us information about movement and shapes.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 642, 'page_label': '643'}, page_content='616 CHAPTER 21 Machine Vision\\nFigure 21.9\\nFour different textures\\nT exture, in the visual sense, can be defined as the pattern that we perceive\\non the surface of an object or in an area. The photos in Figure 21.9 show a\\nvariety of textures.\\nClearly, the textures of the images in Figure 21.9 show us that we are look-\\ning at grass, pebbles, clouds, and roofing tiles. As we will see, the textures\\nalso tell us a great deal more than this.\\n21.4.1 Identifying Textures\\nT o make use of texture information from an image, a computer system must\\nfirst analyze the image and determine the nature of its texture or textures.\\nThe simplest type of texture is the texture we usually expect to find on\\nblocks in the simple blocks world—this is a completely plain, vanilla tex-\\nture, which could be described as textureless, or smooth. In dealing with\\nthe blocks world, we tend to assume that our blocks are textureless and\\ntherefore pay no attention to texture. In fact, of course, in a real blocks'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 643, 'page_label': '644'}, page_content='21.4 Using Texture 617\\nworld, the blocks must be made of something (wood, metal, plastic) and\\nmust therefore have a texture.\\nIn examining a blocks world scene, texture usually is not terribly important\\nbecause the shapes are so simple that determining their position, orienta-\\ntion, and so on can be done using edge detection and simple mathematical\\nalgorithms. In more complex environments, a system must make use of\\ntexture to make these kinds of analyses.\\nThere are a number of statistical methods that can be used to categorize a\\nparticular texture in an image. We will now examine one such method,\\nbased on the use of cooccurrence matrices.\\nThe idea of this method is to determine the relationships between pixels in\\nthe image of particular intensities. We will represent our image as a matrix\\nof pixel values, which for this example will range from 0 to 4. Let us define\\na matrix P, which we will use for this example as the matrix representing\\nthe intensity values of the grey pixels in our image:\\nClearly,P defines a rather uninteresting image, but for a real image the matrix\\nwould be significantly larger and would have a greater range of values.P will\\nsuffice for us to illustrate this statistical method for analyzing textures.\\nEach pixel in P is defined as P(x, y), so that for example:\\nP(0, 0) = 1\\nP(1, 1) = 3\\nP(3, 2) = 3\\nWe will now define a new matrix,D, which is defined as follows:\\nD(m, n) is the number of pairs of pixels in P for which\\nP(i, j) = m\\nP(i + /H9254i, j + /H9254j) = n\\nwhere i and j are any pixels in P, and /H9254i and /H9254j are small increments defined\\nfor this particular matrix D.\\nP =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\uf8fa\\n1032\\n2301\\n1413\\n3224'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 644, 'page_label': '645'}, page_content='618 CHAPTER 21 Machine Vision\\nIn other words, D defines how likely it is that any two pixels a particular\\ndistance apart (/H9254i and /H9254j) will have a particular pair of values.\\nWe will see how this works for our matrix.\\nLet us first define (/H9254i, /H9254j) = (1, 1). In other words, we are interested in pairs\\nof pixels that are diagonally one pixel apart.\\nWe can now define the matrix D.\\nD(0, 0) is equal to the number of pairs of pixels inP that are (1, 1) apart, and\\nwhich are both valued 0. Looking atP, we can see that there is one such pair:\\nP(1, 0) = 0\\nP(2, 1) = 0\\nHence, D(0, 0) = 1.\\nD(1, 0) is equal to the number of pairs of pixels in P that are (1, 1) apart\\nand which are values 1 and 0, respectively. Y ou should be able to see that no\\nsuch pairs exist in P.H e n c e ,D(1, 0) = 0.\\nWe can define the whole of matrix D in the same way. D is a 5 /H110035 matrix\\nbecause there are five possible pixel values in P:\\nLet us see now the difference when we apply this method to a different matrix:\\nClearly this matrix represents an image where the texture is far more\\nnoticeable than in the previous matrix, P. We should expect to see this rep-\\nresented in the corresponding matrix D\\n1, which is defined as follows:\\nP1\\n1043\\n2104\\n3210\\n4321\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\uf8fa\\nD =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n10010\\n00111\\n00001\\n01000\\n00100'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 645, 'page_label': '646'}, page_content='21.4 Using Texture 619\\nFinally, let us examine the following matrix:\\nThis matrix, P2, is clearly similar to P1, but reflected about the Y-axis. Let us\\nsee how D2 turns out:\\nThe difference between D2 and D1 is as we would expect. The values in D1\\nare on the main diagonal, whereas in D2 they are on two minor diagonals.\\nThis reflects the relationship between the vector (/H9254i, /H9254j) = (1, 1) and the tex-\\ntures in the images.\\nOne extension to this method is to assume that we should not distinguish\\nbetween D(m, n) and D(n, m). Hence, we produce the cooccurrence\\nmatrix, C, which is defined as follows:\\nC = D + DT\\nWhere DT is the transposition of matrix D.\\nC has the property that C(m, n) = C(n, m) because\\nC(m, n)=  D(m, n) + D(n, m)\\n= D(n, m) + D(m, n)\\n= C(n, m)\\nD2\\n00300\\n00020\\n00001\\n10000\\n02000\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nP2\\n3401\\n4012\\n0123\\n1234\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\uf8fa\\nD1\\n20000\\n03000\\n00200\\n00010\\n00001\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 646, 'page_label': '647'}, page_content='620 CHAPTER 21 Machine Vision\\nThis matrix, C, gives us a useful statistical analysis of the texture contained\\nwithin the image, relative to the vector ( /H9254i, /H9254j) that we have chosen. By\\nusing a number of different ( /H9254i, /H9254j) vectors, we can determine more infor-\\nmation about the texture.\\n21.4.2 Structural Texture Analysis\\nAn alternative to the statistical approach is to analyze textures using a\\nstructural approach, based on units of texture called texels. A texel is a sin-\\ngle texture element or a piece of image that is repeated throughout the\\nimage to produce the texture. Looking at the final photo in Figure 21.9, of\\nroofing tiles, we can see that this texture is made up of a single tile, repeated\\nover the image. This tile is the texel. Note that in fact, due to perspective\\nand other distortions, the texels will not all be identical (e.g., perspective\\nwill cause them to have different shapes and sizes, and distortions will also\\nbe caused by mapping the texture onto a curved surface).\\nT exel analysis thus involves searching for repeated components within an\\nimage, taking into account distortions such as elongation, rotation, com-\\npression, and so on. As we see in the next sections, once the texture has\\nbeen determined in this way, this information can be used to determine a\\nnumber of useful properties about the object whose texture we are\\nlooking at.\\n21.4.3 Determining Shape and Orientation from Texture\\nUsually, when examining a texture, we are able to use information gained\\nfrom the texture to determine the shape of the surface. This assumes, of\\ncourse, that the picture involves a surface. This is certainly the case for three\\nof the images in Figure 21.9 (the pebbles, the grass, and the tiles are all\\nplaced on some surface). For the fourth image, of clouds, the texture tells us\\nsomething about the “surface” of the clouds, although this is somewhat\\nillusory because the clouds are not solid. Still, the techniques used by com-\\nputer vision systems will draw mostly the same conclusions about the shape\\nand the “surface” of the clouds that we would when examining that picture.\\nNotice how the variation of the shape of the bricks in the photo in Figure\\n21.10 enables us to see, without any external help, that the wall in the pic-\\nture is curved. The main reason that we can tell that the wall is curved is the\\nforeshortening of the bricks on the right-hand side of the picture, suggest-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 647, 'page_label': '648'}, page_content='21.4 Using Texture 621\\nFigure 21.10\\nA section of curved wall,\\nshowing how surface\\nshape can be determined,\\nto some extent, from the\\ntexture\\nFigure 21.11\\nAn illustration showing\\nhow the texture on the\\nsurface of a golf ball helps\\nus to understand the\\nshape of the ball\\ning that they are further away, and seen from a sharper angle, than those on\\nthe left-hand side of the picture.\\nIn a similar way, a photograph of a golf ball can be used to determine the\\nshape of the golf ball, by examining the way in which the small circular\\nindentations on the surface of the ball vary in apparent shape. See the illus-\\ntration in Figure 21.11, for example.\\nThese circles on the surface of the ball, and the bricks in Figure 21.10, are\\nthe texels that make up the texture, which we use to determine the shape of\\nthe object in the picture.\\nAlthough the image in Figure 21.11 is flat, it is translated in our minds into\\na sphere because that is the simplest explanation for the way the circles dis-\\ntort as they get farther from the center of the image.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 648, 'page_label': '649'}, page_content='622 CHAPTER 21 Machine Vision\\nFigure 21.12\\nA photograph of a section\\nof wall. The angle between\\nthe perpendicular to this\\nwall and the camera can\\nbe determined by examin-\\ning the distortion of the\\ntexels (in this case, the\\nindividual bricks).\\nY -axis\\nX-axis\\nZ-axis\\nP\\nImage plane\\nSurface Normal (n)\\nσ\\nτ\\nFigure 21.13\\nShowing how slant (/H9268) and\\ntilt (/H9270) are measured.\\nA simple way to extract shape from a texture of this kind is to assume that\\neach texel is flat (which with a curved surface is probably not true but is a rea-\\nsonable approximation). By determining the extent and direction of distor-\\ntion of a given texel, the slant of the texel can be determined and a\\nperpendicular line projected from it. Once perpendiculars have been deter-\\nmined for all the texels in an image, the surface shape has been determined.\\nIn the same way, the orientation of a flat surface, such as the section of wall\\nshown in Figure 21.12, can be determined.\\nA similar, though less obviously intuitive, method can be applied to tex-\\ntures such as those shown in Figure 21.9. In these cases, the orientation of\\nthe surfaces beneath the grass, pebbles, and roofing tiles would be deter-\\nmined, as would the apparent shape of the clouds.\\nWhen determining orientation, we are interested in two factors: slant and\\ntilt. Slant and tilt are measured between a vector perpendicular to the sur-\\nface of an object and the z- and x-axes. This is illustrated in Figure 21.13.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 649, 'page_label': '650'}, page_content='21.5 Interpreting Motion 623\\nSlant, which is usually written as the Greek letter sigma— /H9268, is measured\\nbetween the surface normal (the vector n, in Figure 21.13), which is per-\\npendicular to the object we are observing at the point we are interested in,\\nand the z-axis. This is shown in Figure 21.13.\\nWe measure tilt (often written as the Greek letter tau— /H9270) as the angle\\nbetween the x-axis and the projection, p, of the normal vector n onto the\\nplane of the image. In other words, the tilt is an apparent angle, determined\\nby the position and orientation of the viewer.\\nIn the diagram in Figure 21.13, we are measuring slant and tilt of a specific\\npoint on the surface of a sphere. This point is the point on the surface from\\nwhich the normal vector, n, has been measured.\\n21.5 Interpreting Motion\\nOne of the most important aspects of mammalian vision is the ability to\\ndetect (and thus react to) motion. For hunters, it is important to be able to\\nspot prey and follow it as it attempts to flee, and for the prey, it is important\\nto detect the hunter as quickly as possible. In a world full of confusing\\nvisual information, most animals (including humans) use motion to pro-\\nvide additional information about what is being seen.\\nSimilarly, for an agent that has the ability of vision, it is important to be\\nable to detect motion.\\nThere are two main types of motion that an observer is interested in—\\nmotion of other objects and the apparent motion of the environment\\ncaused by the observer’s own motion.\\nWe will start with the latter type of motion—the apparent motion caused\\nby the movement of the camera or other image capture device.\\nThe photograph in Figure 21.14, for example, was taken using a camera on\\na moving train. A subsequent photo, taken a second later, would show that\\nthe buildings, trees, and other objects in the photograph had apparently\\nmoved. Of course, this is in fact due to the fact that the train, and therefore\\nthe camera, has moved. This apparent motion in an image is known as\\noptical flow , and the vectors that define the apparent motion make up\\nwhat is known as the motion field. Some of these vectors have been drawn\\nonto the photograph in Figure 21.14. The photograph was taken from the'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 650, 'page_label': '651'}, page_content='624 CHAPTER 21 Machine Vision\\nFigure 21.14\\nA photograph taken from a\\ntrain, illustrating the idea\\nof the motion field. Some\\nof the motion field vectors\\nhave been drawn in as\\narrows, moving away from\\nthe camera.\\nback of the train, so the vectors show that the objects are apparently mov-\\ning away from the camera.\\nThe direction of the motion field will clearly depend on the direction in\\nwhich the camera is moving. If the photograph in Figure 21.14 had been\\ntaken from a car crossing a level crossing, the arrows would have gone hor-\\nizontally across the image, for example, instead of heading toward the van-\\nishing point, the point toward which perspective causes all parallel lines in\\nthe image to converge.\\nHence, by examining a sequence of images taken from a moving camera, if\\nthe direction and speed of the optical flow can be determined, then this can\\nprovide information about the direction and speed of travel of the camera,\\nrelative to the background.\\nIt is possible to estimate the nature of the motion field, and thus the optical\\nflow, in a sequence of images by comparing the features of the images. First,\\nwe assume that the objects in a sequence of images will not themselves\\nchange, and so any changes that occur to them are caused by the movement\\nof the camera. This will clearly not apply if there are moving objects (e.g.,'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 651, 'page_label': '652'}, page_content='21.6 Making Use of Vision 625\\ncars, people, animals) in the image, but it will still apply to the majority of the\\nfeatures within most images, and the anomalies can be dealt with separately.\\nBy computing common points in a sequence of images, we can thus calcu-\\nlate the optical flow vectors and thus determine the speed of motion of the\\ncamera. This technique can also be applied in cases where the camera is still\\nand is capturing a sequence of images of a moving or rotating object.\\n21.6 Making Use of Vision\\nWe have thus far described techniques that can enable a computer system\\nto extract information from a visual scene that has been recorded on a\\ndevice such as a camera. We will now look at ways in which this informa-\\ntion can be used for practical purposes.\\nImages such as the one shown in Figure 21.14 might be used to control the\\nmotion of a vehicle. In this case, the visual information could be used to\\ncontrol the speed of travel of the train. If another train or other obstacle\\nappeared on the tracks in front of the train, the brakes could be applied, for\\nexample. A more complex system could control the motion of a car, which\\ncould be designed to negotiate traffic, stop at traffic lights, and avoid pedes-\\ntrians, other vehicles, and the sidewalk.\\nOne of the most common uses of machine vision in robotic agents is to\\nidentify objects in the agent’s path. In simple cases, these objects will be\\nlimited to blocks of various shapes and sizes, but in real-world systems, the\\nobjects could be almost anything.\\nThe main task is therefore to map the image that has been received to an\\ninternal representation of an object. The method that is usually used\\ndepends on the principle that there are some properties of any object that\\nare invariant. In other words, whatever angle you view the object from,\\nwhatever lighting conditions it is in, whatever changes occur to its shape,\\nthe invariant properties will remain constant.\\nIn the case of a sphere, this is fairly simple: the sphere will appear in a two-\\ndimensional image as a circle under almost any conditions. This is of course\\ncomplicated by the possibility of other objects obscuring the object we are\\nlooking at and also by the complications of texture. A sphere with a checker-\\nboard pattern on it might look rather different when placed against a\\ncheckerboard background than if it were placed against a plain background.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 652, 'page_label': '653'}, page_content='626 CHAPTER 21 Machine Vision\\nThe method that is usually used to identify objects is known as the parts\\ndecomposition method, which involves breaking an object into its constituent\\nparts and then attempting to identify those parts in the image. For example, a\\ncat could be broken down into a head, eyes, mouth, tail, legs, fur, etc.\\nAnother method, which does not work so well with cats, but works well\\nwith more rigid objects, is to assume that an object will look the same once\\na set of transformations (e.g., rotation, translation, and increase or decrease\\nin size) are applied. Hence, if an image is found that looks somewhat like a\\ncube, but not exactly the same as the image of a cube that the system has\\nbeen trained with, the image can be transformed using rotation, transla-\\ntion, and resizing, until it matches more closely the training image. This\\nmethod is known as the alignment method. This method thus involves\\nfinding an object whose internal representation matches that being seen in\\nthe image after applying one or more allowable transformations.\\nHaving identified an object, the agent can use its behavior model (defined\\nperhaps using the subsumption architecture—see Chapter 19) to deter-\\nmine what to do—it may need to move toward the object to examine it in\\nmore detail, it may want to avoid the object, or it may want to pick it up. If\\nthe agent cannot detect objects using vision, then it can be very difficult for\\nit to decide what to do. Hence, agents that need to interact with objects in\\nthe real world in any way more complex than simply moving past them\\nneed to be able to receive some kind of visual input from the world and\\nthen to analyze that visual information.\\n21.7 Face Recognition\\nOne very popular area of computer vision at present is the study of automatic\\nface recognition. This problem is an excellent example of the kinds of prob-\\nlems that Artificial Intelligence techniques are usually applied to: it is a prob-\\nlem that humans find so simple that we take it for granted, yet it is a problem\\nthat traditional computer science has found almost impossible to solve.\\nThe difficulties with automatic face recognition are numerous. First of all,\\nthe conditions in which a face can be seen, such as lighting, distance from\\ncamera to face, and angle, can dramatically alter the appearance of the face.\\nThis problem is faced with most object recognition systems. Face recogni-\\ntion is further complicated by the fact that human faces are so flexible and\\nso capable of being altered. Facial expressions are one complexity, but peo-'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 653, 'page_label': '654'}, page_content='21.7 Face Recognition 627\\nple also are able to grow beards; cut or grow their hair; wear glasses, sun-\\nglasses, hats, and earrings; and grow older, all of which can significantly\\naffect the appearance of a face.\\nAs a result, identifying invariant properties of a given face is an important\\nfirst step in automating the process of face recognition. These properties\\nneed to be invariant regardless of distance, angle, orientation, and lighting,\\nbut also regardless of what has happened to the face—whether it is wearing\\nglasses, whether it has its eyes shut or open, and so on.\\nOne early approach to face recognition was to identify particular facial fea-\\ntures, such as eyes, nose, mouth, eyebrows, and so on, and to store informa-\\ntion about the relative positions of those features. These features could be\\ncompared in a new face to determine if it is one that has been seen before.\\nThis method works in some circumstances, but is not particularly robust.\\nOne problem with this method is that it assumes that the best way to tell\\nthe difference between two faces is to note the locations of the features such\\nas eyes, mouth, and so on. This might not be the case.\\nThis observation led to another face recognition method that uses eigen-\\nfaces. Eigenfaces are based on the idea of principle component analysis .\\nPrinciple component analysis is an important idea in computer science.\\nThe idea is that to learn to recognize any type of items of data, the best way\\nis to determine the features of the data that vary most from one item to\\nanother. This idea was applied, for example, in Chapter 20, when we saw\\nthat the way in which a system could search for responses to a query from a\\ncorpus of text was to treat the words that are most infrequent within the\\ncorpus as being the most important in queries. In the same way, if we look\\nat a selection of ten faces and note that the position of the tip of the nose\\nrelative to the end of the chin is the feature that varies the most, then this is\\na principle component and should be treated as an important feature for\\nidentifying faces. This is the idea behind eigenfaces.\\nThe eigenfaces are the components chosen to represent the faces in the\\ntraining set. These features are chosen as being the features that provide the\\ngreatest differentiation between the faces in the training set and thus pro-\\nvide the greatest likelihood of giving a correct match when presented with\\na new face.\\nThe eigenfaces can be viewed graphically and tend to look like morphed\\nimages of several faces, which is indeed what they are. When attempting to'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 654, 'page_label': '655'}, page_content='628 CHAPTER 21 Machine Vision\\nmatch a single face, a number of these eigenfaces are combined together,\\nand it is the manner in which these faces are combined together that is used\\nto identify which face is being viewed.\\nThis method can also be thought of in terms of vectors. The eigenfaces are\\nthe vectors (or eigenvectors) that form the principle components of the\\ntraining data, and thus define the face space. When a new face is examined,\\nit is constructed as a sum of some combination of the eigenvectors, and this\\nvector is compared with the vectors that were already calculated for the\\nfaces in the training data. The face with the closest vector is the match.\\nUsually a number of faces are used for each person in the training set, with\\na variety of expressions and with different additional elements such as hats,\\nglasses, and so on. In this way, the eigenfaces method provides a very robust\\nway of recognizing faces. In experimentation, this method has given an\\naccuracy of over 90% at matching faces from a small database of training\\nimages (around 50 images).\\n21.8 Chapter Summary\\n■ The mammalian vision system is a remarkably sophisticated system,\\nand most computer vision systems are based to some extent on it.\\n■ Edge detection is often the first stage in image-processing systems.\\nEdge detection involves determining the location of high-fre-\\nquency areas of an image, which usually correspond to edges or\\nchanges in depth in the image.\\n■ Convolution is used by the Canny edge detector to find edges in\\nimages.\\n■ Once edges have been detected in an image, the image is usually\\nsegmented into homogeneous areas, which correspond roughly to\\nparticular objects or textures.\\n■ Edges in a three-dimensional line drawing can be classified using\\nthe Waltz algorithm.\\n■ T exture can be used to provide a great deal of information about a\\nscene, such as shape and orientation.\\n■ Detecting and interpreting motion is an important part of the\\nmammalian vision system and is also useful in many computer'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 655, 'page_label': '656'}, page_content='21.10 Exercises 629\\nimage recognition systems (particularly in robotic agents that need\\nto navigate in the real world).\\n■ Face recognition is one of the hardest problems of image recogni-\\ntion, but one in which a great deal of success has already been\\nachieved. One popular method is to use eigenfaces, which are\\nbased on the idea of principle component analysis.\\n21.9 Review Questions\\n21.1 Why does it make sense to model computer vision systems on the\\nhuman vision system? Can you think of any suitable alternative\\nmodels?\\n21.2 Why is edge detection used as an early stage of computer vision?\\nCan you think of any situations in which edge detection methods\\nwould fail completely?\\n21.3 Explain how convolution is used to detect edges in images.\\n21.4 What is the purpose of segmentation?\\n21.5 Explain the purpose of the Waltz algorithm. Describe in detail how\\nit works.\\n21.6 Why is texture so important for computer vision systems? What dif-\\nferences would there be if the world had no texture and all objects\\nwere smooth and uniformly colored? Would this make it easier or\\nharder for machine vision systems? What if there were no shadows\\nand everything was uniformly illuminated from all sides as well?\\n21.7 How do computer vision systems make use of motion?\\n21.8 Explain the way that eigenfaces are used in face recognition systems.\\n21.10 Exercises\\n21.1 Apply the Waltz algorithm using pen and paper to the three-\\ndimensional scene shown in Figure 21.6. Is it possible that you will\\nend up with a different labeling from that shown in Figure 21.7? If\\nyour answer was yes, how could this happen? If your answer was\\nno, why not, and can you imagine any situation in which there is\\nmore than one possible labeling for such an image?'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 656, 'page_label': '657'}, page_content='630 CHAPTER 21 Machine Vision\\n21.2 Implement an edge detection system in the programming language\\nof your choice. Y ou will need first to find a way to obtain pixel data\\nfrom an image and convert this into a two-dimensional array of\\npixels. Y our edge detection system should be capable of outputting\\nan image showing the edges.\\n21.11 Further Reading\\nThere are many books available on the subject of image recognition, com-\\nputer vision, face recognition, and other related subjects. Shapiro (2001)\\nand Forsyth (2002) both provide excellent coverage of the subject. Nalwa\\n(1993) provides a very readable introduction. Hoffman (1998) provides a\\ndifferent perspective, with a cognitive scientist’s view of human vision.\\n2D Object Detection and Recognition: Models, Algorithms, and Networks ,b y\\nY ali Amit (2002 – MIT Press)\\nIntelligent Machine Vision: Techniques, Implementations and Applications ,\\nby Bruce G. Batchelor and Frederick M. Waltz (2001 – Springer V erlag)\\nAdvances in Image Understanding: A Festschrift for Azriel Rosenfeld , edited\\nby Kevin W. Bowyer and Narendra Ahuja (1996 – Wiley IEEE Press)\\nNeural Networks for Vision and Image Processing, edited by Gail A. Carpen-\\nter and Stephen Grossberg (1992 – MIT Press)\\nMachine Vision: Theory, Algorithms, Practicalities , by E. R. Davies (1996 –\\nAcademic Press)\\nThree-Dimensional Computer Vision, by Olivier Faugeras (1993 – MIT Press)\\nComputer Vision: A Modern Approach, by David A. Forsyth and Jean Ponce\\n(2002 – Prentice Hall)\\nDynamic Vision: From Images to Face Recognition , by Shaogang Gong and\\nStephen J. McKenna (2000 – Imperial College Press)\\nComputer and Robot Vision (Volume II), by Robert M. Haralick and Linda\\nG. Shapiro (2002 – Pearson Education)\\nVisual Intelligence: How We Create What We See , by Donald D. Hoffman\\n(1998 – W. W. Norton & Company)\\nRobot Vision, by Berthold K. Horn (1986 – McGraw Hill Higher Education)\\nMachine Vision, by Ramesh Jain, Rangachar Kasturi, and Brian G. Schunck\\n(1995 –McGraw Hill)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 657, 'page_label': '658'}, page_content='21.11 Further Reading 631\\nComputer Vision and Fuzzy Neural Systems , by Arun D. Kulkarni (2001 –\\nPrentice Hall)\\nA Guided Tour of Computer Vision , by Vishvjit S. Nalwa (1993 – Addison\\nWesley)\\nFeature Extraction in Computer Vision and Image Processing ,b y  M a r k  S .\\nNixon and Alberto Aguado (2002 – Butterworth-Heinemann)\\nAlgorithms for Image Processing and Computer Vision, by J. R. Parker (1996 –\\nJohn Wiley & Sons)\\nLearning-Based Robot Vision, edited by Josef Pauli (2001 – Springer V erlag)\\nComputer Vision, by Linda G. Shapiro and George C. Stockman (2001 –\\nPrentice Hall)\\nImage Processing: Analysis and Machine Vision , by Milan Sonka, Vaclav\\nHlavac, and Roger Boyle (1998 – Brooks Cole)\\nIntroductory Techniques for 3-D Computer Vision, by Emanuele Trucco and\\nAlessandro V erri (1998 – Prentice Hall)\\nHuman Face Recognition Using Third-Order Synthetic Neural Networks ,b y\\nOkechukwu A. Uwechue and Abhijit S. Pandya (1997 – Kluwer Academic\\nPublishers)\\nFace Recognition: From Theory to Applications , by Harry Wechsler (1998 –\\nSpringer V erlag)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 658, 'page_label': '659'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 659, 'page_label': '660'}, page_content='Glossary\\nThis glossary includes definitions and descriptions of the most important\\nterms used in this book.\\nA\\nAbduction\\nA *nonmonotonic form of reasoning that helps us to find plausible expla-\\nnations for observed phenomena.\\nAccepting state\\nA *state in a *finite state machine that represents a “yes” response.\\nAcquaintance algorithm\\nA vector-based approach to identifying languages using *n-grams.\\nAction description language (ADL)\\nA more expressive variation of the *STRIPS planning language.\\nActivation function\\nThe function applied to the inputs of a *neuron in a *neural network. The\\noutput of this function is compared with the *activation level to determine\\nif the neuron fires.\\nActivation level\\nThe level of output that a *neuron must reach in order to fire.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 660, 'page_label': '661'}, page_content='634 Glossary\\nActivity product rule\\nThe rule used in *neural networks that use *Hebbian learning to determine\\nhow the *weights between *neurons change.\\nAdmissibility\\nA *heuristic method is defined as admissible if it never overestimates the\\ncost of getting from a given *state to a *goal state.\\nAdversarial methods\\nMethods used by game-playing systems to search a *game tree for a *path\\nthat will lead to a win over an opponent.\\nAgent\\nAn entity (usually a software entity) that exists to assist humans in carrying\\nout some task or solving some problem. Types of agents include *software\\nagents, *interface agents, *mobile agents, and *information agents.\\nAgent team\\nA group of *agents that collaborate together to reach a common goal.\\nAlphabet\\nThe set of symbols available for a logical system.\\nAlpha–beta pruning\\nA method used to make searching a *game tree more efficient. It relies on\\nthe principle that if a part of a game tree is going to give a bad result, it is\\nnot worth further examining that part of the tree.\\nAmbiguity\\nThe problem that an *utterance in a *human language can have more than\\none possible meaning. Types of ambiguity include lexical, semantic, syntac-\\ntic, referential, and local.\\nAncestor\\nAn ancestor, a, of a *node,n, in a *tree is a node that is further up a *path in\\nthe tree than n. n is a *descendant of a.\\nAnd-goal\\nAnd-goals are *subgoals in a *goal tree that must all be satisfied in order to\\nsatisfy the goal tree.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 661, 'page_label': '662'}, page_content='Glossary 635\\nAnd-node\\nA *node that represents an *and-goal.\\nAnd–or tree\\nSee *goal tree.\\nAntecedent\\nThe part of the *rule that comes before the *implication. In the rule A → B,\\nA is the antecedent. See *consequent.\\nArtificial Intelligence\\nThe subject of this book. See Chapters 1 to 21 for more information. See\\nalso *weak AI, *strong AI.\\nArtificial Life\\nMethods modeled on life that often use *emergent behavior to find better\\nsolutions to problems than can be found using traditional methods.\\nArtificial neural network\\nSee *neural network.\\nAssociativity\\nA property of mathematical and logical operators. Operator o is associative\\nif A o (B o C) ≡ (A o B) o C.\\nAtomic action\\nIndividual actions used by *planning systems. An atomic action can consist\\nof a number of smaller actions, but it is treated as one indivisible action for\\nthe purpose of constructing a plan.\\nAtomic formula\\nA *well-formed formula of the form P(x1, x2, x3, ...,x n).\\nAttractor network\\nSee *recurrent network.\\nAugmented finite state machine (AFSM)\\nA type of *finite state machine that uses *situated action rules. AFSMs are\\nused in Brooks’s *subsumption architecture.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 662, 'page_label': '663'}, page_content='636 Glossary\\nAugmented transition network (ATN)\\nA type of *transition network that is used for *parsing sentences. An ATN\\nhas *procedures and tests that are attached to its arcs.\\nAutoassociative memory\\nA type of memory that can recognize an object but cannot associate one\\nobject or piece of data with another. A *Hopfield network is an autoasso-\\nciative memory. See *heteroassociative memory.\\nAutonomy\\nA property of *agents. An autonomous agent has the ability to act inde-\\npendently to some extent of its owner or programmer. This is often a\\ndesired property for *intelligent agents.\\nAxon\\nThe part of a *neuron in the human brain that provides output to other\\nneurons via a *synapse.\\nB\\nBackpropagation\\nA method used to modify the *weights in a multilayer *neural network.\\nErrors at the output layer are fed back through the network, correcting the\\nweights. Over a number of iterations, this usually leads to a network that\\ngives mostly correct responses to the *training data.\\nBackus–Naur form (BNF)\\nA language used to define the grammar of *formal and *informal lan-\\nguages. BNF uses *terminal and *nonterminal symbols, and *rewrite rules\\nthat express how sentences can be legally built up out of terminal symbols.\\nBackward chaining\\nSee *goal-driven search.\\nBayesian belief network\\nAn acyclic directed *graph, where the *nodes in the graph represent evi-\\ndence or hypotheses, and where an edge that connects two nodes represents\\na dependence between those two nodes. Each node is labeled with a set of\\nprobabilities that express how that node depends on other nodes.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 663, 'page_label': '664'}, page_content='Glossary 637\\nBayes’ optimal classifier\\nA system that uses *Bayes’ theorem to learn to classify data. It can be shown\\nthat this classifier is optimal, which means that it provides the best possible\\nmechanism for classifying data.\\nBayes’ theorem\\nBayes’ theorem can be used to calculate the probability that a certain event\\nwill occur or that a certain proposition is true, given that we already know\\na related piece of information. It is written as follows:\\nBelief desire intention architecture (BDI)\\nAn architecture used by *agents that uses beliefs about the world and\\ndesires to develop intentions about how the agent should behave.\\nBidirectional associative memory (BAM)\\nA *neural network used to associate items from one set with items in\\nanother set.\\nBinary operator\\nA logical or mathematical operator that takes two arguments, such as logi-\\ncal and (\\n∧) and logical or (∨). See *unary operator.\\nBivalent logic\\nA logical system that has two *truth values. Classical logic is bivalent\\nbecause a logical expression can either be true or false. See *multivalent\\nlogic and *fuzzy logic.\\nBlackboard architecture\\nA method for structure knowledge representation that combines informa-\\ntion from a number of knowledge sources (such as human experts) in\\norder to solve a problem.\\nBlind search method\\nA *search method that does not use *heuristics. Also known as *uninformed\\nsearch. *Depth-first search and *breadth-first search are blind search methods.\\nPB A\\nPA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 664, 'page_label': '665'}, page_content='638 Glossary\\nBlocks world\\nA scenario that is used to explain planning techniques. The blocks world\\nconsists of a table with a number of blocks, which are usually cubes. The\\nblocks world has very simple properties, and there are usually a limited set\\nof actions that can be taken to interact with the world, such as “pick up”\\nand “put on.”\\nBottom up\\nAn approach to solving problems that involves first solving the smaller sub-\\nproblems, and repeatedly combining these solutions together until a com-\\nplete solution is found. See *top down.\\nBounded lookahead\\nA method used when searching *game trees that involves cutting off\\n*search when a specified depth in the tree is reached. This is particularly\\nuseful in games such as chess or Go that have very deep search trees.\\nBound variable\\nA bound variable in a logical expression is one that has been quantified\\nwithin the same scope. For example, in the expression \\n∀x(x → y), x is\\nbound because it is quantified by the ∀ *quantifier. See *free variable.\\nBraitenberg vehicle\\nA type of *robotic vehicle that is used in thought experiments to study the\\nnature of intelligence. Braitenberg vehicles range from very simple robots\\nthat follow or avoid light to more complex systems. None of them have any\\nreal intelligence, but they often display behavior that appears intelligent.\\nBranch\\nA connection between two *nodes within a *tree.\\nBranching factor\\nA node within a tree has a branching factor of n if that node has n *chil-\\ndren. In a tree that has a branching factor of n, all nodes (apart from *leaf\\nnodes) have n children.\\nBreadth-first search\\nA *blind, *exhaustive search method that visits all *nodes at a given depth\\nbefore moving on to the next depth in the *tree.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 665, 'page_label': '666'}, page_content='Glossary 639\\nBrute-force search\\nA *search method that examines every *path in a *search tree until it finds\\na goal. See *exhaustive search.\\nBucket-brigade algorithm\\nA method used by *classifier systems that assigns blame and credit to indi-\\nvidual components within the system.\\nBuilding-block hypothesis\\nA consequence of the *schema theorem, which can be stated as: “*Genetic\\nalgorithms manipulate short, low-order, high-fitness *schemata in order to\\nfind optimal solutions to problems.”\\nC\\nCandidate elimination\\nA *learning method that uses *version spaces to learn to classify data. The\\nmethod uses two sets of hypotheses, which start out as the most general\\npossible hypothesis and the most specific hypothesis. On successive itera-\\ntions these hypotheses converge until a match is found.\\nCanny edge detector\\nAn *edge detection method based on *convolution.\\nCase-based planning\\nA case-based planning system stores the *plans it formulates for solving\\nproblems and is able to reuse whole plans or parts of plans to solve similar\\nproblems in the future. Case-based planners use *case-based reasoning to\\nsolve problems.\\nCase-based reasoning\\nSee *case-based planning.\\nCausal link\\nIn *partial order planning, a causal link is a link between an action and one\\nor more conditions, which shows that that action causes the conditions to\\nbecome true. See *protected link.\\nCellular automaton\\nA set of cells that live or die according to a set of rules. A cellular automaton\\nof sufficient complexity can reproduce itself. See *Conway’s Life.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 666, 'page_label': '667'}, page_content='640 Glossary\\nCenter of gravity\\nSee *centroid.\\nCentroid\\nThe point in a two-dimensional shape that has equal area on all sides of it.\\nThis is the *center of gravity of the shape.\\nCertainty factor\\nA representation of the degree of belief in a hypothesis. Used by *MYCIN.\\nChart parser\\nAn efficient method of *parsing natural language sentences that uses a\\nchart to store information about the sentence being parsed.\\nChinese room\\nA thought experiment that is used to claim that a computer is not capable\\nof thought in the same way that a human is. The experiment consists of a\\nroom with a person inside it who does not speak or understand any Chi-\\nnese. This person has a set of symbols and a set of rules for how to manip-\\nulate the symbols. A question in Chinese is passed into the room, and the\\nhuman uses the rules to construct an answer in Chinese. Although the\\nhuman clearly does not understand Chinese, the room as a whole appears\\nto have understood the question and given a sensible answer, thus display-\\ning the kinds of behavior that a computer might display when answering\\nquestions using Artificial Intelligence.\\nChomsky’ s hierarchy\\nA hierarchy of *grammars invented by Noam Chomsky, which includes\\n*regular grammars, *context-free grammars, *context-sensitive grammars,\\nand *recursively enumerable grammars.\\nChromosome\\nA representation of a single solution to a problem as used by a *genetic\\nalgorithm. A chromosome is also a structure contained in biological cells\\nthat contains genetic information.\\nChronological backtracking\\nA method of backtracking (used by depth-first search) that backtracks to\\nthe next available *path that has not yet been taken. This contrasts with\\n*nonchronological backtracking, which can often be more efficient.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 667, 'page_label': '668'}, page_content='Glossary 641\\nCircumscription\\nA form of *nonmonotonic reasoning that is designed to deal with situa-\\ntions in which not all facts are either stated or denied. See *closed-world\\nassumption.\\nClass\\nA group of objects that is defined by some shared property. For example,\\nwe might consider the class of humans or the class of things with three\\nsides. An *object is an instantiation of a class.\\nClass frame\\nA *frame within a *frame system that represents a *class.\\nClassical logic\\nThe logical system based on that proposed by Aristotle. This system contrasts\\nwith nonclassical logics such as *nonmonotonic logics and *modal logics.\\nClassifier system\\nAn *expert system that uses *genetic algorithms and a *bucket-brigade\\nalgorithm to improve its ability to solve problems.\\nClause\\nA *sentence in *conjunctive normal form consists of a *conjunction of\\nclauses, where each clause is of the following form:\\nB\\n1 ∨ B2 ∨ B3 ∨ ... ∨ Bn\\nCLIPS (C Language Integrated Production System)\\nAn *expert system shell.\\nCloning\\nA reproductive method in *genetic algorithms that does not use\\n*crossover. A cloned offspring is an exact replica of its parent, apart from\\nany effects of *mutation.\\nClosed-world assumption\\nThe assumption that any fact not specifically known to be true must be\\nfalse. Also known as *negation by failure. The closed-world assumption is\\nused by *PROLOG.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 668, 'page_label': '669'}, page_content='642 Glossary\\nCoevolution\\nThe process whereby the evolution of two species is tightly connected. Usu-\\nally this applies to a predator species and a prey species. As the predator\\nspecies becomes better at catching the prey, the prey must evolve tech-\\nniques to enable it to escape. In turn this causes the predator to evolve new\\nabilities. Coevolution can often cause species to evolve much faster than\\nthey otherwise would and to reach levels of sophistication that would not\\notherwise be possible. This was used successfully by Danny Hillis in devel-\\noping his ramps to solve problems.\\nCognitive psychology\\nA branch of psychology that studies the way in which the human brain\\nprocesses knowledge or data to solve problems.\\nCollaborative agent\\nAn *agent that is part of a *multiagent system and that cooperates with\\nother agents to achieve a common goal.\\nCollaborative filtering\\nA method used to determine an individual’s likes or dislikes by comparing\\nhis or her past behavior with that of other individuals.\\nCombinatorial explosion\\nThe problem encountered when computers attempt to solve problems\\nwhose complexity grows *exponentially.\\nCombinatorial problem\\nA problem that involves assigning values to a number of variables in order\\nto find some optimal solution. The eight-queens problem is an example of\\na combinatorial problem.\\nCommutativity\\nA property of mathematical and logical operators. Operator o is commuta-\\ntive if a o b\\n≡ b o a.\\nCompetitive learning\\nA form of *unsupervised learning used by *Kohonen maps.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 669, 'page_label': '670'}, page_content='Glossary 643\\nCompleteness\\nA property of *search methods. A search method is complete if it guaran-\\ntees that it will find a solution if one exists. Completeness is also a property\\nof logical systems: A logical system is complete if every *valid statement in\\nthe logic can be proved by applying the rules of deduction to the axioms.\\nBoth *propositional logic and *first-order predicate logic are complete. See\\n*soundness.\\nComplete path\\nA *path in a *tree that leads from the *root node to a *goal node.\\nComposition\\nAn operator that can be combined to two *substitutions to produce a new\\n*substitution that is the same as applying the two original substitutions\\nconsecutively.\\nComputation tree logic (CTL)\\nA form of *temporal logic that uses a *tree to represent time.\\nConcave edge\\nAn edge in a two-dimensional line drawing that is between two faces that\\nare at an angle of less than 180/H11034from each other.\\nConcept learning\\nConcept learning involves learning to map from a set of input variables to a\\nBoolean value. Concept-learning systems can thus learn to determine\\nwhether or not an object meets a particular criterion based on a number of\\nthat object’s attributes.\\nConditional planning\\nA *planning method that does not start with complete information about\\nthe problem, so it allows for several possible results of each action.\\nConditional probability\\nThe *probability that one fact will be true given that another fact is known\\nto be true. This is written P(A|B), which is read “the probability of A given\\nB” . See *posterior probability, *prior probability.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 670, 'page_label': '671'}, page_content='644 Glossary\\nConditional probability table\\nA table that shows the probabilities that one variable will be true given the\\npossible values of other variables on which it depends.\\nConflict\\nA situation that arises in multiple inheritance or in *rule-based systems in\\nwhich two contradictory pieces of data arise. For example, two *rules in a\\nrule-based system fire that recommend contradictory actions.\\nConflict resolution\\nThe methods used in a *rule-based system to decide which *rule to use\\nwhen a *conflict occurs. See *expert system, *metarule.\\nConjunction\\nThe conjunction of two logical variables is the logical and (\\n∧) of those two\\nvariables, written A ∧ B. A ∧ B is true if and only if A and B are both true.\\nSee *disjunction.\\nConjunctive normal form\\nAn expression is in conjunctive normal form if it consists of a *conjunction\\nof a set of *clauses. See *disjunctive normal form.\\nConsequent\\nThe part of a rule that comes after the implication. The consequent of a\\nrule in an *expert system represents the diagnosis or recommended action\\nthat is the consequence of the *antecedent.\\nConstant\\nA symbol in *first-order predicate calculus that names a specific object. A\\nconstant cannot be quantified as a variable can.\\nConstraint\\nA rule that dictates limitations on the possible values variables can take in\\nsolving a problem.\\nConstraint satisfaction problem\\nA problem in which a set of *constraints dictate possible values for vari-\\nables. The eight-queens problem is an example of a constraint satisfaction\\nproblem. In this case, the constraint is that no two queens can be on the\\nsame row, column, or diagonal.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 671, 'page_label': '672'}, page_content='Glossary 645\\nContext-free grammar\\nA *grammar with *rewrite rules that can have at most one *terminal sym-\\nbol on the right-hand side. This type of grammar does not specify how\\nwords should agree with each other in case, number, or gender. See *con-\\ntext-sensitive grammar, *Chomsky’s hierarchy.\\nContext-sensitive grammar\\nA *grammar with *rewrite rules that can have more than one *terminal\\nsymbol on the right-hand side, and which can therefore specify rules con-\\ncerning the agreement of case, number, and gender. Context-sensitive\\ngrammars are often used for *natural language processing. See *context-\\nfree grammar, *Chomsky’s hierarchy.\\nContingent\\nA logical statement whose *truth value is not fixed, but varies depending\\non circumstances, is contingent. For example, A\\n∧ B is true if and only if\\nboth A and B are true. See *noncontingent, *interpretation.\\nContradiction\\nIf a logical system has two facts that disagree with each other, there is a con-\\ntradiction. For example, it would be a contradiction, in *classical logic, to\\nbelieve both A and\\n¬A.\\nConvex edge\\nAn edge between two faces that are at an angle of more than 180 /H11034from\\neach other.\\nConvolution\\nA mathematical operator used in *edge detection. The convolution of two\\ndiscrete functions f(a, b) and g(a, b) is defined as follows:\\nThe convolution of continuous functionsf(a, b) and g(a, b) is defined as fol-\\nlows:\\nApplying convolution to an image is one way to *smooth an image.\\nfab g ab fab g a u b v d u d v,, , ,( ) ∗ ( ) = ( ) −−( )\\n−∞\\n∞\\n−∞\\n∞\\n∫∫\\nfab g ab fab g a u b v\\nvu\\n,, , ,( ) ∗ ( ) = ( ) −−( )\\n=−∞\\n∞\\n=−∞\\n∞\\n∑∑'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 672, 'page_label': '673'}, page_content='646 Glossary\\nConway’ s Life\\nA two-dimensional *cellular automaton that consists of a grid of cells. Each\\ncell can be either alive or dead, and rules are used to determine from one\\ngeneration to the next which cells will live, which will die, and which will\\ncome to life.\\nCo-occurrence matrix\\nA matrix used in *image recognition. The co-occurrence matrix, C,i s\\ndefined as follows:\\nC = D + D\\nT\\nWhere DT is the transposition of the matrix D.\\nCopycat architecture\\nA system designed to solve analogy problems such as “ ABC is to CBA as\\nDEF is to ???.”\\nCorpus\\nA body of text, usually used in *information retrieval problems.\\nCredit assignment\\nThe technique used to decide which parts of a system contributed to its\\nsuccess. This method is used by *classifier systems and other systems that\\nuse a *bucket-brigade algorithm. See *reinforcement learning, *winner\\ntakes all algorithm.\\nCrisp set\\nAn ordinary, nonfuzzy set. Each item in the world either is or is not a mem-\\nber of a given crisp set. There is no idea of “degree of membership” of a\\ncrisp set. See *fuzzy set.\\nCrossover\\nAn operator used in *genetic algorithms that combines genetic informa-\\ntion from two *chromosomes to produce one or two offspring.\\nCYC\\nA *frame-based knowledge representation system that uses a database of mil-\\nlions of facts and *rules to make common-sense deductions about the world.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 673, 'page_label': '674'}, page_content='Glossary 647\\nCycle\\nA *path through a *semantic net or other *graph that visits the same *node\\ntwice. A *tree is a net that does not have any cycles.\\nD\\nData-driven search\\nA *search method that works from a start *state toward a goal state. See\\n*goal-driven search.\\nDeception\\nA problem that arises in *genetic algorithms, due to the use of building blocks.\\nDeception can be avoided by using *inversion or *messy genetic algorithms.\\nDecidability\\nA logical system is decidable if it is possible to produce an algorithm that\\nwill determine whether any *well-formed formula is a *theorem. In other\\nwords, if a logical system is decidable, then a computer can be used to\\ndetermine whether logical expressions in that system are *valid or not.\\nDecision tree\\nA *tree in which each *node represents a question and the answers to the\\nquestion determine which *path is to be followed from that *node. *Leaf\\nnodes represent classifications determined by the decision tree. See *ID3,\\n*decision-tree induction.\\nDecision-tree induction\\nA method that learns to classify data by building a *decision tree based on\\nthe *training data.\\nDeduction\\nA process that applies a set of inference rules to a set of assumptions to lead\\nlogically to a conclusion. We write\\n{A\\n1, A2,..., An} ⊢C\\nwhere A1, A2,..., An are the assumptions, and C is the conclusion that can\\nbe deduced from them.\\nDefault reasoning\\nA form of *nonmonotonic reasoning that uses default rules to assume that\\ncertain facts are true unless there is evidence to contradict them. See\\n*closed-world assumption.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 674, 'page_label': '675'}, page_content='648 Glossary\\nDefault value\\nThe value that is assigned to a *slot in a *frame-based system unless it is\\noverridden.\\nDefining length\\nThe defining length of a *genetic algorithm *schema is defined as the dis-\\ntance between the first and last defined bits (bits that are not) in the schema.\\nDefuzzification\\nThe process whereby a crisp value can be obtained from the *fuzzy sets\\nderived by a *fuzzy system.\\nDemon\\nA *procedure in a *frame-based system that is run automatically when the\\nvalue in a particular *slot is changed.\\nDeMorgan’ s laws\\nA pair of complementary logical rules that can be expressed as follows:\\nA ∧ B ≡¬ (¬A ∨¬ B)\\nA ∨ B ≡¬ (¬A ∧¬ B)\\nDempster–Shafer theory\\nA method that is used to reason about degrees of belief in a theory.\\nDepth-first search\\nA *blind search method that follows one *path to its first *leaf node before\\n*backtracking chronologically to the next deepest choice. See *breadth-\\nfirst search.\\nDepth threshold\\nA limit that is applied in *depth-first search that cuts search off at a speci-\\nfied depth. This avoids the problem that occurs when a *tree has a *path\\nthat is of infinite length, meaning that the search might never find a *goal.\\nDerivation tree\\nA *parse tree that is built from the *top down.\\nDescendant\\nA descendant, d,o fa  * n o d e ,n, in a *tree is a node that is further down a\\n*path in the tree than n. n is an *ancestor of a.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 675, 'page_label': '676'}, page_content='Glossary 649\\nDescribe and match\\nA method that uses a *decision tree to identify an object, by asking ques-\\ntions about the object.\\nDiagnosis\\nA process of explaining the cause of some observed phenomenon. Often\\nused in medicine to explain the cause of a patient’s symptoms.\\nDirected graph\\nA graph in which directions are attached to the edges between *nodes,\\nmeaning that if one edge exists between two nodes, it is only possible to\\ntravel in one direction between those nodes, but if two edges exist between\\ntwo nodes, it is possible to travel in both directions between those nodes.\\nDiscontinuity\\nA line in a two-dimensional line drawing that has one plane on one side\\nand another plane on its other side. Discontinuities can be caused by two\\nfaces meeting, by perception of depth, by lighting, shadows, or *texture.\\nDisjunction\\nThe disjunction of two logical variables is the logical or (\\n∨) of those two vari-\\nables, writtenA ∨ B. A ∨ B is true if eitherA or B is true. See *conjunction.\\nDisjunctive normal form\\nAn expression is in disjunctive normal form if it consists of a *disjunction\\nof a set of *clauses. See *conjunctive normal form.\\nDiversity\\nA measure of the difference between chromosomes in a population.\\nDomain expert\\nA human expert who provides domain knowledge to an *expert system.\\nFor example, a medical expert system would have input from a number of\\ndomain experts, most of whom would probably be doctors.\\nDualism\\nThe philosophical idea that mind and matter are the two distinct con-\\nstituents of the universe.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 676, 'page_label': '677'}, page_content='650 Glossary\\nDynamic planning\\n*Planning methods that take account of unforeseen circumstances by\\nallowing the execution of the plan to change in reaction to events and\\nchanges in the environment.\\nE\\nEdge\\nA line in a *graph that directly connects two *nodes. In *image recognition,\\nan edge is a perceived line that exists between two areas of different depth,\\n*texture, orientation, or color.\\nEdge detection\\nA method in *image recognition that locates the *edges in an image,\\noften by looking for areas of high frequency, which indicate a change in\\ncolor or *texture.\\nEffect axiom\\nIn *situation calculus, a rule that describes the effect of an action.\\nEffective branching factor\\nIf a search method expands n *nodes of a *search tree when solving a par-\\nticular problem, then the effective *branching factor ( b) of the *search is\\nthe branching factor of a *uniform tree that contains n nodes.\\nEmergent behavior\\nComplex behavior that emerges from an apparently simple system. Emer-\\ngent behavior usually involves a system developing some useful behavior\\nthat was not built in by its designer.\\nEntropy\\nThe extent to which a system is disordered. See *information gain, *ID3.\\nEpoch\\nA complete iteration of the training cycle of a *perceptron.\\nEquivalence\\nTwo logical expressions that must always have the same *truth value for all\\ninterpretations are equivalent. This is written A\\n≡ B. For example, A ∧ B ≡\\nB ∧ A because whatever truth values are assigned to A and B, A ∧ B must\\nhave the same truth value as B ∧ A.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 677, 'page_label': '678'}, page_content='Glossary 651\\nError gradient\\nA measure that *neural networks employ in *backpropagation. The error\\ngradient for an output node k is defined as the *error value for this node\\nmultiplied by the derivative of the *activation function:\\nxk is the weighted sum of the input values to the node k.\\nError value\\nThe difference between the expected output of a *node in a *neural net-\\nwork and the actual output.\\nEvent calculus\\nA method for reasoning about entities that vary over time. See *situa-\\ntion calculus.\\nEvolution\\nThe biologic process by which species change over a number of genera-\\ntions. Evolution is modeled in many *Artificial Life methods, particularly\\nin *genetic algorithms.\\nEvolutionary programming\\nA method that evolves *finite state automata to find a solution to the prob-\\nlem of determining the next symbol in a finite sequence of symbols, a\\n1, a2,\\na3, a4, a5,..., an. See *genetic algorithm, *Artificial Life.\\nExcluded middle, law of\\nA law from Aristotelian logic that says that it is not possible to assert both A\\nand ¬A. Similarly, it states that either A must be true, or ¬A is true. These\\ncan be written as the two logically *equivalent statements:\\n¬(A ∧¬ A)\\nA ∨¬ A\\nSee *fuzzy logic, *classical logic.\\nExecution\\nThe process of carrying out the steps determined by a *planner for solving\\na problem.\\nδk\\nk\\nk\\nk\\ny\\nx e= ∂\\n∂ ⋅'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 678, 'page_label': '679'}, page_content='652 Glossary\\nExecution monitoring\\nA method that is used during the *execution of a *plan to ensure that the\\nplan is still a sensible solution to the problem, by checking that the *pre-\\nconditions of the planned actions still hold.\\nExhaustive search\\nSee *brute-force search.\\nExistential quantifier\\nThe existential quantifier \\n∃ is read as “there exists, ” and is used to refer to\\nsome variable of which at least one must exist, but which is not explicitly\\ndefined. For example, we can write \\n∃x (P(x)), which can be read as “there\\nexists an x for which P(x) is true. ” See *universal quantifier, *first-order\\npredicate calculus.\\nExpectiminimax\\nA version of the *minimax algorithm that works with games of chance, by\\ntaking into account the *probability that each *path through the game tree\\nwill be taken.\\nExpert system\\nA system, usually built using a set of *rules, that uses expert knowledge to\\nsolve problems and explain phenomena such as symptoms. See *expert sys-\\ntem shell, *production system.\\nExpert system shell\\nA toolkit that can be used to build *expert systems. *CLIPS is an example of\\nan expert system shell.\\nExponential growth\\nA function grows exponentially if its output grows as a function of some\\nvalue raised to the power of its input. For example,f(x) = 2\\nx is an exponen-\\ntial function. A problem whose complexity grows exponentially as the\\nproblem grows is usually very hard to solve. See *NP-Complete, *combina-\\ntorial explosion.\\nF\\nFace recognition\\nMethods used to identify an individual by examining his or her face.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 679, 'page_label': '680'}, page_content='Glossary 653\\nFact\\nA *clause in *PROLOG that has no negative *literals and thus has nothing\\non the right-hand side of the implication, as in the following example:\\nA :-\\nFailure node\\nAn *or-node in a *goal tree that is also a *leaf node and is thus impossi-\\nble to solve.\\nFalse negative\\nIn *information retrieval, a result that is classified as not being of interest\\nbut which in fact is interesting is a false negative. The fewer false negatives\\nan information retrieval system gives, the higher its *recall. See *false posi-\\ntive, *precision.\\nFalse positive\\nIn *information retrieval, a result that is classified as being of interest but\\nwhich in fact is not interesting is a false positive. The fewer false positives an\\ninformation retrieval system gives, the higher its *precision. See *false neg-\\native, *recall.\\nFalsum\\nThe symbol ⊥ is called falsum, which is used to indicate an absurdity, or a\\n*contradiction.\\nFeasible region\\nThe part of a *search space that contains possible solutions to the problem.\\nFeed-forward network\\nA *multilayer neural network with an input layer, an output layer, and one\\nor more hidden layers.\\nFilter\\nA function that, when applied to an image, removes all undesired parts of the\\nimage. For example, a filter might remove all non-edge regions from an image.\\nFinite state automaton (FSA)\\nA finite state automaton is a simple device that has a finite set of states and\\nan input string (often thought of as being on a tape, running through a\\ndevice that can read one symbol at a time). Each symbol that the finite state'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 680, 'page_label': '681'}, page_content='654 Glossary\\nautomaton reads in is compared with a rule that dictates to which state to\\nmove from that state, with that input. After reading the entire input, the\\nfinite state machine is either in an accepting state, which means its answer\\nis “yes” to some question, or it is in some other state, in which case the\\nanswer is “no. ”\\nFirst-order predicate logic\\nA logical system in which *quantifiers can be applied to terms but not to\\nfunctions or predicates. See *propositional calculus, *propositional logic,\\n*classical logic, *monotonic logic, *nonmonotonic logic.\\nFitness\\nA *metric that is used to measure how successful a given *chromosome is at\\nsolving the *genetic algorithm’s problem. It is usually the case that a chro-\\nmosome with higher fitness will produce more offspring or have a greater\\nchance of reproducing than a chromosome with lower fitness.\\nFluent\\nA function that varies with time.\\nFoothill\\nA *local maximum.\\nForgetting factor\\nA value used in *Hebbian learning systems to reduce the *weights of *nodes.\\nFormal language\\nA language such as PROLOG or C++, as compared with a *natural language.\\nForward chaining\\nSee *data-driven search.\\nForward pruning\\nA method used by game-playing systems that involves cutting off examina-\\ntion of the *game tree at a point where the position has become unaccept-\\nably poor for the computer.\\nFrame\\nIn a *frame system, a frame defines either a *class or an *instance of a class\\nand contains one or more *slots that hold values for attributes of that class\\nor instance.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 681, 'page_label': '682'}, page_content='Glossary 655\\nFrame axiom\\nA rule that states what aspects of the world do not change when an action\\ntakes place. See *effect axiom, *frame problem.\\nFrame problem\\nThe problem that it is usually easy to determine the effects of an action but\\noften very difficult to work out what does not change as a result of the\\naction. See *frame axiom, *ramification problem.\\nFrame system\\nA *semantic network that consists of a set of *frames, connected together\\nby relations.\\nFree variable\\nA free variable in a logical expression is one that has not been quantified\\nwithin the same scope. For example, in the expression ∀x(x → y), y is free\\nbecause it is not quantified by the ∀ *quantifier. See *bound variable.\\nFundamental memory\\nOne of the stable values of a *recurrent network.\\nFuzzification\\nThe process of converting a crisp input value into a fuzzy value.\\nFuzzy expert system\\nAn *expert system that uses *fuzzy logic rules.\\nFuzzy inference\\nThe process by which a fuzzy system applies fuzzy rules to a set of crisp\\ninput values to derive a single crisp value. See *Mamdani inference.\\nFuzzy logic\\nAn alternative to *classical logic in which the *law of the excluded middle\\ndoes not apply and in which logical variables can take on any real number\\nvalue between 0 and 1. See *multivalent logic.\\nFuzzy reasoning\\nThe process of reasoning using fuzzy logic.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 682, 'page_label': '683'}, page_content='656 Glossary\\nFuzzy rule\\nA rule used by a *fuzzy expert system that takes the form\\nIF A op x then B = y\\nwhere op is some mathematical operator (such as “=, ” “>, ” or “<”).\\nFuzzy set\\nA set with a membership function that determines the extent to which any\\nitem is a member. See *crisp set.\\nG\\nGame tree\\nA *tree that represents the moves in a game. The *root node represents the\\nstate before any moves have been made, the *nodes in the tree represent\\npossible states of the game (or positions), and *edges in the tree represent\\nmoves in the game.\\nGene\\nA single unit of *genetic algorithm contained within a *chromosome.\\nGeneral problem solver (GPS)\\nA computer program invented in the 1950s that uses *means–ends analysis\\nto solve logical problems.\\nGeneralization\\nThe act of moving from an *instance of a *class to the class itself. Also\\nknown as the “is–a” relationship. For example, one can generalize from\\nRonald Reagan to the class of presidents of the United States.\\nGenerate and test\\nA *search method that involves examining every *node in the *search space\\nuntil one is found that matches some criteria that describe the *goal state.\\nGenetic algorithm\\nA system that uses methods modeled on natural *evolution to solve com-\\nplex problems. See *chromosome, *gene.\\nGenetic programming\\nA method that evolves *LISP programs or *S-expressions. The method\\nsearches through the *search space of possible S-expressions until it finds\\none that best solves the current problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 683, 'page_label': '684'}, page_content='Glossary 657\\nGenotype\\nThe genetic information represented by a set of *genes that make up an\\nindividual person or other biologic creature. See *phenotype.\\nGeometric progression\\nA sequence of numbers where each number in the progression is obtained\\nby multiplying the previous number by some constant.\\nGlobal maximum\\nThe best possible solution in a *search space. When the search space is rep-\\nresented as a curved surface, the global maximum is the highest peak in the\\nsurface. See *foothill, *plateau, *ridge.\\nGoal\\nThe solution to a problem. See *goal node.\\nGoal-based agent\\nAn *agent that uses *search or *planning to achieve some *goal.\\nGoal-driven search\\nA search method that works from a goal state toward the start state. See\\n*data-driven search.\\nGoal node\\nThe *node (of which there may be more than one) in a *search tree that\\nrepresents the solution of the problem that is being solved. The aim of all\\n*search methods is to find a goal node. See *goal.\\nGoal reduction\\nSee *problem reduction, *goal tree.\\nGoal tree\\nA *tree that is used to represent the way in which a problem can be broken\\ndown into subgoals. Each *node in the tree represents a subgoal. See\\n*and–or tree, *and-node, *or-node, *success node, *failure node.\\nGödel implication\\nA form of logical *implication that is used in *fuzzy logic. It is defined\\nas follows:\\nA → B\\n≡ (A ≤ B) ∨ B'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 684, 'page_label': '685'}, page_content='658 Glossary\\nGradient descent\\nA method used in training *backpropagation neural networks, which\\ninvolves descending the steepest part of the *error gradient.\\nGrammar\\nA set of rules that define the syntax and structure of a language. See\\n*Backus–Naur form, *context-sensitive grammar, *context-free grammar.\\nGraph\\nA data structure that consists of *nodes connected by *edges. See *tree,\\n*semantic net, *cycle.\\nGraphPlan\\nA *planning method that uses *planning graphs to solve problems that are\\nexpressed in *STRIPS notation.\\nGround instance\\nA ground instance of a *clause is a version of that clause in which any variables\\nit contains have been replaced by *ground terms from the *Herbrand universe.\\nGround term\\nA *constant or *function that does not contain any *variables.\\nH\\nHalting Problem\\nThe problem of determining whether a given computer program will ever halt.\\nIt can be proved that no computer program can ever be written that can solve\\nthe Halting problem. The proof is as follows: Imagine a program H, which\\nwhen given an argumentP, which is another program, determines whetherP\\nhalts or not. IfH determines thatP does halt, thenH enters an infinite loop and\\ntherefore never halts. IfH determines thatP does not halt, then it reports a pos-\\nitive response and exits. Now further imagine thatH is applied to itself. Let us\\nsuppose thatH determines thatH does halt. In that case, it will enter an infinite\\nloop and never halt. Hence, its assessment was wrong. Similarly, if it decides\\nthat H does not halt, then it halts, disproving its own assessment again.\\nHamming distance\\nThe Hamming distance measures the number of elements of two vectors\\nthat differ. The Hamming distance between two vectors,X and Y, is written\\n||X, Y||. For example,||(1,0,0,1),(1,0,1,1)|| = 1 because there is only one ele-\\nment of the vectors that differs.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 685, 'page_label': '686'}, page_content='Glossary 659\\nHEARSAY II\\nA system that used a *blackboard architecture and an index of computer\\nscience papers to answer spoken questions on the subject.\\nHebbian learning\\nA method of *unsupervised learning used in *neural networks. Hebbian\\nlearning is based on the idea that if two *neurons in a neural network are\\nconnected together, and they fire at the same time when a particular input\\nis given to the network, then the connection between those two neurons\\nshould be strengthened.\\nHedge\\nA fuzzy set qualifier, such as very, quite, extremely,o r  somewhat.\\nHerbrand base\\nThe Herbrand base of a set of *clauses, S, is defined as the set of\\n*ground atoms that can be obtained by replacing variables in S by\\nmembers of the *Herbrand universe for S. The Herbrand base of S is\\nwritten H\\nS(S).\\nHerbrand interpretation\\nA Herbrand Interpretation for a set of *clauses, S, is defined as a set of\\nassignments of true and false to the elements of the *Herbrand base,HS(S).\\nHerbrand universe\\nFor a set of *clauses, S, the Herbrand universe, HS, is defined as being the\\nset of constants that are contained within S and the set of functions in S\\napplied to those constants. See *ground term.\\nHeteroassociative memory\\nA type of memory that can associate one object or piece of data with another.\\nHeuristic\\nA rule or piece of information that is used to make *search or another\\nproblem-solving method more effective or more efficient.\\nHeuristic evaluation function\\nA function that when applied to a *node gives a value that represents a\\ngood estimate of the distance of the node from the *goal.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 686, 'page_label': '687'}, page_content='660 Glossary\\nHeuristic repair\\nA method of solving *combinatorial problems that involves generating a\\nrandom solution to the problem and iterating toward a better solution by\\nmaking simple changes that reduce the number of errors.\\nHidden layer\\nA layer of *neurons within a *neural network that is between an input layer\\nand an output layer. The hidden layer usually carries out the calculations of\\nthe network.\\nHill climbing\\nAn *informed search method that acts by always moving toward a better\\nsolution one step at a time, ensuring that every step improves the current\\nstate. Hill climbing is very susceptible to problems such as *ridges,\\n*foothills, and *plateaus.\\nHopfield network\\nA *recurrent neural network that usually uses the sign activation function:\\nHorizon problem\\nThis problem involves an extremely long sequence of moves in a game that\\nclearly lead to a strong advantage for one player, but where the sequence of\\nmoves, although potentially obvious to a human player, takes more moves\\nthan can be examined by a computer using *bounded lookahead. Hence,\\nthe significant end of the sequence has been pushed over the horizon.\\nHorn clause\\nA *clause that has at most one positive *literal.\\nHuman language\\nOne of the many hundreds of languages spoken and written by human\\nbeings around the world, such as English, Japanese, Russian, Swahili, and\\nFrench. See *formal language, *natural language.\\nHybrid agent\\nAn *agent that exhibits properties of more than one agent type.\\nSign X for x\\nfor x( ) = +>\\n−<\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n10\\n10'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 687, 'page_label': '688'}, page_content='Glossary 661\\nHypothesis\\nA possible explanation for a set of observed phenomena.\\nI\\nID3\\nA *decision tree induction algorithm that builds a decision tree from the\\ntop down. The nodes in the decision tree are selected by choosing features\\nof the *training data set that provide the most information about the data\\nand turning those features into questions.\\nImage capture\\nThe process of obtaining an image from a real-world scene so that some\\nkind of image processing can be carried out on the image.\\nImage recognition\\nThe process of identifying features and making informed decisions about a\\nscene by examining an image of the scene. See *edge detection, *segmentation.\\nImplication\\nA logical operator that is defined by the following *truth table:\\nABA → B\\nfalse false true\\nfalse true true\\ntrue false false\\ntrue true true\\nIndependence\\nIf the value of one variable does not in any way affect the value of another,\\nthen the two variables are said to be independent. Hence, for example, the\\ncolor of the sky is independent of my name, but the color of the sky is not\\nindependent of the time of day.\\nInductive bias\\nRestrictions imposed on a *learning method by its assumptions.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 688, 'page_label': '689'}, page_content='662 Glossary\\nInductive reasoning\\nReasoning about what will happen in the future based on evidence from\\nthe past. See *deduction, *abduction.\\nInference engine\\nThe part of an *expert system that controls the process of deriving conclu-\\nsions and recommended actions from a set of rules and facts.\\nInference rule\\nA rule that is used in the process of logical *deduction.\\nInformation agent\\nAn *agent that gathers information on behalf of a human user. Usually\\ninformation agents are used to gather information from the Internet and so\\nare also called *internet agents.\\nInformation gain\\nThe reduction in *entropy caused by some change in a system.\\nInformation retrieval\\nInformation retrieval involves matching the text contained in a query or a\\ndocument to a set of other documents. Often, the task involves finding the\\ndocuments from a *corpus of documents that are relevant to a user’s query.\\nInformed search method\\nA *search method that uses a *heuristic to ensure that it searches the\\n*search space as efficiently as it can.\\nInheritance\\nThe way in which one *class or *object can derive features from a *super-\\nclass from which it is itself derived.\\nInitial state\\nThe *state in which a problem-solving system starts to solve its problem.\\nInstance\\nAn instance of a *class is an *object that has properties of that class.\\nInstance constructor\\nA *procedure that creates an *instance of a *class.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 689, 'page_label': '690'}, page_content='Glossary 663\\nInstance frame\\nA *frame within a *frame system that represents an *instance of a *class.\\nIntelligent agent\\n*Software agents are often referred to as intelligent agents, but an intelli-\\ngent agent is really an agent that has the particular property of intelligence.\\nThis means, for example, that it has the ability to reason independently\\nabout data. Many intelligent agents have the ability to learn. Another\\nimportant property of most intelligent agents is *autonomy.\\nInterface agent\\nAn *autonomous *intelligent agent that acts as a personal assistant for a user.\\nInternet agent\\nSee *information agent.\\nInterpretation\\nAn interpretation is a specific choice of assignment of values to a set of\\nvariables. A logical expression can be true under one interpretation and\\nfalse under another.\\nInversion\\nA *unary operator that reverses the order of a subset of the bits within a\\n*chromosome. Inversion is used to avoid the problem of *deception.\\nIrrevocability\\nA *search method is irrevocable if it does not employ *backtracking—in\\nother words, it explores just one *path through a *search tree. *Hill climb-\\ning is an example of an irrevocable search method.\\nIterated local search\\nA *local search method that is repeated iteratively using different starting\\npoints in order to avoid the problem of *local maxima.\\nJ\\nJoint probability distribution (joint)\\nThe distribution of *probabilities of a combination of two logical variables.\\nFor example, we could refer to the joint probability distribution of A ∧ B,\\nwhich would be represented by a table such as the following:'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 690, 'page_label': '691'}, page_content='664 Glossary\\nA ¬A\\nB 0.11 0.09\\n¬B 0.63 0.17\\nK\\nKnowledge base\\nThe database of *rules used by an *expert system. The knowledge base con-\\ntains the data that represent the system’s knowledge about the domain.\\nKnowledge engineer\\nThe human being who is responsible for a large part of the creation of an\\n*expert system. Specifically the knowledge engineer is responsible for\\ninputting *metaknowledge into the system.\\nKohonen map\\nAn *unsupervised learning *neural network. A Kohonen map is capable of\\nclassifying data into a set of classifications without being given the specific\\nclassifications in advance. This kind of classification is also known as auto-\\nmatic clustering.\\nL\\nLeaf node\\nA *node in a *tree that has no *successors. All *goal nodes are leaf nodes,\\nbut not all leaf nodes are goal nodes. In other words, it is possible to reach\\na leaf node at the end of a path in a tree without successfully finding a\\ngoal node.\\nLearning agent\\nAn *agent that is capable of *learning and is therefore able to acquire new\\nknowledge and skills and is able to use the new knowledge and skills to improve\\nits performance. This learning is often carried out using a *neural network.\\nLexicon\\nA dictionary of words and other linguistic units that make up a language.\\nLife, game of\\nSee *Conway’s life.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 691, 'page_label': '692'}, page_content='Glossary 665\\nLikelihood\\nThe *probability that a given piece of evidence (E) would occur, given that\\na particular hypothesis (H) were true:\\nP (E | H)\\nThis is the likelihood of E,g i v e n  H, as compared with the probability of H\\ngiven E, which would be written P(H | E).\\nLinear threshold function\\nA step function used as an *activation function by artificial *neurons. The\\nlinear threshold function gives a value of 0 for inputs below a threshold (t)\\nand a value of 1 for inputs above t.\\nLinearly separable function\\nA function that can be drawn in a two-dimensional graph such that a\\nstraight line can be drawn between the values so that inputs that are classi-\\nfied into one classification are on one side of the line, and inputs that are\\nclassified into the other are on the other side of the line. Logical-OR is a lin-\\nearly separable function, but exclusive-OR is not. *Perceptrons can only be\\nused to learn linearly separable functions.\\nLinguistic variable\\nA fuzzy variable such asheight or age that is defined not in objective numer-\\nical terms but in terms of fuzzy values such as tall, short, old, and young.\\nLISP\\n(LISt Programming). A programming language widely used in Artificial\\nIntelligence research.\\nLiteral\\nOne of the basic symbols of *propositional logic. For example, in the fol-\\nlowing expression\\n¬A ∧ (B ∨ C)\\nthe literals are ¬A, B, and C.\\nLocal maximum\\nA peak within a *search space that is higher than all the points immediately\\naround it but is lower than the global maximum, which is the highest point\\nin the entire search space and the most desirable goal. Many search meth-\\nods are prone to identifying local maxima rather than *global maxima.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 692, 'page_label': '693'}, page_content='666 Glossary\\nT echniques such as *simulated annealing and *iterated local search are\\ndesigned to avoid this problem. See *local optimization.\\nLocal optimization\\nA method of solving *combinatorial problems that involves finding a *local\\nmaximum by moving toward a better solution in the *search space. See\\n*hill climbing.\\nLocal search\\nLocal search methods work by starting from some initial configuration\\n(usually random) and making small changes to the configuration until a\\nstate is reached from which no better state can be achieved. See *meta-\\nheuristic, *local optimization, *hill climbing, *simulated annealing.\\nL-systems\\nA system that describes the way a set of artificial “trees” grow using a set of\\nsimple rules:\\nRule 1: a → ab\\nRule 2: b → a\\nM\\nMachine translation\\nThe use of computer software to translate text from one *human language\\nto another.\\nMamdani inference\\nA form of *fuzzy inference that allows a system to take in a set of *crisp\\ninput values (from a set of sensors or inputs from a human operator, for\\nexample) and apply a set of *fuzzy rules to those values in order to derive a\\nsingle, crisp, output value or action recommendation.\\nMap coloring\\nThe problem of finding a way to color the countries in a map so that no\\ntwo adjoining countries are the same color. It can be shown that any nor-\\nmal two-dimensional map can be colored using at most 5 colors.\\nMaximum a posteriori hypothesis (MAP)\\nThe *hypothesis that has the greatest *posterior probability for explaining\\nan observed piece of evidence. The MAP classification for a piece of data is\\nthe classification that is the most likely one for the data.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 693, 'page_label': '694'}, page_content='Glossary 667\\nMeans–ends analysis\\nAn approach to *planning that involves identifying the differences between\\nthe goal state and the current state, and selecting actions that aim to lessen\\nthose differences.\\nMembership function\\nA function that defines the membership of a *fuzzy set. Membership of\\nfuzzy sets is defined as a value between 0 and 1, where 1 means complete\\nmembership of the fuzzy set, and 0 means nonmembership. For example,\\nthe following membership function:\\ndefines a membership function for a fuzzy set, B. Input values above 2 are\\nnot members of B at all. The number 0 is completely a member ofB, with a\\nmembership value of 1.\\nMemory\\nA property of *recurrent neural networks that enables it to change its\\n*weights as new inputs are presented to it.\\nMental situation calculus\\nA form of *situation calculus that allows an agent to reason about the\\neffects that events have on an its beliefs about the world.\\nMessy genetic algorithm (MGA)\\nAn alternative to a standard *genetic algorithm. Each bit in a messy genetic\\nalgorithm *chromosome is represented by a pair of numbers: the first\\nnumber represents the position within the chromosome, and the second\\nnumber is the bit value (0 or 1). A chromosome in a messy genetic algo-\\nrithm can be *overspecified (a bit is defined more than once) or *under-\\nspecified (a bit is not defined at all). See *schema.\\nMetaheuristic\\nA *heuristic used by a *local search method. See *simulated annealing,\\n*tabu search, *local optimization.\\nMetaknowledge\\nKnowledge about knowledge. See *metarule.\\nMx\\nx for x\\nfor x\\nB( ) = −≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n1 2 2\\n02'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 694, 'page_label': '695'}, page_content='668 Glossary\\nMetarule\\nRules that define how *conflict resolution and other aspects of an *expert\\nsystem work.\\nMetric\\nA measure that is used to quantify the performance of a system. See *fitness.\\nMinimax\\nAn algorithm that is used by game-playing software to determine the best\\nmove to make. The algorithm assumes that it is playing against a *rational\\nopponent and uses a *static evaluator at *leaf nodes. See *alpha–beta prun-\\ning, *expectiminimax.\\nMobile agent\\nAn *agent that is capable of moving, either in the physical world or across\\nnetworks such as the Internet.\\nModal logic\\nAn extended version of a *classical logic that allows reasoning about cer-\\ntainty and possibility.\\nModal operator, M\\nA logical operator that indicates that an expression is consistent with an\\nagent’s beliefs. See *modal logic, *nonmonotonic logic.\\nModus ponens\\nA logical rule that is used in *deduction, which states that if A implies B,\\nand we know that A is true, then we can deduce that B is true:\\nMonotonicity\\n1. A search method is described as monotone if it always reaches a\\ngiven node by the shortest possible path.\\n2. A function that increases monotonically is one that never decreases\\nif its argument increases.\\n3. A logical system is described as being monotonic if a valid proof in\\nthe system cannot be made invalid by adding additional premises\\nAA B\\nB\\n→'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 695, 'page_label': '696'}, page_content='Glossary 669\\nor assumptions. In other words, if we find that we can prove a con-\\nclusion C by applying rules of deduction to a premise B with\\nassumptions A, then adding additional assumptions A/H11032and B/H11032will\\nnot stop us from being able to deduce C.\\nSee *nonmonotonic.\\nMorphologic analysis\\nAnalysis of the structure of individual words within an *utterance in a\\n*human language. See *natural language processing, *semantic analysis,\\n*syntactic analysis, *pragmatic analysis.\\nMost general hypothesis\\nThe most general hypothesis is defined as a vector such as <?, ?, ?, ?, ?, ?>,\\nwhich allows for any set of data values. See *most specific hypothesis,\\n*hypothesis.\\nMost general unifier (mgu)\\nA *unifier, u\\n1, is called a most general unifier if any other unifier, u2, can\\nbe expressed as the composition of u1 with some other substitution (i.e.,\\nu2 = u1 o u3).\\nA most general unifier is a unique unifier that provides the most general set\\nof *substitutions to unify a set of *clauses.\\nMost specific hypothesis\\nThe most specific hypothesis is defined as a vector such as <∅, ∅, ∅, ∅, ∅,\\n∅>, which does not allow for any set of data values. See *most general\\nhypothesis, *hypothesis.\\nMotion field\\nThe vectors that define the apparent motion in a still photograph. See\\n*optical flow.\\nMultiagent system\\nA system that uses a number of *agents to solve a single problem. See\\n*agent team, *collaboration, *intelligent agent.\\nMultilayer neural network\\nAn artificial *neural network that has more than one layer of *neurons. See\\n*perceptron, *Hebbian learning, *Kohonen map, *Hopfield network.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 696, 'page_label': '697'}, page_content='670 Glossary\\nMultiple inheritance\\nThe *inheritance of properties from more than one *frame or *class.\\nMultivalent logic\\nA logical system that has more than two logical values. See *bivalent logic,\\n*fuzzy logic.\\nMutation\\nA *unary operator that flips a single bit within a *chromosome from zero\\nto one or from one to zero. See *crossover, *genetic algorithm.\\nMutual exclusion (mutex)\\nIn a *planning graph, a mutex exists between two actions or effects that are\\nmutually exclusive—in other words, they cannot both exist at the same\\ntime. Hence, if a mutex exists between two actions and one action is taken,\\nthen the other action cannot be taken.\\nMYCIN\\nA medical *expert system that uses *certainty factors to diagnose symptoms.\\nN\\nN-gram\\nA grouping of n letters (Some examples of trigrams that occur commonly\\nin English are ing, the, ant, and ize). N-grams are used to identify a language\\nfrom a piece of text, using the *acquaintance algorithm.\\nNaïve Bayes classifier\\nA system that uses *Bayes’ theorem to learn to classify data.\\nNatural language\\nSee *human language.\\nNatural language processing (NLP)\\nThe analysis of the *syntax, *semantics, *morphology, *phonology, and\\n*pragmatics of *utterances in *human languages. Natural language pro-\\ncessing is used to enable a computer system to “hear” spoken human lan-\\nguage, interpret the words, and carry out some action (such as a database\\nquery) on the basis of the words.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 697, 'page_label': '698'}, page_content='Glossary 671\\nNearest neighbor algorithm\\nAn instance-based learning method. See *Shepard’s method.\\nNearest neighbor heuristic\\nA *heuristic used to solve problems such as the traveling salesman\\nproblem, which functions by extending the *path to the nearest unvis-\\nited city.\\nNegation by failure\\nSee *closed-world assumption, *PROLOG.\\nNeural network\\nA network of simple processing *nodes (*neurons), which is roughly mod-\\neled on the human brain. See also *backpropagation, *bidirectional asso-\\nciative memory, *Hebbian learning, *activation function, *hidden layer,\\n*Hopfield network, *linear threshold function, *multilayer neural network.\\nNeuro-fuzzy system\\nA *neural network that learns to classify data using *fuzzy rules and *fuzzy sets.\\nNeuron\\nThe individual computation devices that make up the human brain. Neu-\\nrons are also the building blocks of artificial *neural networks. A neuron\\ngenerally takes in one or more inputs to which an *activation function is\\napplied. The result of this function is compared with the *activation level\\nto determine if the neuron should fire.\\nNode\\n1. A *neuron within an artificial *neural network.\\n2. The building block of *graphs, *nets, and *trees. A graph consists\\nof a set of nodes, which are connected by *edges. Each node repre-\\nsents a decision or a piece of data within the graph. See *leaf node,\\n*goal node, *and-node, *or-node.\\nNonchronological backtracking\\nA form of *backtracking that involves using additional information about\\nthe search problem to backtrack to a more helpful *node than the last one\\nin the *tree. See *chronological backtracking.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 698, 'page_label': '699'}, page_content='672 Glossary\\nNoncontingent\\nA logical statement whose *truth value is fixed and does not vary with cir-\\ncumstances is noncontingent. For example, A ∧¬ A is always false, regard-\\nless of the value of A. See *contingent, *interpretation.\\nNondirected graph\\nA *graph in which an *edge between two *nodes goes in both directions.\\nSee *directed graph.\\nNonmonotonic\\nIn a nonmonotonic logical system, a valid proof can be made invalid by\\nadding additional premises or assumptions. See *monotonic, *abduction,\\n*circumscription, *classical logic, *default reasoning, *modal operator M,\\n*truth maintenance system.\\nNonterminal symbol\\nA symbol in a *grammar that is used to represent a number of *terminal\\nsymbols or nonterminal symbols.\\nNormal distribution\\nAlso known as the Gaussian distribution or bell curve, the normal distribu-\\ntion is defined as follows:\\nNormalization\\nNormalization is the process whereby the *posterior probabilities of a pair\\nof variables are divided by the normalizing constant to ensure that they\\nsum to 1. The normalizing constant is defined as follows:\\nNoun\\nA word in a *human language that is used to define a thing, a person, a\\nplace, or an abstract thing such as “happiness. ” See *noun phrase, *verb.\\nα =\\n( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\n1\\nPA B PB PA B P B\\nPx e d t\\nt\\n−∞( ) =\\n−\\n−∞\\n∞\\n∫, 1\\n2\\n2\\n2\\nπ'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 699, 'page_label': '700'}, page_content='Glossary 673\\nNoun phrase\\nA phrase that has the same properties within a *grammar as a noun and can\\nthus be used interchangeably with a noun, at least as far as the syntactic rules\\nof the grammar are concerned. For example, the following are all noun\\nphrases: America, a big green dog, the house that I lived in when I was younger.\\nNP-complete\\nA problem that is NP can be solved nondeterministically in polynomial time.\\nThis means that if a possible solution to the problem is presented to the com-\\nputer, it will be able to determine whether it is a solution or not in polynomial\\ntime. The hardest NP problems are termed NP-complete. All NP-complete\\nproblems can be mapped directly onto the satisfiability problem.\\nO\\nOccam’ s razor\\nThe assertion that the simplest possible solution to a problem should\\nalways be selected. See *inductive bias.\\nOccluding edge\\nA depth *discontinuity within a two-dimensional line drawing.\\nOpening book\\nA database of opening moves for use in playing games such as chess and\\ncheckers.\\nOperator schema\\nA template that defines a number of operators within a *planning system.\\nOptical flow\\nThe apparent motion within a still photograph. See *motion field.\\nOptimality\\nAn optimal *search method is one that will find the solution that involves\\ntaking the least number of steps to a *goal node, if such a solution exists.\\nOptimal path\\nThe shortest *path from the *root node in a *search tree to a *goal node.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 700, 'page_label': '701'}, page_content='674 Glossary\\nOr-goal\\nA *goal that can be solved by solving any one of its *subgoals. Represented\\nin a *goal tree by an *or-node. See *problem reduction.\\nOr-node\\nA *node in a *goal tree that represents an *or-goal.\\nOverfitting\\nA problem that affects *learning systems, whereby the system performs well\\nat classifying the *training data but, due to fitting its model of the data too\\nclosely to inaccurate training data, does not perform well at classifying\\nunseen data.\\nOverriding\\nThe act of assigning a new value to an inherited default value.\\nOverspecified chromosome\\nA *chromosome where one or more bits have more than one value assigned\\nto them. See *messy genetic algorithm, *underspecified chromosome.\\nP\\nParallel search\\n*Search methods that are designed to take advantage of multiple processor\\ncomputers by running parts of the search in parallel with each other.\\nParser\\nA tool that breaks down a *sentence in a *human language to its compo-\\nnent parts by matching the sentence with the structure imposed by the lan-\\nguage’s *grammar.\\nPartial order\\nA relation on a set that is reflexive, transitive, and antisymmetric. For\\nexample, ≤ defines a partial order on the set of integers. This can be proved\\nas follows:\\na ≤ a for all integers. Hence, ≤ is reflexive.\\na ≤ b\\n∧ b ≤ c → a ≤ c.H e n c e ,≤ is transitive.\\na ≤ b ∧ b ≤ a ⇔ a = b.H e n c e ,≤ is antisymmetric.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 701, 'page_label': '702'}, page_content='Glossary 675\\nPartial order planning\\nA *planning method in which the order of actions that are not dependent\\non each other is not necessarily defined.\\nPartial path\\nA *path in a *tree that leads from the *root node to a *leaf node that is not\\na *goal node.\\nPath\\nA route through a *tree or *graph. The shortest path consists of just one\\n*node. Normally, a path consists of more than one node, connected\\ntogether by one or more edges.\\nPattern matching\\nThe identification of patterns in images, text, or other data by comparing\\nthe data with a set of templates or regular expressions.\\nPerceptron\\nA simple *neuron that is used to classify input data into one of two cate-\\ngories. See *linearly separable function.\\nPhenotype\\nThe physical characteristics of a creature, as determined by the creature’s\\n*genotype and its environment.\\nPixel\\nA picture element. A single unit of light and color as displayed on a com-\\nputer screen.\\nPlan\\nA sequence of actions that has been determined by the act of *planning to\\nbe a way to solve a particular problem.\\nPlanning\\nThe act of taking a starting state and a goal state and building a *plan that\\nconsists of a sequence of actions that when carried out should lead from\\nthe start state to the goal state. See *partial order planning, *case-based\\nplanning, *atomic action, *STRIPS.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 702, 'page_label': '703'}, page_content='676 Glossary\\nPlanning Domain Definition Language (PDDL)\\nA planning language that can be used to represent problems expressed in\\n*STRIPS or *ADL.\\nPlanning graph\\nA *graph that contains a number of levels that represent states and actions\\nthat is used by algorithms such as *GraphPlan to devise plans for solving\\nproblems. See *GraphPlan, *mutual exclusion.\\nPlateau\\nA flat region in the *search space, where moving in any direction leads to\\nan area at the same height as the current height. See *hill climbing, *local\\nmaximum, *global maximum.\\nPly\\nOne level in a *game tree.\\nPopulation\\nThe complete collection of *chromosomes that a *genetic algorithm has\\ndeveloped in a given generation.\\nPossible world\\nA universe that could logically exist but is not necessarily the same as the\\none we live in now.\\nPosterior probability\\nThe *probability of a variable given that we know that another variable is\\ntrue. The posterior probability of B is written P(B | A). See *prior probabil-\\nity, *conditional probability.\\nPragmatic analysis\\nThe analysis of the real meaning of an *utterance in a human language, by\\ndisambiguating and contextualizing the utterance. See *semantic analysis,\\n*syntactic analysis, *morphologic analysis, *natural language processing.\\nPrecision\\nA measure of the success of an *information retrieval system. A system that\\ngives no *false positives has 100% precision. See *recall, *false negative.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 703, 'page_label': '704'}, page_content='Glossary 677\\nPrecondition\\nA requirement that must be met for a particular action to be carried out by\\na *planning system.\\nPredecessor\\nThe *node immediately above a given node in a *tree. Each node has\\nexactly one predecessor, except for the root node, which has no predeces-\\nsors. See *ancestor, *descendant, *successor.\\nPredicate calculus\\nSee *first-order predicate calculus.\\nPremises\\nA set of logical statements from which a conclusion is drawn using logical\\n*deduction.\\nPrenex normal form\\nAn expression that is in *conjunctive normal form and in which all *quan-\\ntifiers are at the beginning is in prenex normal form.\\nPrinciple component analysis\\nAnalysis of data by determining the features of the data that vary the most\\ngreatly from one item to another.\\nPrior probability\\nThe *probability of a variable, regardless of any other variables. The prior\\nprobability of B is written P(B). See *posterior probability, *conditional\\nprobability.\\nPrisoner’ s Dilemma\\nA two-player game based on the following scenario:\\nTwo prisoners have been arrested on suspicion of committing a crime.\\nThey are kept in separate cells, and each is told that if he betrays his friend\\nhe will receive a reward. If his friend does not betray him, then he will go\\nfree, and receive a reward, while his friend is tortured. If both betray each\\nother, they will both be tortured, and if neither betrays the other, they will\\nbe set free.\\nProbabilistic planning\\n*Planning in nondeterministic environments, in which an action will cause\\nan effect with a given probability.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 704, 'page_label': '705'}, page_content='678 Glossary\\nProbabilistic reasoning\\nReasoning about the probabilities of events or attributes.\\nProbability\\nThe probability of A is a measure of how likely it is that A will occur\\nunder ordinary circumstances. This is written P(A). See *conditional\\nprobability, *joint probability distribution, *posterior probability, *prior\\nprobability.\\nProblem reduction\\nA method of solving problems by breaking each problem down into a\\nnumber of subproblems, each of which may in turn be further broken\\ndown. By solving all of the subproblems and combining the results cor-\\nrectly, the original problem can be solved. See *goal reduction, *goal tree.\\nProcedural attachment\\nA *procedure that is associated with a *frame.\\nProcedure\\nA method that is associated with a *frame. Each procedure is a set of\\ninstructions that can be executed on request. Some procedures are executed\\nautomatically when particular events occur, such as upon creation. See\\n*demon, *procedural attachment.\\nProduct rule\\nThe rule used in *Hebbian learning to determine the extent to which the\\nweights attached to each *node are increased or decreased during learning.\\nProduction rule\\n1. A rule used by an *expert system. Each rule has the form input ->\\naction or input -> diagnosis.\\n2. A rule used to define a part of a grammar. Each rule explains how\\none *nonterminal symbol is made up of one or more terminal or\\nnonterminal symbols. See *rewrite rule.\\nProduction system\\nSee *expert system.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 705, 'page_label': '706'}, page_content='Glossary 679\\nPROLOG\\n(PROgramming in LOGic). A language that is widely used in Artificial\\nIntelligence research. PROLOG programs are based around a database of\\nfacts and rules.\\nPropositional calculus\\nThe language that is used to express the concepts of *propositional logic.\\nPropositional logic\\nA *monotonic logical system based around logical operators (such as,\\n∧, ∨,\\nand ¬) and proposition terms. See *classical logic, *first-order predicate\\ncalculus, *propositional calculus.\\nPropositional planning\\nA *planning system that models plans purely using *propositional calculus.\\nProtected link\\nA *causal link is said to be protected when it is needed to establish the *pre-\\nconditions of an operator below it in the plan that is being developed.\\nPruning\\nCutting off sections of a *search tree that are (probably) not worth examin-\\ning. See *alpha–beta pruning.\\nPure AND-OR tree\\nA *goal tree that has the following properties: the *tree has an *or-node at\\nthe top, each or-node has *and-nodes as its direct *successors, and each\\nand-node has or-nodes as its direct successors. Another condition of a pure\\nAND-OR tree is that it does not have any *constraints that affect which\\nchoices can be made. A *game tree is a pure AND-OR tree.\\nQ\\nQuantifier\\nSee *universal quantifier, *existential quantifier, *first-order predicate logic.\\nR\\nRamification problem\\nThe problem of identifying all consequences of an action including trivial\\nones or ones that are hard to foresee. See *frame problem.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 706, 'page_label': '707'}, page_content='680 Glossary\\nRationality\\nAn *agent or other computer program that behaves rationally is one that\\nacts to maximize some *utility function. In playing games, for example, a\\nrational opponent is one that is attempting to win. An irrational opponent\\nwould be one that wanted to win but did not always play its best move.\\nReactive agent\\nAn *agent that simply responds to inputs from its environment. A reactive\\nagent has a set of rules (like an *expert system) that instruct it how it\\nshould behave based on any given input from the environment.\\nRecall\\nA measure of the success of an *information retrieval system. A system that\\ngives no *false negatives has 100% recall. See *precision, *false negative.\\nRecurrent network\\nA *multilayer neural network that is able to feed information back from its\\noutputs to its inputs, and thus is able to act as a *memory.\\nRecursively enumerable\\nA class of *grammars that has no restrictions on the *rewrite rules that can\\nbe used to define the grammar. Recursively enumerable grammars are also\\nknown as unrestricted grammars. See *context-sensitive grammar, *con-\\ntext-free grammar.\\nReductio ad absurdum\\nA rule that states that if we assume that some expression E is false, and by a\\nprocess of logical deduction starting from this assumption can deduce fal-\\nsum, then E must in fact be true. See *refutation proof.\\nReflex agent\\nSee *reactive agent.\\nRefutation proof\\nA method that proves a logical *deduction is valid by first negating the con-\\nclusion and then using the resulting *clauses to deduce falsum. See *resolu-\\ntion, *reductio ad absurdum.\\nRegular expression\\nA *sentence defined by a *regular grammar.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 707, 'page_label': '708'}, page_content='Glossary 681\\nRegular grammar\\nA *grammar that defines the syntax of a *regular language.\\nRegular language\\nThe simplest *grammar from *Chomsky’s hierarchy of grammars. A simple\\nlanguage is one that can be described by a *finite state automaton.\\nReinforcement learning\\nA *learning system that uses positive reinforcement when it succeeds and\\nnegative reinforcement when it fails. See *bucket-brigade algorithm,\\n*credit assignment.\\nRelative likelihood\\nThe relative likelihood of two hypotheses, H\\n1 and H2, given evidence E is\\ndefined as follows:\\nThe relative likelihood thus tells us how much more likely one explanation\\nis than another for a piece of observed evidence.\\nRelaxed problem\\nA version of a *combinatorial problem that has fewer *constraints.\\nReplanning\\nThe process of devising a new *plan during *execution when circumstances\\nhave changed such that the existing plan is no longer suitable—for exam-\\nple, because one of the *preconditions of the next planned action is no\\nlonger satisfied.\\nRepresentation\\nA model used by a computer to represent a real-world situation or to store\\nsome data that are used in solving a problem.\\nRepresentational adequacy\\nThe ability of a representational system to represent different situations is\\nmeasured by representational adequacy. If representational system A can\\nmodel more situations than representational system B, then A has a higher\\nrepresentational adequacy than B.\\nPH E\\nPH E\\n1\\n2\\n( )\\n( )'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 708, 'page_label': '709'}, page_content='682 Glossary\\nResolution\\nA method used in *propositional logic and *first-order predicate logic to\\nprove theorems by *contradiction. See *unification, *skolemization,\\n*skolem normal form, *most general unifier, *substitution.\\nRete\\nA directed, acyclic *graph or *tree used by *expert systems to make the\\nprocess of modifying stored facts in the database efficient.\\nRewrite rule\\nSee *production rule.\\nRidge\\nA narrow high region in a *search space that can cause problems for search\\nmethods such as *hill climbing.\\nRobot\\nA physical *agent. A robot can take many forms, such as an arm, an insect,\\nor a bucket on wheels.\\nRobotic agent\\nSee *robot, *software agent.\\nRoot goal\\nThe overall *goal of a problem that is being solved by *problem reduction.\\nThe root goal is represented by the *root node of the *goal tree.\\nRoot node\\nThe only *node in a *tree that has no predecessor. The top node in the tree.\\nRote learning\\n*Learning by simply storing each piece of *training data and its classification.\\nRoulette-wheel selection\\nA method that is used to make a random selection in which some items are\\nmore likely to be selected than others. Roulette-wheel selection is used, for\\nexample, to select *chromosomes to reproduce in *genetic algorithms.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 709, 'page_label': '710'}, page_content='Glossary 683\\nRule\\nA method of *representing the *knowledge used by a *rule-based system.\\nEach rule has an *antecedent and a *consequent. A rule can be written A →\\nB or IF A THEN B, which are equivalent.\\nRule-based system\\nA system whose behavior in a given situation is defined by a set of rules. See\\n*production system, *expert system.\\nS\\nSatisfiability\\nAn expression is satisfiable if it true under some *interpretation.\\nSAT planning\\nA method of determining whether a suitable *plan exists to solve a given\\nproblem. The problem is represented as a set of expressions, and if those\\nexpressions can be shown to be *satisfiable, then a plan can be devised to\\nsolve the problem.\\nScheduling\\nA method for allocating resources to machines (or other agents).\\nScheduling takes account of the time each task takes, which can be com-\\npared with *planning in which the time taken to carry out each task is\\nusually ignored.\\nSchema\\nA template used to represent a set of *chromosomes, using the symbol * to\\nrepresent any value. See *messy genetic algorithm, *genetic algorithm.\\nSchema theorem\\nA theorem that states that short, low-order schemata that are fitter than the\\naverage fitness of the population will appear with exponentially increasing\\nregularity in subsequent generations. See *genetic algorithm, *schema.\\nScript\\nA structured *representation for a scenario that involves a sequence of\\nevents, such as buying a house or going to a restaurant.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 710, 'page_label': '711'}, page_content='684 Glossary\\nSearch\\nThe process of locating a solution to a problem by systematically looking at\\nnodes in a *search tree or *search space until a *goal node is located. See\\n*heuristic, *blind search method, *informed search method.\\nSearch space\\nThe set of possible permutations that can be examined by a *search method in\\norder to find a solution. The search space represents every possible solution\\nand all the arrangements that do not satisfy the problem’s *constraints.\\nSearch tree\\nA *tree that is used to represent a *search problem and is examined by a\\nsearch method to search for a solution.\\nSegmentation\\nThe process of breaking an image down into homogeneous areas. See *edge\\ndetection, *image recognition.\\nSemantic analysis\\nThe analysis of the meaning of words in an *utterance in a human lan-\\nguage. See *syntactic analysis, morphologic analysis, *pragmatic analysis,\\n*natural language processing.\\nSemantic net\\nA *graph in which the *nodes represent objects and the *edges between\\nnodes represent relationships between the objects. See *semantic tree.\\nSemantic tree\\nA *semantic net in which each *node has exactly one *predecessor, apart\\nfrom the *root node, which has none.\\nSentence\\nSee *well-formed formula.\\nS-expression\\nA symbolic expression used by *LISP as either data or as a program to\\nbe executed.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 711, 'page_label': '712'}, page_content='Glossary 685\\nShepard’ s method\\nA variant of the *nearest neighbor algorithm in which the contribution of\\neach neighbor is determined by its distance from the point that is being\\nclassified.\\nSigmoid function\\nA mathematical function that is defined as follows:\\nThe sigmoid function is often used as the *activation function in *back-\\npropagation *neural networks because it is easy to differentiate.\\nSign activation function\\nA mathematical function that is usually used as the *activation function in\\n*Hopfield networks:\\nSimulated annealing\\nA *local search method based on the way in which metal or glass can be\\nmade very strong by being heated and then cooled very slowly.\\nSituated action rule\\nA *rule used by an *augmented finite state automaton that takes the form\\ninput -> action.\\nSituation calculus\\nA form of *first-order predicate calculus that is able to represent change\\nand the way in which variables relate to each other over time.\\nSituation variable\\nA variable used in a *situation calculus expression that represents the situ-\\nation that that expression represents. For example, in the following expres-\\nsion, S\\n1 is the situation variable:\\n∃x(In(Robot, x, S1) ∧ In(cheese, x, S1))\\nSign X for X\\nfor X( ) = +>\\n−<\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n10\\n10\\nσ x\\ne x( ) =\\n+ −\\n1\\n1'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 712, 'page_label': '713'}, page_content='686 Glossary\\nSkolem constant\\nA variable that is used to replace an *existentially quantified variable when\\n*skolemizing an expression.\\nFor example, the expression ∃x.x → b would be skolemized as x → c,w h e r e\\nc is the skolem constant.\\nSkolem function\\nA function that is used to replace an *existentially quantified variable that\\ncomes after a *universal quantification when *skolemizing an expression.\\nFor example, ∀x ∃y (x ∧ y) → b would be skolemized as ∀x (x ∧ f(x)) → b),\\nwhere f(x) is the skolem function.\\nSkolemization\\nThe process of replacing an *existentially quantified variable in an expres-\\nsion with a *skolem constant or *skolem function. Skolemization is a part\\nof the process of *resolution and results in an expression that is in *skolem\\nnormal form.\\nSkolem normal form\\nThe normal form produced by applying the process of *skolemization to\\nan expression.\\nSlipnet\\nA structure that represents the long-term memory of the *copycat architecture.\\nSlot\\nA named variable that is used in a *frame system to store an item of data.\\nSlot reader\\nA *procedure that returns the value that is stored in a *slot.\\nSlot value\\nThe item of data that is stored in a *slot in a *frame system.\\nSlot writer\\nA *procedure that inserts a value into a *slot.\\nSmart agent\\nA fully *autonomous, *intelligent, cooperative *agent. Smart agents are the\\nultimate goal of *agent research.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 713, 'page_label': '714'}, page_content='Glossary 687\\nSmoothing\\nThe process of removing noise from an image. See *convolution.\\nSoftware agent\\nAn *agent that exists only as a computer program, as compared with a\\n*robotic agent.\\nSoundness\\nA logical system is sound if every *theorem is a *tautology. See *completeness.\\nSpidering\\nThe process of retrieving documents from the Internet by following hyper-\\ntext links from one page to another. Spidering systems usually follow some\\nsearch strategy such as *depth-first or *breadth-first search.\\nStack\\nA data structure that stores its data sequentially and has just two operators:\\n“push, ” which places a new item onto the top of the stack, and “pop, ” which\\nremoves the top item from the stack. A stack is a “LIFO” or last-in-first-out\\ndata structure.\\nState\\nThe set of variables that define the current situation in a *world model.\\nState space\\nSee *search space.\\nStatic evaluator\\nA function used to evaluate a single position in a game, such as chess.\\nStemming\\nThe process of removing suffixes from words to render words with a com-\\nmon stem into the same form. For example, the following words would all\\nbe stemmed to swim: swimmer, swimming, swimmers, swims. Most stem-\\nmers would not successfully convert swam or swum into swim. Stemmers\\nare used in *information retrieval systems to increase *recall.\\nStop list\\nA list of words that an *information retrieval system is instructed to ignore\\nin queries and in the corpus it is searching against. The stop list usually\\ncontains extremely common words such as and, the, and of.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 714, 'page_label': '715'}, page_content='688 Glossary\\nSTRIPS\\nAn operator-based *planning approach and the corresponding planning lan-\\nguage that uses *well-formed formulae to represent the *state of the world.\\nSTRIPS assumption\\nThe assumption used by *STRIPS *planning systems that any statement\\nthat is true before applying an operator is also true after applying the oper-\\nator, unless it is included in the operator’s delete list.\\nStrong AI\\nThe belief that a computer system that is given sufficient processing power\\nand sufficiently powerful artificial intelligence would actually be capable of\\nhaving mental states in the way that humans are. See *weak AI.\\nStrong methods\\nArtificial Intelligence methods that rely on systems with a great deal of\\nknowledge built in. See *expert system, *weak methods.\\nSubclass\\nThe *class that *inherits the properties of a *superclass.\\nSubgoal\\nSee *subproblem.\\nSubproblem\\nOne of the smaller problems into which a large problem is broken down by\\nthe process of *problem reduction. By achieving all of the subproblems of a\\nproblem, the complete problem can be solved.\\nSubset\\nSet A is a subset of set B if A is wholly contained by B. For example, the set\\nof all men is a subset of the set of all humans.\\nSubstitution\\nThe process of replacing a *free variable in an expression with another free\\nvariable in order to facilitate the process of *unification. See *resolution.\\nSubsumption architecture\\nA layered architecture designed for the control of reactive *robotic agents.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 715, 'page_label': '716'}, page_content='Glossary 689\\nSuccess node\\nAn *and-node in a *goal tree that is a *leaf node.\\nSuccessor\\nThe *node immediately below a given node in a *tree. Each node has one or\\nmore successors, except for leaf nodes, which have no successors. See\\n*ancestor, *descendant, *predecessor.\\nSuperclass\\nThe *class from which a subclass *inherits certain properties.\\nSupervised learning\\nA form of *learning method in which the learning system is presented with\\na set of preclassified data before being expected to classify unseen data. See\\n*backpropagation, *competitive learning, *unsupervised learning.\\nSyllogism\\nA logical argument that contains a set of statements (premises) from which\\nis logically derived a conclusion. An example of a syllogism is:\\nAll cats have two ears.\\nMavis is a cat.\\nTherefore, Mavis has two ears.\\nSynapse\\nA connection between two *neurons in the human brain.\\nSyntactic analysis\\nThe analysis of the grammatical or syntactic structure of an *utterance in a\\n*human language. See *parser.\\nT\\nTabu search\\nA *metaheuristic that uses a list of states that have already been visited to\\nattempt to avoid repeating *paths.\\nTautology\\nAn expression that is true under all *interpretations.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 716, 'page_label': '717'}, page_content='690 Glossary\\nTemporal logic\\nA form of *modal logic that was designed to reason about change and the\\neffects of time.\\nTerm\\nA *constant, a variable, or a function of terms in *first-order predicate calculus.\\nTerm frequency - Inverse document frequency (TF-IDF)\\nA method used in *information retrieval to identify words that occur rarely\\nin a *corpus of text, but frequently in a particular document.\\nTerminal symbol\\nA symbol used in expressing a *grammar for a language that represents a\\nreal word that appears in the language. See *nonterminal symbol.\\nTexel\\nA single *texture element that is repeated in an image to produce an area of\\na particular texture.\\nTexture\\nA perceived pattern on the surface of an object in an image. See *texel,\\n*image recognition.\\nTheorem\\nA theorem of a logical system is a statement that can be proved by applying\\nthe rules of *deduction to the axioms in the system.\\nThree-coloring problem\\nSee *map coloring.\\nTit for tat\\nA strategy employed when playing the *Prisoner’s Dilemma game. The\\nstrategy involves cooperating on the first iteration of the game, and then\\nfor each subsequent iteration, doing what the opponent did in the previous\\niteration.\\nTop down\\nAn approach to solving problems that involves recursively breaking a prob-\\nlem down into smaller *subproblems until trivial problems are obtained\\nand the whole problem can be solved. See *bottom up, *problem reduction.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 717, 'page_label': '718'}, page_content='Glossary 691\\nTowers of Hanoi\\nA problem that involves moving a number of discs from one peg to\\nanother. The *constraints that are applied are that no disc can ever be\\nplaced on top of a smaller disc and that a disc cannot be moved if it has\\nanother disc on top of it.\\nTraining\\nThe process of teaching a *learning system to classify data by showing it\\npreclassified *training data.\\nTraining data\\nThe preclassified data that is shown to a *learning system in the *train-\\ning phase.\\nTransition network\\nA *finite state automaton used to represent a part of a *grammar.\\nTrihedral vertex\\nA vertex at which three faces meet.\\nTruth maintenance system (TMS)\\nA system that stores a set of beliefs along with information about how those\\nbeliefs were derived. The TMS is able to use this information to retract beliefs\\nif conflicting information later arrives. See *nonmonotonic reasoning.\\nTruth table\\nA table that represents the behavior of a logical operator by showing the\\npossible input *truth values for the operator and the corresponding output\\ntruth values.\\nTruth value\\nA representation of whether an expression is correct or not. In *classical\\nlogic, the truth values aretrue and false. In *multivalent logics, there are more\\ntruth values. *Fuzzy logic has an infinite range of truth values from 0 to 1.\\nTuring Test\\nA test devised by Alan Turing to determine whether an attempt to create a\\ntruly intelligent computer has been successful or not, by seeing whether the\\ncomputer can fool a human into thinking that it might actually be human\\ntoo. See *strong AI.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 718, 'page_label': '719'}, page_content='692 Glossary\\nU\\nUnary operator\\nA logical or mathematical operator that takes just one argument, such as ¬\\n(logical negation).\\nUncertainty\\nA lack of knowledge about the world. Most real-world problems are full of\\nuncertainty, but many Artificial Intelligence techniques deal very poorly\\nwith uncertainty.\\nUnderspecified chromosome\\nA *chromosome in which one or more bits do not have any value assigned\\nto them. See *messy genetic algorithm, *Overspecified chromosome.\\nUnification\\nThe process of applying a *substitution to a set of *clauses that enables\\nthose clauses to be *resolved.\\nUnifier\\nA *substitution that is applied to a set of clauses to enable those clauses to\\nbe resolved.\\nUniform crossover\\nA form of *crossover in which a probability, p, is used to determine\\nwhether a given bit from parent 1 will be used or from parent 2.\\nUniform tree\\nA *tree in which each non-leaf *node has the same *branching factor.\\nUninformed Search\\nA *search method that does not use *heuristics.\\nUniversal quantifier\\nThe quantifier \\n∀, which can be read “for all” and indicates that a property\\nholds for all possible values of the quantified variable. Hence, ∀x.P(x) can\\nbe read as “for all values ofx, P(x) is true. ” See *existential quantifier, *first-\\norder predicate calculus.\\nUniverse of discourse\\nThe range of possible values for a *linguistic variable.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 719, 'page_label': '720'}, page_content='Glossary 693\\nUnsupervised learning\\nA form of *learning method that does not require training or any other human\\nintervention. See *Hebbian learning, *Kohonen map, *supervised learning.\\nUtility-based agent\\nAn *agent that seeks to maximize some *utility function.\\nUtility function\\nA function that defines for any state how successful an *agent has been. A\\nhigh utility function is the goal of any *rational agent.\\nUtterance\\nA *sentence or partial sentence in a *human language that is spoken or\\nwritten by a human being or by an agent.\\nV\\nValidity\\nA logical *deduction is valid if its conclusions follow logically from its premises.\\nVanishing point\\nThe point to which perspective causes all parallel lines to appear to vanish.\\nVerb\\nA word in a *human language that is used to define an action. See *verb\\nphrase, *noun.\\nVerb phrase\\nA phrase that has the same properties within a *grammar as a *verb and can\\nthus be used interchangeably with a verb, at least as far as the syntactic rules of\\nthe grammar are concerned. For example, the following are all verb phrases:\\njump, jump over the moon, jumping over the table which is next to the door.\\nVersion space\\nThe set of *hypotheses that correctly map each of a set of *training data to\\nits classification.\\nW\\nWeak AI\\nThe view that intelligent behavior can be modeled and used by computers\\nto solve complex problems. See *strong AI.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 720, 'page_label': '721'}, page_content='694 Glossary\\nWeak methods\\nArtificial Intelligence methods that use logic or other representational sys-\\ntems to solve problems but do not rely on any real-world knowledge. See\\n*strong methods.\\nWeight\\nA value associated with each connection between *neurons in an artificial\\n*neural network that indicates how important that connection is and how\\nmuch of a role in the learning process that connection plays.\\nWeighted linear function\\nA function that takes the form ax + by + cz +  ...,w h e r e  a  n u m b e r  o f v a r i -\\nables (in this case x, y, z...)  a r e  e a c h  m u l tiplied by a weight (in this case a,\\nb, c...)  and the results summed.\\nWeight vector\\nA vector that represents the *weights of all connections from a given *neu-\\nron in an artificial *neural network.\\nWell-formed formula (wff)\\nAn expression that is correctly constructed according to the syntactic rules\\nof *propositional calculus or *first-order predicate calculus.\\nWHEN-CHANGED procedure\\nA *procedure that is run automatically whenever the value of a *slot is changed.\\nWHEN-NEEDED procedure\\nA *procedure that is run automatically when the value of a given *slot\\nneeds to be determined.\\nWHEN-READ procedure\\nA *procedure that is run automatically whenever the value of a *slot is read.\\nWHEN-WRITTEN procedure\\nSee  *WHEN-CHANGED procedure.\\nWinner takes all algorithm\\nThe algorithm used by *Kohonen maps to assign credit to nodes in the net-\\nwork. See *competitive learning, *credit assignment.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 721, 'page_label': '722'}, page_content='Glossary 695\\nWorkspace\\nA data structure similar to a *blackboard that is used by the *copycat\\narchitecture.\\nWorld model\\nA representation of the state of the world or environment as it affects an\\n*agent.\\nZ\\nZero sum game\\nA game in which if one player wins, the other player loses.'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 722, 'page_label': '723'}, page_content='This page intentionally left blank'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 723, 'page_label': '724'}, page_content='Bibliography\\nA\\nIntroduction to Artificial Life, by Christoph Adami (1997 – T elos)\\nNatural Language Understanding, by James Allen (1995 – Addison Wesley)\\nReasoning About Plans, by James F. Allen, Henry A. Kautz, Josh T enenberg,\\nand Richard Pelavin (1991 – Morgan Kaufmann)\\n2D Object Detection and Recognition: Models, Algorithms, and Networks ,b y\\nY ali Amit (2002 – MIT Press)\\nNonmonotonic Reasoning, by Grigoris Antoniou (1997 – MIT Press)\\nThe Handbook of Brain Theory and Neural Networks: Second Edition, edited\\nby Michael A. Arbib (2002 – MIT Press)\\nBehavior-Based Robotics, by Ronald C. Arkin (1998 – MIT Press)\\nReal-Time and Multi-Agent Systems, by Ammar Attoui (2000 – Springer V erlag)\\nGenetic Algorithm for the Prisoner Dilemma Problem , by R. Axelrod (1987 -\\nin Genetic Algorithms and Simulated Annealing, edited by L. Davis – Hyper-\\nion Books)\\nB\\nHandbook of Evolutionary Computation, edited by T. Bäck, D. B. Fogel, and\\nZ. Michalewicz (1997- Institute of Physics Publishing)\\nFrancis Bacon: The New Organon , by Francis Bacon; edited by Lisa Jardine\\nand Michael Silverthorne (2002 – Cambridge University Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 724, 'page_label': '725'}, page_content='698 Bibliography\\nModern Information Retrieval , by Ricardo Baeza-Y ates and Berthier\\nRibeiro-Neto (1999 – Addison Wesley)\\nModeling the Internet and the Web: Probabilistic Methods and Algorithms,b y\\nPierre Baldi, Paolo Frasconi, and Padhraic Smyth (2003 – John Wiley &\\nSons)\\nGenetic Programming: An Introduction: On the Automatic Evolution of Com-\\nputer Programs and Its Applications , by Wolfgang Banzhaf, Peter Nordin,\\nRobert E. Keller, and Frank D. Francone (1997 – Morgan Kaufmann)\\nKnowledge Representation, Reasoning and Declarative Problem Solving ,b y\\nChitta Baral (2003 – Cambridge University Press)\\nThe Handbook of Artificial Intelligence, edited by A. Barr and E. Feigenbaum\\n(1989 – William Kaufman)\\nIntelligent Machine Vision: Techniques, Implementations and Applications ,\\nby Bruce G. Batchelor and Frederick M. Waltz (2001 – Springer V erlag)\\nDigital Biology, by Peter Bentley (2002 – Simon & Schuster)\\nBayesian Theory, by José M. Bernardo and Adrian F. M. Smith (2001 – John\\nWiley & Sons)\\nNeural Networks for Pattern Recognition, by Christopher M. Bishop (1996 –\\nOxford University Press)\\nFundamentals of Expert System Technology: Principles and Concepts ,b y\\nSamuel J. Biondo (1990 – Intellect)\\nRecent Advances in AI Planning, by Susanne Biundo and Maria Fox (2000 –\\nSpringer V erlag)\\nFast Planning Through Planning Graph Analysis , by A. Blum and M. Furst\\n(1997 – in Artificial Intelligence, Vol. 90, pp. 281–300)\\nA Logical Theory of Nonmonotonic Inference and Belief Change , by Alexan-\\nder Bochman (2001 – Springer V erlag)\\nThe Philosophy of Artificial Life, by Margaret A. Boden (1996 – Oxford Uni-\\nversity Press)\\nSwarm Intelligence: From Natural to Artificial Systems , by Eric Bonabeau,\\nMarco Dorigo, and Guy Theraulaz (1999 – Oxford University Press)\\nUnderstanding 99% of Artificial Neural Networks: Introduction & Tricks ,b y\\nMarcelo Bosque (2002 – Writers Club Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 725, 'page_label': '726'}, page_content='Bibliography 699\\nAdvances in Image Understanding: A Festschrift for Azriel Rosenfeld , edited\\nby Kevin W. Bowyer and Narendra Ahuja (1996 – Wiley IEEE Press)\\nSoftware Agents, edited by Jeffrey M. Bradshaw (1997 – AAAI Press)\\nRobot Motion: Planning and Control, edited by Michael Brady, John Holler-\\nbach, Timothy Johnson, T omás Lozano-Pérez, and Matthew T. Mason\\n(1983 – MIT Press)\\nEmpirical Analysis of Predictive Algorithms for Collaborative Filtering ,b y\\nJohn S. Breese, David Heckerman, and Carl Kadie (1998 – in Proceedings of\\nthe Fourteenth Conference on Uncertainty in Artificial Intelligence ,M o r g a n\\nKaufmann)\\nNonmonotonic Reasoning: An Overview , by Gerhard Brewka, Jürgen Dix,\\nand Kurt Konolige (1995 – Cambridge University Press)\\nNonmonotonic Reasoning: From Theoretical Foundation to Efficient Compu-\\ntation, by G. Brewka (1991 – Cambridge University Press)\\nCambrian Intelligence: The Early History of the New AI , by Rodney A.\\nBrooks (1999 – MIT Press)\\nRule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuris-\\ntic Programming Project , by B. G. Buchanan and E. H. Shortliffe (1984 –\\nAddison Wesley)\\nPropositional Logic: Deduction and Algorithms, by Hans Kleine Büning and\\nTheodor Lettmann (1999 – Cambridge University Press)\\nA Resolution Principle for a Logic with Restricted Quantifiers, by H. J. Burck-\\nert (1992 – Springer V erlag)\\nC\\nThe Essence of Neural Networks, by Robert Callan (1999 – Prentice Hall)\\nProlog Programming for Students: With Expert Systems and Artificial Intelli-\\ngence Topics, by David Callear (2001 – Continuum)\\nEfficient and Accurate Parallel Genetic Algorithms, by Erick Cantu-Paz (2002\\n– Kluwer Academic Publishers)\\nPlan Recognition in Natural Language Dialogue , by Sandra Carberry (1990\\n– MIT Press)\\nNeural Networks for Vision and Image Processing, edited by Gail A. Carpen-\\nter and Stephen Grossberg (1992 – MIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 726, 'page_label': '727'}, page_content='700 Bibliography\\nSymbolic Logic and Game of Logic, by Lewis Carroll (Published in one vol-\\nume – 1958 – Dover Books)\\nExpert Systems and Probabilistic Network Models , by Enrique Castillo, Jose\\nManuel Gutierrez, and Ali S. Hadi (1997 – Springer V erlag)\\nThe Essence of Artificial Intelligence , by Alison Cawsey (1998 – Prentice\\nHall)\\nArtificial Intelligence, by Jack Challoner (2002 – Dorling Kindersley, Essen-\\ntial Science)\\nPractical Handbook of Genetic Algorithms, by Lance Chambers (1995 – CRC\\nPress)\\nSymbolic Logic and Mechanical Theorem Proving, by Chin-Liang Chang and\\nRichard Char-Tung Lee (1973 – Academic Press)\\nIntroduction to Artificial Intelligence , by Eugene Charniak and Drew\\nMcDermott (1985 – Addison Wesley; out of print)\\nGenetic Algorithms and Genetic Programming in Computational Finance ,\\nedited by Shu-Heng Chen (2002 – Kluwer Academic Publishers)\\nLearning from Data: Concepts, Theory, and Methods , by Vladimir\\nCherkassky and Filip Mulier (1998 – Wiley Interscience)\\nThe Computational Brain , by Patricia S. Churchland and T errence J.\\nSejnowski (1992 – The MIT Press)\\nAn Introduction to Genetic Algorithms for Scientists and Engineers, by David\\nA. Coley (1999 – World Scientific Publishing Company)\\nAdaptive Parallel Iterative Deepening Search, by Diane J. Cook and R. Craig\\nVarnell (1998 – in Journal of Artificial Intelligence Research, Vol. 9, pp.\\n139–166)\\nIntroduction to Algorithms , by Thomas H. Cormen, Charles E. Leiserson,\\nRonald L. Rivest, and Clifford Stein (2001 – MIT Press)\\nProbabilistic Networks and Expert Systems , edited by Robert G. Cowell\\n(1999 – Springer V erlag)\\nThe Fuzzy Systems Handbook: A Practitioner’s Guide to Building, Using, &\\nMaintaining Fuzzy Systems, by Earl Cox (1999 – Morgan Kaufmann)\\nFuzzy Logic for Business and Industry , by Earl Cox (2000 – Charles River\\nMedia)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 727, 'page_label': '728'}, page_content='Bibliography 701\\nAI: The Tumultuous History of the Search for Artificial Intelligence, by Daniel\\nCrevier (1999 – Basic Books)\\nThe Turing Test and the Frame Problem: AI’s Mistaken Understanding of\\nIntelligence, by Larry J. Crockett (1994 – Intellect)\\nD\\nThe Origin of Species, by Charles Darwin (1859 – reprinted, by Penguin)\\nArtificial Immune Systems and Their Applications, edited by Dipankar Das-\\ngupta (1999 – Springer V erlag)\\nMultiobjective Heuristic Search: An Introduction to Intelligent Search Meth-\\nods for Multicriteria Optimization , by Pallab Dasgupta, P . P . Chakrabarti,\\nand S. C. Desarkar (1999 - Friedrich Vieweg & Sohn)\\nSocially Intelligent Agents - Creating Relationships with Computers and\\nRobots, edited by Kerstin Dautenhahn, Alan H. Bond, Lola Canamero, and\\nBruce Edmonds (2002 – Kluwer Academic Publishers)\\nMachine Vision: Theory, Algorithms, Practicalities , by E. R. Davies (1996 –\\nAcademic Press)\\nAdaptive Learning ,b y  Genetic Algorithms: Analytical Results and Applica-\\ntions to Economic Models, by Herbert Dawid (1999 – Springer V erlag)\\nThe Blind Watchmaker, by Richard Dawkins (1996 – W. W. Norton & Com-\\npany)\\nArtificial Immune Systems: A New Computational Intelligence Paradigm ,b y\\nLeandro N. de Castro and Jonathan Timmis (2002 – Springer V erlag)\\nA Generalization of Bayesian Inference, by A. P . Dempster (1968 - in Journal\\nof the Royal Statistical Society Vol. 30 (pp. 205–217))\\nBayesian Methods for Nonlinear Classification and Regression , by David\\nG. T. Denison, Christopher C. Holmes, Bani K. Mallick, and Adrian F. M.\\nSmith (2002 – John Wiley & Sons)\\nBrainstorms: Philosophical Essays on Mind and Psychology , by Daniel Den-\\nnett (1978 – Bradford)\\nConsciousness Explained, by Daniel Dennett (1992 – Little, Brown & Co.)\\nProbabilistic Theory of Pattern Recognition , by Luc Devroye, Laszlo Gyorfi,\\nand Gabor Lugosi (1998 – Springer V erlag)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 728, 'page_label': '729'}, page_content='702 Bibliography\\nUnderstanding Agent Systems, edited by Mark D’Inverno and Michael Luck\\n(2001 – Springer V erlag)\\nA Truth Maintenance System, by Jon Doyle (1979 – in Computation & Intel-\\nligence – Collected Readings, edited by George F. Luger, The MIT Press)\\nWhat Computers Still Can’t Do , by Hubert L. Dreyfus (1999 – The MIT\\nPress)\\nAn Introduction to Fuzzy Control, by Dimiter Driankov, Hans Hellendoorn\\nand M. Reinfrank (1996 – Springer V erlag)\\nHow to Solve it, by Computer, by R. G. Dromey (1982 – out of print)\\nArtificial Intelligence: Strategies, Applications, and Models Through Search ,\\nby Benedict Du Boulay and Christopher James Thornton (1999 – AMA-\\nCOM)\\nE\\nIntelligent Agents for Mobile and Virtual Media , edited by Rae Earnshaw,\\nJohn Vince, and Margaret A. Arden (2002 – Springer V erlag)\\nNeural Darwinism: The Theory of Neuronal Group Selection , by Gerald M.\\nEdelman (1990 – Oxford University Press)\\nComputational Intelligence: An Introduction , by Andries P . Engelbrecht\\n(2003 – John Wiley & Sons)\\nPredicate Logic: The Semantic Foundations of Logic , by Richard L. Epstein\\n(2000 – Wadsworth Publishing)\\nPropositional Logics: The Semantic Foundations of Logic , by Richard L.\\nEpstein (2000 – Wadsworth Publishing)\\nF\\nFuzzy Control: Synthesis and Analysis, edited by Shehu S. Farinwata, Dimi-\\ntar P . Filev, and Reza Langari (2000 – John Wiley & Sons)\\nThree-Dimensional Computer Vision , by Olivier Faugeras (1993 – MIT\\nPress)\\nFundamentals of Neural Networks , by Laurene V . Fausett (1994 – Prentice\\nHall)\\nSTRIPS: A New Approach to the Application of Theorem Proving to Problem\\nSolving, by Richard E. Fikes and Nils J. Nilsson (1971 – in Computation &\\nIntelligence, edited by George F. Luger, 1995, MIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 729, 'page_label': '730'}, page_content='Bibliography 703\\nArtificial Intelligence: A Knowledge-Based Approach, by Morris W. Firebaugh\\n(1988 – Boyd & Fraser Publishing Company – out of print)\\nThe Anatomy of Programming Languages, by Alice E. Fischer and Frances S.\\nGrodzinsky (1993 – Prentice Hall)\\nBlondie 24: Playing at the Edge of AI , by David B. Fogel (2001 – Morgan\\nKaufmann)\\nEvolutionary Computation in Bioinformatics , edited by Gary B. Fogel and\\nDavid W. Corne (2002 – Morgan Kaufmann)\\nThe Robots Dilemma Revisited: The Frame Problem in Artificial Intelligence ,\\nby Kenneth M. Ford and Zenon W. Pylyshyn (1996 – Ablex Publishing)\\nComputer Vision: A Modern Approach, by David A. Forsyth and Jean Ponce\\n(2002 – Prentice Hall)\\nParallel Computing Works, by G. C. Fox, R. D. Williams, and P . C. Messina\\n(1994 – Morgan Kaufmann)\\nUnderstanding Artificial Intelligence (Science Made Accessible), compiled by\\nSandy Fritz (2002 – Warner Books)\\nG\\nHandbook of Logic in Artificial Intelligence and Logic Programming: Nonmo-\\nnotonic Reasoning and Uncertain Reasoning, edited by Dov M. Gabbay, J. A.\\nRobinson, and Christopher John Hogger (1994 – Oxford University Press)\\nBayesian Data Analysis, by Andrew Gelman, Donald B. Rubin, and Hal S.\\nStern (2003 – CRC Press)\\nGenetic Algorithms and Engineering Optimization, by Mitsuo Gen and Run-\\nwei Cheng (1991 – Wiley Interscience)\\nExpert Systems: Principles and Programming, by Joseph C. Giarratano (1998\\n– Brooks Cole)\\nT abu Search, by Fred W. Glover and Manuel Laguna (1998 – Kluwer Acade-\\nmic Publishers)\\nGenetic Algorithms in Search, Optimization and Machine Learning, by David\\nE. Goldberg (1989 – Addison Wesley)\\nMessy Genetic Algorithms: Motivation, Analysis and First Results , by D. E.\\nGoldberg (1989 - in Complex Systems, Vol. 3, pp. 493–530)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 730, 'page_label': '731'}, page_content='704 Bibliography\\nRapid, Accurate Optimization of Difficult Problems Using Fast Messy Genetic\\nAlgorithms, by David E. Goldberg, Kalyanmoy Deb, Hillol Kargupta, and\\nGeorges Harik (1993 – in Proceedings of the Fifth International Conference\\non Genetic Algorithms, pp. 56–64, Morgan Kaufmann)\\nDynamic Vision: From Images to Face Recognition , by Shaogang Gong,\\nStephen J. McKenna, and Stephen J. McKenna (2000 – Imperial College\\nPress)\\nCreation: Life and How to Make It, by Steve Grand (2001 – Harvard Univer-\\nsity Press)\\nCross-Language Information Retrieval , edited by Gregory Grefenstette\\n(1998 – Kluwer Academic Publishing)\\nManaging Uncertainty in Expert Systems, by Jerzy W. Grzymala-Busse (1991\\n– Kluwer Academic Publishers)\\nAn Introduction to Neural Networks, by Kevin Gurney (1997 – UCL Press)\\nH\\nFrom Animals to Animats 7: Proceedings of the Seventh International Confer-\\nence on Simulation of Adaptive Behavior , edited by Bridget Hallam, Dario\\nFloreano, John Hallam, Gillian Hayes, and Jean-Arcady Meyer (2002 – MIT\\nPress; also available are the proceedings from the first to sixth conferences)\\nComputer and Robot Vision (Volume II), by Robert M. Haralick and Linda\\nG. Shapiro (2002 – Pearson Education)\\nAlgorithmics: The Spirit of Computing , by David Harel (1987 – Addison\\nWesley)\\nExpert Systems: Artificial Intelligence in Business , by Paul Harmon (1985 –\\nJohn Wiley & Sons – out of print)\\nArtificial Intelligence: The Very Idea , by J. Haugeland (1985 – The MIT\\nPress)\\nPractical Genetic Algorithms, by Randy L. Haupt and Sue Ellen Haupt (1998\\n– Wiley Interscience)\\nFoundations of Computational Linguistics: Human-Computer Communica-\\ntion in Natural Language, by Roland R. Hausser (2001 – Springer V erlag)\\nNeural Networks: A Comprehensive Foundation, by Simon S. Haykin (1998 –\\nPrentice Hall)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 731, 'page_label': '732'}, page_content='Bibliography 705\\nThe Organisation of Behavior: A Neuropsychological Theory ,b y  D .O .H e b b\\n(1949 – republished in 2002, by Lawrence Erlbaum Assoc.)\\nProbabilistic Interpretations for Mycin’s Certainty Factors, by O. Heckerman\\n(1986 – in Uncertainty in Artificial Intelligence , edited by L. N. Kanal and\\nJ. F. Lemmer, Elsevier Science, pp. 167–196)\\nSilicon Second Nature: Culturing Artificial Life in a Digital World , by Stefan\\nHelmreich (2000 – University of California Press)\\nInformation Retrieval, by William R. Hersh (2002 – Springer V erlag)\\nFuzzy and Neural Approaches in Engineering , by J. Wesley Hines (1997 –\\nWiley Interscience)\\nVisual Intelligence: How We Create What We See , by Donald D. Hoffman\\n(1998 – W. W. Norton & Company)\\nBraitenberg Creatures, by David W. Hogg, Fred Martin, and Mitchel Resnick\\n(1991 - originally published as Epistemology and Learning Memo #13)\\nAdaptation in Natural and Artificial Systems: An Introductory Analysis with\\nApplications to Biology, Control, and Artificial Intelligence , by John H. Hol-\\nland (1992 – MIT Press)\\nRobot Vision, by Berthold K. Horn (1986 – McGraw Hill Higher Education)\\nBehind Deep Blue: Building the Computer That Defeated the World Chess\\nChampion, by Feng-Hsiung Hsu (2002 – Princeton University Press)\\nSpoken Language Processing: A Guide to Theory, Algorithm and System\\nDevelopment, by Xuedong Huang, Alex Acero, Hsiao-Wuen Hon, and Raj\\nReddy (2001 – Prentice Hall)\\nI\\nNatural Language Processing and Knowledge Representation: Language for\\nKnowledge and Knowledge for Language , edited by Lucja M. Iwanska and\\nStuart C. Shapiro (2000 – AAAI Press)\\nJ\\nIntroduction to Expert Systems, by Peter Jackson (1999 – Addison Wesley)\\nIntroduction to Artificial Intelligence , by Philip C. Jackson (1985 – Dover\\nPublications)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 732, 'page_label': '733'}, page_content='706 Bibliography\\nNatural Language Processing for Online Applications: Text Retrieval, Extrac-\\ntion, and Categorization , by Peter Jackson and Isabelle Moulinier (2002 –\\nJohn Benjamins Publishing Company)\\nText-Based Intelligent Systems: Current Research and Practice in Information\\nExtraction and Retrieval , edited by Paul Schafran Jacobs (1992 – Lawrence\\nErlbaum Assoc.)\\nIncreased Rates of Convergence Through Learning Rate Adaptation , by R. A.\\nJacobs (1987 - in Neural Networks, Vol. 1, pp. 295–307)\\nSpotting and Discovering Terms through Natural Language Processing ,b y\\nChristian Jacquemin (2001 – MIT Press)\\nMachine Vision, by Ramesh Jain, Rangachar Kasturi, and Brian G. Schunck\\n(1995 –McGraw Hill)\\nApplications of Fuzzy Logic: Towards High Machine Intelligence Quotient\\nSystems, edited by Mohammad Jamshidi, Andre Titli, Lotfi Zadeh, and\\nSerge Boverie (1997 – Prentice Hall)\\nNeuro-Fuzzy and Soft Computing: A Computational Approach to Learning\\nand Machine Intelligence , by Jyh-Shing Roger Jang, Chuen-Tsai Sun, and\\nEiji Mizutani (1996 – Prentice Hall)\\nSimulated Annealing for Query Results ranking , by B. J. Jansen (1997 – in\\nACM Computer Science Education Conference, ACM Press)\\nAgent Technology: Foundations, Applications, and Markets , edited by\\nNicholas R. Jennings and Michael J. Wooldridge (1998 – Springer V erlag)\\nEmergence: The Connected Lives of Ants, Brains, Cities, and Software ,b y\\nSteven Johnson (2001 – Scribner)\\nAI Application Programming, by M. Tim Jones (2003 – Charles River Media)\\nSpeech and Language Processing: An Introduction to Natural Language Pro-\\ncessing, Computational Linguistics and Speech Recognition, by Dan Jurafsky,\\nJames H. Martin, Keith Vander Linden, and Nigel Ward (2000 – Prentice\\nHall)\\nK\\nMultistage Fuzzy Control: A Model-Based Approach to Fuzzy Control and\\nDecision Making, by Janusz Kacprzyk (1997 – John Wiley & Sons)\\nChoices, Values, and Frames, by Daniel Kahneman (Editor) and Amos Tver-\\nsky (2000 – Cambridge University Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 733, 'page_label': '734'}, page_content='Bibliography 707\\nAn Introduction to Computational Learning Theory , by Michael J. Kearns\\nand Umesh V . Vazirani (1994 – MIT Press)\\nLearning and Soft Computing: Support Vector Machines, Neural Networks,\\nand Fuzzy Logic Models (Complex Adaptive Systems) , by Vojislav Kecman\\n(2001 – MIT Press)\\nThe Essence of Logic, by John Kelly (1997 – Prentice Hall)\\nOut of Control: The New Biology of Machines, by Kevin Kelly (1994 – Fourth\\nEstate)\\nSwarm Intelligence, by James Kennedy, Russell C. Eberhart, and Yuhui Shi\\n(2001 – Morgan Kaufmann)\\nKnowledge Acquisition for Expert Systems: A Practical Handbook , by Alison\\nL. Kidd (1987 – Plenum Publishing Corporation)\\nCommitment and Effectiveness of Situated Agents , by D. Kinny and M.\\nGeorgeff (1991 – in Proceedings of the Twelfth International Joint Conference\\non Artificial Intelligence, pp. 82–88, Morgan Kaufmann)\\nIntelligent Information Agents: The Agentlink Perspective (Lecture Notes in\\nComputer Science, 2586) , edited by Matthias Klusch, Sonia Bergamaschi,\\nand Pete Edwards (2003 – Springer V erlag)\\nAn Analysis of Alpha Beta Pruning , by Donald Knuth and R. W. Moore\\n(1975 - in Artificial Intelligence, Vol. 6 (4), pp. 293–326)\\nArt of Computer Programming: Sorting and Searching , by Donald Knuth\\n(1973 – Pearson Addison Wesley)\\nSelf-Organizing Maps, by T euvo Kohonen (2000 – Springer V erlag)\\nCase-Based Reasoning, by Janet Kolodner (1993 – Morgan Kaufmann)\\nLearning to Solve Problems, by Searching for Macro-Operators (Research\\nNotes in Artificial Intelligence, Vol. 5), by Richard E. Korf (1985 – Longman\\nGroup United Kingdom)\\nSearch, by Richard E. Korf (1987 – in Encyclopedia of Artificial Intelligence,\\nedited by E. Shapiro – Wiley)\\nBidirectional Associative Memories, by Bart Kosko (1988 - in IEEE Transac-\\ntions Systems, Man & Cybernetics, Vol. 18, pp. 49–60)\\nFuzzy Thinking: The New Science of Fuzzy Logic , by Bart Kosko (1994 –\\nHyperion)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 734, 'page_label': '735'}, page_content='708 Bibliography\\nGenetic Programming: On the Programming of Computers,b y  Means of Nat-\\nural Selection, by John R. Koza (1992 – MIT Press)\\nGenetic Programming II: Automatic Discovery of Reusable Programs, by John\\nR. Koza (1994 – MIT Press)\\nA Learning Interface Agent for Scheduling Meetings , by R. Kozierok and P .\\nMaes (1993 – in Proceedings of the ACM-SIGCHI International Workshop on\\nIntelligent User Interfaces, ACM Press)\\nStrategic Negotiation in Multiagent Environments , by Sarit Kraus (2001 –\\nMIT Press)\\nComputer Vision and Fuzzy Neural Systems , by Arun D. Kulkarni (2001 –\\nPrentice Hall)\\nThe Age of Spiritual Machines, by Ray Kurzweil (1999 – Viking Penguin)\\nL\\nArtificial Life: An Overview , edited by Christopher Langton (1995 – MIT\\nPress)\\nCase-Based Reasoning: Experiences, Lessons, and Future Directions, edited by\\nDavid B. Leake (1996 –AAAI Press)\\nThe Resolution Calculus, by Alexander Leitsch (1997 – Springer V erlag)\\nBuilding Large Knowledge-Based Systems: Representation and Inference in\\nthe CYC Project, by Douglas B. Lenat and R. V . Guha (1990 – Addison Wes-\\nley)\\nThe Logic of Knowledge Bases , by Hector J. Levesque and Gerhard Lake-\\nmeyer (2001 – MIT Press)\\nFor the Sake of the Argument: Ramsey Test Conditionals, Inductive Inference\\nand Nonmonotonic Reasoning, by Isaac Levi (1996 – Cambridge University\\nPress)\\nArtificial Life: A Report from the Frontier Where Computers Meet Biology,b y\\nSteven Levy (1993 – Vintage Books)\\nMaking Decisions, by D. V . Lindley (1991 – John Wiley & Sons)\\nKnowledge Representation and Defeasible Reasoning (Studies in Cognitive\\nSystems, Vol. 5), edited by Ronald P . Loui and Greg N. Carlson (1990 –\\nKluwer Academic Publishers)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 735, 'page_label': '736'}, page_content='Bibliography 709\\nAutomated Theorem Proving: A Logical Basis, by Donald W. Loveland (1978\\n– Elsevier Science – Out of Print)\\nArtificial Intelligence & Manufacturing Research Planning Workshop , edited\\nby George F. Luger (1998 – AAAI)\\nArtificial Intelligence: Structures and Strategies for Complex Problem-Solving,\\nby George F. Luger (2002 – Addison Wesley)\\nComputation & Intelligence: Collected Readings , edited by George F. Luger\\n(1995 – The AAAI Press / The MIT Press)\\nM\\nFoundations of Statistical Natural Language Processing , by Christopher D.\\nManning and Hinrich Schütze (1999 – MIT Press)\\nNonmonotonic Logic: Context-Dependent Reasoning, by V . W. Marek and M.\\nTruszczynski (1993 – Springer V erlag)\\nProbabilistic Situation Calculus , by Paulo Mateus, António Pacheco, Javier\\nPinto, Amílear Sernadas, and Cristina Sernadas (2001 – in Annals of Math-\\nematics and Artificial Intelligence)\\nIntelligent Multimedia Information Retrieval , edited by Mark T. Maybury\\n(1997 – AAAI Press)\\nCircumscription: A Form of Non-Monotonic Reasoning , by John McCarthy\\n(1980 – in Computation & Intelligence – Collected Readings , edited by\\nGeorge F. Luger, The MIT Press)\\nSome Expert Systems Need Common Sense , by John McCarthy (1983 – in\\nComputer Culture: The Scientific, Intellectual and Social Impact of the Com-\\nputer, edited by Heinz Pagels, Vol. 426)\\nA Production System Version of the Hearsay-II Speech Understanding System,\\nby Donald McCracken (1981 - UMI Research)\\nA Logical Calculus of the Ideas Immanent in Nervous Activity , by W. S.\\nMcCulloch and W. Pitts (1943 - in Bulletin of Mathematical Biophysics, Vol.\\n5, pp. 115–137)\\nComputational Linguistics, by T ony McEnery (1992 – Coronet Books – out\\nof print)\\nFuzzy Logic: The Revolutionary Computer Technology That Is Changing Our\\nWorld, by Daniel McNeill (1994 – Simon & Schuster)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 736, 'page_label': '737'}, page_content='710 Bibliography\\nText Information Retrieval Systems , by Charles T. Meadow, Bert R. Boyce,\\nand Donald H. Kraft (2000 – Academic Press)\\nUncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions,\\nby Jerry M. Mendel (2000 – Prentice Hall)\\nBuilding Expert Systems in Prolog, by Dennis Merritt (1995 – Springer V er-\\nlag)\\nGenetic Algorithms + Data Structures = Evolution Programs , by Zbigniew\\nMichalewicz (1999 - Springer)\\nHow to Solve It: Modern Heuristics, by Zbigniew Michalewicz and David B.\\nFogel (1999 – Springer V erlag)\\nA Framework for Representing Knowledge , by Marvin Minsky (1975 – in\\nComputation & Intelligence – Collected Readings, edited by George F. Luger,\\nThe MIT Press)\\nPerceptrons, by Marvin Minsky and Seymour A. Papert (1969 – now avail-\\nable in an extended edition: Perceptrons - Expanded Edition: An Introduc-\\ntion to Computational Geometry, 1987 – MIT Press)\\nThe Society of Mind, by Marvin Minsky (1988 – Simon & Schuster)\\nSteps towards Artificial Intelligence, by Marvin Minsky (1961 – in Computa-\\ntion & Intelligence – Collected Readings , edited by George F. Luger, MIT\\nPress)\\nLearning Search Control Knowledge: An Explanation Based Approach ,b y\\nStephen Minton (1988 – Kluwer Academic Publishers)\\nMinimizing Conflicts: A Heuristic Repair Method for Constraint Satisfaction\\nand Scheduling Problems, by S.Minton, M. D. Johnson, A. B. Philips, and P .\\nLaird (1992 – Artificial Intelligence, Vol. 58)\\nAn Introduction to Genetic Algorithms , by Melanie Mitchell (1998 – MIT\\nPress)\\nThe Royal Road for Genetic Algorithms: Fitness Landscapes and GA Perfor-\\nmance, by Melanie Mitchell, Stephanie Forrest, and John H. Holland (1992\\n- In Towards a Practice of Autonomous Systems: Proceedings of the First Euro-\\npean Conference on Artificial Life , edited by Francisco J. Varela and Paul\\nBourgine, pp. 245–254, MIT Press)\\nMachine Learning, by T om M. Mitchell (1997 – McGraw Hill)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 737, 'page_label': '738'}, page_content='Bibliography 711\\nThe Turing Test: The Elusive Standard of Artificial Intelligence , edited by\\nJames H. Moor (2003 – Kluwer Academic Publishers)\\nRobot: Mere Machine to Transcendent Mind , by Hans P . Moravec (2000 –\\nOxford University Press)\\nAn Introduction to AI Robotics, by Robin R. Murphy (2000 – MIT Press)\\nN\\nA Guided Tour of Computer Vision , by Vishvjit S. Nalwa (1993 – Addison\\nWesley)\\nLocal Search for Planning and Scheduling: Ecai 2000 Workshop, Berlin, Ger-\\nmany, August 21, 2000: Revised Papers (Lecture Notes in Computer Science,\\n2148), edited by Alexander Nareyek (2001 – Springer V erlag)\\nMachine Learning: A Theoretical Approach , by Balas K. Natarajan (1991 –\\nMorgan Kaufmann)\\nLearning Bayesian Networks , by Richard E. Neapolitan (2003 – Prentice\\nHall)\\nArtificial Intelligence: A Guide to Intelligent Systems, by Michael Negnevitsky\\n(2002 – Addison Wesley)\\nAutomated Theorem Proving: Theory and Practice , by Monty Newborn\\n(2001 – Springer V erlag)\\nDeep Blue: An Artificial Intelligence Milestone , by Monty Newborn (2003 –\\nSpringer V erlag)\\nKasparov Versus Deep Blue: Computer Chess Comes of Age, by Monty New-\\nborn (1997 – Springer V erlag)\\nComputer Science as Empirical Enquiry: Symbols and Search , by Allen\\nNewell and Herbert A. Simon (1976 - in Computation & Intelligence – Col-\\nlected Readings, edited by George F. Luger, MIT Press)\\nGPS, A Program That Simulates Human Thought, by Alan Newell and Her-\\nbert A. Simon (1963 – in Computation & Intelligence , edited by George F.\\nLuger 1995 – MIT Press)\\nReport on a General Problem Solving Program , by Alan Newell, J. C. Shaw,\\nand Herbert A. Simon (1959 – in Proceedings of the International Conference\\non Information Processing, pp. 256–264, UNESCO)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 738, 'page_label': '739'}, page_content='712 Bibliography\\nBlackboard Systems: The Blackboard Model of Problem Solving and the Evo-\\nlution of Blackboard Architectures, by H. Penny Nii (1986 – in Computation\\n& Intelligence – Collected Readings , edited by George F. Luger, The MIT\\nPress)\\nArtificial Intelligence: A New Synthesis , by N. J. Nilsson (1998 – Morgan\\nKauffman)\\nReadings in Machine Translation , edited by Sergei Nirenburg, Harold L.\\nSomers, and Y orick A. Wilks (2002 – MIT Press)\\nFeature Extraction in Computer Vision and Image Processing ,b y  M a r k  S .\\nNixon and Alberto Aguado (2002 – Butterworth-Heinemann)\\nEvolutionary Robotics: The Biology, Intelligence, and Technology of Self-Orga-\\nnizing Machines, by Stefano Nolfi and Dario Floreano (2000 – MIT Press)\\nO\\nComputational Explorations in Cognitive Neuroscience: Understanding the\\nMind, by Simulating the Brain , by Randall C. O’Reilly (Author) and Yuko\\nMunakata (2000 – MIT Press)\\nEvolutionary Algorithms for Single and Multicriteria Design Optimization ,\\nby Andrzej Osyczka (2001 – Physica V erlag)\\nP\\nSoft Computing in Case Based Reasoning, edited by Sankar K. Pal, Tharam S.\\nDillon, and Daniel S. Y eung (2000 – Springer V erlag)\\nKasparov and Deep Blue, by Bruce Pandolfini (1997 – Fireside)\\nCombinatorial Optimization: Algorithms and Complexity , by Christos H.\\nPapadimitriou and Kenneth Steiglitz (1998 – Dover Publications)\\nEvolving Hexapod Gaits Using a Cyclic Genetic Algorithm , by Gary Parker\\n(1997 – in Proceedings of the IASTED International Conference on Artificial\\nIntelligence and Soft Computing, pp. 141–144, IASTED/ACTA Press)\\nGenerating Arachnid Robot Gaits with Cyclic Genetic Algorithms , by Gary\\nParker (1998 - in Genetic Programming III, pp. 576–583)\\nMetachronal Wave Gait Generation for Hexapod Robots , by Gary Parker\\n(1998 – in Proceedings of the Seventh International Symposium on Robotics\\nwith Applications, ISORA)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 739, 'page_label': '740'}, page_content='Bibliography 713\\nAlgorithms for Image Processing and Computer Vision, by J. R. Parker (1996\\n– John Wiley & Sons)\\nLearning-Based Robot Vision, edited by Josef Pauli (2001 – Springer V erlag)\\nHeuristics: Intelligent Search Strategies for Computer Problem Solving ,b y\\nJudea Pearl (1984 – Addison Wesley)\\nProbabilistic Reasoning in Intelligent Systems: Networks of Plausible Infer-\\nence, by Judea Pearl (1997 – Morgan Kaufmann)\\nAn Introduction to Fuzzy Sets: Analysis and Design , by Witold Pedrycz and\\nFernando Gomide (1998 – MIT Press)\\nThe Emperor’s New Mind: Concerning Computers, Minds, and the Laws of\\nPhysics, by Roger Penrose (1989 – Oxford University Press)\\nNatural Language Processing , by Fernando C. N. Pereira and Barbara J.\\nGrosz (1994 – MIT Press)\\nUnderstanding Intelligence, by Rolf Pfeiffer and Christian Scheier (2000 –\\nISBN: The MIT Press)\\nAn Algorithm for Suffix Stripping , by M. F. Porter (1980 - in Spärck Jones\\nand Willett, 1997)\\nIntroduction to Logic: Propositional Logic , by Howard Pospesel (1999 –\\nPrentice Hall)\\nPrisoner’s Dilemma: John Von Neumann, Game Theory and the Puzzle of the\\nBomb, by William Poundstone (1994 – MIT Press)\\nViews into the Chinese Room: New Essays on Searle and Artificial Intelligence,\\nedited by John Preston and Mark Bishop (2002 – Oxford University Press)\\nThe Robots Dilemma: The Frame Problem in Artificial Intelligence, by Zenon\\nW. Pylyshyn (1987 – Ablex Publishing)\\nQ\\nInduction of Decision Trees, by J. R. Quinlan (1986 – from Machine Learn-\\ning, Vol. 1 (1), pp. 81–106)\\nR\\nFundamentals of Speech Recognition , by Lawrence Rabiner and Biing-\\nHwang Juang (1993 – Pearson Education)\\nModern Heuristic Search Methods , edited by V . J. Rayward-Smith, I. H.\\nOsman, Colin R. Reeves, and G. D. Smith (1996 – John Wiley & Sons)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 740, 'page_label': '741'}, page_content='714 Bibliography\\nLogic for Computer Science , by Steve Reeves and Michael Clarke (1993 –\\nAddison Wesley)\\nAre We Spiritual Machines?: Ray Kurzweil vs. the Critics of Strong A.I., edited\\nby Jay W. Richards (2002 – Discovery Institute)\\nWord of Mouse: The Marketing Power of Collaborative Filtering , by John\\nRiedl and Joseph Konstan (2002 – Warner Books)\\nInside Case-Based Reasoning , by Christopher K. Riesbeck and Roger C.\\nSchank (1989 – Lawrence Erlbaum)\\nThe Bayesian Choice: From Decision-Theoretic Foundations to Computa-\\ntional Implementation, by Christian P . Robert (2001 – Springer V erlag)\\nMonte Carlo Statistical Methods, by Christian P . Robert and George Casella\\n(1999 – Springer V erlag)\\nLogic, Form and Function: The Mechanization of Deductive Reasoning ,b y\\nJohn Alan Robinson (1980 – Elsevier Science)\\nThe Perceptron: A Probabilistic Model for Information Storage and Organiza-\\ntion in the Brain , by F. Rosenblatt (1958 - in Psychological Review, Vol. 65,\\npp. 386–408)\\nRepresentations for Genetic and Evolutionary Algorithms, by Franz Rothlauf\\nand David E. Goldberg (2002 – Springer V erlag)\\nChange, Choice and Inference: A Study of Belief Revision and Nonmonotonic\\nReasoning, by Hans Rott (2002 – Oxford University Press)\\nArtificial Intelligence: A Modern Approach , by Stuart Russell and Peter\\nNorvig (1995 – Prentice Hall)\\nS\\nLogical Forms: An Introduction to Philosophical Logic , by Mark Sainsbury\\n(1991 – Blackwell)\\nEvolutionary Language Understanding, by Geoffrey Sampson (1996 – Con-\\ntinuum)\\nSome Studies in Machine Learning Using the Game of Checkers , by Arthur\\nSamuel (1959 - in Computation & Intelligence – Collected Readings, edited\\nby George F. Luger - MIT Press)\\nUsing Sophisticated Models in Resolution Theorem Proving (Lecture Notes in\\nComputer Science, Vol. 90), by David M. Sandford (1981 – Springer V erlag)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 741, 'page_label': '742'}, page_content='Bibliography 715\\nThe Importance of Being Fuzzy , by Arturo Sangalli (1998 – Princeton Uni-\\nversity Press)\\nOne Jump Ahead: Challenging Human Supremacy in Checkers , by Jonathan\\nSchaeffer (1997 – Springer V erlag)\\nA Re-examination of Brute-force Search , by Jonathan Schaeffer, Paul Lu,\\nDuane Szafron, and Robert Lake (1993 – in Games: Planning and Learning,\\nAAAI 1993 Fall Symposium, Report FS9302, pp. 51–58)\\nA World Championship Caliber Checkers Program , by Jonathan Schaeffer,\\nJoseph Culberson, Norman Treloar, Brent Knight, Paul Lu, and Duane\\nSzafron (1992 – in Artificial Intelligence, Vol. 53 (2–3), pp. 273–290)\\nArtificial Intelligence: An Engineering Approach, by Robert J. Schalkoff (1990\\n– McGraw Hill)\\nThe Structure of Episodes in Memory , by Roger C. Schank (1975 – in Com-\\nputation & Intelligence – Collected Readings, edited by George F. Luger, The\\nMIT Press)\\nThe Evidential Foundations of Probabilistic Reasoning , by David A. Schum\\n(2001 – Northwestern University Press)\\nMinds, Brains, and Programs , by John R. Searle (1980 – in The Behavioral\\nand Brain Sciences, Vol. 3, Cambridge University Press)\\nMinds, Brains and Science , by John R. Searle (1986 – Harvard University\\nPress)\\nAlgorithms, by Robert Sedgewick (1988 – Addison Wesley)\\nA New Method for Solving Hard Satisfiability Problems , by B. Selman, H.\\nLevesque, and D. Mitchell (1992 – in Proceedings of the T enth National\\nConference on Artificial Intelligence, pp. 440–446, AAAI)\\nNoise Strategies for Improving Local Search, by B. Selman, H.A. Kautz, and B.\\nCohen (1994 – Proceedings of the T enth National Conference on Artificial\\nIntelligence, pp. 337–343, AAAI)\\nComputer Vision, by Linda G. Shapiro and George C. Stockman (2001 –\\nPrentice Hall)\\nThe Encylopedia of Artificial Intelligence , edited by S. C. Shapiro (1992 –\\nWiley)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 742, 'page_label': '743'}, page_content='716 Bibliography\\nSocial Information Filtering: Algorithms for Automating “Word of Mouth,” by\\nU. Shardanand and P . Maes (1995 – in Proceedings of CHI’95 - Human Fac-\\ntors in Computing Systems, pp. 210–217)\\nA Two Dimensional Interpolation Function for Irregularly Spaced Data,b y  D .\\nShepard (1968 - Proceedings of the 23rd National Conference of the ACM,p p .\\n517–523, ACM Press)\\nComputer Based Medical Consultations: Mycin , by Edward Shortliffe (1976\\n– Elsevier Science, out of print)\\nArtificial Evolution for Computer Graphics , by Karl Sims (1991 – Siggraph\\n’91 - Annual Conference Proceedings, 1991, pp. 319–328, Eurographics Asso-\\nciation)\\nEvolving Virtual Creatures, by Karl Sims (1994 - Siggraph ’94 - Annual Con-\\nference Proceedings, 1994, pp. 43–50, Eurographics Association)\\nThe Algorithm Design Manual, by Steven S. Skiena (1997 – T elos)\\nData Analysis: A Bayesian Tutorial, by D. S. Sivia (1996 – Oxford University\\nPress)\\nImage Processing: Analysis and Machine Vision , by Milan Sonka, Vaclav\\nHlavac, and Roger Boyle (1998 – Brooks Cole)\\nKnowledge Representation: Logical, Philosophical, and Computational Foun-\\ndations, by John F. Sowa and David Dietz (1999 – Brooks Cole)\\nComputer Viruses as Artificial Life, by Eugene Spafford (1989 – in Artificial\\nLife An Overview, edited by Christopher G. Langton, 1995, MIT Press, pp.\\n249–265)\\nEvaluating Natural Language Processing Systems: An Analysis and Review,b y\\nKaren Spärck Jones and Julia R. Galliers (1996 – Springer V erlag)\\nReadings in Information Retrieval , edited by Karen Spärck Jones and Peter\\nWillett (1997 – Morgan Kaufmann)\\nLogic and Prolog, by Richard Spencer-Smith (1991 – Harvester Wheatsheaf)\\nResolution Proof Systems: An Algebraic Theory (Automated Reasoning Series,\\nVol. 4), by Zbigniew Stachniak (1996 – Kluwer Academic Publishers)\\nArtificial Life VIII: Proceedings of the Eighth International Conference on\\nArtificial Life, edited by Russell Standish, Mark A. Bedau, and Hussein A.\\nAbbass (2003 – MIT Press; also available are the proceedings from the first\\nthrough the seventh conferences)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 743, 'page_label': '744'}, page_content='Bibliography 717\\nLayered Learning in Multiagent Systems: A Winning Approach to Robotic\\nSoccer, by Peter Stone (2000 – MIT Press)\\nReinforcement Learning: An Introduction (Adaptive Computation and\\nMachine Learning), by Richard S. Sutton and Andrew G. Barto (1998 – MIT\\nPress)\\nBayes’s Theorem (Proceedings of the British Academy, Vol. 113) , edited by\\nRichard Swinburne (2002 – British Academy)\\nT\\nEvolutionary Art and Computers , by Stephen T odd and William Latham\\n(1992 – Academic Press)\\nIntroductory Techniques for 3-D Computer Vision, by Emanuele Trucco and\\nAlessandro V erri (1998 – Prentice Hall)\\nTranslation Engines: Techniques for Machine Translation, by Arturo Trujillo\\n(1999 – Springer V erlag)\\nManaging Expert Systems, edited by Efraim Turban and Jay Liebowitz (1992\\n– Idea Group Publishing)\\nU\\nHuman Face Recognition Using Third-Order Synthetic Neural Networks ,b y\\nOkechukwu A. Uwechue and Abhijit S. Pandya (1997 – Kluwer Academic\\nPublishers)\\nV\\nSimulated Annealing: Theory and Applications , by P . J. M. Van Laarhoven\\nand E. H. L. Aarts (1987 - D Reidel Publishing Company – out of Print)\\nStatistical Learning Theory , by Vladimir N. Vapnik (1998 – Wiley Inter-\\nscience)\\nPlanning and Learning ,b y  Analogical Reasoning , by Manuela M. V eloso\\n(1994 – Springer V erlag T elos)\\nLearning and Generalization: With Applications to Neural Networks ,b y\\nMathukumalli Vidyasagar (2002 – Springer V erlag)\\nThe Simple Genetic Algorithm: Foundations and Theory, by Michael D. Vose\\n(1999 – MIT Press)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 744, 'page_label': '745'}, page_content='718 Bibliography\\nW\\nVirtual Organisms: The Startling World of Artificial Life ,b y  M a r k  W a r d\\n(2000 – St Martin’s Press)\\nIn the Mind of the Machine: The Breakthrough in Artificial Intelligence ,b y\\nKevin Warwick (1998 – Random House)\\nFace Recognition: From Theory to Applications , by Harry Wechsler (1998 –\\nSpringer V erlag)\\nMultiagent Systems: A Modern Approach to Distributed Artificial Intelligence,\\nedited by Gerhard Weiss (1999 – MIT Press)\\nRecent Advances in AI Planning, by Daniel S. Weld in AI Magazine, Summer\\n1999\\nPractical Planning: Extending the Classical AI Planning Paradigm , by David\\nE. Wilkins (1989 – Morgan Kaufman)\\nArguing A. I.: The Battle for Twenty-First Century Science, by Sam Williams\\n(2002 – Random House)\\nArtificial Intelligence, by Patrick Henry Winston (1992 – Addison Wesley)\\nIntroduction to MultiAgent Systems , by Michael Wooldridge (2002 – John\\nWiley & Sons)\\nX\\nY\\nIntelligent Planning: A Decomposition and Abstraction Based Approach ,b y\\nQiang Y ang (1998 – Springer V erlag)\\nZ\\nIntelligent Scheduling, edited by Monte Zweben and Mark S. Fox (1998 –\\nMorgan Kaufmann)'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 745, 'page_label': '746'}, page_content='Index\\nA\\nA* algorithms, 108–109\\nAbduction, 201–202, 633\\nand Bayesian theory, 338–339\\nAbductive reasoning, 467, 482–483\\nAbelard, Peter,Dialectica, 6–7\\nAccepting state, 366, 633\\nAccessibility, 560–561\\nAcquaintance algorithm, 633\\nAction, 242\\nAction description language (ADL), 633\\nAction potential, of neuron, 293\\nActivation function, 293, 633\\nActivation level, 294f, 633\\nActivity product rule, 634\\nAdaptation in Natural and Artificial Systems\\n(Holland), 387\\nAdd list, 436\\nADL (action description language), 434, 455–456, 633\\nand open world assumption, 480\\nAdmissibility, 93, 634\\nAdmissible heuristic, 96\\nAdventures of Sherlock Holmes (Conan Doyle), 605\\nAdversarial methods, of game playing, 146, 634\\nAgent architectures, 556–560\\nAgent team, 555, 634\\nAgents, 196–197, 634\\ndefinition, 543\\nintelligent, 135, 543–569\\naccessibility, 560–561\\narchitectures for, 556–560\\nbold, 558\\nBraitenberg vehicles, 562–565\\ncollaborative, 642\\ngoal-based, 548–549, 657\\nhybrid, 660\\ninformation (internet), 553–554, 572, 662\\ninterface, 551–552, 663\\nlearning, 561–562\\nmobile, 552–553\\nmultiagent systems, 554–556\\nproperties of, 544–546\\nreactive (reflex), 547–548, 554\\nrobotic, 561–562\\nstatic, 552\\nutility-based, 549–551\\nmobile, 668\\nproperties of, 544–546\\nreactive (reflex), 547–548, 559, 680\\nsoftware, 543–544, 687\\nutility-based, 693\\nAggregation, 34\\nAI. see Artificial intelligence (AI)\\nAI: Artificial Intelligence (film), 23\\nAIBO robotic dog, 23\\nAlan Turing the Enigma of Intelligence(Hodge), 291\\nAlfonso the Wise, 117\\nAlgorithms\\nacquaintance, 633\\nAD3, 268, 278, 281, 661\\nbucket brigade, 286, 377, 378, 379–380, 639\\ngenetic. See Genetic algorithms\\nGraphPlan, 434, 451, 454–455, 658\\nwinner-take-all, 316, 694'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 746, 'page_label': '747'}, page_content='720 Index\\nAlice’s Adventures in Wonderland(Carroll), 241\\nAlignment method, and machine vision, 626\\nAlpha-beta pruning, 69, 133, 153–159, 634\\neffectiveness, 154–155\\nimplementation, 155–159\\nanalysis of search, 158t\\nAlphabet, 634\\nAmbiguity, 589–592, 634\\nand disambiguation, 591–592\\nAn Anatomy of the World(Donne), 503\\nAn Essay on Man (Pope), 433\\nAnalogy, and knowledge representation, 9.See also Copy-\\ncat architecture\\nAnalogy (Evans), 9\\nAnalogy of Religion, The (Butler), 327\\nAnalytic engine, 7\\nAnaphoric expressions, 590\\nAncestor, 45, 634\\nAnd-goals, 58–59, 634\\nAnd-elimination, 191\\nAnd-introduction, 191\\nAnd-nodes, 58–59, 635\\nAnd (operator), 178–179, 182\\nAnd-or trees, 635. See also Goal trees\\nAnnealing schedule, 130\\nAnnealing, simulated. See Simulated annealing\\nAnt colony optimization (ACO), 128\\nAntecedent, in logical statement, 183, 242\\nAPL2, 42\\nApplicable operators, action, 437\\nArchitectures, agent, 556–560\\nbelief desire intention (BDI), 558\\nhorizontal vs. vertical, 559–560\\nsubsumption, 556–557\\nT ouringMachines, 559, 560\\nAristotelian logic. See Logic, classical\\nAristotle, 6, 10, 364\\nPoetics, 327\\nArs Rhetorica (Dionysius), 3\\nART, 253\\nArt, and artificial evolution, 412–413\\nArtificial immune systems (AIS), 381–382\\nArtificial Intelligence: A Knowledge-Based Approach (Fire-\\nbaugh), 241\\nArtificial intelligence (AI)\\ndefinition, 4–5, 635\\nhistory of, 3–18\\nimportant areas of study, 10\\nintroduction of term, 9\\nweak vs. strong, 4, 5, 23\\nArtificial life, 128, 266, 635\\ntechniques, 363–364\\nArtificial Life Roots of Artificial Intelligence(Steels), 363\\nArtificial neural network, 284, 635\\nAsexual reproduction, 373\\nAssociation, 34\\nAssociativity (associative property), 187, 635\\nAssumption-based truth maintenance system (ATMS),\\n479\\nAtilla (robot), 563\\nAtomic actions, 422, 635\\nAtomic formula, 199, 635\\nAttractor networks, 307, 635\\nAugmented finite state machines (AFSMs), 556–557, 635\\nAugmented transition network (ATN), 585, 636\\nAutoassociative memory, 313, 636\\nAutomated reasoning, 5\\nAutomated translation, 572\\nAutonomy, of agents, 545, 636\\nAxelrod, R., 411\\nAxioms, 200\\nAxons, 292–293, 636\\nB\\nBabbage, Charles, 7\\nBackgammon, 167\\nBackpropagation algorithm, 291–292\\nin multilayer networks, 302–306, 636\\nimproving performance, 305–306\\nBacktracking, 76\\nnonchronological, 479\\nBacktracking search, 135\\nBackus-Naur form (BNF), 575–579, 636\\nBackward chaining, 242, 243, 248–251, 469.See also\\nGoal-driven search\\ncompared to forward chaining, 249–251\\nin rule-based systems, 257–259\\nin STRIPS, 440–441\\nBaldrick (from Blackadder), 433\\nBates, Marston, 71\\nBayes’ optimal classifier, 637\\nBayes’ Theorem, 327, 330–337, 637\\napplications, 331–337\\ncomparing conditional probabilities, 334–335'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 747, 'page_label': '748'}, page_content='Index 721\\nmedical diagnosis, 331–332\\nnormalization, 335–337\\nwitness reliability, 332–334\\nstated, 331\\nBayes, Thomas, 327\\nBayesian belief networks, 339–346, 347f, 636\\nexamples, 342–346\\nBayesian classifiers, 327, 349–356\\nnaive, 351–356\\nBayesian concept learning, 337–339\\nBayesian reasoning, 266, 504\\nand abduction and induction, 338–339\\nand collaborative filtering, 356–357\\nBeam search, 105–107\\nanalysis of search, 106t\\nBehavior, asynchronous, 557\\nBehavioral psychology, 12\\nBelief desire intention architecture (BDI), 558, 637\\nBelief networks, 339–346\\nBenevolence, 556\\nBest-first search, 69, 103–105\\nanalysis of search, 104t\\nBible, The, 19, 71, 267, 465, 543, 605\\nBidding system, of classifiers, 378, 379–380\\nBidirectional associative memories (BAMs), 292,\\n313–316, 637\\ncompared to Hopfield networks, 314, 315–316\\nBidirectional search, 136\\nBinary operators, 182, 637\\nBiology, and AI, 3, 4, 12, 128\\nBiomorphs, 372, 372–373, 412\\nBivalent logic, 504, 637\\nBlackboard architecture, 467, 469–472, 637\\nimplementing, 471–472\\nBletchley Park, 7\\nBlind search methods, 72, 91, 440, 637.See also Generate\\nand test\\nBlind Watchmaker, The(Dawkins), 363, 372, 372–373\\nBlocks world, 428–430, 433, 438–443, 638\\nBlondie, 24, 164\\nBlum, Avrim, 454\\nBoids system, 366, 367–368, 555\\nBoltzmann acceptance criterion, 129\\nBoole, George, 7\\nBoolean algebra, 7, 276–277, 296\\nBoolean operators, and perceptrons, 296, 299–300\\nBottom up or top down, 60–61, 278, 638\\nbuilding parse tree, 581–582\\nBound variables, 638\\nBounded lookahead, 151–153, 638\\nBraitenberg, Valentino, 563\\nBraitenberg vehicles, 464, 563–565, 638\\nBranch, 45, 638\\nBranch and bound search, 109–110\\nBranching factor, 45, 638\\nBranching time temporal logic, 488\\nBreadth-first search, 71, 76–78, 80, 638\\nimplementing, 83–90, 86–88\\nanalysis of search, 87t\\nBridge (game), 165, 167\\nBritish museum procedure, 107–108\\nBrooks, Rodney A., 556, 557\\nBrowne, Thomas, Religio Medici, 363\\nBrowning, Robert, Rabbi Ben Ezra, 421\\nBrute-force search (exhaustive search), 53, 639, 652.See\\nalso Breadth-first search; Depth-first search;\\nGenerate and test\\nBucket brigade algorithm, 286, 377, 639\\nand classifier systems, 378, 379–380\\nBuilding-block hypothesis, 403–404, 639\\nBurke, Edmund, Letter to a Member of the National\\nAssembly, 421\\nBurns, Robert, To a Mouse, 421\\nBuro, Michael, 166\\nButler, Joseph,The Analogy of Religion, 327\\nByron, Manfred, 571\\nC\\nC++, 12–13, 32, 41–42, 173\\nCandidate elimination, 275, 639\\nCandide (Voltaire), 71\\nCanny edge detector, 611–612, 639\\nCarlyle, Thomas, Critical and Miscellaneous Essays ,3 ,\\n465\\nCarroll, Lewis\\nAlice’s Adventures in Wonderland, 241\\nThrough the Looking Glass, 19, 175\\nCarruth, William Herbert, Each in His Own Tongue,\\n387\\nCase-based planning systems, 457–458, 639\\nCase-based reasoning, 467, 495–496, 639\\nCausal links, 446–447, 639\\nCells, in Conway’s Life, 368'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 748, 'page_label': '749'}, page_content='722 Index\\nCellular automata, 364, 368–371, 639\\ndefinition, 369\\none-dimensional, 370–371\\nCenter of gravity. See Centroid\\nCentroid, 521–522, 640\\nCertainty, 328\\nCertainty factor algebra, 487\\nCertainty factors, 467, 485–487, 640\\nChange, and logical systems, 205, 210, 426, 487–494.\\nSee also Situation calculus\\nevent calculus, 490–492\\nmental situation calculus, 492–494\\ntemporal logic, 487–490\\nChart parser, 640\\nChart parsing, 585–588\\nCheckers, playing, 159–164, 165\\nWorld Checkers Championship, 161\\nCHEF, 457–458\\nChess, playing, 22, 164–165, 268–269\\nChinese room, 20–21, 640\\nChinook, 152\\nevaluation function, 162–163\\nforward pruning, 163\\nChomsky, Noam, 11, 579, 580\\nChomsky’s hierarchy, 640\\nChromosomes, 131, 388–389, 640\\noverspecified, 674\\nsize, 389\\nsplicing, 405–406\\ntemplate, 405\\nunder- and overspecified, 405–406\\nunderspecified, 692\\nChronological backtracking, 76, 640\\nCircumscription, 467, 480–482, 641\\nClass, 30, 641\\nClass frame, 32, 641\\nClassical logic, 641\\nClassification, of data, 268\\nClassifier systems, 286, 364, 377–381, 388, 389, 641\\nBayesian, 327, 349–356\\nnaive, 351–356\\noptimal, 349–351\\ncomponents of, 377\\noperation of, 377–381\\nreproduction in, 380–381\\nrules, 378–379\\nClause, 641\\nCLIPS (C language integrated production system), 253,\\n255–257, 641\\nCloning, 392, 641\\nClosed world assumption, 467, 480, 641\\nCluster, 316\\nCluster layer, 316\\nCNF-satisfiability problem, 414\\nCo-evolution, 413–414\\nCo-occurence matrix, 646\\nCodelets, 475\\nCoderack, 474–475\\nCoevolution, 642\\nCognitive psychology, 12, 642\\nCollaboration, 555\\nCollaborative agents, 555–556, 642\\nCollaborative filtering, 327–328, 356–357, 642\\nColony optimization, 126\\nColoring problems, 216–218\\nColossus, 161\\nCombinatorial explosion, 57, 125\\nCombinatorial problems, 24, 117, 173, 414, 642\\noptimization, 125–126\\nsearch, 216\\nCommunative property, 187, 331\\nCommunication, 555\\nCommutativity, 642\\nCompetitive agents, 555\\nCompetitive learning, 316, 642\\nComplete path, 45, 643\\nCompleteness, 173, 175, 200, 643\\nof search methods, 79\\nComplexity, of search methods, 78–79\\nComposition, 223–224, 643\\nComputation tree logic (CTL), 488, 643\\nComputing Machinery and Intelligence (Turing), 7\\nConan Doyle, Arthur\\nThe Adventures of Sherlock Holmes, 605\\nThe Sign of Four, 209\\nConcave edge, 613–614, 643\\nConcept learning, 270–271, 643\\nBayesian, 337–339\\nConclusion, 242\\nConditional effects, 455–456\\nConditional planning, 457, 643\\nConditional probabilities, 329–330, 643\\ncalculating, 344–345\\ncomparing, 335–337'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 749, 'page_label': '750'}, page_content='Index 723\\ntables, 340–341, 343–344, 644\\nConflict resolution, 242, 245–247, 644\\nConflicts, 37, 644\\nConjunction, 644\\nConjunctive normal forms (CNF), 210–211, 212, 644\\nConjunctive operators, 182\\nConnect-4 (game), 166\\nConsequent, 183, 242, 644\\nConsistent, 273–274\\nConstant, 644\\nConstraint satisfaction problems (CSPs), 118, 644\\nConstraint satisfaction search, 118–121\\nConstraints, 48, 62, 644\\nrelaxing, 95\\nConstructor, 41\\nContext-free grammar, 580, 645\\nContext-sensitive grammar, 580, 645\\nContingent statements, 203–204, 645\\nContradiction, proof by, 192–193, 214–216, 645\\nContradictory assumptions, 201\\nContradictory expressions, 186–187, 192–193\\nConvex edge, 613–614, 645\\nConvolution, 611–612, 645\\nConway’s Life, 368–369, 646\\nCook, Stephen, 51, 134\\nCooperation\\nof agents, 545–546\\nin Prisoner’s Dilemma, 406\\nCopycat architecture, 9, 463, 467, 646\\nCorpus, 572–573, 646\\nCost, 107\\nCrabbe, George, Gretna Green, 143\\nCreatures, 388, 390\\nCredit assignment, 286, 302, 378, 646\\nCrichton, Michael, Prey, 543\\nCrisp set, 646\\nCritical and Miscellaneous Essays (Carlyle), 3, 465\\nCrossover, 387, 389, 390–392, 646\\napplication, 390–391\\nand genetic programming, 375, 378\\nand schemata, 401–402\\nsingle-point, 391\\ntwo-point, 391\\nuniform, 391–392, 692\\nCrossover position, 380\\nCumulative selection, 372\\nCut and splice operators, 405–406\\nCYC, 259–260, 365, 646\\nCycles, 45, 49, 647\\nD\\nDartmouth College, 9\\nDarwin, Charles, 372, 413\\nData\\ndirty, 554\\nnoisy, 282–283, 283–284\\nData-driven reasoning, 244\\nData-driven search, 73–74, 647\\nDatabase\\naccess, 572\\nof facts, 252f, 253\\nof rules, 243\\nDawkins, Richard, 390, 392, 393\\nbiomorphs, 412\\nThe Blind Watchmaker, 363, 372, 372–373\\nDeception, in genetic algorithms, 404, 647\\ncombatting, 405–406\\nDecidability, 173, 175, 200–201, 647\\nDecision tree, 56–57, 277f, 647\\nfor collaborative filtering, 357\\nDecision-tree induction, 276–278, 647\\nDecision-tree learning, 268\\nDeclarative semantics, 38–39\\nDeduction, 243, 244–245. See also Forward chaining,\\nrules of, 189, 191–195, 647\\nDeduction theorem, rule of, 195–196\\nDeductive reasoning, 201–202\\nDeep Blue, 22, 133, 143, 165\\nDeep, definition, 133\\nDeep Fritz, 165\\nDeep Junior, 143, 165\\nDeep Thought, 133\\nDefault reasoning, 467, 477–478, 647\\nDefault value, 32, 648\\nDefection, in Prisoner’s Dilemma, 406\\nDefining length, 648\\nDefuzzification, 521–522, 531–533, 648\\nof neuro-fuzzy system, 538\\nDelete list, 436\\nDemons, 38, 648\\nDeMorgan’s laws, 188, 198, 442, 443, 648\\nDemoted operators, 447\\nDempster-Shafer theory, 467, 483–485, 648\\nDendrites, 292–293'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 750, 'page_label': '751'}, page_content='724 Index\\nDennett, Daniel, 10, 11\\nDependence, 339-349\\nDependency-directed backtracking, 121, 479.\\nSee also Nonchronological backtracking\\nDepth-first iterative deepening (DFID), 88–90\\nDepth-first search, 71, 75–76, 107, 228, 648. See also\\nBacktracking search; Hill climbing, steepest\\nascent\\nand the eight queens problem, 118–121\\nexamples, 80–83\\nmaze, 81\\nsearching for gift, 81–83\\nimplementing, 83–90\\nanalysis of search, 85t\\nand minimax, 149–150\\nDepth threshold, 78, 648\\nDerivation tree, 582, 648\\nDescartes, Rene, 10, 11\\nDescendent, 45, 648\\nDescribe and match, 649\\nDiagnosis, 649\\nDialectica (Abelard), 6–7\\nDionysius, Ars Rhetorica,3\\nDirected graphs, 45, 649\\nDirectives, 243\\nDirty data, 554\\nDisagreement set, 225\\nDisambiguation, 591–592\\nDiscontinuity, 649\\nDisjunction, 649\\nDisjunctive normal forms (DNF), 211, 649\\nDisjunctive operators, 182–183\\nDistributed computing architecture, 553\\nDistributed tree search (DTS), 134\\nDistributive property, 188\\nDiversity, 649\\nDocument frequency, 595\\nDomain expert, 252, 649\\nDonne, John, An Anatomy of the World, 503\\nDouble negation, 193\\nDoyle, Jon, 478\\nDraughts, 159\\nDreyfus, Hubert, 10\\nDualism, 11, 649\\nDynamic world planning, 419, 456–457, 650\\nE\\nEach in His Own Tongue (Carruth), 387\\nEclipse, 253\\nEdelman, Gerald, M., 285\\nEdge detection, 609–612, 650\\ncanny edge detector, 611–612\\nEdges, 29, 31, 586, 650\\nconcave, 613–614\\nconvex, 613–614\\noccluding, 613–614\\nEffect axioms, 422, 427, 450–451, 650\\nEffective branching factor, 650\\nEigenfaces, 627–628\\n8-puzzle, 92–95, 425\\nEight queens problem, 118–121, 425\\ndiagram, 119f\\nand heuristic repair, 123–125\\nrelaxed version, 125–126\\nsolution, 120f\\nEisenhower, Dwight D., 421\\nElimination, rule of, 191\\nexamples, 194\\nELIZA, 8\\nEmergent behavior, 266, 365–366, 650\\nEnd-user, of expert system, 251\\nEnergy of the system, 128\\nEntropy, 278–281, 650\\nEpoch, 297, 650\\nEquilibrium, of neural networks, 306–307\\nEquivalence, logical, 175, 187–189, 198, 442, 650\\nEquivalent sample size, 354\\nError gradient, 651\\nError value, 651\\nEspecially When the October Wind (Thomas), 291\\nEstablishes links, 446\\nEuclidean distance, calculating, 317\\nEvaluation functions, in game playing, 146–148\\nas weighted linear functions, 147\\nEvans, Thomas, Analogy,9\\nEvent calculus, 467, 490–492, 651\\nEvent, defined, 491\\nEvolution, 651\\nEvolution, and artificial life, 365, 372–381, 396–397\\npredators, 413–414\\nstrategies, 373–374\\nin visual arts, 412–413\\nEvolution, theory of, 266\\nEvolutionary programming (EP), 364, 375–376, 651'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 751, 'page_label': '752'}, page_content='Index 725\\nExchanging heuristic, 126–127\\nExcluded middle, law of, 203, 515, 651\\nExecution, 422, 651\\nmonitoring, 457, 652\\nExhaustive search (Brute-force search), 53, 639, 652.\\nSee also Breadth-first search; Depth-first search;\\nGenerate and test\\nExistential quantifiers, 197, 652\\neliminating, 220–222\\nExpected value, 167\\nExpectiminimax, 167, 652\\nExpert system shell, 252f, 254–255, 652\\nExpert systems, 23–24, 241–263, 652\\narchitecture of, 252–254\\nbuilding, 32–33\\nend-user, 251\\nand frames, 34\\nfuzzy, 503, 522–533, 655\\nrule-based, 251–254\\npeople involved in, 251–252\\nExplanation system, 252, 252f\\nExponential growth, 57, 652\\nEye, human, diagram of, 607f\\nF\\nFables (Gay), 267\\nFace recognition, 464, 626–628, 652\\nFace space, 628\\nFact, 228, 653\\nFact database, 253\\nFailure nodes, 59, 653\\nFalse negatives, 598–599, 653\\nFalse positives, 598–599, 653\\nFalsum, 192, 193–194, 653\\nFeasible region, 125, 653\\nFeed-forward networks, 301, 306, 653\\nFikes, Richard E., 434\\nFilter, 612, 653\\nFinite state automaton (FSA), 366–368,\\n375–376, 579, 653–654. See also\\nTransition networks\\nFirebaugh, Morris W.,Artificial Intelligence: A Knowl-\\nedge-Based Approach, 241\\nFired rule, 244\\nFiring, of classifiers, 378, 379\\nFirst-order predicate calculus, 468.See alsoFirst-order\\npredicate logic (FOPL); Situation calculus\\nand blackboard architecture, 469, 471\\nFirst-order predicate logic (FOPL), 30, 199–200, 201,\\n558–559\\nmonotonicity, 201\\nrepresentational adequacy of, 40–41\\nresolution in, 218–219\\nFitness, 131, 387, 389, 393, 654\\ndetermining, 373–374, 378, 379, 390\\ncalculating fitness ratio, 393–396\\nmetrics for, 411–412\\nof offspring, 380\\nuser choice, 412\\nFluents, 490, 654\\nFocus of attention, 472\\nFogel, David, 164\\nFogel, Lawrence, 375\\nFoothills, 101–103, 654\\nFOPL (first-order predicate logic). See First-order predi-\\ncate logic (FOPL)\\nForgetting factor, 654\\nFormal language, 571–572, 654\\nForward chaining, 242, 244–245, 469, 654.See also CLIPS\\n(C language integrated production system);\\nData-driven search; Deduction\\ncompared to backward chaining, 249–251\\nand STRIPS, 439\\nForward checking, 121\\nForward pruning, 654\\nFox, G. C., 133\\nFractional knapsack problem, 110–111\\nFrame axioms, 422, 427, 434–435, 450–451, 655\\nFrame-based representations\\nand FOPL, 40–41\\nof knowledge, 259–260\\nFrame problem, 419, 422, 427–428, 480, 655\\nFrame system, 32, 655\\ngraphic representations, 33f\\nFrames, 32–41, 173, 469, 654.See Semantic nets\\ncombining, with rules, 40–41\\ninstance, 663\\nslots as, 35–36\\nFrankenstein (Shelley), 3\\nFree variable, 655\\nFunctions\\nand classification of data, 268\\nlinearly separable, 299–300, 301\\nin predicate calculus, 199\\nFundamental memories, 307, 655\\nFurst, Merrick, 454'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 752, 'page_label': '753'}, page_content='726 Index\\nFuzzification, 517–519, 655\\nof neuro-fuzzy system, 536–537\\nFuzzy expert systems, 522–533, 655\\nbuilding, 522–533\\ndefining fuzzy rules, 527–528\\ndefining fuzzy sets, 523–527\\nusing, 528–533\\ndefuzzification, 531–533\\nFuzzy inference, 516–522, 655\\napplying fuzzy values, 519–520\\ndefuzzification, 521–522\\nfuzzification, 517–519\\nFuzzy logic, 23, 347, 463, 511–515, 655\\napplication, 515–516\\nand fuzzy variables, 511–512\\nand truth tables, 512–515\\nFuzzy operators, 503\\nFuzzy reasoning, 463, 503–541, 655\\nbivalent and multivalent logics, 504\\nfuzzy sets, 505–511\\nand linguistic variables, 504–505\\nsystems that learn, 534–538\\nFuzzy rules, 516, 656\\ndefining, 527–528\\nFuzzy sets, 503, 505–507\\ndefining, 523–527\\nhedges, 510–511\\nmembership functions, 507\\noperators, 508–510\\nFuzzy variables, 511–515\\nG\\nGame of life, 368–369, 664\\nGame playing, 143–171, 560–561\\nassumptions, 145–146\\nBackgammon, 167\\nBridge, 165\\nCheckers, 159–164, 165\\nChess, 22, 164–165, 268–269\\ndraughts, 159\\nevaluation functions in, 146–148\\ngames of chance, 166–167\\nGo, 165–166\\nGo-Moku, 166\\nOthello, 165–166\\nPrisoner’s Dilemma. See Prisoner’s Dilemma\\nReversi, 166\\nTic-tac-toe, 144–145, 166\\nzero-sum, 146, 695\\nGame trees, 63–64, 78, 144–145, 656\\nsearching, 148–149\\nGaussian function, 611\\nGay, John,Fables, 267\\nGelatt, C. D., 130\\nGeneral Magic, 552\\nGeneral Problem Solver (GPS), 6, 9, 422, 430, 656\\nGeneral-to-specific ordering, 272–273, 273–274\\nGeneralization, 33, 656\\nGeneralized delta rule, 305\\nGenerate and test, 74–75, 98, 656\\nand cyyptographic problems, 122–123\\nGeneration, in Conway’s Life, 368\\nGenerator, 74\\nGenes, 372, 388–389, 656\\nGenetic algorithms, 126, 131, 266, 321–322, 387–418,\\n555, 656\\ndeception, 404\\nmessy, 405–406\\nand optimization of mathematical function, 393–396\\nfor Prisoner’s Dilemma, 410\\nproblems applied to, 414\\nrepresentations, 388–389\\nrunning, 389\\ntermination criteria, 392–393\\nwhy they work, 396–404\\nbuilding-block hypothesis, 403–404\\nschemata, 397–404\\nGenetic programming, 364, 374–375, 656\\nGenghis, 563\\nGenotype, 390, 412, 657\\nGeometric progression, 657\\nGeorgeff, M., 558\\nGlider gun, in Conway’s Life, 369\\nGlider, in Conway’s Life, 369\\nGlobal maximum, 102, 657\\nGo (game), 165–166\\nGo-Moku (game), 166\\nGoal, 228, 248, 657\\nGoal-based agents, 548–549, 657\\nGoal-driven reasoning, 248\\nGoal-driven search, 61, 73–74, 657\\nGoal nodes, 45, 657\\nGoal reduction, 57, 657\\nGoal state, 72\\ndiagram, 452f\\nin STRIPS, 437'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 753, 'page_label': '754'}, page_content='Index 727\\nGoal trees, 58–64, 657\\nuses of, 61–64\\ngames, 63–64\\nmap coloring, 61–62\\nparsing sentences, 63\\nproving theorems, 62–63\\nGoals\\nand- and or-, 58–59\\nand planning, 428–430\\nvs. plans, 62\\nand PROLOG, 228, 229\\nroot, 59, 682\\nGödel implication, 515, 516, 657\\ndefinition, 513\\nGödel’s incompleteness theorem, 21\\nGoldberg, David E., 405, 406\\nGradient descent, 304, 658\\nGrammars, 575, 579–580, 658\\nGranularity, 181\\nGrapes of Wrath, The(Steinbeck), 291\\nGraphPlan algorithm, 434, 451, 454–455, 658\\nGraphs, 29, 658\\ncoloring, 217–218\\ndirected vs. nondirected, 45\\nleveling off, 454\\nplanning, 451–455, 676\\nGreedy search, 110–112\\nGretna Green (Crabbe), 143\\nGround atoms, 230–231\\nGround instance, 230, 658\\nGround terms, 229, 658\\nGSAT system, 450\\nH\\nHAL, 19, 21–22, 22–23\\nHalting problem, 21, 658\\nHamlet (Shakespeare), 503\\nHamming distance, 313, 658\\nHeadless clause, 228\\nHEARSAY, 472\\nHEARSAY II, 469, 471, 472, 659\\nHebb, Donald O., 285, 320\\nHebbian learning, 285, 295, 304, 320–321\\nHebb’s law, 292, 320\\nHedges, 503, 510–511, 659\\nHeisenberg, Werner,Physics and Beyond, 241\\nHenry V (Shakespeare), 143\\nHerbrand universes, 229–233, 659\\ndefinition, 229\\nexample, 232–233\\nHerbrand base, 230–231, 659\\nHerbrand interpretations, 231–232, 659\\nHeteroassociative memory, 313, 659\\nHeuristic repair method, 123–125, 660\\nHeuristic search methods, 90–98\\nchoosing, 92–93\\nevaluation function, 91, 659\\nexamples, 92–98\\nHeuristics, 53, 72, 91, 659\\nexchanging, 126–127\\nHidden layer, of multilayer network, 301, 660\\nHill climbing, 98–103, 126, 392, 396, 660\\nanalysis of search, 100t\\nand evolution strategies, 373\\nand foothills, plateaus and ridges, 101–103\\nsteepest ascent, 98–100\\nHillis, Danny, 413\\nHodge, A.,Alan Turing the Enigma of Intelligence, 291\\nHolland, John H., 377, 380, 388, 397, 403\\nAdaptation in Natural and Artificial Systems, 387\\nHopfield, John, 307\\nHopfield networks, 292, 307–313, 660\\napplication, 310–313\\nstages of, 312–313\\ncompared to BAMs, 314, 315–316\\nHorizon problem, 152, 660\\nHorizontal layer architecture, 559.See also Subsumption\\narchitecture\\nHorn clauses, 173, 227–229, 660\\nHuman language, 571, 660\\nand ambiguity, 589–592\\nand  knowledge representation, 465–466\\nHybrid agent, 660\\nHyperbolic tangent function, 305\\nHypothesis, 248, 271, 661\\nmost general, 272\\nmost specific, 272\\nI\\nIBM corporation, 42, 165\\nID3 algorithm, 268, 278, 661\\ninductive bias of, 281\\nIF...THEN statements, 242, 243, 328. See also\\nImplication\\nIff (if and only if), 177–178, 184\\nImage capture, 661'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 754, 'page_label': '755'}, page_content='728 Index\\nImage processing, 608–615\\nedge detection, 609–612\\nImage recognition, 661\\nImplementation, 38–39\\nImplication, 178, 180, 183–184, 661.See also IF...THEN\\nstatements\\nand fuzzy logic, 513–515\\nmaterial, 183\\nImplied introduction, 193\\nIncompleteness theorem, of Gödel, 21\\nIndependence, 661\\nInductive bias, 265, 276, 283, 661\\nof ID3 algorithm, 281\\nof nearest neighbor algorithm, 283–284\\nInductive-learning methods, 270–271\\nInductive reasoning, 201–202, 662\\nand Bayesian theory, 338–339\\nInference engine, 243, 252f, 253, 662\\nInference rules, 191–195, 662\\nInformation agents (Internet agents), 553–554, 572, 662\\nInformation gain, 278–281, 662\\nInformation retrieval (IR), 464, 572–573, 594–598, 662\\nInformed search methods, 72, 662\\nInge, Charles, On Monsieur Coue, 433\\nInheritance, 31–32, 662\\nand frames, 34–35, 469\\nmultiple, 36–37, 42\\nInitial state, 72, 662\\nInput layer, of multilayer network, 301\\nInstance-based learning, 283–284\\nInstance constructor, 662\\nInstance frame, 32, 663\\nInstances, 30, 41–42, 436–437, 662\\nInstantiated variables, 436–437\\nIntelligence\\nof agents, 544–545\\ndefining, 4, 10–11\\nIntelligent agents, 23, 434, 463, 543–569, 663\\nInterface agents, 551–552, 663\\nInternet agents (information agents), 553–554, 572, 663\\nInterpretation, 468, 663\\nInterpreter, 243\\nInteRRaP , 559–560\\nIntroduction, rule of, 191\\nexamples, 194, 194–195\\nInverse, 36\\nInversion, and deception, 404, 663\\nIR (information retrieval), 464\\nIrrevocability, of search methods, 80, 663\\nIsland, 137–138\\nIsland-driven search, 137\\nIterated local search, 127, 663\\nIterative approach, to segmenting, 613\\nIterative-deepening A* (IDA), 132, 160\\nIterative deepening search (IDS), 88–90\\nJ\\nJacobs, R. A., 305–306\\nJava, 11, 12–13, 32, 41–42, 173, 552\\nJESS, 253\\nJob shop scheduling, 458\\nJohnson, Samuel, 465\\nJoint. See Joint probability distribution\\nJoint probability distribution, 330, 344–345, 663–664\\ncomputing, 342\\nand dependence, 347–349\\nJustification-based truth maintenance system (JTMS),\\n478–479\\nK\\nK-exchange, 126–127\\nKahneman, Daniel, 550\\nKarp, Richard,Randomized Parallel Algorithms for Back-\\ntrack Search and Branch-and-Bound Computa-\\ntion, 135\\nKasparov, Garry, 22, 143, 165\\nKing, Ron, 161\\nKinny, D., 558\\nKirkpatrick, S., 130\\nKlee, Paul, Pedagogical Sketchbook, 543\\nKnapsack problem, 110–112, 414\\nKnight’s T our, 414\\nKnowledge\\nincomplete, 202\\nmeta, 247\\nKnowledge acquisition, 494\\nKnowledge base, 243, 252f, 664\\nKnowledge base editor, 252f, 253\\nKnowledge base engineer, 664\\nKnowledge engineer, 252, 253\\nKnowledge engineering, 254, 467, 494–495\\nKnowledge, importance of, 6\\nKnowledge Level, The (Newell), 209\\nKnowledge representation, 11-12, 28–67, 465–501'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 755, 'page_label': '756'}, page_content='Index 729\\nblackboard architecture, 469–472\\ncopycat architecture, 474–476\\nand human language, 465–466\\nimportance of, 28–29, 467\\nnonmonotonic reasoning, 476–487\\nrules for, 242–243\\nscripts, 472–474\\nstructured, 469\\nKnowledge sources, 469–470\\nKohonen maps, 265, 285, 292, 316–320, 664\\npurpose of, 316\\nKosko, Bart, 313\\nKozierok, R., 551\\nL\\nL-systems, 266, 376–377, 666\\nLa Pensee Sauvage (Levi-Strauss), 571\\nLa Vie d’Henri Brulard (Stendahl), 503\\nLafferty, Don, 161\\nLangton, Christopher G., 371\\nLanguage. See also Linguistics, and AI\\nformal, 571–572\\nhuman, 468\\nand knowledge representation, 465–466\\nspoken, 472\\nnatural, 571–572\\nLanguage identification, 593–594\\nLanguages, of logic. See First-order predicate calculus;\\nPropositional calculus\\nLanguages, programming, 468\\nC++, 12–13, 32\\nJava, 11, 12–13, 32\\nLISP , 9, 11, 13, 388\\noverview, 14–15\\nobject-oriented. See C++; Java\\nPROLOG, 13, 13–14\\nLatham, William, 412\\nLaw of the excluded middle, 203, 515, 651\\nLeaf nodes, 45, 664\\nLeak nodes, 348\\nLearning ability, of agents, 545\\nLearning agents, 561–562, 664\\nLearning algorithm, 273–274\\nLearning\\ncentralized vs. decentralized, 562\\ncompetitive, 316, 642 \\nmachine. See Machine learning\\nin multiagent systems, 267\\nLearning neural networks, 284–285\\nLearning rate, 297\\nLeast commitment, principle of, 447–448\\nLeast-constraining value, 122\\nLegal rules, 370\\nLeibniz, Gottfried, 7\\nLenat, Douglas B., Programming Artificial \\nIntelligence, 241\\nLevi-Strauss, Claude, La Pensee Sauvage, 571\\nLexical ambiguity, 589\\nLexicon, 664\\nLife, defining, 364–365\\nLife, game of, 664\\nLIFO, 84\\nLii, H. Penny, 469, 470\\nLikelihood, 338, 339, 504, 665\\nrelative, 681\\nLindenmayer, Aristid, 376\\nLinear threshold function, 665\\nof artificial neurons, 294\\nLinear time temporal logic, 488\\nLinearly separable functions, 299–300, 301, 665\\nLinguistic variables, 503, 504–505, 665\\nLinguistics, and AI, 3, 4, 11–12\\nLinks, protected, 679\\nLISP , 9, 11, 13, 255, 388, 665\\nLiteral, definition, 211\\nLiterals, 227, 665\\nLoad balancing, 134\\nLocal ambiguity, 590\\nLocal maxima, 101, 126, 392, 411, 665–666\\nLocal minima, 130\\nLocal optimization, 126, 666\\nLocal search methods, 117, 126–128, 666.See also\\nGenetic algorithms\\nLogic, 5, 6–7, 175–208\\nclassical, 177, 203, 504, 641\\nand change, 205\\ndefinition, 176\\nfirst-order predicate (FOPL). See First-order predi-\\ncate logic (FOPL)\\nfuzzy. See Fuzzy logic\\nlogical operators, 177–181\\ntranslating, 178–181\\nmodal, 668\\npropositional, 175–196'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 756, 'page_label': '757'}, page_content='730 Index\\npropositional calculus, 189–196\\nuse, in AI, 176–177\\nLogical systems, properties of, 175\\nLogics, nonclassical, 467\\nLogistello, 166\\nLongest-matching strategy, 246–247\\nLoops, as self-reproducing systems, 371\\nLove’s Labours Lost (Shakespeare), 28\\nM\\nM-estimate, 354\\nMachine learning, 265, 267–289\\nalgorithms, 273–274, 281, 283–284, 286\\nand artificial neural networks, 284–285\\ncandidate elimination, 275\\nconcept learning, 270–271\\nand decision-tree induction, 276–278\\ngeneral-to-specific ordering, 272–273, 273–274\\nand information gain, 278–281\\nand the problem of overfitting, 282–283\\nreinforcement, 286\\nrote learning, 270\\nand supervised learning, 285\\ntraining, 268–270\\nand unsupervised learning, 285\\nversion spaces, 274–275\\nMachine translation, 592, 666\\nMachine vision, 464, 605–628\\nand face recognition, 626–628\\nimage processing, 608–615\\nmotion in, 623–625\\nparts decomposition method, 626\\nusing, 625–626\\nusing texture in, 615–623\\nMaes, P ., 551\\nMamdani, Ebrahim, 516\\nMamdani inference, 516, 666\\nManfred (Byron), 571\\nManhattan distances, 93–94\\nMap coloring, 666\\nMAP hypothesis, 351\\nMarkov decision processes (MDPs), 561\\nMassively parallel, 133\\nMaterial implication, 183\\nMateus, Paulo, 456\\nMathematical function, optimization of, 393–396\\nMatrix arithmetic, and Hopfield networks, 307–310\\nMax node, 149\\nMaximum a posteriori, 351, 666\\nMcCarthy, John, 9, 480\\nMcCulloch, W. S., 12, 291, 293\\nMeans-ends analysis, 419, 422, 428–430, 667\\nand STRIPS, 435, 440–441\\nMedical uses, for AI, 23, 73\\nmedical diagnosis, 331–332, 485–487\\nMembership function, 667\\nMemory, 306, 307, 667\\nautoassociative, 313\\nand Hopfield networks, 312–313\\nMental situation calculus, 467, 492–494, 667\\nMessy genetic algorithms (mGAs), 405–406, 667\\nMeta knowledge, 247, 667\\nMeta rules, 247–248, 668\\nMetaheuristics, 126, 667\\nMethods, weak vs. strong, 5–6\\nMetrics, 69, 668\\nand artificial evolution, 373–374, 390\\nfor determining fitness, 411–412\\nMetropolis Monte Carlo simulation, 128, 129\\nMin-conflicts heuristic, 123–125\\nMin node, 149–150\\nMinimax, 149–151\\nMinimax algorithm, 69, 149–153, 668\\nExpectiminimax, 167\\nlimitations of, 163–164\\nMinsky, Marvin, Steps Toward Artificial Intelligence,28\\nMIPS (millions of instructions per second), 160\\nMissionaries and cannibals, 47\\nMIT Mobot Lab, 563\\nMitchell, Melanie, 9, 474\\nMobile agents, 546, 552–553, 668\\nModal logics, 177, 203–204, 668\\nreasoning in, 204\\nModal operator, M, 668\\nModus ponens, rule of, 192, 201, 668\\nexamples, 193–194, 194–195\\nand fuzzy logic, 514, 515\\nMomentum, and backpropagation, 305\\nMonitor, 470\\nMonotonic heuristic, 96\\nMonotonic reasoning systems. See also Predicate logic;\\nPropositional logic\\ndefined, 476–477\\nMonotonicity, 41, 95–96, 175, 196, 201, 668–669'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 757, 'page_label': '758'}, page_content='Index 731\\nMonte Carlo simulation, 128–130\\nMorphologic analysis, 573, 574–575, 669\\nMost-constrained variables, 121–122\\nMost general hypothesis, 272, 669\\nMost general unifier (MGU), 224, 669\\nMost specific hypothesis, 272, 669\\nMotion field, 669\\nMotion, interpreting, 623–625\\nMove notation, 426\\nMultiagent system, 669\\nMultiagent systems, 545, 554–556\\nlearning, 562\\nMultilayer neural networks, 291–292, 300–306, 669\\nMultiple inheritance, 36–37, 42, 670\\nMultivalent logic, 504, 670\\nMurakami, Takeshi, 166\\nMutation, artificial, 372–373, 373, 375, 378, 380–381,\\n387, 392, 405–406, 670\\nand schemata, 402–403\\nMutex conditions, 455\\nMutexes, 452, 453–454, 454–455\\nMutual exclusion information (mutex).See Mutexes\\nMYCIN, 255–256, 485–487, 670\\nN\\nN-gram, 670\\nNaive Bayes’ classifier, 351–356, 670\\nNatural language, 571–572\\nNatural language processing (NPL), 11, 12, 464, 472,\\n573–592, 670\\nand ambiguity, 589–592\\nBackus-Naur form (BNF), 575–579\\ndefinition, 572\\nmorphological analysis, 574–575\\nNatural selection, 372\\nNatural Selection, Incorporated, 164\\nNearest neighbor algorithm, 283–284, 671\\nNearest neighbor heuristic, 53, 671\\nNegation, 179–180, 181–182\\nNegation by failure, 480, 671\\nNegation, double, 193\\nNegative training example, 271\\nNegatives, false, 598–599, 653\\nNeural networks, 126, 265, 284–285, 291–326, 671\\nevolving, 321–322\\nmultilayer, 291–292, 300–306\\narchitecture of, 301–302\\nbackpropagation in, 302–306\\nBidirectional associative memories (BAMs),\\n313–316\\nfeed-forward, 301, 306\\nKohonen maps, 316–320\\nrecurrent, 292, 306–313\\nstability or equilibrium of, 306–307\\nunstable, 307\\nunsupervised learning networks, 316–321\\nNeuro-fuzzy systems, 503, 534–538, 671\\ndefuzzification layer, 538\\nfuzzification layer, 536–537\\nfuzzy rule layer, 537\\ninput layer, 536\\nlearning mechanism, 538\\noutput membership function layer, 537\\nNeurons, 284, 671\\nartificial, 293–295\\nbiological, 292–293\\nNewborn, Monty, 164\\nNewell, Alan, 6, 9, 430\\nThe Knowledge Level, 209\\nNewton, Isaac, 7\\nNight Thoughts (Y oung), 209\\nNilsson, Nils J., 434\\nNodes, 29, 31, 671\\nand- and or-, 58–59\\nand-nodes, 58–59\\ngoal, 45\\nleak, 348\\nmax, 149–150\\nmin, 149–150\\nroot, 45, 682\\nsuccess and failure, 59\\nNoise parameters, 348\\nNoisy data, 282–283, 283–284\\nNoisy logical relationships, 346, 347–349\\nNoisy-v function, 346, 347–349\\nNonchronological backtracking, 76, 121, 137–138, 671\\nNoncontingent statements, 203–204, 672\\nNondeterministic search, 136–137\\nNondirected graphs, 45, 672\\nNonmonotonic, 672\\nNonmonotonic logics, 467\\nNonmonotonic reasoning, 467, 476–487.See also Fuzzy\\nlogic\\nabductive, 482–483'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 758, 'page_label': '759'}, page_content='732 Index\\ncircumscription, 480–482\\nclosed world assumption, 480\\ndefault reasoning, 477–478\\nDempster-Shafer theory, 483–485\\nMYCIN, 485–487\\nnonmonotonic logic, 477\\nramification problem, 480\\ntruth maintenance systems (TMS), 478–479\\nNonterminal symbol, 577, 672\\nNormal distribution, 373, 672\\nNormal forms, 210–212\\nfor predicate logic, 219–220\\nprenex, 219–220\\nNormalization, 335–337, 672\\nN o r v i g ,P e t e r ,1 6 7\\nNot (operator), 179–180, 181–182\\nNoun, 672\\nNoun phrases, 576, 581–582, 673\\nNP-complete, 50–51, 673\\nNPL (natural language processing), 464\\nO\\nObject, 242\\nObject-oriented programming, 41–42\\nOccam’s razor, 276, 283, 673\\nOccluding edge, 613–614, 673\\nOffspring, in artificial evolution, 373\\ndetermining fitness of, 380\\nOpen world assumption, 480\\nOpening book, 161, 673\\nOperator-based planning, 434\\nOperator schema, 436–437, 673\\nOperators, action\\ndemoted vs. promoted, 447\\nand STRIPS, 435–437\\nOperators, logical, 177–184\\nand, 178, 242, 296, 299\\nbinary, 182, 637\\nimplication, 180–181, 183–184, 242\\nnot, 179–180, 181–182\\nor, 182–183, 242, 296, 299, 299–300\\nunary, 181, 392, 692\\nusing, in truth tables, 181–184\\nOpportunistic reasoning model, 469–470\\nOPS5, 253\\nOptical field, 623\\nOptical flow, 623, 673\\nOptimal classification, Bayes’ , 349–351\\nOptimal classifier, Bayes’ , 637\\nOptimal path, 91, 673\\nidentifying, 107–112\\nOptimality, of search methods, 79–80, 673\\nOr-goals, 58–59, 674\\nOr-introduction, rule of, 192\\nOr-nodes, 58–59, 674\\nOr (operator), 179, 182–183\\nOthello (game), 165, 166\\nOutput layer, of multilayer network, 301\\nOverfitting, problem of, 282–283, 674\\nOverridden, 32\\nOverriding, 674\\nOverspecified chromosome, 674\\nP\\nP class problems, 50–51\\nParadoxes, well-known, and fuzzy logic, 515–516\\nParallel search, 117–118, 132–133, 674\\nParallel window search (PWS), 134\\nParent, in artificial evolution, 373\\nParse trees, 581–582, 590\\nParser, 63, 674\\nParsing, 575, 581–588\\nchart, 585–588\\nTransition networks, 582–585\\nPartial order, 674\\nPartial order planning, 434, 444–447, 675.See also\\nGraphPlan algorithm\\nPartial path, 45, 675\\nPartially observable Markov decision processes\\n(POMDPs), 561\\nParts decomposition method, and machine vision, 626\\nPath, 45, 675\\nPath-based evaluation function, 108\\nPattern-matching, 675\\nPattern-matching clauses, 40–41\\nPDDL. See planning domain definition language\\nPedagogical Sketchbook (Klee), 543\\nPerceptron training rule, 297\\nPerceptrons, 291, 295–300, 675\\nand Boolean operators, 296, 299–300\\nPersistence actions, 451, 453\\nPhenotype, 390, 412, 675\\nPhilosophische Untersuchunge (Wittgenstein), 571\\nPhilosophy, and AI, 3, 4, 10–11\\nPhonology, 573\\nPhysics and Beyond (Heisenberg), 241'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 759, 'page_label': '760'}, page_content='Index 733\\nPictures, evolving, 412–413\\nPitts, W., 12, 291, 293\\nPixel, 675\\nPlan, 675\\nPlanner, 421–422\\nPlanning, 469, 675\\nAction description language (ADL), 455–456\\ncase-based systems, 457–458\\nconditional, 457, 643\\ndynamic world, 419, 456–457, 650\\nand goal-based agents, 548\\ngraphs, 451–455\\nmutexes, 452\\npartial order, 434, 444–447\\nprobabilistic, 419, 456, 677\\npropositional, 448–450\\nSAT, 450–451, 683\\nand scheduling, 458–459\\nas search, 423–425\\nvs. executing, 435–436\\nPlanning domain definition language (PDDL), 456, 675\\nPlanning graph, 451–455, 676\\nPlanning methods, 433–462\\nPlans\\nvs. goals, 62\\npartial order. See also GraphPlan algorithm\\nPlateaus, 101–103\\nPlato, 6, 10\\nPly, 77, 676\\nPoetics (Aristotle), 327\\nPoint fluent, 494\\nPolaroid, 381\\nPolynomial time, 50–51\\nPope, Alexander,An Essay on Man, 433\\nPopulation, 131\\nin genetic algorithm, 388, 676\\nsize, 389\\nPorter’s stemmer, 597–598\\nPositives, false, 598–599, 653\\nPossible world, 479, 676\\nPosterior probability, 331, 676\\ncalculating, 345–346, 350-354\\nestimating, 354–355\\nhighest, 351\\nPraed, Winthrop Mackworth,The T alented Man, 175\\nPragmatic analysis, 676\\nand natural language processing, 573\\nPrecedence, 180\\nPrecision, 553, 598–599, 676\\nPrecondition, 677\\nPredators, and co-evolution, 413–414\\nPredecessor, 677\\nPredicate calculus, 173, 175, 196–199.See also First-order\\npredicate logic (FOPL)\\nand change, 210\\ninterpretation, 468\\nsyntax of, 196–197\\nPredicate logic, 30, 179, 476\\nnormal forms for, 219–220\\nPremises, 176, 677\\nPrenex normal form, 219–220, 677\\nPrey (Crichton), 543\\nPrinciple component analysis, 627–628, 677\\nPrinciple of least commitment, 447–448\\nPrior probability, 331, 677\\nPrisoner’s Dilemma, 266, 388, 406–411, 677\\nchoice of opponents, 410–411\\ndiversity, 411–412\\nand predators, 413–414\\nstrategies, 407–410\\nevolution of, 410\\nrepresentation of, 407–408\\ntit-for-tat, 409\\nProbabilistic planning, 419, 456, 677\\nProbabilistic reasoning, 266, 327–361, 504, 678. See also\\nBayesian; Bayes’ Theoerem and propositional\\nlogic, 328-330\\nProbability, 504, 678\\nand ambiguity, 591\\ncalculating, 330-332, 350, 351\\nconditional, 329-330, 643\\ncomparing, 334-335\\ntables, 340-341\\nestimating, 354–355\\njoint probability distributions, 330\\nposterior. See Posterior probability\\nProbability theory, 201, 328–330\\nand dependence, 339-349\\nProblem reduction, 57–58, 678\\ntop down vs. bottom up, 60–61\\nProcedural attachments, 37, 678\\nProcedural semantics, 38–39\\nProcedures, 37–38, 678\\nProduct rule, 331, 678\\nProduction rule, 678\\nProduction systems, 469. See Expert systems'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 760, 'page_label': '761'}, page_content='734 Index\\nProgramming\\nevolutionary, 375–376\\ngenetic, 374–375\\nProgramming Artificial Intelligence (Lenat), 241\\nPROLOG, 13, 173, 679\\nand closed world assumption, 480\\nand Horn clauses, 227–229\\noverview, 13–14\\nand resolution, 210\\nPromoted operators, 447\\nProof by contradiction, 192–193, 214–216\\nProof by refutation, 173, 214–216\\nProposition letters, 189\\nPropositional calculus, 173, 175, 189, 189–196, 679\\nrules of deduction, 190–196\\nsemantics, 190\\nsyntax, 189–190\\nPropositional logic, 175–196, 347, 468, 476, 679\\nlogical operators, 177–178\\nmonotonicity, 201\\npropositional calculus, 189–196\\nresolution in, 210–216\\nrules of inference, 191–196\\nsemantics, 190\\nsyntax, 189–190\\ntranslating, 178–181\\nand truth tables, 181–184\\ncomplex, 184–189\\nPropositional planning, 448–450, 679\\nPropositional symbols, 189\\nProtected causal links, 446–447\\nProtected links, 679\\nPruning, 679\\nPsychology, and AI, 3, 4, 12\\nPure and-or tree, 64, 679\\nPush, 435\\nQ\\nQuantifiers, 679\\napplication, 199–200\\nexistential, 197\\neliminating, 220–222\\nmoving, 220–222\\nuniversal, 197\\nQuantum physics, 504\\nQuenching, 130\\nQueue, 83, 84\\nQuinlan, J. R., 278\\nR\\nRabbi Ben Ezra (Browning), 421\\nRamification problem, 480, 679\\nRamps, 413\\nRandomized Parallel Algorithms for Backtrack Search\\nand Branch-and Bound Computation (Karp and\\nZhang), 135\\nRationality, 550\\nand game playing, 146\\nRationality, and game playing, 149, 680\\nReactive agents, 547–548, 559, 680\\nReal-time A*, 131–132\\nReal-world systems, 573–574\\nRecall, 553, 598–599, 680\\nRechenberg, Ingo, 373\\nRecommendation rule, 243\\nRecurrent networks, 301, 306–313, 680\\nHopfield networks, 307–313\\nRecursive depth, 84\\nRecursively enumerable, 680\\nRecursively enumerable grammars, 580\\nReductio ad absurdum, 192–193, 680\\nexamples, 194–195\\nReferential ambiguity, 589–590\\nReflex agents, 547–548, 680.See also Reactive agents\\nRefutation proof, 192–193, 214–216, 680\\nRegular expression, 580, 680\\nRegular grammar, 681\\nRegular languages, 579–580, 681\\nReinforcement learning, 286, 681\\nRelative likelihood, 681\\nRelaxed problem, 95, 125–126, 681\\nRelaxing, 94\\nReligio Medici (Browne), 363\\nReplanning, 457, 681\\nRepresentational adequacy, 40–41, 467, 681\\nRepresentational frame problem, 427–428\\nRepresentational methods\\nblackboard architecture, 467\\ncopycat architecture, 467\\nscripts, 467\\nRepresentations, 681\\nof frame problem, 427–428\\nfor genetic algorithms, 388–389\\nof strategy, for Prisoner’s Dilemma, 407–408\\nReproduction, 389–390, 395–396.See also Crossover\\nin classifier system, 380–381\\nand messy genetic algorithms, 405–406'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 761, 'page_label': '762'}, page_content='Index 735\\nand schemata, 399–403\\nsexual vs. asexual, 373\\nRepublic: The Revolution, 24\\nResolution, 209–239, 682\\nalgorithm, 226–227\\napplication, 216–218\\nexample, 233–236\\nin predicate logic, 218–219\\nin propositional logic, 210–216\\nnormal forms, 210–212\\nrule, 213–214\\nin PROLOG, 228\\nrules, 212–216\\nand STRIPS, 441–443\\nResolution, automation of, 173\\nResolution (Robinson), 209\\nResolvent, 213\\nResult function, 426\\nRete algorithm, 242, 253–254, 682\\nReversi (game), 166\\nRewrite rule, 682\\nRewrite rules, 577–579, 580\\nReynolds, Craig, 366, 555\\nRidges, 101–103, 682\\nRobinson, Alan, Resolution, 209\\nRobot navigation, 414\\nRobotic agents, 562–563, 682\\nRobots, 23, 423, 434, 438–443, 682\\ncontrol mechanisms, 557\\nmobile, 552\\nRomeo and Juliet (Shakespeare), 267\\nRoot goals, 59, 682\\nRoot nodes, 45, 682\\nRosenblatt, F., 295, 296\\nRote learning, 270, 682\\nRoulette-wheel selection, 394, 682\\nRule-based systems, 30, 34, 243–251, 683\\nand backward chaining, 248–251\\nbackward chaining in, 257–259\\nand conflict resolution, 245–247\\nand deduction, 244–245\\nand forward chaining, 244–245\\nand longest-matching strategy, 246–247\\nand meta rules, 247–248\\nRule relation, 227\\nRules, 173, 228, 683\\nas directives, 243\\nof inference, 191–195\\nlegal, 370\\npurpose of, 242\\nrecommendation, 243\\nresolution, 212–216\\ntotalistic, 370\\nRun, of genetic algorithm, 392–393\\nRuskin, John, Seven Lamps of Architecture, 28\\nRussell, Stuart, 167\\nRussell’s paradox, 515\\nS\\nS-expressions, 388, 684\\nSamuel, Arthur,Some Studies in Machine Learning Using\\nthe Game of Checkers, 160\\nSAT planning, 450–451, 683\\nSatisfiability, 175, 187, 231–233, 683\\nSatisfiability problem, 51\\nSatisfiability problem (SAT)\\nand propositional notation, 450–451\\nsystematic approach, 450\\nSchaeffer, Jonathan, 143, 160, 163, 164\\nSchank, 473\\nScheduling, 683\\nScheduling, and planning, 458–459\\nSchema, 472, 559, 683.See also Scripts\\nSchema theorem, 403, 683\\nSchemata, 387, 397–403\\nbuilding blocks, 403–404\\nand crossover, 401–402\\nand mutation, 402–403\\nand reproduction, 399–403\\nSchwefel, Hans-Paul, 373\\nScrabble, 167\\nScripts, 20, 467, 683\\ndefinition, 472\\nSearch, 684\\ndata-driven, 73\\nand goal-based agents, 548\\ngoal-driven, 73, 657\\niterated, 663\\nproblem solving as, 72\\nSearch engines, 88, 135–136\\nSearch methods, 71–116\\nbreadth-first, 76–78\\nimplementing, 86–88\\ndata-driven or goal-driven, 73–74\\ndepth-first, 75–76, 77, 80–83\\nimplementing, 83–86'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 762, 'page_label': '763'}, page_content='736 Index\\ndepth-first iterative deepening, 88–90\\ngenerate and test, 74–75\\nheuristics, 90–97\\nhill climbing, 98–103\\ninformed vs. uninformed, 91–92\\nlocal, 666\\nparallel, 674\\nand planning, 423–425\\nproblem solving as, 72\\nproperties of, 78–80\\ncompleteness, 79\\ncomplexity, 78–79\\nirrevocability, 80\\nmonotonicity, 95–96\\noptimality, 79–80\\nSearch spaces, 42–44, 72, 684\\nSearch trees, 684\\ndiagram, 46f\\nexamples\\ndescribe and match, 56–57\\nmissionaries and cannibals, 47–50\\ntowers of Hanoi, 54–56\\ntraveling salesman, 50–54\\nfor plan, 424–425\\nRete algorithm. See Rete algorithm\\nand STRIPS, 439–440\\nSearching, 49\\nSearle, John, 20\\nSegmentation, 612–613, 684\\nSelection, artificial, 372–373\\nSelection, evolutionary, 372\\nSelf-organizing feature map, 316. See also Kohonen maps\\nSelf-reproducing systems, 371–372\\nSelman, B., 450\\nSemantic ambiguity, 589\\nSemantic analysis, 588–589, 684\\nSemantic nets, 29–31, 588–589, 684\\ndiagram, 30f\\nframe system for, 32–33\\nSemantic trees, 44–57, 684\\ndiagram, 44f\\nsearch trees, 46–57\\nSemantics, 38–39\\nand natural language processing, 573\\nof propositional logic, 190\\nand representations, 468 \\nSen, Sandip, Learning in Multiagent Systems, 267\\nSentence, well-formed, 189–190, 199\\nSet notation, of propositional logic, 189\\nSeven Lamps of Architecture (Ruskin), 28\\nSexual reproduction, 373\\nShakespeare, William\\nHamlet, 503\\nHenry V ,143\\nLove’s Labours Lost, 28\\nRomeo and Juliet, 267\\nShaw, George Bernard, 71\\nShaw, J. C., 430\\nShelley, Mary,Frankenstein, 3\\nShepard, D., 283\\nShepard’s method, 283, 685\\nSigmoid function, 294f, 302, 305, 685\\nSign activation function, 307, 685\\nSign of Four, The (Conan Doyle), 209\\nSimon, Herbert A., 6, 9, 430\\nSimple Monte Carlo simulation, 128–129\\nSimplification, of logical expressions, 188–189\\nSims, Karl, 390, 413\\nSimulated annealing, 69, 117, 126, 128–131, 396, 685\\nuses of, 130–131\\nSingle-step selection, 372\\nSingular-extension heuristic, 153\\nSituated, 557\\nSituated action rules, 557, 685\\nSituation action rules. See Situated action rules\\nSituation calculus, 419, 422, 426–427, 467, 685\\nSituation variables, 426, 434–435, 685\\nSkolem constant, 221, 686\\nSkolem function, 686\\nSkolem normal form, 686\\nSkolemization, 173, 220–222, 686\\nexample, 221–222\\nSlipnet, 474–475, 686\\nSlippage, 476\\nSlot reader, 37, 686\\nSlot values, 32–33, 686\\nSlot writer, 38\\nSlots, 32–33, 686\\nas frames, 35–36\\nSmart agent, 686\\nSmoothing, 611, 687\\nSocial interaction, of agents, 545–546\\nSocrates, 10\\nSoftware agents, 543–544, 687'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 763, 'page_label': '764'}, page_content='Index 737\\nmobile, 552\\nSolved, 166\\nSoma, 292–293\\nSome Studies in Machine Learning Using the Game of\\nCheckers (Samuel), 160\\nSony corporation, 23\\nSoundness, 173, 175, 200, 687\\nSpace complexity, 78\\nSpärck Jones, Karen, 597\\nSpidering the web, 72, 88, 135–136, 687\\nSpielberg, Stephen, 23\\nSplice and cut operators, 405–406\\nSplitting and merging, 613\\nSQL, 572\\nStability, of neural networks, 306–307\\nStack, 84, 687\\nStagnant, 371\\nStagnate, 395, 411\\nStanford University, 255, 485–486\\nStart state, diagram, 452f\\nStart symbol, 577\\nState, 687\\nState spaces, 43–44, 72, 83, 84, 687\\ndiagram, 44f\\nStatic agents, 552\\nStatic evaluators, in game playing, 146–148, 687\\nSteels, Luc, The Artificial Life Roots of Artificial Intelli-\\ngence, 363\\nSteepest ascent hill climbing, 98–100\\nSteinbeck, John, The Grapes of Wrath, 291\\nStemming, 596–598, 687\\nStendahl, Henri Beyle, La Vie d’Henri Brulard, 503\\nStep function, of artificial neuron, 294, 302\\nSteps Toward Artificial Intelligence(Minsky), 28\\nStevenson, Adlai E., Jr., 117\\nStochastic, 560\\nStochastic methods, 450\\nStop list, 594, 687\\nSTRIPS (Stanford Research Institute Problem Solver),\\n419, 422, 430, 434–443, 688\\nassumption, 688\\nand closed world assumption, 480\\nand GraphPlan, 454–455\\nimplementing, 437–443\\nbackward chaining, 440–441\\nforward chaining, 439\\nmeans-ends analysis, 440–441\\nand resolution, 441–443\\nsearch trees, 439–440\\noperators, 435–437\\nand principle of least commitment, 447–448\\nand propositional planning, 448–450\\nSussman anomaly, 443–444\\nStrong AI, 688\\nStrong methods, 688\\nStructural texture analysis, 620\\nStructured knowledge representation, 469\\nSubclass, 31–32, 42, 688\\nSubgoal, 58, 688\\nSubproblems, 57, 688\\nSubset, 688\\nSubstitution, in logical expressions, 198, 222–223, 229,\\n688\\nSubsumption architecture, 556–557, 559, 688\\nSuccess nodes, 59, 689\\nSuccessor state axioms, 422, 427–428\\nSuccessors, 83, 689\\nSun corporation, 552\\nSuperclass, 31–32, 689\\nSupervised learning, 285, 689\\nin multilayer networks, 292\\nSurvival of the fittest, 372\\nSussman anomaly, 443–444\\nSyllogism, 6, 689\\nSymbolic representation, 558–559\\nSynapses, 292–293, 689\\nSyntactic ambiguity, 589\\nSyntactic analysis, 581–582, 689\\nSyntactic structures, 11\\nSyntax\\nand natural language processing, 573\\nof predicate calculus, 196–197\\nof propositional logic, 189–190\\nSystematic approach, to SAT, 450\\nSystems reply, to Chinese room, 21\\nT\\nTabu search, 69, 126, 127–128, 689\\nT alented Man, The(Praed), 175\\nTanh, 305\\nTask distribution, 134, 134–135\\nTautologies, 175, 186–187, 201, 689\\nT elescript, 552\\nT emperature, 129'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 764, 'page_label': '765'}, page_content='738 Index\\nT emplate chromosomes, 405\\nT emporal logic, 467, 487–490, 690\\nlinear time, 488\\nusing, 489–490\\nT ennyson, Alfred, 465\\nT erm frequency-inverse document frequency (TF-IDF),\\n594–596, 690\\nT erminal symbols, 577, 580, 690\\nT ermination criteria, for genetic algorithm, 392–393\\nT erms, 199, 690\\nT exels, 620, 690\\nT exture, 690\\nin machine vision, 615–623\\ndetermining shape and orientation, 620–623\\nidentifying, 616–620\\nstructural texture analysis, 620\\nTF-IDF (term frequency-inverse document frequency),\\n594–596, 690\\nTheorems, 200, 690\\nThomas, Dylan, Especially When the October Wind, 291\\nThreat tree, 60\\nThreatened causal links, 446–447\\nThree-coloring problem, 216–218, 690\\nThresholding, 612–613\\nThrough the Looking Glass (Carroll), 19, 175\\nTic-tac-toe, 144–145, 166\\nTime complexity, 78\\nTimetable problem, 414\\nTinsley, Marion, 160–161, 163\\nTit-for-tat strategy, 409, 690\\nTo a Mouse (Burns), 421\\nT odd, Stephen, 412\\nT op down or bottom up, 60–61, 278, 690\\nbuilding parse tree, 581–582\\nT otal order plans, 444\\nT otalistic rules, 370\\nT ouringMachines, 559, 560\\nT owers of Hanoi, 691\\nTractatus Logico-Philosophicus (Wittgenstein), 19\\nTraining, 691\\nand machine learning, 268–270\\nTraining data, 691\\npositive and negative, 282\\nTransition model, 560\\nTransition networks, 582–585, 691\\naugmented, 585\\nTranslating logical operators, 178–181\\nTranslation, 8–9\\nautomated, 572\\nmachine, 592\\nTraveling salesman problem, 50–54, 96–98, 414\\nTree ordering, 134, 135\\nTrihedral vertex, 614–615, 691\\nTruth maintenance system (TMS), 467, 478–479, 691\\nTruth tables, 173, 181–184, 691\\ncomplex, 184–186\\nand fuzzy logic, 512–515\\ntautologies, 186–187, 201\\nTruth values, 176, 691\\nTuring, Alan, 7–8, 21, 291\\nComputing Machinery and Intelligence,7\\nTuring test, 8, 22, 691\\nTversky, A., 550\\n2001: A Space Odyssey (film), 19, 21–22\\nU\\nUlam, Stanislaw, 369\\nUnary operators, 181, 392, 692\\nUncertainty, and logic, 177, 467, 564\\nUncertainty principle, 504\\nUnderspecified chromosomes, 692\\nUnification, 222–226, 228, 229\\nexample, 225–226\\nand STRIPS, 437, 442\\nUnification algorithm, 224–225\\nUnifiers, 692\\nUniform cost search, 109–110\\nUniform crossover, 391–392, 692\\nUniform tree, 692\\nUninformed search, 692\\nUniversal quantifiers, 197, 692\\nUniverse of discourse, 504–505, 692\\nUniversity of Alberta, Canada, 160\\nUnrestricted grammars, 580\\nUnstable neural networks, 307\\nUnsupervised learning, 285, 292, 693\\nUnsupervised learning networks, 316–321\\nKohonen maps, 316–320\\nexample, 318–320\\nUser interface, 252f, 253\\nUtility-based agents, 549, 551, 693\\nUtility functions, 549–551, 693\\nUtterance, 693'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 765, 'page_label': '766'}, page_content='Index 739\\nV\\nVagueness, 504, 590\\nValidity, 176, 693\\nVanishing point, 624, 693\\nVariables, 181\\nbound vs. free, 198, 638\\nV ecchi, M. P ., 130\\nV erb, 693\\nV erb phrase, 693\\nV erb phrases, 576, 581–582\\nV ersatility, of agents, 546\\nV ersion spaces, 274–275, 693\\nV ertical layer architecture, 559, 559–560\\nVision\\nhuman, 606–608\\nmachine, 605–628\\nVisual data, analyzing. See Machine vision\\nVLSI (very large-scale integration), 130\\nVoltaire,Candide,7 1\\nVon Neumann, John, 369–370, 372\\nW\\nWalksat, 450\\nWatts, Isaac, 327\\nWave search, 136\\nWeak AI, 693\\nWeak methods, 694\\nWeb spidering, 88, 135–136\\nWeight, 694\\nWeight vector, 694\\nWeighted linear functions, 147, 694\\nWeiss, Gerhard,Learning in Multiagent Systems, 267\\nWeizenbaum, Joseph, 8\\nWell-formed formula (WFF ), 189–190, 694\\ndefined, 199–200\\nand STRIPS, 435, 436, 437\\nWHEN-CHANGED procedures, 39–40, 694\\nWHEN-NEEDED procedures, 38, 40, 694\\nWHEN-READ procedures, 694\\nWHEN-WRITTEN procedures, 694\\nWillett, Peter, 597\\nWilliam of Occam, 276\\nWilson, Stewart, 381\\nWinner-take-all algorithm, 316, 694\\nWinston, Patrick Henry, 412\\nWittgenstein, Ludwig\\nPhilosophische Untersuchungen, 571\\nTractatus Logico-Philosophicus,1 9\\nWordsworth, William, 605\\nWorkspace, 474–475, 695\\nWorld model, 436–437, 591–592, 695\\nY\\nY oung, Edward,Night Thoughts, 209\\nZ\\nZero-sum games, 146, 695\\nZhang, Y anjun,Randomized Parallel Algorithms for Back-\\ntrack Search and Branch-and-Bound Computa-\\ntions, 135'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 766, 'page_label': '767'}, page_content='Computer Science Illuminated, Second Edition\\nNell Dale and John Lewis\\nISBN: 0-7637-0799-6\\n©2004\\nProgramming and Problem Solving with Java\\nNell Dale, Chip Weems, \\nand Mark R. Headington\\nISBN: 0-7637-0490-3\\n©2003\\nDatabases Illuminated\\nCatherine Ricardo\\nISBN: 0-7637-3314-8\\n©2004\\nFoundations of Algorithms Using Java\\nPseudocode\\nRichard Neapolitan and Kumarss Naimipour\\nISBN: 0-7637-2129-8\\n©2004\\nArtificial Intelligence Illuminated\\nBen Coppin\\nISBN: 0-7637-3230-3\\n©2004\\nThe Essentials of Computer Organization and\\nArchitecture\\nLinda Null and Julia Lobur\\nISBN: 0-7637-0444-X\\n©2003\\nA Complete Guide to C#\\nDavid Bishop\\nISBN: 0-7637-2249-9\\n©2004\\nA First Course in Complex Analysis \\nwith Applications\\nDennis G. Zill and Patrick Shanahan\\nISBN: 0-7637-1437-2\\n©2003\\nProgramming and Problem Solving with C++,\\nFourth Edition\\nNell Dale and Chip Weems\\nISBN: 0-7637-0798-8\\n©2004\\nC++ Plus Data Structures, Third Edition\\nNell Dale\\nISBN: 0-7637-0481-4\\n©2003\\nApplied Data Structures with C++\\nPeter Smith\\nISBN: 0-7637-2562-5\\n©2004\\nFoundations of Algorithms Using C++\\nPseudocode, Third Edition\\nRichard Neapolitan and Kumarss Naimipour\\nISBN: 0-7637-2387-8\\n©2004\\nManaging Software Projects\\nFrank Tsui\\nISBN: 0-7637-2546-3\\n©2004\\nReadings in CyberEthics, Second Edition\\nRichard Spinello and Herman Tavani\\nISBN: 0-7637-2410-6\\n©2004\\nC#.NET Illuminated\\nArt Gittleman\\nISBN: 0-7637-2593-5\\n©2004\\nDiscrete Mathematics, Second Edition\\nJames L. Hein\\nISBN: 0-7637-2210-3\\n©2003\\nOutstanding New Titles:\\nhttp://www.jbpub.com/ 1.800.832.0034'),\n",
              " Document(metadata={'producer': 'Acrobat Distiller 5.0 (Windows)', 'creator': 'PyPDF', 'creationdate': '2007-05-24T11:04:58+05:30', 'moddate': '2007-05-24T11:05:18+05:30', 'title': 'Artificial Intelligence Illuminated', 'author': 'Coppin, Ben.', 'source': '/content/AI2.pdf', 'total_pages': 768, 'page': 767, 'page_label': '768'}, page_content='Take Your Courses to the Next Level\\nTurn the page to preview new and forthcoming titles\\nin Computer Science and Math from \\nJones and Bartlett…\\nProviding solutions for students and educators in the following \\ndisciplines:\\nPlease visit http://computerscience.jbpub.com/ and\\nhttp://math.jbpub.com/ to learn more about our exciting publishing \\nprograms in these disciplines.\\n• Introductory Computer Science\\n• Java \\n• C\\n++\\n• Databases \\n• C# \\n• Data Structures \\n• Algorithms\\n• Network Security \\n• Software Engineering \\n• Discrete Mathematics \\n• Engineering Mathematics \\n• Complex Analysis\\nhttp://www.jbpub.com/ 1.800.832.0034')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [doc.page_content for doc in docs]\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB9FB4HEsiTp",
        "outputId": "82af05f0-2833-4ae6-bc8e-69aff8ed702d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
        "docs = splitter.create_documents(text)\n",
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-rQX-U6tk8h",
        "outputId": "c52d68e1-cbd3-4375-b285-f1013b9d8c45"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={}, page_content='Artificial Intelligence \\nIlluminated\\nBen Coppin\\nJONES AND BARTLETT PUBLISHERS'), Document(metadata={}, page_content='Copyright © 2004 by Jones and Bartlett Publishers, Inc.\\nCover image © Photodisc\\nLibrary of Congress Cataloging-in-Publication Data\\nCoppin, Ben.\\nArtificial intelligence illuminated / by Ben Coppin.--1\\nst ed.\\np. cm.\\nIncludes bibliographical references and index.\\nISBN 0-7637-3230-3\\n1. Artificial intelligence. I. Title.\\nQ335.C586  2004\\n006.3--dc22\\n2003020604\\n3382\\nAll rights reserved. No part of the material protected by this copyright notice may be'), Document(metadata={}, page_content='reproduced or utilized in any form, electronic or mechanical, including photocopying,\\nrecording, or any information storage or retrieval system, without written permission\\nfrom the copyright owner.\\nAcquisitions Editor: Stephen Solomon\\nProduction Manager: Amy Rose\\nMarketing Manager: Matthew Bennett\\nEditorial Assistant: Caroline Senay\\nManufacturing Buyer: Therese Bräuer\\nCover Design: Kristin E. Ohlin\\nT ext Design: Kristin E. Ohlin\\nComposition: Northeast Compositors'), Document(metadata={}, page_content='Composition: Northeast Compositors\\nT echnical Artist: George Nichols\\nPrinting and Binding: Malloy, Inc.\\nCover Printing: Malloy, Inc.\\nPrinted in the United States of America\\n08  07  06  05  04        10  9  8  7  6  5  4  3  2  1\\nWorld Headquarters\\nJones and Bartlett Publishers \\n40 Tall Pine Drive\\nSudbury, MA 01776\\n978-443-5000\\ninfo@jbpub.com\\nwww.jbpub.com\\nJones and Bartlett Publishers \\nCanada\\n2406 Nikanna Road\\nMississauga, ON L5C 2W6\\nCANADA\\nJones and Bartlett Publishers \\nInternational'), Document(metadata={}, page_content='Jones and Bartlett Publishers \\nInternational \\nBarb House, Barb Mews\\nLondon W6 7PA\\nUK'), Document(metadata={}, page_content='For Erin'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='Preface\\nWho Should Read This Book\\nThis book is intended for students of computer science at the college level,\\nor students of other subjects that cover Artificial Intelligence. It also is\\nintended to be an interesting and relevant introduction to the subject for\\nother students or individuals who simply have an interest in the subject.\\nThe book assumes very little knowledge of computer science, but does\\nassume some familiarity with basic concepts of algorithms and computer'), Document(metadata={}, page_content='systems. Data structures such as trees, graphs, and stacks are explained\\nbriefly in this book, but if you do not already have some familiarity with\\nthese concepts, you should probably seek out a suitable book on algorithms\\nor data structures.\\nIt would be an advantage to have some experience in a programming lan-\\nguage such as C++ or Java, or one of the languages commonly used in Arti-\\nficial Intelligence research, such as PROLOG and LISP , but this experience\\nis neither necessary nor assumed.'), Document(metadata={}, page_content='is neither necessary nor assumed.\\nMany of the chapters include practical exercises that require the reader to\\ndevelop an algorithm or program in a programming language of his or her\\nchoice. Most readers should have no difficulty with these exercises. How-\\never, if any reader does not have the necessary skills he or she simply should\\ndescribe in words (or in pseudocode) how his or her programs work, giving\\nas much detail as possible.'), Document(metadata={}, page_content='How to Read This Book\\nThis book can be read in several ways. Some readers will choose to read the\\nchapters through in order from Chapter 1 through Chapter 21. Any chapter\\nthat uses material which is presented in another chapter gives a clear refer-\\nence to that chapter, and readers following the book from start to finish\\nshould not need to jump forward at any point, as the chapter dependencies\\ntend to work in a forward direction.'), Document(metadata={}, page_content='tend to work in a forward direction.\\nAnother perfectly reasonable way to use this book is as a reference. When a\\nreader needs to know more about a particular subject, he or she can pick up\\nthis book and select the appropriate chapter or chapters, and can be illumi-\\nnated on the subject (at least, that is the author’s intent!)\\nChapter 12 contains a diagram that shows how the dependencies between\\nchapters work (Section 12.6.2). This diagram shows, for example, that if a'), Document(metadata={}, page_content='reader wants to read Chapter 8, it would be a good idea to already have read\\nChapter 7.\\nThis book is divided into six parts, each of which is further divided into a\\nnumber of chapters. The chapters are laid out as follows:\\nPart 1: Introduction to Artificial Intelligence\\nChapter 1: A Brief History of Artificial Intelligence\\nChapter 2: Uses and Limitations\\nChapter 3: Knowledge Representation \\nPart 2: Search\\nChapter 4: Search Methodologies \\nChapter 5: Advanced Search\\nChapter 6: Game Playing'), Document(metadata={}, page_content='Chapter 6: Game Playing\\nPart 3: Logic\\nChapter 7: Propositional and Predicate Logic\\nChapter 8: Inference and Resolution for Problem Solving\\nChapter 9: Rules and Expert Systems\\nvi Preface'), Document(metadata={}, page_content='Part 4: Machine Learning \\nChapter 10: Introduction to Machine Learning\\nChapter 11: Neural Networks\\nChapter 12: Probabilistic Reasoning and Bayesian Belief Networks \\nChapter 13: Artificial Life: Learning through Emergent Behavior \\nChapter 14: Genetic Algorithms\\nPart 5: Planning \\nChapter 15: Introduction to Planning\\nChapter 16: Planning Methods\\nPart 6: Advanced Topics \\nChapter 17: Advanced Knowledge Representation \\nChapter 18: Fuzzy Reasoning\\nChapter 19: Intelligent Agents'), Document(metadata={}, page_content='Chapter 19: Intelligent Agents\\nChapter 20: Understanding Language\\nChapter 21: Machine Vision\\nEach chapter includes an introduction that explains what the chapter cov-\\ners, a summary of the chapter, some exercises and review questions, and\\nsome suggestions for further reading. There is a complete bibliography at\\nthe back of the book.\\nThis book also has a glossary, which includes a brief definition of most of\\nthe important terms used in this book. When a new term is introduced in'), Document(metadata={}, page_content='the text it is highlighted in bold, and most of these words are included in\\nthe glossary. The only such terms that are not included in the glossary are\\nthe ones that are defined in the text, but that are not used elsewhere in the\\nbook.\\nThe use of third person pronouns is always a contentious issue for authors\\nof text books, and this author has chosen to use he and she interchangeably.\\nIn some cases the word “he” is used, and in other cases “she. ” This is not'), Document(metadata={}, page_content='intended to follow any particular pattern, or to make any representations\\nabout the genders, but simply is in the interests of balance.\\nPreface vii'), Document(metadata={}, page_content='The first few chapters of this book provide introductory material, explain-\\ning the nature of Artificial Intelligence and providing a historical back-\\nground, as well as describing some of the connections with other\\ndisciplines. Some readers will prefer to skip these chapters, but it is advis-\\nable to at least glance through Chapter 3 to ensure that you are familiar\\nwith the concepts of that chapter, as they are vital to the understanding of\\nmost of the rest of the book.\\nAcknowledgments'), Document(metadata={}, page_content='most of the rest of the book.\\nAcknowledgments\\nAlthough I wrote this book single-handedly, it was not without help. I\\nwould like to thank, in chronological order, Frank Abelson; Neil Salkind\\nand everyone at Studio B; Michael Stranz, Caroline Senay, Stephen\\nSolomon, and Tracey Chapman at Jones & Bartlett; also a number of peo-\\nple who read chapters of the book: Martin Charlesworth, Patrick Coyle,\\nPeter and Petra Farrell, Robert Kealey, Geoffrey Price, Nick Pycraft, Chris'), Document(metadata={}, page_content='Swannack, Edwin Y oung, my parents, T ony and Frances, and of course\\nErin—better late than never.\\nThanks also to:\\nThe MIT Press for the excerpt from ‘Learning in Multiagent Systems’ by\\nSandip Sen and Gerhard Weiss, © 2001, The MIT Press.\\nThe MIT Press for the excerpt from ‘Adaptation in Natural and Artificial\\nSystems’ by John H. Holland, © 1992, The MIT Press.\\nThe MIT Press for the excerpt from ‘The Artificial Life Roots of Artificial'), Document(metadata={}, page_content='Intelligence’ by Luc Steels, © 1994, the Massachusetts Institute of T echnol-\\nogy.\\nThe IEEE for the excerpt from ‘Steps T owards Artificial Intelligence’ by\\nMarvin Minsky, © 2001, IEEE.\\nI have attempted to contact the copyright holders of all copyrighted quotes\\nused in this book. If I have used any quotes without permission, then this\\nwas inadvertent, and I apologize. I will take all measures possible to rectify\\nthe situation in future printings of the book.\\nviii Preface'), Document(metadata={}, page_content='Contents\\nPreface v\\nP A R T  1 Introduction to Artificial Intelligence 1\\nChapter 1 A Brief History of Artificial Intelligence 3\\n1.1 Introduction 3\\n1.2 What Is Artificial Intelligence? 4\\n1.3 Strong Methods and Weak Methods 5\\n1.4 From Aristotle to Babbage 6\\n1.5 Alan Turing and the 1950s 7\\n1.6 The 1960s to the 1990s 9\\n1.7 Philosophy 10\\n1.8 Linguistics 11\\n1.9 Human Psychology and Biology 12\\n1.10 All Programming Languages 12\\n1.10.1 PROLOG 13\\n1.10.2 LISP 14\\n1.11 Chapter Summary 15'), Document(metadata={}, page_content='1.10.2 LISP 14\\n1.11 Chapter Summary 15\\n1.12 Review Questions 16\\n1.13 Further Reading 17'), Document(metadata={}, page_content='Chapter 2 Uses and Limitations 19\\n2.1 Introduction 19\\n2.2 The Chinese Room 20\\n2.3 HAL—Fantasy or Reality? 21\\n2.4 AI in the 21\\nst Century 23\\n2.5 Chapter Summary 24\\n2.6 Review Questions 24\\n2.7 Further Reading 25\\nChapter 3 Knowledge Representation 27\\n3.1 Introduction 27\\n3.2 The Need for a Good Representation 28\\n3.3 Semantic Nets 29\\n3.4 Inheritance 31\\n3.5 Frames 32\\n3.5.1 Why Are Frames Useful? 34\\n3.5.2 Inheritance 34\\n3.5.3 Slots as Frames 35\\n3.5.4 Multiple Inheritance 36\\n3.5.5 Procedures 37'), Document(metadata={}, page_content='3.5.4 Multiple Inheritance 36\\n3.5.5 Procedures 37\\n3.5.6 Demons 38\\n3.5.7 Implementation 38\\n3.5.8 Combining Frames with Rules 40\\n3.5.9 Representational Adequacy 40\\n3.6 Object-Oriented Programming 41\\n3.7 Search Spaces 42\\n3.8 Semantic Trees 44\\n3.9 Search Trees 46\\n3.9.1 Example 1: Missionaries and Cannibals 47\\n3.9.2 Improving the Representation 49\\n3.9.3 Example 2: The Traveling Salesman 50\\n3.9.4 Example 3: The T owers of Hanoi 54\\n3.9.5 Example 4: Describe and Match 56\\n3.10 Combinatorial Explosion 57'), Document(metadata={}, page_content='3.10 Combinatorial Explosion 57\\n3.11 Problem Reduction 57\\nx Contents'), Document(metadata={}, page_content='3.12 Goal Trees 58\\n3.12.1 T op Down or Bottom Up? 60\\n3.12.2 Uses of Goal Trees 61\\nExample 1: Map Coloring\\nExample 2: Proving Theorems \\nExample 3: Parsing Sentences 63\\nExample 4: Games\\n3.13 Chapter Summary 64\\n3.14 Review Questions 65\\n3.15 Exercises 65\\n3.16 Further Reading 66\\nP A R T  2 Search 69\\nChapter 4 Search Methodologies 71\\n4.1 Introduction 71\\n4.2 Problem Solving as Search 72\\n4.3 Data-Driven or Goal-Driven Search 73\\n4.4 Generate and T est 74\\n4.5 Depth-First Search 75'), Document(metadata={}, page_content='4.5 Depth-First Search 75\\n4.6 Breadth-First Search 76\\n4.7 Properties of Search Methods 78\\n4.7.1 Complexity 78\\n4.7.2 Completeness 79\\n4.7.3 Optimality 79\\n4.7.4 Irrevocability 80\\n4.8 Why Humans Use Depth-First Search? 80\\n4.8.1 Example 1: Traversing a Maze 81\\n4.8.2 Example 2: Searching for a Gift 81\\n4.9 Implementing Depth-First and Breadth-First Search 83\\n4.10 Example: Web Spidering 88\\n4.11 Depth-First Iterative Deepening 88\\n4.12 Using Heuristics for Search 90'), Document(metadata={}, page_content='4.12 Using Heuristics for Search 90\\n4.12.1 Informed and Uninformed Methods 91\\n4.12.2 Choosing a Good Heuristic 92\\n4.12.3 The 8-Puzzle 92\\nContents xi'), Document(metadata={}, page_content='4.12.4 Monotonicity 95\\n4.12.5 Example: The Modified Traveling Salesman \\nProblem 96\\n4.13 Hill Climbing 98\\n4.13.1 Steepest Ascent Hill Climbing 98\\n4.13.2 Foothills, Plateaus, and Ridges 101\\n4.14 Best-First Search 104\\n4.15 Beam Search 106\\n4.16 Identifying Optimal Paths 107\\n4.16.1 A* Algorithms 108\\n4.16.2 Uniform Cost Search 110\\n4.16.3 Greedy Search 111\\n4.16.4 Example: The Knapsack Problem 111\\n4.17 Chapter Summary 113\\n4.18 Review Questions 114\\n4.19 Exercises 115\\n4.20 Further Reading 116'), Document(metadata={}, page_content='4.19 Exercises 115\\n4.20 Further Reading 116\\nChapter 5 Advanced Search 117\\n5.1 Introduction 117\\n5.2 Constraint Satisfaction Search 118\\n5.3 Forward Checking 121\\n5.4 Most-Constrained Variables 121\\n5.5 Example: Cryptographic Problems 122\\n5.6 Heuristic Repair 123\\n5.7 Combinatorial Optimization Problems 125\\n5.8 Local Search and Metaheuristics 126\\n5.8.1 Exchanging Heuristics 126\\n5.8.2 Iterated Local Search 127\\n5.8.3 Tabu Search 127\\n5.8.4 Ant Colony Optimization 128\\n5.9 Simulated Annealing 128'), Document(metadata={}, page_content='5.9 Simulated Annealing 128\\n5.9.1 Uses of Simulated Annealing 130\\n5.10 Genetic Algorithms for Search 131\\n5.11 Real-Time A* 131\\nxii Contents'), Document(metadata={}, page_content='5.12 Iterative-Deepening A* (IDA*) 132\\n5.13 Parallel Search 132\\n5.13.1 Task Distribution 134\\n5.13.2 Tree Ordering 135\\n5.13.3 Search Engines 135\\n5.14 Bidirectional Search 136\\n5.15 Nondeterministic Search 136\\n5.16 Island-Driven Search 137\\n5.17 Nonchronological Backtracking 137\\n5.18 Chapter Summary 138\\n5.19 Review Questions 139\\n5.20 Exercises 140\\n5.21 Further Reading 141\\nChapter 6 Game Playing 143\\n6.1 Introduction 143\\n6.2 Game Trees 144\\n6.2.1 Rationality, Zero Sum, and Other \\nAssumptions 145'), Document(metadata={}, page_content='Assumptions 145\\n6.2.2 Evaluation Functions 146\\n6.2.3 Searching Game Trees 148\\n6.3 Minimax 149\\n6.3.1 Bounded Lookahead 151\\n6.4 Alpha-Beta Pruning 153\\n6.4.1 The Effectiveness of Alpha-Beta Pruning 154\\n6.4.2 Implementation 155\\n6.5 Checkers 159\\n6.5.1 Chinook 160\\n6.5.2 Chinook’s Databases 161\\n6.5.3 Chinook’s Evaluation Function 162\\n6.5.4 Forward Pruning 163\\n6.5.5 Limitations of Minimax 163\\n6.5.6 Blondie 24 164\\n6.6 Chess 164\\nContents xiii'), Document(metadata={}, page_content='6.7 Go 165\\n6.7.1 Go-Moku 166\\n6.8 Othello (Reversi) 166\\n6.9 Games of Chance 166\\n6.9.1 Expectiminimax 167\\n6.10 Chapter Summary 167\\n6.11 Review Questions 168\\n6.12 Exercises 169\\n6.13 Further Reading 170\\nP A R T  3 Knowledge Representation and Automated \\nReasoning 173\\nChapter 7 Propositional and Predicate Logic 175\\n7.1 Introduction 175\\n7.2 What Is Logic? 176\\n7.3 Why Logic Is Used in Artificial Intelligence 176\\n7.4 Logical Operators 177\\n7.5 Translating between English and Logic Notation 178'), Document(metadata={}, page_content='7.6 Truth Tables 181\\n7.6.1 Not 181\\n7.6.2 And 182\\n7.6.3 Or 182\\n7.6.4 Implies 183\\n7.6.5 iff 184\\n7.7 Complex Truth Tables 184\\n7.8 Tautology 186\\n7.9 Equivalence 187\\n7.10 Propositional Logic 189\\n7.10.1 Syntax 189\\n7.10.2 Semantics 190\\n7.11 Deduction 191\\n7.11.1 ^-Introduction 191\\n7.11.2 ^-Eliminations 191\\n7.11.3 Or-Introduction 192\\n7.11.4 ?Elimination 192\\nxiv Contents'), Document(metadata={}, page_content='7.11.5 Reductio Ad Absurdum 192\\n7.11.6 ?Introduction 193\\n7.11.7 ¬¬Elimination 193\\n7.11.8 Example 1 193\\n7.11.9 Example 2 194\\n7.11.10 Example 3 194\\n7.11.11 Example 4 195\\n7.12 The Deduction Theorem 195\\n7.13 Predicate Calculus 196\\n7.13.1 Syntax 196\\n7.13.2 Relationships between \" and $ 197\\n7.13.3 Functions 199\\n7.14 First-Order Predicate Logic 199\\n7.15 Soundness 200\\n7.16 Completeness 200\\n7.17 Decidability 200\\n7.18 Monotonicity 201\\n7.19 Abduction and Inductive Reasoning 201'), Document(metadata={}, page_content='7.19 Abduction and Inductive Reasoning 201\\n7.20 Modal Logics and Possible Worlds 203\\n7.20.1 Reasoning in Modal Logic 204\\n7.21 Dealing with Change 205\\n7.22 Chapter Summary 205\\n7.23 Review Questions 205\\n7.24 Exercises 206\\n7.25 Further Reading 208\\nChapter 8 Inference and Resolution for Problem Solving 209\\n8.1 Introduction 209\\n8.2 Resolution in Propositional Logic 210\\n8.2.1 Normal Forms 210\\n8.2.2 The Resolution Rule 212\\n8.2.3 Resolution Refutation 213\\n8.2.4 Proof by Refutation 214'), Document(metadata={}, page_content='8.2.4 Proof by Refutation 214\\n8.3 Applications of Resolution 216\\n8.4 Resolution in Predicate Logic 218\\nContents xv'), Document(metadata={}, page_content='8.5 Normal Forms for Predicate Logic 219\\n8.6 Skolemization 220\\n8.6.1 Example of Skolemization 221\\n8.6.2 Second Example of Skolemization 222\\n8.6.3 Unification 222\\n8.6.4 Most General Unifiers 224\\n8.6.5 Unification Algorithm 224\\n8.6.6 Unification Example 225\\n8.7 Resolution Algorithm 226\\n8.8 Horn Clauses and PROLOG 227\\n8.9 Herbrand Universes 229\\n8.9.1 The Herbrand Base 230\\n8.9.2 Herbrand Interpretations 231\\n8.9.3 Example 232\\n8.10 Resolution for Problem Solving 233\\n8.11 Chapter Summary 237'), Document(metadata={}, page_content='8.11 Chapter Summary 237\\n8.12 Review Questions 238\\n8.13 Exercises 238\\n8.14 Further Reading 239\\nChapter 9 Rules and Expert Systems 241\\n9.1 Introduction 241\\n9.2 Rules for Knowledge Representation 242\\n9.3 Rule-Based Systems 243\\n9.3.1 Forward Chaining 244\\n9.3.2 Conflict Resolution 246\\n9.3.3 Meta Rules 247\\n9.3.4 Backward Chaining 248\\n9.3.5 Comparing Forward and Backward Chaining 249\\n9.4 Rule-Based Expert Systems 251\\n9.4.1 The People Involved in an Expert System 251'), Document(metadata={}, page_content='9.4.1 The People Involved in an Expert System 251\\n9.4.2 Architecture of an Expert System 252\\n9.4.3 The Expert Shell System 253\\n9.4.4 The Rete Algorithm 253\\n9.4.5 Knowledge Engineering 254\\n9.5 CLIPS (C Language Integrated Production System) 255\\nxvi Contents'), Document(metadata={}, page_content='9.6 Backward Chaining in Rule-Based Expert Systems 257\\n9.7 CYC 259\\n9.8 Chapter Summary 260\\n9.9 Review Questions 261\\n9.10 Exercises 261\\n9.11 Further Reading 261\\nP A R T  4 Machine Learning 265\\nChapter 10 Introduction to Machine Learning 267\\n10.1 Introduction 267\\n10.2 Training 268\\n10.3 Rote Learning 270\\n10.4 Learning Concepts 270\\n10.5 General-to-Specific Ordering 272\\n10.5.1 A Simple Learning Algorithm 273\\n10.6 V ersion Spaces 274\\n10.7 Candidate Elimination 275\\n10.8 Inductive Bias 276'), Document(metadata={}, page_content='10.8 Inductive Bias 276\\n10.9 Decision-Tree Induction 276\\n10.9.1 Information Gain 278\\n10.9.2 Example 279\\n10.9.3 Inductive Bias of ID3 281\\n10.10 The Problem of Overfitting 282\\n10.11 The Nearest Neighbor Algorithm 283\\n10.12 Learning Neural Networks 284\\n10.13 Supervised Learning 285\\n10.14 Unsupervised Learning 285\\n10.15 Reinforcement Learning 286\\n10.16 Chapter Summary 286\\n10.17 Review Questions 287\\n10.18 Exercises 288\\n10.19 Further Reading 288\\nChapter 11 Neural Networks 291\\n11.1 Introduction 291'), Document(metadata={}, page_content='11.1 Introduction 291\\n11.2 Neurons 292\\nContents xvii'), Document(metadata={}, page_content='11.2.1 Biological Neurons 292\\n11.2.2 Artificial Neurons 293\\n11.3 Perceptrons 295\\n11.4 Multilayer Neural Networks 300\\n11.4.1 Backpropagation 302\\n11.4.2 Improving the Performance of\\nBackpropagation 305\\n11.5 Recurrent Networks 306\\n11.5.1 Hopfield Networks 307\\n11.5.2 Bidirectional Associative Memories (BAMs) 314\\n11.6 Unsupervised Learning Networks 317\\n11.6.1 Kohonen Maps 317\\n11.6.2 Kohonen Map Example 319\\n11.6.3 Hebbian Learning 321\\n11.7 Evolving Neural Networks 322\\n11.8 Chapter Summary 323'), Document(metadata={}, page_content='11.8 Chapter Summary 323\\n11.9 Review Questions 324\\n11.10 Exercises 325\\n11.11 Further Reading 326\\nChapter 12 Probabilistic Reasoning and Bayesian Belief \\nNetworks 327\\n12.1 Introduction 327\\n12.2 Probabilistic Reasoning 328\\n12.3 Joint Probability Distributions 330\\n12.4 Bayes’ Theorem 330\\n12.4.1 Example: Medical Diagnosis 331\\n12.4.2 Example: Witness Reliability 332\\n12.4.3 Comparing Conditional Probabilities 334\\n12.4.4 Normalization 335\\n12.5 Simple Bayesian Concept Learning 337'), Document(metadata={}, page_content='12.5 Simple Bayesian Concept Learning 337\\n12.6 Bayesian Belief Networks 339\\n12.6.1 Example: Life at College 342\\n12.6.2 Example: Chapter Dependencies 346\\n12.7 The Noisy-V Function 346\\nxviii Contents'), Document(metadata={}, page_content='12.8 Bayes’ Optimal Classifier 349\\n12.9 The Naïve Bayes Classifier 351\\n12.10 Collaborative Filtering 356\\n12.11 Chapter Summary 357\\n12.12 Review Questions 358\\n12.13 Exercises 359\\n12.14 Further Reading 359\\nChapter 13 Artificial Life: Learning through Emergent \\nBehavior 363\\n13.1 Introduction 363\\n13.2 What Is Life? 364\\n13.3 Emergent Behavior 365\\n13.4 Finite State Automata 366\\n13.5 Cellular Automata 368\\n13.5.1 Conway’s Life 368\\n13.5.2 One-Dimensional Cellular Automata 370'), Document(metadata={}, page_content='13.5.2 One-Dimensional Cellular Automata 370\\n13.6 Self-Reproducing Systems 371\\n13.7 Evolution 372\\n13.7.1 Ramps 373\\n13.8 Evolution Strategies 373\\n13.9 Genetic Programming 374\\n13.10 Evolutionary Programming 375\\n13.11 L-Systems 376\\n13.12 Classifier Systems 377\\n13.13 Artificial Immune Systems 381\\n13.14 Chapter Summary 382\\n13.15 Review Questions 382\\n13.16 Further Reading 383\\nChapter 14 Genetic Algorithms 387\\n14.1 Introduction 387\\n14.2 Representations 388\\n14.3 The Algorithm 389\\n14.4 Fitness 390'), Document(metadata={}, page_content='14.3 The Algorithm 389\\n14.4 Fitness 390\\nContents xix'), Document(metadata={}, page_content='14.5 Crossover 390\\n14.6 Mutation 392\\n14.7 T ermination Criteria 392\\n14.8 Optimization of a Mathematic Function 393\\n14.9 Why Genetic Algorithms Work 396\\n14.9.1 Schemata 397\\n14.9.2 How Reproduction Affects Schemata 399\\n14.9.3 How Mutation and Crossover Affect \\nSchemata 401\\n14.9.4 The Building-Block Hypothesis 403\\n14.9.5 Deception 404\\n14.10 Messy Genetic Algorithms 405\\n14.11 Prisoner’s Dilemma 406\\n14.11.1 Strategy Representation 407\\n14.11.2 Possible Strategies 408'), Document(metadata={}, page_content='14.11.2 Possible Strategies 408\\n14.11.3 Evolution of Strategies 410\\n14.11.4 Choice of Opponents 410\\n14.12 Diversity 411\\n14.13 Evolving Pictures 412\\n14.14 Predators and Coevolution 413\\n14.15 Other Problems 414\\n14.16 Chapter Summary 414\\n14.17 Review Questions 415\\n14.18 Exercises 416\\n14.19 Further Reading 417\\nP A R T  5 Planning 419\\nChapter 15 Introduction to Planning 421\\n15.1 Introduction 421\\n15.2 Planning as Search 423\\n15.3 Situation Calculus 426\\n15.4 The Frame Problem 427'), Document(metadata={}, page_content='15.4 The Frame Problem 427\\n15.5 Means-Ends Analysis 428\\n15.6 Chapter Summary 430\\nxx Contents'), Document(metadata={}, page_content='15.7 Review Questions 431\\n15.8 Exercises 431\\n15.9 Further Reading 432\\nChapter 16 Planning Methods 433\\n16.1 Introduction 433\\n16.2 STRIPS 434\\n16.2.1 Planning and Executing 435\\n16.2.2 Operators 436\\n16.2.3 Implementation of STRIPS 437\\n16.2.4 Example: STRIPS 438\\n16.2.5 Example: STRIPS and Resolution 441\\n16.3 The Sussman Anomaly 443\\n16.4 Partial Order Planning 444\\n16.5 The Principle of Least Commitment 447\\n16.6 Propositional Planning 448\\n16.7 SAT Planning 450\\n16.8 Planning Graphs 451'), Document(metadata={}, page_content='16.7 SAT Planning 450\\n16.8 Planning Graphs 451\\n16.8.1 GraphPlan 454\\n16.8.2 Mutex Conditions 455\\n16.9 ADL and PDDL 455\\n16.10 Probabilistic Planning 456\\n16.11 Dynamic World Planning 456\\n16.12 Case-Based Planning Systems 457\\n16.13 Planning and Scheduling 458\\n16.14 Chapter Summary 459\\n16.15 Review Questions 460\\n16.16 Exercises 461\\n16.17 Further Reading 461\\nP A R T  6 Advanced Topics 463\\nChapter 17 Advanced Knowledge Representation 465\\n17.1 Introduction 465\\n17.2 Representations and Semantics 468'), Document(metadata={}, page_content='17.2 Representations and Semantics 468\\nContents xxi'), Document(metadata={}, page_content='17.3 The Blackboard Architecture 469\\n17.3.1 Implementation 471\\n17.3.2 HEARSAY 472\\n17.4 Scripts 472\\n17.5 Copycat Architecture 474\\n17.6 Nonmonotonic Reasoning 476\\n17.6.1 Nonmonotonic Logic with the Modal \\nOperator 477\\n17.6.2 Default Reasoning 477\\n17.6.3 Truth Maintenance Systems 478\\n17.6.4 Closed-World Assumption 480\\n17.6.5 The Ramification Problem 480\\n17.6.6 Circumscription 480\\n17.6.7 Abductive Reasoning 482\\n17.6.8 The Dempster-Shafer Theory 483\\n17.6.9 MYCIN and Certainty Factors 485'), Document(metadata={}, page_content='17.6.9 MYCIN and Certainty Factors 485\\n17.7 Reasoning about Change 487\\n17.7.1 T emporal Logic 487\\n17.7.2 Using T emporal Logic 488\\n17.7.3 Event Calculus 490\\n17.7.4 Mental Situation Calculus 492\\n17.8 Knowledge Engineering 494\\n17.9 Case-Based Reasoning 495\\n17.10 Chapter Summary 496\\n17.11 Review Questions 497\\n17.12 Exercises 498\\n17.13 Further Reading 500\\nChapter 18 Fuzzy Reasoning 503\\n18.1 Introduction 503\\n18.2 Bivalent and Multivalent Logics 504\\n18.3 Linguistic Variables 504\\n18.4 Fuzzy Sets 505'), Document(metadata={}, page_content='18.3 Linguistic Variables 504\\n18.4 Fuzzy Sets 505\\n18.4.1 Fuzzy Set Membership Functions 507\\n18.4.2 Fuzzy Set Operators 508\\nxxii Contents'), Document(metadata={}, page_content='18.4.3 Hedges 510\\n18.5 Fuzzy Logic 511\\n18.6 Fuzzy Logic as Applied to Traditional Logical \\nParadoxes 515\\n18.7 Fuzzy Rules 516\\n18.8 Fuzzy Inference 516\\n18.9 Fuzzy Expert Systems 522\\n18.9.1 Defining the Fuzzy Sets 523\\n18.9.2 Defining Fuzzy Rules 527\\n18.9.3 Relating Observations to Fuzzy Sets 528\\n18.9.4 Evaluating Each Case for the Fuzzy Rules 530\\n18.9.5 Defuzzification 531\\n18.10 Fuzzy Systems that Learn 534\\n18.10.1 Neuro-fuzzy Systems 534\\n18.10.2 Layer 1: The Input Layer 536'), Document(metadata={}, page_content='18.10.2 Layer 1: The Input Layer 536\\n18.10.3 Layer 2: The Fuzzification Layer 536\\n18.10.4 Layer 3: The Fuzzy Rule Layer 537\\n18.10.5 Layer 4: The Output Membership Function \\nLayer 537\\n18.10.6 Layer 5: The Defuzzification Layer 538\\n18.10.7 How the System Learns 538\\n18.11 Chapter Summary 539\\n18.12 Review Questions 539\\n18.13 Exercises 540\\n18.14 Further Reading 540\\nChapter 19 Intelligent Agents 543\\n19.1 Introduction 543\\n19.2 Properties of Agents 544\\n19.2.1 Intelligence 544\\n19.2.2 Autonomy 545'), Document(metadata={}, page_content='19.2.1 Intelligence 544\\n19.2.2 Autonomy 545\\n19.2.3 Ability to Learn 545\\n19.2.4 Cooperation 545\\n19.2.5 Other Agent Properties 546\\n19.3 Agent Classification 546\\nContents xxiii'), Document(metadata={}, page_content='19.4 Reactive Agents 547\\n19.4.1 Goal-based Agents 548\\n19.4.2 Utility-based Agents 549\\n19.4.3 Utility Functions 549\\n19.5 Interface Agents 551\\n19.6 Mobile Agents 552\\n19.7 Information Agents 553\\n19.8 Multiagent Systems 554\\n19.9 Collaborative Agents 556\\n19.10 Agent Architectures 556\\n19.10.1 Subsumption Architecture 556\\n19.10.2 BDI Architectures 558\\n19.10.3 Other Architectures 558\\n19.11 Accessibility 560\\n19.12 Learning Agents 561\\n19.12.1 Multiagent Learning 562\\n19.13 Robotic Agents 562'), Document(metadata={}, page_content='19.13 Robotic Agents 562\\n19.14 Braitenberg V ehicles 563\\n19.15 Chapter Summary 565\\n19.16 Review Questions 566\\n19.17 Exercises 567\\n19.18 Further Reading 567\\nChapter 20 Understanding Language 571\\n20.1 Introduction 571\\n20.2 Natural Language Processing 573\\n20.2.1 Morphologic Analysis 574\\n20.2.2 BNF 575\\n20.2.3 Grammers 579\\n20.2.4 Parsing: Syntactic Analysis 581\\n20.2.5 Transition Networks 582\\n20.2.6 Augmented Transition Networks 585\\n20.2.7 Chart Parsing 585\\n20.2.8 Semantic Analysis 588'), Document(metadata={}, page_content='20.2.8 Semantic Analysis 588\\n20.2.9 Ambiguity and Pragmatic Analysis 589\\nxxiv Contents'), Document(metadata={}, page_content='20.3 Machine Translation 592\\n20.3.1 Language Identification 593\\n20.4 Information Retrieval 594\\n20.4.1 Stemming 596\\n20.4.2 Precision and Recall 598\\n20.5 Chapter Summary 599\\n20.6 Review Questions 600\\n20.7 Exercises 600\\n20.8 Further Reading 601\\nChapter 21 Machine Vision 605\\n21.1 Introduction 605\\n21.2 Human Vision 606\\n21.3 Image Processing 608\\n21.3.1 Edge Detection 609\\n21.3.2 Convolution and the Canny Edge Detector 611\\n21.3.3 Segmentation 612\\n21.3.4 Classifying Edges in Line Drawings 613'), Document(metadata={}, page_content='21.3.4 Classifying Edges in Line Drawings 613\\n21.4 Using T exture 616\\n21.4.1 Identifying T extures 616\\n21.4.2 Structural T exture Analysis 620\\n21.4.3 Determining Shape and Orientation from \\nT exture 620\\n21.5 Interpreting Motion 623\\n21.6 Making Use of Vision 625\\n21.7 Face Recognition 627\\n21.8 Chapter Summary 628\\n21.9 Review Questions 629\\n21.10 Exercises 630\\n21.11 Further Reading 630\\nGlossary 633\\nBibliography 697\\nIndex 719\\nContents xxv'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='Introduction to Artificial\\nIntelligence\\n1\\nIntroduction to Part 1\\nPart 1 is divided into three chapters.\\nA Brief History of Artificial Intelligence\\nThis chapter provides a brief overview of the history of the\\nstudy of Artificial Intelligence. It also provides background\\nfrom philosophy, psychology, biology, and linguistics and\\nexplains how these subjects have contributed to the subject.\\nUses and Limitations\\nThe second chapter discusses the prevalence of Artificial'), Document(metadata={}, page_content='Intelligence in our world today, at the beginning of the 21st\\ncentury. It also looks at the limitations of Artificial Intelli-\\ngence and discusses some of the arguments against the\\nprinciple of strong AI, which claims that a machine that\\ncan behave in an intelligent way is actually capable of hav-\\ning mental states, much like a human being.\\nKnowledge Representation\\nThis chapter introduces an idea that is used throughout\\nthis book: knowledge representation. It explains why repre-'), Document(metadata={}, page_content='sentation is so important and why it is vital to choose the\\nright representation to solve a problem.\\nIt also explains some common representational methods\\nused in Artificial Intelligence, such as frames, semantic\\nnets, and search trees, which are used more extensively in\\nChapters 4 and 5.\\nThis chapter also provides a number of example problems\\nand explains how to use the representational methods\\nintroduced to solve the problems.\\nPART\\n1\\nCHAPTER\\n2\\nCHAPTER\\n3\\nCHAPTER'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='1CHAPTER\\nA Brief History of Artificial\\nIntelligence\\nWhat is all knowledge too but recorded experience, and a product of history; of\\nwhich, therefore, reasoning and belief, no less than action and passion, are\\nessential materials?\\n—Thomas Carlyle, Critical and Miscellaneous Essays\\nHistory is Philosophy from Examples.\\n—Dionysius, Ars Rhetorica\\nScience is built upon facts, as a house is built of stones; but an accumulation of\\nfacts is no more a science than a heap of stones is a house.'), Document(metadata={}, page_content='—Henri Poincaré, Science and Hypothesis\\nYou seek for knowledge and wisdom as I once did; and I ardently hope that the\\ngratification of your wishes may not be a serpent to sting you, as mine has been.\\n—Mary Shelley, Frankenstein\\n1.1 Introduction\\nAlthough Artificial Intelligence is one of the newest fields of intellectual\\nresearch, its foundations began thousands of years ago. In studying Artifi-\\ncial Intelligence, it is useful to have an understanding of the background of'), Document(metadata={}, page_content='a number of other subjects, primarily philosophy, linguistics, psychology,\\nand biology.\\nThis chapter will present a selected history of the thinking and research\\nthat led up to the present state of what we now call Artificial Intelligence.'), Document(metadata={}, page_content='4 CHAPTER 1 A Brief History of Artificial Intelligence\\nIn this chapter, we will look at the contributions made by philosophy, lin-\\nguistics, psychology, and biology to Artificial Intelligence. We will also look\\nat the difference between the claims made by proponents of weak AI (AI is\\na commonly used abbreviation for Artificial Intelligence) compared with\\nthose who support strong AI, as well as look at the difference between\\nstrong methods and weak methods in Artificial Intelligence.'), Document(metadata={}, page_content='We will begin by looking at Artificial Intelligence itself and trying to find a\\ndefinition for the subject.\\n1.2 What Is Artificial Intelligence?\\nPerhaps a better starting point would be to ask, “What is intelligence?” This\\nis a complex question with no well-defined answer that has puzzled biolo-\\ngists, psychologists, and philosophers for centuries. In Chapter 13 we pose\\na similar question when we ask, “What is life?” in order to help us under-'), Document(metadata={}, page_content='stand what Artificial Life, a branch of Artificial Intelligence, is.\\nOne could certainly define intelligence by the properties it exhibits: an abil-\\nity to deal with new situations; the ability to solve problems, to answer\\nquestions, to devise plans, and so on. It is perhaps harder to define the dif-\\nference between the intelligence exhibited by humans and that exhibited by\\ndolphins or apes.\\nFor now we will confine ourselves, then, to the somewhat simpler question'), Document(metadata={}, page_content='that is posed by the title of this section: What Is Artificial Intelligence?\\nA simple definition might be as follows:\\nArtificial intelligence is the study of systems that act in a way that to any\\nobserver would appear to be intelligent.\\nThis definition is fine, but in fact it does not cover the whole of Artificial\\nIntelligence. In many cases, Artificial Intelligence techniques are used to\\nsolve relatively simple problems or complex problems that are internal to'), Document(metadata={}, page_content='more complex systems. For example, the search techniques described in\\nChapter 4 are rarely used to provide a robot with the ability to find its way\\nout of a maze, but are frequently used for much more prosaic problems.\\nThis may lead us to another definition of Artificial Intelligence, as follows:\\nArtificial Intelligence involves using methods based on the intelligent behavior\\nof humans and other animals to solve complex problems.'), Document(metadata={}, page_content='1.3 Strong Methods and Weak Methods 5\\nHence, in Chapter 20, we look at systems that are able to “understand”\\nhuman speech, or at least are able to extract some meaning from human\\nutterances, and carry out actions based on those utterances. Such systems\\nmay not be designed to behave in an intelligent way, but simply to provide\\nsome useful function. The methods they use, however, are based on the\\nintelligent behavior of humans.'), Document(metadata={}, page_content='intelligent behavior of humans.\\nThis distinction is brought into sharper contrast when we look at the dif-\\nference between so-called strong AI and weak AI.\\nThe followers of strong AI believe that by giving a computer program suffi-\\ncient processing power, and by providing it with enough intelligence, one\\ncan create a computer that can literally think and is conscious in the same\\nway that a human is conscious.\\nMany philosophers and Artificial Intelligence researchers consider this view'), Document(metadata={}, page_content='to be false, and even ludicrous. The possibility of creating a robot with emo-\\ntions and real consciousness is one that is often explored in the realms of\\nscience fiction but is rarely considered to be a goal of Artificial Intelligence.\\nWeak AI, in contrast, is simply the view that intelligent behavior can be\\nmodeled and used by computers to solve complex problems. This point of\\nview argues that just because a computer behaves intelligently does not'), Document(metadata={}, page_content='prove that it is actually intelligent in the way that a human is. We will exam-\\nine this argument in more detail in Chapter 2, when we look at the Chinese\\nRoom thought experiment and the arguments around it.\\n1.3 Strong Methods and Weak Methods\\nWe have discussed the difference between the claims of weak AI and strong\\nAI. This difference is not to be confused with the difference between strong\\nmethods and weak methods.\\nWeak methods in Artificial Intelligence use systems such as logic, auto-'), Document(metadata={}, page_content='mated reasoning, and other general structures that can be applied to a wide\\nrange of problems but that do not necessarily incorporate any real knowl-\\nedge about the world of the problem that is being solved.\\nIn contrast, strong method problem solving depends on a system being\\ngiven a great deal of knowledge about its world and the problems that it\\nmight encounter. Strong method problem solving depends on the weak'), Document(metadata={}, page_content='6 CHAPTER 1 A Brief History of Artificial Intelligence\\nmethods because a system with knowledge is useless without some\\nmethodology for handling that knowledge.\\nHence, the production systems we will examine in Chapter 9 are based on\\nthe weak method expert system shells but use strong method rules to\\nencode their knowledge.\\nThe earliest research in Artificial Intelligence focused on weak methods.\\nNewell and Simon’s General Problem Solver (GPS), which is discussed in'), Document(metadata={}, page_content='Chapter 15, was an attempt to use weak methods to build a system that\\ncould solve a wide range of general problems. That this approach ulti-\\nmately failed led to a realization that more was needed than simple repre-\\nsentations and algorithms to make Artificial Intelligence work: knowledge\\nwas the key ingredient.\\nA great number of the subjects covered in this book are weak methods.\\nThis does not mean that they are not worth studying, or even that they are'), Document(metadata={}, page_content='not useful. In many situations, weak methods are ideal for solving prob-\\nlems. However, the addition of knowledge is almost always essential to\\nbuild systems that are able to deal intelligently with new problems; if our\\naim is to build systems that appear to behave intelligently, then strong\\nmethods are certainly essential.\\n1.4 From Aristotle to Babbage\\nIn Chapter 7 of this book, we present the propositional and predicate log-'), Document(metadata={}, page_content='ics. These systems for logical reasoning are based on the logic invented by\\nAristotle, a philosopher from ancient Greece, who lived from 384 to 322\\nB.C. and who studied under Plato during that time. The writings of Aristo-\\ntle (on this and many other subjects) have formed the basis for a great deal\\nof our modern scientific thinking.\\nFrom the point of view of Artificial Intelligence, the most interesting aspect\\nof Aristotle’s work is his study of logic. He invented the idea of the syllo-'), Document(metadata={}, page_content='gism, which he defined as follows:\\n“A discourse in which certain things having been stated, something else\\nfollows of necessity from their being so. ”\\nAristotle’s logic was developed and expanded on by later philosophers,\\nmathematicians, and logicians. The first real steps in the study of logic after\\nAristotle took place in the 12th century, when Peter Abelard (who lived'), Document(metadata={}, page_content='1.5 Alan Turing and the 1950s 7\\nfrom 1079 to 1142 A.D.) wrote Dialectica, a treatise on logic. In the follow-\\ning centuries, more work was carried out, but the greatest developments\\nwere made in the last few centuries.\\nIn the late 17th to early 18th centuries, Gottfried Leibniz, the German\\nmathematician and philosopher who along with Isaac Newton had a part\\nin the invention of the calculus used by mathematicians today, invented the'), Document(metadata={}, page_content='idea of developing a formal mathematical language for reasoning. His uni-\\nversal language would allow us to express with great precision problems of\\nall kinds, and then go about solving them. Leibniz did not succeed in creat-\\ning this universal language, but his work provided the basis for the propo-\\nsitional and predicate logics that are so important to Artificial Intelligence\\nresearch today.\\nIn the 19th century, George Boole, an English mathematician, who lived'), Document(metadata={}, page_content='from 1815 to 1864, developed Boolean algebra, the logical system we still\\nuse as part of propositional and predicate logics. Boolean algebra is widely\\nused by electronics engineers in developing logical gates for silicon chips\\nand is also used by computer scientists. Boolean algebra provides a language\\nfor expressing concepts such as “A is true” and “A is true but B is false. ”\\nAround the same time that Boole was inventing his algebra, Charles Babbage'), Document(metadata={}, page_content='invented the world’s first computer—the Analytic Engine. He didn’t ever\\nmanage to build the computer, but his designs were later used to build a work-\\ning model. The designs of computers in the 20th century didn’t bear much\\nresemblance to Babbage’s computer, but they certainly owed a great deal to it.\\nBabbage’s idea of a digital computer remained a dream until around the\\nmiddle of the 20th century. By the 1950s, a number of working computers'), Document(metadata={}, page_content='had been built. Unlike Babbage’s mechanical engines, these computers were\\nelectronic. The very first electromechanical computers were soon replaced\\nby computers based on vacuum tubes.\\n1.5 Alan Turing and the 1950s\\nOne of the great figures in the history of Artificial Intelligence is Alan Tur-\\ning. During World War II, Turing famously worked in Bletchley Park, help-\\ning to solve the Germans’ codes. After the war, he began to work on the idea'), Document(metadata={}, page_content='of the possibility of building a computer that could think. His paper pub-\\nlished in 1950, Computing Machinery & Intelligence , was one of the first\\npapers to be written on this subject.'), Document(metadata={}, page_content='8 CHAPTER 1 A Brief History of Artificial Intelligence\\nThe Turing test was designed by Turing as a way to judge the success or\\notherwise of an attempt to produce a thinking computer. More specifically,\\nit was based on the idea that if a person who interrogated the computer\\ncould not tell if it was a human or a computer, then to all intents and pur-\\nposes, Turing said, it is intelligent.\\nThe test is designed as follows:\\nThe interrogator is given access to two individuals, one of whom is a'), Document(metadata={}, page_content='human and the other of whom is a computer. The interrogator can ask the\\ntwo individuals questions, but cannot directly interact with them. Probably\\nthe questions are entered into a computer via a keyboard, and the responses\\nappear on the computer screen.\\nThe human is intended to attempt to help the interrogator, but if the com-\\nputer is really intelligent enough, it should be able to fool the interrogator\\ninto being uncertain about which is the computer and which is the human.'), Document(metadata={}, page_content='The human can give answers such as “I’m the human—the other one is the\\ncomputer, ” but of course, so can the computer. The real way in which the\\nhuman proves his or her humanity is by giving complex answers that a\\ncomputer could not be expected to comprehend. Of course, the inventors\\nof the truly intelligent computer program would have given their program\\nthe ability to anticipate all such complexities.\\nTuring’s test has resulted in a number of computer programs (such as'), Document(metadata={}, page_content='Weizenbaum’s ELIZA, designed in 1965) that were designed to mimic\\nhuman conversation. Of course, this in itself is not a particularly useful\\nfunction, but the attempt has led to improvements in understanding of\\nareas such as natural language processing. T o date, no program has passed\\nthe Turing test, although cash prizes are regularly offered to the inventor of\\nthe first computer program to do so.\\nLater in the 1950s computer programs began to be developed that could'), Document(metadata={}, page_content='play games such as checkers and chess (see Chapter 6), and also the first\\nwork was carried out into developing computer programs that could\\nunderstand human language (Chapter 20).\\nA great deal of work at this stage was done in computer translation. It was,\\nindeed, widely believed that computers could eventually be programmed to\\ntranslate accurately from one human language to another. It has since been\\nfound that the task of machine translation is actually an extremely difficult'), Document(metadata={}, page_content='1.6 The 1960s to the 1990s 9\\none, and not one that has yet been completely solved. This subject is dis-\\ncussed in more detail in Chapter 20.\\nIn 1956, the term Artificial Intelligence was first used by John McCarthy at\\na conference in Dartmouth College, in Hanover, New Hampshire.\\nIn 1957, Newell and Simon invented the idea of the GPS, whose purpose\\nwas, as the name suggests, to solve almost any logical problem. The program\\nused a methodology known as means ends analysis, which is based on the'), Document(metadata={}, page_content='idea of determining what needs to be done and then working out a way to\\ndo it. This works well enough for simple problems, but AI researchers soon\\nrealized that this kind of method could not be applied in such a general\\nway—the GPS could solve some fairly specific problems for which it was\\nideally suited, but its name was really a misnomer.\\nAt this time there was a great deal of optimism about Artificial Intelligence.\\nPredictions that with hindsight appear rash were widespread. Many com-'), Document(metadata={}, page_content='mentators were predicting that it would be only a few years before comput-\\ners could be designed that would be at least as intelligent as real human\\nbeings and able to perform such tasks as beating the world champion at\\nchess, translating from Russian into English, and navigating a car through a\\nbusy street. Some success has been made in the past 50 years with these\\nproblems and other similar ones, but no one has yet designed a computer'), Document(metadata={}, page_content='that anyone would describe reasonably as being intelligent.\\nIn 1958, McCarthy invented the LISP programming language, which is still\\nwidely used today in Artificial Intelligence research.\\n1.6 The 1960s to the 1990s\\nSince the 1950s, a great deal of the original optimism has gone out of Arti-\\nficial Intelligence and has been replaced with a degree of realism.\\nThe aim of the study of Artificial Intelligence is no longer to create a robot'), Document(metadata={}, page_content='as intelligent as a human, but rather to use algorithms, heuristics, and\\nmethodologies based on the ways in which the human brain solves prob-\\nlems. Hence, systems have been designed such as Thomas Evans’ Analogy\\nand Melanie Mitchell’s Copycat Architecture, which were designed to be\\nable to solve problems that involve analogies. Mitchell’s Copycat, for exam-\\nple, can solve problems such as “ABC is to CBA as DEF is to ???. ”'), Document(metadata={}, page_content='10 CHAPTER 1 A Brief History of Artificial Intelligence\\nThe ability to solve problems of this kind does not represent intelligence, but\\nthe development of systems that can solve such problems is the mainstay of\\nArtificial Intelligence research and arguably an extremely useful step along\\nthe way to producing more and more useful computer software systems.\\nIn Chapter 2, we will discuss the subject of whether a computer program\\ncan really be “intelligent. ”'), Document(metadata={}, page_content='can really be “intelligent. ”\\nIn the most recent decades, the study of Artificial Intelligence has flour-\\nished. Areas of particular importance include the following:\\n■ machine learning\\n■ multi-agent systems\\n■ artificial life\\n■ computer vision\\n■ planning\\n■ playing games (chess in particular)\\nIn Chapter 2, we will look at the prevalence of Artificial Intelligence in\\nthe world today. This prevalence has more than justified the work of the\\npast 50 years.\\n1.7 Philosophy'), Document(metadata={}, page_content='past 50 years.\\n1.7 Philosophy\\nThe philosophy of great thinkers, from Plato to Descartes and to Daniel\\nDennett, has had a great deal of influence on the modern study of Artificial\\nIntelligence.\\nThe influence of Aristotle has already been mentioned, but it has been\\nargued (Dreyfus, 1972) that the history of Artificial Intelligence begins\\nwhen Plato wrote that his teacher Socrates said, “I want to know what is\\ncharacteristic of piety which makes all actions pious. . . that I may have it to'), Document(metadata={}, page_content='turn to, and to use as a standard whereby to judge your actions and those of\\nother men.”\\nSocrates was claiming that an algorithm could be defined that described\\nthe behavior of humans and determined whether a person’s behavior was\\ngood or bad.\\nThis leads us to a fundamental question that has been asked by philoso-\\nphers and students of Artificial Intelligence for many years: Is there more to'), Document(metadata={}, page_content='1.8 Linguistics 11\\nthe mind than simply a collection of neurons? Or, to put it another way, if\\neach neuron in the human brain was replaced by an equivalent computa-\\ntional device, would the resultant be the same person? Would it indeed be\\ncapable of intelligent thought?\\nThis kind of question is regularly debated by modern philosophers such as\\nDaniel Dennett, and while the answer is far from clear, it is an instructive'), Document(metadata={}, page_content='debate to follow, and its implications for Artificial Intelligence are enormous.\\nIn the 17th century, the great philosopher René Descartes was a strong believer\\nin dualism, the idea that the universe consists of two entirely separate things:\\nmind and matter. Descartes’s view was that the mind (or soul) was entirely\\nseparate from the physical body and not constrained by it in any way.\\nImportantly, Descartes did not believe that this dualism extended to ani-'), Document(metadata={}, page_content='mals. In other words, in his view a cat or a dog is simply a machine: a highly\\ncomplex machine, but a machine nonetheless. This view gives hope to the\\nproponents of Artificial Intelligence who believe that by simply putting\\nenough computing power together and programming it in the correct way,\\na machine could be made to behave in the same way as an animal, or even a\\nhuman being.\\n1.8 Linguistics\\nThe study of human language has a vital role to play in Artificial Intelli-'), Document(metadata={}, page_content='gence. As is discussed in some detail in Chapter 20, compared with com-\\nputer languages such as Java and LISP , human languages are extraordinarily\\ncomplex and are full of pitfalls that almost seem designed to trap anyone\\n(human or computer) inexperienced in the use of the language.\\nThis complexity, combined with a sense of optimism, may well have been\\npart of the reason that natural language processing was such a popular\\nresearch area in the early days of Artificial Intelligence.'), Document(metadata={}, page_content='Some of the optimism surrounding Natural Language Processing came\\nfrom the writings of Noam Chomsky, who in the 1950s proposed his the-\\nory of Syntactic Structures, which was a formal theory of the structure of\\nhuman language. His theory also attempted to provide a structure for\\nhuman knowledge, based on the knowledge of language.\\nThis idea of knowledge representation is at the very core of Artificial Intel-\\nligence and is a recurring theme throughout this book.'), Document(metadata={}, page_content='12 CHAPTER 1 A Brief History of Artificial Intelligence\\nAlmost all of the techniques described in this book depend on a formal\\nmethod of representation for knowledge that enables a computer to use\\ninformation from the world, or concerning the problems it is to solve,\\nwithout necessarily needing to understand that knowledge.\\nThere is a close relationship between linguistics and Artificial Intelligence,\\nand the two fields join together in the study of natural language processing,'), Document(metadata={}, page_content='which is discussed in some detail in Chapter 20.\\n1.9 Human Psychology and Biology\\nSome of the techniques, such as search algorithms, described in this book\\ndo not clearly map onto any specific biological or psychological function of\\nhuman beings. On the other hand, many of them do. For example, McCul-\\nloch and Pitts’s electronic neurons, which are used today to build neural\\nnetworks, are directly based on the way in which neurons in the human\\nbrain function.'), Document(metadata={}, page_content='brain function.\\nIn a similar way, much research in Artificial Intelligence has been related to\\ncognitive psychology , which is based on the idea that the human brain\\nuses knowledge or information that it is capable of processing in order to\\nsolve problems, make decisions, draw conclusions, and carry out other\\nintelligent acts.\\nThis form of psychology was in contrast to behaviorism, which prevailed\\nfor much of the first half of the 20th century. Behaviorism relates behavior'), Document(metadata={}, page_content='directly to stimuli, without taking into account knowledge or information\\nthat might be contained in the brain. This is the kind of psychology that\\nPavlov was demonstrating in his famous experiment with dogs.\\nPsychology is certainly useful to the study of Artificial Intelligence in one\\nrespect: it helps to answer the important question, “What is intelligence?”\\nAs we have seen already, this is a difficult question to answer, but in study-'), Document(metadata={}, page_content='ing it, psychologists give us a great deal of information that is useful in\\nforming the ideas behind Artificial Intelligence.\\n1.10 AI Programming Languages\\nA number of programming languages exist that are used to build Artificial\\nIntelligence systems. General programming languages such as C++ and\\nJava are often used because these are the languages with which most com-'), Document(metadata={}, page_content='1.10 AI Programming Languages 13\\nputer scientists have experience. There also exist two programming lan-\\nguages that have features that make them particularly useful for program-\\nming Artificial Intelligence projects—PROLOG and LISP .\\nWe will now provide a brief overview of these two languages and explain\\nhow they are used in Artificial Intelligence research. Of course, a number of\\nother programming languages exist that are also widely used for Artificial'), Document(metadata={}, page_content='Intelligence, but we will focus on PROLOG and LISP because these are cer-\\ntainly the most widely used and the ones on which there is the widest range\\nof relevant literature.\\n1.10.1 PROLOG\\nPROLOG (PROgramming in LOGic) is a language designed to enable pro-\\ngrammers to build a database of facts and rules, and then to have the sys-\\ntem answer questions by a process of logical deduction using the facts and\\nrules in the database.\\nFacts entered into a PROLOG database might look as follows:'), Document(metadata={}, page_content='tasty (cheese).\\nmade_from (cheese, milk).\\ncontains (milk, calcium).\\nThese facts can be expressed as the following English statements:\\nCheese is tasty.\\nCheese is made from milk.\\nMilk contains calcium.\\nWe can also specify rules in a similar way, which express relationships between\\nobjects and also provide the instructions that the PROLOG theorem prover\\nwill use to answer queries. The following is an example of a rule in PROLOG:\\ncontains (X, Y) :- made_from (X, Z), contains (Z, Y).'), Document(metadata={}, page_content='This rule is made up of two main parts, separated by the symbol “:-” .\\nThe rule thus takes the form:\\nB :- A\\nwhich means “if A is true, then B is true, ” or “A implies B. ”\\nHence, the rule given above can be translated as “If X is made from Z and Z\\ncontains Y then X contains Y. ”'), Document(metadata={}, page_content='14 CHAPTER 1 A Brief History of Artificial Intelligence\\nIn Chapters 7, 8, and 9, we make a great deal of use of rules of this kind.\\nHaving entered the three facts and one rule given above, the user might\\nwant to ask the system a question:\\n?- contains (cheese, calcium).\\nUsing a process known as resolution (which is described in detail in Chap-\\nter 8), the PROLOG system is able to use the rule and the facts to determine\\nthat because cheese is made from milk, and because milk contains calcium,'), Document(metadata={}, page_content='therefore cheese does contain calcium. It thus responds:\\nyes\\nIt would also be possible to ask the system to name everything that con-\\ntains calcium:\\n?- contains (X, calcium)\\nThe system will use the same rules and facts to deduce that milk and cheese\\nboth contain calcium, and so will respond:\\nX=milk.\\nX=cheese.\\nThis has been a very simple example, but it should serve to illustrate how\\nPROLOG works. Far more complex databases of facts and rules are rou-'), Document(metadata={}, page_content='tinely built using PROLOG, and in some cases simple databases are built\\nthat are able to solve complex mathematical problems.\\nPROLOG is not an efficient programming language, and so for many prob-\\nlems a language such as C++ would be more appropriate. In cases where\\nlogical deduction is all that is required, and the interactive nature of the\\nPROLOG interface is suitable, then PROLOG is the clear choice. PROLOG\\nprovides a way for programmers to manipulate data in the form of rules'), Document(metadata={}, page_content='and facts without needing to select algorithms or methodologies for han-\\ndling those data.\\n1.10.2 LISP\\nLISP (LISt Programming) is a language that more closely resembles the\\nimperative programming languages such as C++ and Pascal than does\\nPROLOG. As its name suggests, LISP is based around handling of lists of\\ndata. A list in LISP is contained within brackets, such as:\\n[A B C]'), Document(metadata={}, page_content='Chapter Summary 15\\nThis is a list of three items. LISP uses lists to represent data, but also to rep-\\nresent programs. Hence, a program in LISP can be treated as data. This\\nintroduces the possibility of writing self-modifying programs in LISP , and\\nas we see in Chapter 13, it also allows us to use evolutionary techniques to\\n“evolve” better LISP programs.\\nLISP is a far more complex language syntactically than PROLOG, and so we'), Document(metadata={}, page_content='will not present any detail on its syntax here. It provides the usual kinds of\\nmechanisms that other programming languages provide, such as assign-\\nment, looping, evaluating functions, and conditional control\\n( i f ...t h e n ...) .I t  also provides a great deal of list manipulation functions,\\nsuch as car and cdr, which are used to return the first entry in a list and all\\nthe entries except for the first entry, respectively.\\n1.11 Chapter Summary'), Document(metadata={}, page_content='1.11 Chapter Summary\\n■ Intelligence is difficult to define, and as a result Artificial Intelli-\\ngence is also hard to define.\\n■ One definition of Artificial Intelligence is:\\nArtificial intelligence is the study of systems that act in a way that to\\nany observer would appear to be intelligent.\\n■ Proponents of strong AI believe that a computer that behaves in an\\nintelligent way is capable of possessing mental states and, there-\\nfore, of being truly conscious and intelligent in the same way that'), Document(metadata={}, page_content='humans are.\\n■ Weak AI is a less controversial idea—that computers can be pro-\\ngrammed to behave in intelligent ways in order to solve specific\\nproblems. This book is concerned with the methods of weak AI.\\n■ Weak and strong AI are not to be confused with weak and\\nstrong methods.\\n■ Weak methods are those that do not rely on any knowledge or\\nunderstanding of the world and the problems being solved. Most\\nof the techniques described in this book are weak methods.'), Document(metadata={}, page_content='■ Strong methods are those that use knowledge about the world and\\nabout the problem being solved. The strong method approach is\\nessential for solving many complex real world problems using Arti-\\nficial Intelligence.'), Document(metadata={}, page_content='16 CHAPTER 1 A Brief History of Artificial Intelligence\\n■ In studying Artificial Intelligence, it is extremely useful to understand\\nthe background of philosophy, linguistics, biology, and psychology.\\n■ Philosophers, from Plato and Aristotle to Searle and Dennett, have\\nasked questions and provided opinions concerning the nature of\\nintelligence and the ability to define it in a way that would enable\\nus to program a computer with real intelligence.'), Document(metadata={}, page_content='us to program a computer with real intelligence.\\n■ The 1950s were a time of great optimism in Artificial Intelligence\\nand also a time of great progress in the field.\\n■ Turing’s test is a way to determine if a computer is truly intelligent,\\nby seeing if it could fool a human in conversation into thinking\\nthat it too was human. It is widely believed today that even if a\\ncomputer could pass the Turing test, it would still not truly be con-\\nscious or intelligent in the way that humans are.'), Document(metadata={}, page_content='scious or intelligent in the way that humans are.\\n■ In 1956 the term Artificial Intelligence was coined by John McCarthy.\\n■ Since the 1950s, the study of Artificial Intelligence has been fla-\\nvored with a great deal more realism. The progress in recent years\\nhas been phenomenal.\\n1.12 Review Questions\\n1.1 What is intelligence?\\n1.2 What is Artificial Intelligence? What do you hope to learn by read-\\ning this book?\\n1.3 Is Artificial Intelligence a branch of computer science or an alter-'), Document(metadata={}, page_content='native to computer science?\\n1.4 Why is Artificial Intelligence a worthwhile subject to study?\\n1.5 Explain the difference between strong and weak methods in Artifi-\\ncial Intelligence. Explain how this dichotomy differs from the dif-\\nference between strong and weak AI.\\n1.6 Why are PROLOG and LISP so well suited to Artificial Intelligence\\nresearch? Do you think languages such as C++ and Java could also\\nbe used for such research?\\n1.7 What do you think led mankind to embark upon the study of Arti-'), Document(metadata={}, page_content='ficial Intelligence? Which fields of study particularly fed into it?'), Document(metadata={}, page_content='Further Reading 17\\nWhat human desires did the study of Artificial Intelligence seek to\\nsatisfy?\\n1.8 When did Artificial Intelligence first begin to be studied? Y our\\nanswer should be more detailed than a simple date.\\n1.13 Further Reading\\nCrevier (1999) gives a fascinating history of the subject of Artificial\\nIntelligence.\\nThroughout this book, details are given of other books that can be refer-\\nenced to learn more about the material covered herein. The following'), Document(metadata={}, page_content='books are general Artificial Intelligence texts that cover almost all of the\\ntopics covered by this book and also provide excellent introductions to the\\nsubject as a whole.\\nEach of these books takes a different approach to the material, and it is\\nworth selecting the text that best fits your personal preferences in studying\\nthis subject.\\nFor example, Russell and Norvig present the material in terms of intelligent\\nagents. Winston explains his material with a great deal of examples but'), Document(metadata={}, page_content='tends not to go into a great deal of detail, while Luger goes into greater\\ndepth, but with fewer examples. Schalkoff gives a good coverage of Artifi-\\ncial Intelligence using examples in PROLOG and LISP; it also therefore\\nserves as a useful text in those languages.\\nComputation & Intelligence, edited by George Luger, contains a number of\\nextremely important papers collected from the whole history of Artificial\\nIntelligence. It includes papers by such pioneers of the subject as Alan Tur-'), Document(metadata={}, page_content='ing, Marvin Minsky, John McCarthy, Allen Newell, and Herbert Simon.\\nThe Handbook of Artificial Intelligence, edited by A. Barr and E. Feigenbaum\\n(1989 – William Kaufman)\\nThe Essence of Artificial Intelligence, by Alison Cawsey (1998 – Prentice Hall)\\nIntroduction to Artificial Intelligence , by Eugene Charniak and Drew\\nMcDermott (1985 – Addison Wesley; out of print)\\nThe Computational Brain , by Patricia S. Churchland and T errence J.\\nSejnowski (1992 – The MIT Press)'), Document(metadata={}, page_content='Sejnowski (1992 – The MIT Press)\\nAI: The Tumultuous History of the Search for Artificial Intelligence, by Daniel\\nCrevier (1999 – Basic Books)'), Document(metadata={}, page_content='18 CHAPTER 1 A Brief History of Artificial Intelligence\\nUnderstanding Artificial Intelligence (Science Made Accessible), compiled by\\nSandy Fritz (2002 – Warner Books)\\nThe Anatomy of Programming Languages, by Alice E. Fischer and Frances S.\\nGrodzinsky (1993 – Prentice Hall)\\nIntroduction to Artificial Intelligence , by Philip C. Jackson (1985 – Dover\\nPublications)\\nAI Application Programming, by M. Tim Jones (2003 – Charles River Media)'), Document(metadata={}, page_content='Artificial Intelligence: Structures and Strategies for Complex Problem-Solving,\\nby George F. Luger (2002 – Addison Wesley)\\nComputation & Intelligence: Collected Readings , edited by George F. Luger\\n(1995 – The AAAI Press / The MIT Press)\\nArtificial Intelligence: A Guide to Intelligent Systems, by Michael Negnevitsky\\n(2002 – Addison Wesley)\\nArtificial Intelligence: A New Synthesis , by N.J. Nilsson (1998 – Morgan\\nKauffman)\\nArtificial Intelligence: A Modern Approach , by Stuart Russell and Peter'), Document(metadata={}, page_content='Norvig (1995 – Prentice Hall)\\nThe Emperor’s New Mind: Concerning Computers, Minds, and the Laws of\\nPhysics, by Roger Penrose (1989 – Oxford University Press)\\nUnderstanding Intelligence, by Rolf Pfeiffer and Christian Scheier (2000 –\\nThe MIT Press)\\nArtificial Intelligence: An Engineering Approach, by Robert J. Schalkoff (1990\\n– McGraw Hill)\\nThe Encyclopedia of Artificial Intelligence, edited by S.C. Shapiro (1992 - Wiley)\\nArtificial Intelligence, by Patrick Henry Winston (1992 – Addison Wesley)'), Document(metadata={}, page_content='2CHAPTER\\nUses and Limitations\\nThe limits of my language mean the limits of my world.\\n—Ludwig Wittgenstein,Tractatus Logico-Philosophicus\\nWhy, sometimes I’ve believed as many as six impossible things before breakfast.\\n—Lewis Carroll, Through the Looking Glass\\nWho hath put wisdom in the inward parts? Or who hath given understanding\\nto the heart?\\n—The Book of Job, Chapter 38, V erse 36\\n2.1 Introduction\\nAs was explained in Chapter 1, the early history of Artificial Intelligence'), Document(metadata={}, page_content='was filled with a great deal of optimism—optimism that today seems at\\nbest to have been unfounded. In this chapter, we look at some of the argu-\\nments against strong AI (the belief that a computer is capable of having\\nmental states) and also look at the prevalence of Artificial Intelligence\\ntoday and explain why it has become such a vital area of study.\\nWe will also look at the extent to which the Artificial Intelligence commu-'), Document(metadata={}, page_content='nity has been successful so far in achieving the goals that were believed to\\nbe possible decades ago. In particular, we will look at whether the computer\\nHAL in the science fiction film 2001: A Space Odyssey is a possibility with\\ntoday’s technologies.\\nWe will also look at the prevalence of Artificial Intelligence, and how it is\\nused in the world today, the 21st century.'), Document(metadata={}, page_content='20 CHAPTER 2 Uses and Limitations\\n2.2 The Chinese Room\\nWe will start by examining philosophical objections to strong AI, in partic-\\nular the Chinese Room argument of John Searle.\\nThe American philosopher John Searle has argued strongly against the pro-\\nponents of strong AI who believe that a computer that behaves sufficiently\\nintelligently could in fact be intelligent and have consciousness, or mental\\nstates, in much the same way that a human does.'), Document(metadata={}, page_content='states, in much the same way that a human does.\\nOne example of this is that it is possible using data structures called scripts\\n(see Chapter 17) to produce a system that can be given a story (for example,\\na story about a man having dinner in a restaurant) and then answer ques-\\ntions (some of which involve a degree of subtlety) about the story. Propo-\\nnents of strong AI would claim that systems that can extend this ability to\\ndeal with arbitrary stories and other problems would be intelligent.'), Document(metadata={}, page_content='Searle’s Chinese Room experiment was based on this idea and is described\\nas follows:\\nAn English-speaking human is placed inside a room. This human does not\\nspeak any language other than English and in particular has no ability to\\nread, speak, or understand Chinese.\\nInside the room with the human are a set of cards, upon which are printed\\nChinese symbols, and a set of instructions that are written in English.\\nA story, in Chinese, is fed into the room through a slot, along with a set of'), Document(metadata={}, page_content='questions about the story. By following the instructions that he has, the\\nhuman is able to construct answers to the questions from the cards with\\nChinese symbols and pass them back out through the slot to the questioner.\\nIf the system were set up properly, the answers to the questions would be suf-\\nficient that the questioner would believe that the room (or the person inside\\nthe room) truly understood the story, the questions, and the answers it gave.'), Document(metadata={}, page_content='Searle’s argument is now a simple one. The man in the room does not\\nunderstand Chinese. The pieces of card do not understand Chinese. The\\nroom itself does not understand Chinese, and yet the system as a whole is\\nable to exhibit properties that lead an observer to believe that the system\\n(or some part of it) does understand Chinese.'), Document(metadata={}, page_content='2.3 HAL—Fantasy or Reality? 21\\nIn other words, running a computer program that behaves in an intelligent\\nway does not necessarily produce understanding, consciousness, or real\\nintelligence.\\nThis argument clearly contrasts with Turing’s view that a computer system\\nthat could fool a human into thinking it was human too would actually be\\nintelligent.\\nOne response to Searle’s Chinese Room argument, the Systems Reply,\\nclaims that although the human in the room does not understand Chinese,'), Document(metadata={}, page_content='the room itself does. In other words, the combination of the room, the\\nhuman, the cards with Chinese characters, and the instructions form a sys-\\ntem that in some sense is capable of understanding Chinese stories. There\\nhave been a great number of other objections to Searle’s argument, and the\\ndebate continues.\\nThere are other objections to the ideas of strong AI. TheHalting Problemand\\nGödel’s incompleteness theorem tell us that there are some functions that a'), Document(metadata={}, page_content='computer cannot be programmed to compute, and as a result, it would seem to\\nbe impossible to program a computer to perform all the computations needed\\nfor real consciousness. This is a difficult argument, and one potential response\\nto it is to claim that the human brain is in fact a computer, and that although it\\nmust also be limited by the Halting Problem, it is still capable of intelligence.\\nThis claim that the human brain is a computer is an interesting one. Upon it'), Document(metadata={}, page_content='is based the idea of neural networks. By combining the processing power of\\nindividual neurons, we are able to produce artificial neural networks that are\\ncapable of solving extremely complex problems, such as recognizing faces.\\nProponents of strong AI might argue that such successes are steps along the\\nway to producing an electronic human being, whereas objectors would point\\nout that this is simply a way to solve one small set of problems—not only'), Document(metadata={}, page_content='does it not solve the whole range of problems that humans are capable of,\\nbut it also does not in any way exhibit anything approaching consciousness.\\n2.3 HAL—Fantasy or Reality?\\nOne of the most famous fictional accounts of Artificial Intelligence comes\\nin the film 2001: A Space Odyssey, based on the story by Arthur C. Clarke.\\nOne of the main characters in the film is HAL, a Heuristically programmed\\nALgorithmic computer. In the film, HAL behaves, speaks, and interacts'), Document(metadata={}, page_content='22 CHAPTER 2 Uses and Limitations\\nwith humans in much the same way that a human would (albeit in a dis-\\nembodied form). In fact, this humanity is taken to extremes by the fact that\\nHAL eventually goes mad.\\nIn the film, HAL played chess, worked out what people were saying by read-\\ning their lips, and engaged in conversation with other humans. How many\\nof these tasks are computers capable of today?\\nWe shall see in Chapter 6 that there has been a great deal of success with'), Document(metadata={}, page_content='developing computers that can play chess. In 1997, a computer, Deep Blue,\\nbeat the chess world champion Garry Kasparov. As we discuss in Chapter 6,\\nthis was not the end of supremacy at chess for mankind, however. The vic-\\ntory was not a particularly convincing one and has not been repeated.\\nChess-playing computers are certainly capable of beating most human\\nchess players, but those who predicted that chess computers would be'), Document(metadata={}, page_content='vastly superior to even the best human players by now were clearly wrong.\\nIn some games, such as Go, the best computers in the world are able to play\\nonly at the level of a reasonably accomplished amateur human player. The\\ngame is so complex that even the best heuristics and Artificial Intelligence\\ntechniques are not able to empower a computer with the ability to come\\nclose to matching the capabilities of the best human players.'), Document(metadata={}, page_content='In Chapter 20, we look at techniques that are used to enable computers to\\nunderstand human language and in theory to enable them to engage in\\nconversation. Clearly no computer program has yet been designed that is\\nable to pass the Turing test and engage fully in conversation in such a way\\nthat would be indistinguishable from a human, and there is no sign that\\nany such program will be designed in the near future.\\nThe ability to interpret spoken words by examining the movement of lips is'), Document(metadata={}, page_content='one that only a few humans have. It combines a number of complex prob-\\nlems: first, the visual problem of identifying sounds from the shape of lips.\\nIn Chapter 21, we will see how computers can be programmed to interpret\\nvisual information in the same kinds of ways that humans do. Interpreting\\nthe shape of human lips would probably not be impossible, and it is likely\\nthat a neural network could be trained to solve such a problem. The next'), Document(metadata={}, page_content='problem is to combine the sounds together into words—again, not a diffi-\\ncult problem given a suitably large lexicon of words. Finally, HAL would\\nhave needed to be able to interpret and understand the words in the same\\nway that he would have done when listening to spoken words.'), Document(metadata={}, page_content='2.4 AI in the 21st Century 23\\nHAL, as portrayed in the film, did have some capabilities that Artificial\\nIntelligence has given to computers today, but it is certainly not the case\\nthat computers exist with the breadth of capabilities and in particular the\\nability to communicate in so human a manner. Finally, the likelihood of a\\ncomputer becoming insane is a rather remote one, although it is of course\\npossible that a malfunction of some kind could cause a computer to exhibit'), Document(metadata={}, page_content='properties not unlike insanity!\\nArtificial Intelligence has been widely represented in other films. The Stephen\\nSpielberg film AI: Artificial Intelligence is a good example. In this film, a cou-\\nple buy a robotic boy to replace their lost son. The audience’s sympathies are\\nfor the boy who feels emotions and is clearly as intelligent (if not more so) as\\na human being. This is strong AI, and while it may be the ultimate goal of'), Document(metadata={}, page_content='some Artificial Intelligence research, even the most optimistic proponents of\\nstrong AI would agree that it is not likely to be achieved in the next century.\\n2.4 AI in the 21st Century\\nArtificial Intelligence is all around us. The techniques described in this\\nbook are used in a staggering array of machines and systems that we use\\nevery day. Fuzzy logic, for example, is widely used in washing machines,\\ncars, and elevator control mechanisms. (Note that no one would claim that'), Document(metadata={}, page_content='as a result those machines were intelligent, or anything like it! They are\\nsimply using techniques that enable them to behave in a more intelligent\\nway than a simpler control mechanism would allow.)\\nIntelligent agents, which are described in Chapter 19, are widely used. For\\nexample, there are agents that help us to solve problems while using our\\ncomputers and agents that traverse the Internet, helping us to find docu-\\nments that might be of interest. The physical embodiment of agents,'), Document(metadata={}, page_content='robots, are also becoming more widely used. Robots are used to explore the\\noceans and other worlds, being able to travel in environments inhospitable\\nto humans. It is still not the case, as was once predicted, that robots are\\nwidely used by households, for example, to carry shopping items or to play\\nwith children, although the AIBO robotic dog produced by Sony and other\\nsimilar toys are a step in this direction.\\nExpert systems are used by doctors to help with symptoms that are hard to'), Document(metadata={}, page_content='diagnose or to prescribe treatments in cases where even human experts\\nhave difficulty.'), Document(metadata={}, page_content='24 CHAPTER 2 Uses and Limitations\\nArtificial Intelligence systems are used in a wide range of industries, from\\nhelping travel agents select suitable holidays to enabling factories to sched-\\nule machines.\\nArtificial Intelligence is particularly useful in situations where traditional\\nmethods would be too slow. Combinatorial problems, such as scheduling\\nteachers and pupils to classrooms, are not well solved by traditional com-'), Document(metadata={}, page_content='puter science techniques. In such cases, the heuristics and techniques pro-\\nvided by Artificial Intelligence can provide excellent solutions.\\nMany computer games have been designed based on Artificial Intelligence.\\nIn order to provide more realistic play, the computer game Republic: The\\nRevolution, launched in 2003, contained a million individual Artificial\\nIntelligences, each capable of interacting with the world and with the player'), Document(metadata={}, page_content='of the game, as well as capable of being manipulated by the player.\\nIt is likely that Artificial Intelligence will become more prevalent in our\\nsociety. And whether or not we eventually create an Artificial Intelligence\\nthat is truly intelligent, we are likely to find computers, machines, and other\\nobjects appearing to become more intelligent—at least in terms of the way\\nthey behave.\\n2.5 Chapter Summary\\n■ The Chinese Room argument is a thought experiment designed by'), Document(metadata={}, page_content='John Searle, which is designed to refute strong AI.\\n■ The computer HAL, as described in the film 2001: A Space Odyssey,\\nis not strictly possible using today’s technology, but many of its\\ncapabilities are not entirely unrealistic today.\\n■ The computer program, Deep Blue, beat world chess champion\\nGarry Kasparov in a six-game chess match in 1997. This feat has\\nnot been repeated, and it does not yet represent the end of human\\nsupremacy at this game.'), Document(metadata={}, page_content='supremacy at this game.\\n■ Artificial Intelligence is all around us and is widely used in indus-\\ntry, computer games, cars, and other devices, as well as being a\\nvaluable tool used in many computer software programs.\\n2.6 Review Questions\\n2.1 Explain the difference between strong AI and weak AI. Which of\\nthe two do you think this book will be about? Why?'), Document(metadata={}, page_content='Further Reading 25\\n2.2 Are there any tasks that a human can do that you think a computer\\ncould never be programmed to do? Why?\\n2.3 What kinds of problems that humans find difficult do you think\\ncomputers are particularly well suited to solve? Are there any such\\nproblems that you know of that computers cannot currently solve\\nbut which you believe computers will one day be able to solve?\\nWhat advances in technology or understanding are necessary\\nbefore those problems can be solved?'), Document(metadata={}, page_content='before those problems can be solved?\\n2.4 Explain the Chinese Room argument, and present some of the\\narguments against it, and the counter-arguments. Which do you\\nfind most convincing? How does this affect your view on the over-\\nall worth of the study of Artificial Intelligence?\\n2.5 If a computer passed the Turing T est, what would that prove? What\\nconditions would you want to be sure had been observed in setting\\nup the test?\\n2.6 If you replaced each of the neurons in your brain one by one with'), Document(metadata={}, page_content='electronic neurons (take on trust for now that electronic neurons\\nare possible), what do you think would be the effect? How would\\nyour perceptions of the world change during the process? At the\\nend of the process, would you still be you? Would you still be con-\\nscious? Would you still be capable of having mental states and\\nemotions? (Note: there are no right answers to these questions. The\\npurpose in asking them is to make you think about them and'), Document(metadata={}, page_content='hopefully to inspire you to read more about the subject.)\\n2.7 Further Reading\\nThe works of Dreyfus and Dennett provide a great introduction to the\\nphilosophical arguments surrounding strong AI. The opposing view can be\\nfound thoroughly explored in Kurzweil’s works, among others. The origi-\\nnal Chinese Room argument can be found in Searle (1980).\\nA number of other books give good coverage of the popularity of Artificial\\nIntelligence in the modern world. Challoner (2002) is probably too basic'), Document(metadata={}, page_content='for most readers but does provide an entertaining introduction to the sub-\\nject that would make a good introduction for a younger relative who was\\ninterested in learning more about the subject.\\nCambrian Intelligence: The Early History of the New AI , by Rodney A.\\nBrooks (1999 – MIT Press)'), Document(metadata={}, page_content='26 CHAPTER 2 Uses and Limitations\\nArtificial Intelligence, by Jack Challoner (2002 – Dorling Kindersley, Essen-\\ntial Science)\\nThe Turing Test and the Frame Problem: AI’s Mistaken Understanding of\\nIntelligence, by Larry J. Crockett (1994 – Intellect)\\nBrainstorms: Philosophical Essays on Mind and Psychology , by Daniel Den-\\nnett (1978 – Bradford)\\nConsciousness Explained, by Daniel Dennett (1992 – Little, Brown & Co.)\\nWhat Computers Still Can’t Do, by Hubert L. Dreyfus (1999 – The MIT Press)'), Document(metadata={}, page_content='Artificial Intelligence: The Very Idea, by J. Haugeland (1985 – The MIT Press)\\nThe Age of Spiritual Machines, by Ray Kurzweil (1999 – Viking Penguin)\\nThe Society of Mind, by Marvin Minsky (1988 – Simon & Schuster)\\nRobot: Mere Machine to Transcendent Mind , by Hans P . Moravec (2000 –\\nOxford University Press)\\nViews into the Chinese Room: New Essays on Searle and Artificial Intelligence,\\nedited by John Preston and Mark Bishop (2002 – Oxford University Press)'), Document(metadata={}, page_content='Are We Spiritual Machines?: Ray Kurzweil vs. the Critics of Strong A.I., edited\\nby Jay W. Richards (2002 – Discovery Institute)\\nThe Turing Test: The Elusive Standard of Artificial Intelligence , edited by\\nJames H. Moor (2003 – Kluwer Academic Publishers)\\nMinds, Brains, and Programs , by John R. Searle (1980 – in The Behavioral\\nand Brain Sciences, vol. 3, Cambridge University Press)\\nMinds, Brains and Science, by John R. Searle (1986 – Harvard University Press)'), Document(metadata={}, page_content='In the Mind of the Machine: The Breakthrough in Artificial Intelligence ,b y\\nKevin Warwick (1998 – Random House)\\nArguing A. I.: The Battle for Twenty-First Century Science, by Sam Williams\\n(2002 – Random House)'), Document(metadata={}, page_content='3CHAPTER\\nKnowledge Representation\\nIf, for a given problem, we have a means of checking a proposed solution, then\\nwe can solve the problem by testing all possible answers. But this always takes\\nmuch too long to be of practical interest. Any device that can reduce this search\\nmay be of value.\\n—Marvin Minsky, Steps Toward Artificial Intelligence\\nStudy is like the heaven’s glorious sun,\\nThat will not be deep-search’d with saucy looks;\\nSmall have continual plodders ever won,'), Document(metadata={}, page_content='Small have continual plodders ever won,\\nSave base authority from others’ books.\\nThese earthly godfathers of Heaven’s lights\\nThat give a name to every fixed star,\\nHave no more profit of their shining nights\\nThan those that walk and wot not what they are.\\n—William Shakespeare, Love’s Labours Lost\\nBetter the rudest work that tells a story or records a fact, than the richest with-\\nout meaning.\\n—John Ruskin, Seven Lamps of Architecture\\n3.1 Introduction'), Document(metadata={}, page_content='3.1 Introduction\\nThroughout this book we will be discussing representations. The reason for\\nthis is that in order for a computer to solve a problem that relates to the real\\nworld, it first needs some way to represent the real world internally. In dealing\\nwith that internal representation, the computer is then able to solve problems.'), Document(metadata={}, page_content='28 CHAPTER 3 Knowledge Representation\\nThis chapter introduces a number of representations that are used else-\\nwhere in this book, such as semantic nets, goal trees, and search trees, and\\nexplains why these representations provide such a powerful way to solve a\\nwide range of problems.\\nThis chapter also introduces frames and the way in which inheritance can\\nbe used to provide a powerful representational system.\\nThis chapter is illustrated with a number of problems and suitable repre-'), Document(metadata={}, page_content='sentations that can be used to solve those problems.\\n3.2 The Need for a Good Representation\\nAs we will see elsewhere in this book, the representation that is used to repre-\\nsent a problem is very important. In other words, the way in which the com-\\nputer represents a problem, the variables it uses, and the operators it applies\\nto those variables can make the difference between an efficient algorithm\\nand an algorithm that doesn’t work at all. This is true of all Artificial Intelli-'), Document(metadata={}, page_content='gence problems, and as we see in the following chapters, it is vital for search.\\nImagine that you are looking for a contact lens that you dropped on a foot-\\nball field. Y ou will probably use some knowledge about where you were on\\nthe field to help you look for it. If you spent time in only half of the field,\\nyou do not need to waste time looking in the other half.\\nNow let us suppose that you are having a computer search the field for the'), Document(metadata={}, page_content='contact lens, and let us further suppose that the computer has access to an\\nomniscient oracle that will answer questions about the field and can accu-\\nrately identify whether the contact lens is in a particular spot.\\nNow we must choose a representation for the computer to use so that it can\\nformulate the correct questions to ask.\\nOne representation might be to have the computer divide the field into\\nfour equal squares and ask the oracle for each square, “Is the lens in this'), Document(metadata={}, page_content='square?” This will identify the location on the field of the lens but will not\\nreally be very helpful to you because you will still have a large area to search\\nonce you find which quarter of the field the lens is in.\\nAnother representation might be for the computer to have a grid con-\\ntaining a representation of every atom contained in the field. For each'), Document(metadata={}, page_content='3.3 Semantic Nets 29\\natom, the computer could ask its oracle, “Is the lens in contact with this\\natom?”\\nThis would give a very accurate answer indeed, but would be an extremely\\ninefficient way of finding the lens. Even an extremely powerful computer\\nwould take a very long time indeed to locate the lens.\\nPerhaps a better representation would be to divide the field up into a grid\\nwhere each square is one foot by one foot and to eliminate all the squares'), Document(metadata={}, page_content='from the grid that you know are nowhere near where you were when you\\nlost the lens. This representation would be much more helpful.\\nIn fact, the representations we have described for the contact lens problem\\nare all really the same representation, but at different levels of granularity.\\nThe more difficult problem is to determine the data structure that will be\\nused to represent the problem we are exploring. As we will see throughout'), Document(metadata={}, page_content='this book, there are a wide range of representations used in Artificial\\nIntelligence.\\nWhen applying Artificial Intelligence to search problems, a useful, efficient,\\nand meaningful representation is essential. In other words, the representa-\\ntion should be such that the computer does not waste too much time on\\npointless computations, it should be such that the representation really\\ndoes relate to the problem that is being solved, and it should provide a'), Document(metadata={}, page_content='means by which the computer can actually solve the problem.\\nIn this chapter, we look at a number of representations that are used in\\nsearch, and in particular we will look at search trees, which are used\\nthroughout this part of the book.\\n3.3 Semantic Nets\\nThe semantic net is a commonly used representation in Artificial Intelli-\\ngence. A semantic net is a graph consisting of nodes that are connected by\\nedges. The nodes represent objects, and the links between nodes represent'), Document(metadata={}, page_content='relationships between those objects. The links are usually labeled to indi-\\ncate the nature of the relationship.'), Document(metadata={}, page_content='30 CHAPTER 3 Knowledge Representation\\nchases\\nchases\\nDog\\nCat\\nFido\\nBob\\nCheese\\nBuilder\\nFang\\nMice\\nowns\\nis a\\nis a\\nis a\\neats\\neat\\nFigure 3.1\\nA simple semantic net\\nA simple example of a semantic net is shown in Figure 3.1.\\nNote that in this semantic net, the links are arrows, meaning that they have\\na direction. In this way, we can tell from the diagram that Fido chases Fang,\\nnot that Fang chases Fido. It may be that Fang does chase Fido as well, but\\nthis information is not presented in this diagram.'), Document(metadata={}, page_content='Semantic nets provide a very intuitive way to represent knowledge about\\nobjects and the relationships that exist between those objects. The data in\\nsemantic nets can be reasoned about in order to produce systems that have\\nknowledge about a particular domain. Semantic nets do have limitations,\\nsuch as the inability to represent negations: “Fido is not a cat. ” As we see in\\nChapter 7, this kind of fact can be expressed easily in first-order predicate'), Document(metadata={}, page_content='logic and can also be managed by rule-based systems.\\nNote that in our semantic net we have represented some specific individu-\\nals, such as Fang, Bob, and Fido, and have also represented some general\\nclasses of things, such as cats and dogs. The specific objects are generally\\nreferred to as instances of a particular class. Fido is an instance of the class\\ndog. Bob is an instance of the class Builder.\\nIt is a little unclear from Figure 3.1 whether cheese is a class or an instance of'), Document(metadata={}, page_content='a class. This information would need to be derived by the system that is\\nmanipulating the semantic net in some way. For example, the system might\\nhave a rule that says “any object that does not have an ‘is-a’ relationship to a\\nclass is considered to represent a class of objects. ” Rules such as this must be\\napplied with caution and must be remembered when building a semantic net.'), Document(metadata={}, page_content='3.4 Inheritance 31\\nAn important feature of semantic nets is that they convey meaning. That is\\nto say, the relationship between nodes and edges in the net conveys infor-\\nmation about some real-world situation. A good example of a semantic net\\nis a family tree diagram. Usually, nodes in these diagrams represent people,\\nand there are edges that represent parental relationships, as well as relation-\\nships by marriage.\\nEach node in a semantic net has a label that identifies what the node repre-'), Document(metadata={}, page_content='sents. Edges are also labeled. Edges represent connections or relationships\\nbetween nodes. In the case of searching a dictionary for a page that con-\\ntains a particular word, each node might represent a single page, and each\\nedge would represent a way of getting from one page to another.\\nThe particular choice of semantic net representation for a problem will\\nhave great bearing on how the problem is solved. A simple representation'), Document(metadata={}, page_content='for searching for a word in a dictionary would be to have the nodes\\narranged in a chain with one connection from the first node to the second,\\nand then from the second to the third, and so on. Clearly, any method that\\nattempts to search this graph will be fairly inefficient because it means vis-\\niting each node in turn until the desired node is found. This is equivalent\\nto flicking through the pages of the dictionary in order until the desired\\npage is found.'), Document(metadata={}, page_content='page is found.\\nAs we see in Section 3.7, representing the dictionary by a different data\\nstructure can give much more efficient ways of searching.\\n3.4 Inheritance\\nInheritance is a relationship that can be particularly useful in AI and in\\nprogramming. The idea of inheritance is one that is easily understood\\nintuitively. For example, if we say that all mammals give birth to live\\nbabies, and we also say that all dogs are mammals, and that Fido is a dog,'), Document(metadata={}, page_content='then we can conclude that Fido gives birth to live mammals. Of course,\\nthis particular piece of reasoning does not take into account the fact that\\nFido might be male, or if Fido is female, might be too young or too old to\\ngive birth.\\nSo, inheritance allows us to specify properties of a superclass and then to\\ndefine a subclass, which inherits the properties of the superclass. In our'), Document(metadata={}, page_content='32 CHAPTER 3 Knowledge Representation\\nexample, mammals are the superclass of dogs and Fido. Dogs are the sub-\\nclass of mammals and the superclass of Fido.\\nIf you have programmed with an object-oriented programming language\\nsuch as C++ or Java, then you will be familiar with the concept of inheri-\\ntance and will appreciate its power. Object-oriented programming is dis-\\ncussed further in Section 3.6.\\nAs has been shown, although inheritance is a useful way to express general-'), Document(metadata={}, page_content='ities about a class of objects, in some cases we need to express exceptions to\\nthose generalities (such as, “Male animals do not give birth” or “Female\\ndogs below the age of six months do not give birth”). In such cases, we say\\nthat thedefault valuehas beenoverriddenin the subclass.\\nAs we will see, it is usually useful to be able to express in our chosen repre-\\nsentation which values can be overridden and which cannot.\\n3.5 Frames'), Document(metadata={}, page_content='3.5 Frames\\nFrame-based representation is a development of semantic nets and allows\\nus to express the idea of inheritance.\\nAs with semantic nets, a frame system consists of a set of frames (or\\nnodes), which are connected together by relations. Each frame describes\\neither an instance (an instance frame) or a class (a class frame).\\nThus far, we have said that instances are “objects” without really saying\\nwhat an object is. In this context, an object can be a physical object, but it'), Document(metadata={}, page_content='does not have to be. An object can be a property (such as a color or a\\nshape), or it can be a place, or a situation, or a feeling. This idea of objects\\nis the same that is used in object-oriented programming languages, such as\\nC++ and Java. Frames are thus an object-oriented representation that can\\nbe used to build expert systems. Object-oriented programming is further\\ndiscussed in Section 3.6.\\nEach frame has one or more slots, which are assigned slot values. This is'), Document(metadata={}, page_content='the way in which the frame system network is built up. Rather than simply\\nhaving links between frames, each relationship is expressed by a value being\\nplaced in a slot. For example, the semantic net in Figure 3.1 might be repre-\\nsented by the following frames:'), Document(metadata={}, page_content='3.5 Frames 33\\nIs a\\nOwns\\nEats\\nBuilder\\nFido\\nCheese\\nBob\\nIs a\\nChases\\nDog\\nFang\\nFido\\nFigure 3.2\\nPartial representation for\\na frame system for the\\nsemantic net shown in Fig-\\nure 3.1\\nFrame Name Slot Slot Value\\nBob is a Builder\\nowns Fido\\neats Cheese\\nFido is a Dog\\nchases Fang\\nFang is a Cat\\nchases Mice\\nMice eat Cheese\\nCheese\\nBuilder\\nDog\\nCat\\nWe can also represent this frame system in a diagrammatic form using rep-\\nresentations such as those shown in Figure 3.2.'), Document(metadata={}, page_content='resentations such as those shown in Figure 3.2.\\nWhen we say, “Fido is a dog, ” we really mean, “Fido is an instance of the\\nclass dog,” or “Fido is a member of the class of dogs.” Hence, the “is-a” rela-\\ntionship is very important in frame-based representations because it\\nenables us to express membership of classes. This relationship is also\\nknown as generalization because referring to the class of mammals is more\\ngeneral than referring to the class of dogs, and referring to the class of dogs'), Document(metadata={}, page_content='is more general than referring to Fido.'), Document(metadata={}, page_content='34 CHAPTER 3 Knowledge Representation\\nIt is also useful to be able to talk about one object being a part of another\\nobject. For example, Fido has a tail, and so the tail is part of Fido. This rela-\\ntionship is known as aggregation because Fido can be considered an aggre-\\ngate of dog parts.\\nOther relationships are known as association. An example of such a relation-\\nship is the “chases” relationship. This explains how Fido and Fang are related'), Document(metadata={}, page_content='or associated with each other. Note that association relationships have mean-\\ning in two directions. The fact that Fido chases Fang means that Fang is chased\\nby Fido, so we are really expressing two relationships in one association.\\n3.5.1 Why Are Frames Useful?\\nFrames can be used as a data structure by Expert Systems, which are dis-\\ncussed in more detail in Chapter 9.\\nThe main advantage of using frame-based systems for expert systems over'), Document(metadata={}, page_content='the rule-based approach is that all the information about a particular\\nobject is stored in one place. In a rule-based system, information about\\nFido might be stored in a number of otherwise unrelated rules, and so if\\nFido changes, or a deduction needs to be made about Fido, time may be\\nwasted examining irrelevant rules and facts in the system, whereas with the\\nframe system, the Fido frame could be quickly examined.\\nThis difference becomes particularly clear when we consider frames that'), Document(metadata={}, page_content='have a very large number of slots and where a large number of relationships\\nexist between frames (i.e., a situation in which objects have a lot of proper-\\nties, and a lot of objects are related to each other). Clearly, many real-world\\nsituations have these properties.\\n3.5.2 Inheritance\\nWe might extend our frame system with the following additional information:\\nDogs chase cats\\nCats chase mice\\nIn expressing these pieces of information, we now do not need to state'), Document(metadata={}, page_content='explicitly that Fido chases Fang or that Fang chases mice. In this case, we\\ncan inherit this information because Fang is an instance of the class Cats,\\nand Fido is an instance of the class Dogs.\\nWe might also add the following additional information:'), Document(metadata={}, page_content='3.5 Frames 35\\nMammals breathe\\nDogs are mammals\\nCats are mammals\\nHence, we have now created a new superclass, mammals, of which dogs and\\ncats are subclasses. In this way, we do not need to express explicitly that cats\\nand dogs breathe because we can inherit this information. Similarly, we do\\nnot need to express explicitly that Fido and Fang breathe—they are\\ninstances of the classes Dogs and Cats, and therefore they inherit from\\nthose classes’ superclasses.\\nNow let us add the following fact:'), Document(metadata={}, page_content='Now let us add the following fact:\\nMammals have four legs\\nOf course, this is not true, because humans do not have four legs, for exam-\\nple. In a frame-based system, we can express that this fact is the default\\nvalue and that it may be overridden. Let us imagine that in fact Fido has\\nhad an unfortunate accident and now has only three legs. This information\\nmight be expressed as follows:\\nFrame Name Slot Slot Value\\nMammal *number of legs four\\nDog subclass Mammal\\nCat subclass Mammal\\nFido is a Dog'), Document(metadata={}, page_content='Cat subclass Mammal\\nFido is a Dog\\nnumber of legs three\\nFang is a Cat\\nHere we have used an asterisk (*) to indicate that the value for the “number\\nof legs” slot for the Mammal class is a default value and can be overridden,\\nas has been done for Fido.\\n3.5.3 Slots as Frames\\nIt is also possible to express a range of values that a slot can take—for\\nexample, the number of legs slot might be allowed a number between 1 and\\n4 (although, for the insects class, it might be allowed 6).'), Document(metadata={}, page_content='36 CHAPTER 3 Knowledge Representation\\nOne way to express this kind of restriction is by allowing slots to be frames.\\nIn other words, the number of legs slot can be represented as a frame,\\nwhich includes information about what range of values it can take:\\nFrame Name Slot Slot Value\\nNumber of legs minimum value 1\\nmaximum value 4\\nIn this way, we can also express more complex ideas about slots, such as the\\ninverse of a slot (e.g., the “chases” slot has an inverse, which is the “chased'), Document(metadata={}, page_content='by” slot). We can also place further limitations on a slot, such as to specify\\nwhether or not it can take multiple values (e.g., the “number of legs” slot\\nshould probably only take one value, whereas the “eats” slot should be\\nallowed to take many values).\\n3.5.4 Multiple Inheritance\\nIt is possible for a frame to inherit properties from more than one other\\nframe. In other words, a class can be a subclass of two superclasses, and an'), Document(metadata={}, page_content='object can be an instance of more than one class. This is known as multiple\\ninheritance.\\nFor example, we might add the following frames to our system:\\nFrame Name Slot Slot Value\\nHuman Subclass Mammal\\nNumber of legs two\\nBuilder Builds houses\\nBob is a Human\\nFrom this, we can see that Bob is a human, as well as being a builder. Hence,\\nwe can inherit the following information about Bob:\\nHe has two legs\\nHe builds houses'), Document(metadata={}, page_content='3.5 Frames 37\\nIn some cases, we will encounter conflicts, where multiple inheritance\\nleads us to conclude contradictory information about a frame. For exam-\\nple, let us consider the following simple frame system:\\nFrame Name Slot Slot Value\\nCheese is smelly\\nThing wrapped in foil is not smelly\\nCheddar is a Cheese\\nis a Thing wrapped in foil\\n(Note: the slot “is” might be more accurately named “has property. ” We\\nhave named it “is” to make the example clearer.)'), Document(metadata={}, page_content='have named it “is” to make the example clearer.)\\nHere we can see that cheddar is a type of cheese and that it comes wrapped\\nin foil. Cheddar should inherit its smelliness from the Cheese class, but it\\nalso inherits nonsmelliness from the Thing wrapped in foil class. In this\\ncase, we need a mechanism to decide which features to inherit from which\\nsuperclasses. One simple method is to simply say that conflicts are resolved'), Document(metadata={}, page_content='by the order in which they appear. So if a fact is established by inheritance,\\nand then that fact is contradicted by inheritance, the first fact is kept\\nbecause it appeared first, and the contradiction is discarded.\\nThis is clearly rather arbitrary, and it would almost certainly be better to\\nbuild the frame system such that conflicts of this kind cannot occur.\\nMultiple inheritance is a key feature of most object-oriented programming\\nlanguages. This is discussed in more detail in Section 3.6.'), Document(metadata={}, page_content='3.5.5 Procedures\\nIn object-oriented programming languages such as C++ or Java, classes\\n(and hence objects) have methods associated with them. This is also true\\nwith frames. Frames have methods associated with them, which are called\\nprocedures. Procedures associated with frames are also called procedural\\nattachments.\\nA procedure is a set of instructions associated with a frame that can be exe-\\ncuted on request. For example, a slot reader procedure might return the'), Document(metadata={}, page_content='value of a particular slot within the frame. Another procedure might insert'), Document(metadata={}, page_content='38 CHAPTER 3 Knowledge Representation\\na value into a slot (a slot writer ). Another important procedure is the\\ninstance constructor, which creates an instance of a class.\\nSuch procedures are called when needed and so are called WHEN-\\nNEEDED procedures. Other procedures can be set up that are called auto-\\nmatically when something changes.\\n3.5.6 Demons\\nA demon is a particular type of procedure that is run automatically when-\\never a particular value changes or when a particular event occurs.'), Document(metadata={}, page_content='Some demons act when a particular value is read. In other words, they are\\ncalled automatically when the user of the system, or the system itself, wants\\nto know what value is placed in a particular slot. Such demons are called\\nWHEN-READ procedures. In this way, complex calculations can be made\\nthat calculate a value to return to the user, rather than simply giving back\\nstatic data that are contained within the slot. This could be useful, for'), Document(metadata={}, page_content='example, in a large financial system with a large number of slots because it\\nwould mean that the system would not necessarily need to calculate every\\nvalue for every slot. It would need to calculate some values only when they\\nwere requested.\\nWHEN-CHANGED procedures (also known as WHEN-WRITTEN pro-\\ncedures) are run automatically when the value of a slot is changed. This\\ntype of function can be particularly useful, for example, for ensuring that'), Document(metadata={}, page_content='the values assigned to a slot fit within a set of constraints. For example, in\\nour example above, a WHEN-WRITTEN procedure might run to ensure\\nthat the “number of legs” slot never has a value greater than 4 or less than 1.\\nIf a value of 7 is entered, a system message might be produced, telling the\\nuser that he or she has entered an incorrect value and that he or she should\\nenter a different value.\\n3.5.7 Implementation\\nWith the addition of procedures and demons, a frame system becomes a'), Document(metadata={}, page_content='very powerful tool for reasoning about objects and relationships. The sys-\\ntem has procedural semantics as opposed to declarative semantics, which'), Document(metadata={}, page_content='3.5 Frames 39\\nmeans that the order in which things occur affects the results that the sys-\\ntem produces. In some cases, this can cause problems and can make it\\nharder to understand how the system will behave in a given situation.\\nThis lack of clarity is usually compensated for by the level of flexibility\\nallowed by demons and the other features that frame systems possess.\\nFrame systems can be implemented by a very simple algorithm if we do not'), Document(metadata={}, page_content='allow multiple inheritance. The following algorithm allows us to find the\\nvalue of a slot S, for a frame F. In this algorithm definition, we will use the\\nnotation\\nF[S]to indicate the value of slot S in frame F. We also use the nota-\\ntion instance (F1, F2) to indicate that frame F1 is an instance of frame F2\\nand subclass (F1, F2)to indicate that frame F1 is a subclass of frame F2.\\nFunction find_slot_value (S, F)\\n{\\nif F[S] == V             // if the slot contains'), Document(metadata={}, page_content='if F[S] == V             // if the slot contains\\nthen return V       // a value, return it.\\nelse if instance (F, F’)\\nthen return find_slot_value (S, F’)\\nelse if subclass (F, Fs)\\nthen return find_slot_value (S, Fs)\\nelse return FAILURE;\\n}\\nIn other words, the slot value of a frame F will either be contained within\\nthat frame, or a superclass of F, or another frame of which F is an instance.\\nIf none of these provides a value, then the algorithm fails.'), Document(metadata={}, page_content='Clearly, frames could also be represented in an object-oriented program-\\nming language such as C++ or Java.\\nA frame-based expert system can be implemented in a similar way to the\\nrule-based systems, which we examine in Chapter 9. T o answer questions\\nabout an object, the system can simply examine that object’s slots or the\\nslots of classes of which the object is an instance or a subclass.\\nIf the system needs additional information to proceed, it can ask the user'), Document(metadata={}, page_content='questions in order to fill in additional information. In the same way as with\\nrule-based systems, WHEN-CHANGED procedures can be set up that'), Document(metadata={}, page_content='40 CHAPTER 3 Knowledge Representation\\nmonitor the values of slots, and when a particular set of values is identified,\\nthis can be used by the system to derive a conclusion and thus recommend\\nan action or deliver an explanation for something.\\n3.5.8 Combining Frames with Rules\\nIt is possible to combine frames with rules, and, in fact, many frame-based\\nexpert systems use rules in much the same way that rule-based systems do,\\nwith the addition of pattern matching clauses, which are used to identify'), Document(metadata={}, page_content='values that match a set of conditions from all the frames in the system.\\nTypically, a frame-based system with rules will use rules to try to derive\\nconclusions, and in some cases where it cannot find a value for a particular\\nslot, a WHEN-NEEDED procedure will run to determine the value for that\\nslot. If no value is found from that procedure, then the user will be asked to\\nsupply a value.\\n3.5.9 Representational Adequacy\\nWe can represent the kinds of relationships that we can describe with'), Document(metadata={}, page_content='frames in first-order predicate logic. For example:\\n/H7001x Dog(x) → Mammal(x)\\nFirst-order predicate logic is discussed in detail in Chapter 7. For now, you\\nsimply need to know how to read that expression. It is read as follows:\\n“For all x’s, if x is a dog, then x is a mammal. ”\\nThis can be rendered in more natural English as:\\n“All dogs are mammals. ”\\nIn fact, we can also express this relationship by the introduction of a new'), Document(metadata={}, page_content='symbol, which more closely mirrors the meaning encompassed by the idea\\nof inheritance:\\nAlmost anything that can be expressed using frames can be expressed using\\nfirst-order predicate logic (FPOL). The same is not true in reverse. For\\nexample, it is not easy to represent negativity (“Fido is not a cat”) or quan-\\ntification (“there is a cat that has only one leg”). We say that FOPL has\\ngreater representational adequacy than frame-based representations.\\nDog Mammalsubset\\uf8e7→\\uf8e7\\uf8e7'), Document(metadata={}, page_content='3.6 Object-Oriented Programming 41\\nIn fact, frame-based representations do have some aspects that cannot be\\neasily represented in FOPL. The most significant of these is the idea of\\nexceptions, or overriding default values.\\nAllowing exceptions to override default values for slots means that the\\nframe-based system is not monotonic (monotonicity is discussed in Chap-\\nter 7). In other words, conclusions can be changed by adding new facts to\\nthe system.'), Document(metadata={}, page_content='the system.\\nIn this section, we have discussed three main representational methods:\\nlogic, rules, and frames (or semantic nets). Each of these has advantages\\nand disadvantages, and each is preferable over the others in different situa-\\ntions. The important thing is that in solving a particular problem, the cor-\\nrect representation must be chosen.\\n3.6 Object-Oriented Programming\\nWe now briefly explore some of the ideas used in object-oriented program-'), Document(metadata={}, page_content='ming, and, in particular, we see how they relate to some of the ideas we have\\nseen in Sections 3.4 and 3.5 on inheritance and frames.\\nTwo of the best-known object-oriented programming languages are Java\\nand C++. These two languages use a similar syntax to define classes and\\nobjects that are instantiations of those classes.\\nA typical class in these languages might be defined as:\\nclass animal\\n{\\nanimal ();\\nEye *eyes;\\nLeg *legs;\\nHead head;\\nTail tail;\\n}'), Document(metadata={}, page_content='Eye *eyes;\\nLeg *legs;\\nHead head;\\nTail tail;\\n}\\nThis defines a class called animal that has a number of fields, which are the\\nvarious body parts. It also has a constructor, which is a function that is\\ncalled when an instantiation of the class is called. Classes can have other\\nfunctions too, and these functions are equivalent to the procedures we saw\\nin Section 3.5.5.\\nWe can create an instance of the class animal as follows:\\nanimal an_animal = new animal ();'), Document(metadata={}, page_content='42 CHAPTER 3 Knowledge Representation\\nThis creates an instance of the class animal. The instance, which is an object,\\nis called “an_animal” . In creating it, the constructor animal () is called.\\nWe can also create a subclass of animal:\\nClass dog : animal\\n{\\nbark ();\\n}\\nHere we have created a subclass of animal called dog. Dog has inherited all of\\nthe properties of animal and also has a new function of its own called bark ().\\nIn some object-oriented programming languages, it is possible to use mul-'), Document(metadata={}, page_content='tiple inheritance. This means that one class inherits properties from more\\nthan one parent class. While C++ does allow multiple inheritance, Java,\\nwhich itself inherited many features from C++, does not allow multiple\\ninheritance. This is because multiple inheritance was seen by the develop-\\ners of Java as an “unclean” idea—one that creates unnecessarily compli-\\ncated object-oriented structures. Additionally, it is always possible to'), Document(metadata={}, page_content='achieve the same results using single inheritance as it is with multiple\\ninheritance.\\nObject-oriented programming languages such as Java and C++ use the\\nprinciples that were invented for the frames structure. There are also\\nobject-oriented programming languages such as IBM’s APL2 that use a\\nframe-based structure.\\nThe ideas explored in Sections 3.4 and 3.5 of this book are thus very rele-\\nvant to object-oriented programming, as well as being an important part of\\nArtificial Intelligence research.'), Document(metadata={}, page_content='Artificial Intelligence research.\\n3.7 Search Spaces\\nMany problems in Artificial Intelligence can be represented as search\\nspaces. In simple terms, a search space is a representation of the set of pos-\\nsible choices in a given problem, one or more of which are the solution to\\nthe problem.\\nFor example, attempting to find a particular word in a dictionary with 100\\npages, a search space will consist of each of the 100 pages. The page that is'), Document(metadata={}, page_content='being searched for is called a goal, and it can be identified by seeing'), Document(metadata={}, page_content='3.7 Search Spaces 43\\nState 1\\nRobot in room A.\\nBlock in room A.\\nState 2\\nRobot in room B.\\nBlock in room A.\\nState 3\\nRobot in room C.\\nBlock in room A.\\nState 4\\nRobot in room A.\\nBlock in room B.\\nState 5\\nRobot in room B.\\nBlock in room B.\\nState 6\\nRobot in room C.\\nBlock in room B.\\nState 7\\nRobot in room A.\\nBlock in room C.\\nState 8\\nRobot in room B.\\nBlock in room C.\\nState 9\\nRobot in room C.\\nBlock in room C.\\nFigure 3.3\\nA simple state-space \\ndiagram'), Document(metadata={}, page_content='Figure 3.3\\nA simple state-space \\ndiagram\\nwhether the word we are looking for is on the page or not. (In fact, this\\nidentification might be a search problem in itself, but for this example we\\nwill assume that this is a simple, atomic action.)\\nThe aim of most search procedures is to identify one or more goals and,\\nusually, to identify one or more paths to those goals (often the shortest\\npath, or path with least cost).\\nBecause a search space consists of a set of states, connected by paths that'), Document(metadata={}, page_content='represent actions, they are also known as state spaces. Many search prob-\\nlems can be represented by a state space, where the aim is to start with the\\nworld in one state and to end with the world in another, more desirable\\nstate. In the missionaries and cannibals problem that is discussed later in\\nthis chapter, the start state has all missionaries and cannibals on one side of\\nthe river, and the goal state has them on the other side. The state space for'), Document(metadata={}, page_content='the problem consists of all possible states in between.\\nFigure 3.3 shows a very simple state-space diagram for a robot that lives in\\nan environment with three rooms (room A, room B, and room C) and with'), Document(metadata={}, page_content='44 CHAPTER 3 Knowledge Representation\\nA A\\nB\\nB\\nC\\nC\\nGFE\\nE\\nD\\nD\\nFigure 3.4\\nA semantic net and a\\nsemantic tree\\na block that he can move from room to room. Each state consists of a pos-\\nsible arrangement of the robot and the block. Hence, for example, in state\\n1, both the robot and the block are in room A. Note that this diagram does\\nnot explain how the robot gets from one room to another or how the block\\nis moved. This kind of representation assumes that the robot has a repre-'), Document(metadata={}, page_content='sentation of a number of actions that it can take. T o determine how to get\\nfrom one state to another state, the robot needs to use a process called\\nplanning, which is covered in detail in Part 5 of this book.\\nIn Figure 3.3, the arrows between states represent state transitions.N o t e\\nthat there are not transitions between every pair of states. For example, it is\\nnot possible to go from state 1 to state 4 without going through state 5. This'), Document(metadata={}, page_content='is because the block cannot move on its own and can only be moved to a\\nroom if the robot moves there. Hence, a state-space diagram is a valuable\\nway to represent the possible actions that can be taken in a given state and\\nthus to represent the possible solutions to a problem.\\n3.8 Semantic Trees\\nA semantic tree is a kind of semantic net that has the following properties:\\n■ Each node (except for the root node, described below) has exactly'), Document(metadata={}, page_content='one predecessor (parent) and one or more successors (children).\\nIn the semantic tree in Figure 3.4, node A is the predecessor of\\nnode B: node A connects by one edge to node B and comes before\\nit in the tree. The successors of node B, nodes D and E, connect\\ndirectly (by one edge each) to node B and come after it in the tree.\\nWe can write these relationships as: succ (B) = D and pred (B) = A.'), Document(metadata={}, page_content='3.8 Semantic Trees 45\\nThe nonsymmetric nature of this relationship means that a seman-\\ntic tree is a directed graph. By contrast, nondirected graphs are\\nones where there is no difference between an arc from A to B and\\nan arc from B to A.\\n■ One node has no predecessors. This node is called the root\\nnode. In general, when searching a semantic tree, we start at the\\nroot node. This is because the root node typically represents a\\nstarting point of the problem. For example, when we look at'), Document(metadata={}, page_content='game trees in Chapter 6, we will see that the game tree for a\\ngame of chess represents all the possible moves of the game,\\nstarting from the initial position in which neither player has\\nmade a move. This initial position corresponds to the root node\\nin the game tree.\\n■ Some nodes have no successors. These nodes are called leaf nodes.\\nOne or more leaf nodes are called goal nodes. These are the nodes\\nthat represent a state where the search has succeeded.'), Document(metadata={}, page_content='■ Apart from leaf nodes, all nodes have one or more successors.\\nApart from the root node, all nodes have exactly one predecessor.\\n■ An ancestor of a node is a node further up the tree in some path. A\\ndescendent comes after a node in a path in the tree.\\nA path is a route through the semantic tree, which may consist of just one\\nnode (a path of length 0). A path of length 1 consists of a node, a branch\\nthat leads from that node, and the successor node to which that branch'), Document(metadata={}, page_content='leads. A path that leads from the root node to a goal node is called a com-\\nplete path. A path that leads from the root node to a leaf node that is not a\\ngoal node is called a partial path.\\nWhen comparing semantic nets and semantic trees visually, one of the\\nmost obvious differences is that semantic nets can contain cycles, but\\nsemantic trees cannot. A cycle is a path through the net that visits the same\\nnode more than once. Figure 3.4 shows a semantic net and a semantic tree.'), Document(metadata={}, page_content='In the semantic net, the path A, B, C, D, A. . . is a cycle.\\nIn semantic trees, an edge that connects two nodes is called a branch.I fa\\nnode has n successors, that node is said to have a branching factor of n.A\\ntree is often said to have a branching factor of n if the average branching\\nfactor of all the nodes in the tree is n.'), Document(metadata={}, page_content='46 CHAPTER 3 Knowledge Representation\\nA\\nBE E CC\\nA\\nECD\\nDB\\nA\\nD\\nBE E\\nA\\nD\\nA\\nC\\nThe root node of a tree is said to be at level 0, and the successors of the root\\nnode are at level 1. Successors of nodes at level n are at level n + 1.\\n3.9 Search Trees\\nSearching a semantic net involves traversing the net systematically (or in\\nsome cases, not so systematically), examining nodes, looking for a goal\\nnode. Clearly following a cyclic path through the net is pointless because'), Document(metadata={}, page_content='following A,B,C,D,A will not lead to any solution that could not be reached\\njust by starting from A. We can represent the possible paths through a\\nsemantic net as a search tree, which is a type of semantic tree.\\nThe search tree shown in Figure 3.5 represents the possible paths through\\nthe semantic net shown in Figure 3.4. Each node in the tree represents a\\npath, with successive layers in the tree representing longer and longer paths.'), Document(metadata={}, page_content='Note that we do not include cyclical paths, which means that some\\nbranches in the search tree end on leaf nodes that are not goal nodes. Also\\nnote that we label each node in the search tree with a single letter, which\\nFigure 3.5\\nA search tree representa-\\ntion for the semantic net\\nin Figure 3.4.'), Document(metadata={}, page_content='3.9 Search Trees 47\\nrepresents the path from the root node to that node in the semantic net in\\nFigure 3.4.\\nHence, searching for a node in a search tree corresponds to searching for a\\ncomplete path in a semantic net.\\n3.9.1 Example 1: Missionaries and Cannibals\\nThe Missionaries and Cannibals problem is a well-known problem that is\\noften used to illustrate AI techniques. The problem is as follows:\\nThree missionaries and three cannibals are on one side of a river, with a canoe.'), Document(metadata={}, page_content='They all want to get to the other side of the river. The canoe can only hold one\\nor two people at a time. At no time should there be more cannibals than mis-\\nsionaries on either side of the river, as this would probably result in the mis-\\nsionaries being eaten.\\nT o solve this problem, we need to use a suitable representation.\\nFirst of all, we can consider a state in the solving of the problem to consist\\nof a certain number of cannibals and a certain number of missionaries on'), Document(metadata={}, page_content='each side of the river, with the boat on one side or the other. We could rep-\\nresent this, for example, as\\n3, 3, 1 0, 0, 0\\nThe left-hand set of numbers represents the number of cannibals, mission-\\naries, and canoes on one side of the river, and the right-hand side repre-\\nsents what is on the other side.\\nBecause the number that is on one side is entirely dependent on the num-\\nber that is on the other side, we can in fact just show how many of each are'), Document(metadata={}, page_content='on the finishing side, meaning that the starting state is represented as\\n0, 0, 0\\nand the goal state is\\n3, 3, 1\\nAn example of a state that must be avoided is\\n2, 1, 1\\nHere, there are two cannibals, one canoe, and just one missionary on the\\nother side of the river. This missionary will probably not last very long.'), Document(metadata={}, page_content='48 CHAPTER 3 Knowledge Representation\\n0,0,0\\n0,0,0 0,0,0 1,0,0 1,0,0\\n1,1,11,0,1 2,0,1\\n11 23\\n215\\nFigure 3.6\\nA partial search tree for\\nthe missionaries and can-\\nnibals problem\\nT o get from one state to another, we need to apply an operator. The opera-\\ntors that we have available are the following:\\n1. Move one cannibal to the other side\\n2. Move two cannibals to the other side\\n3. Move one missionary to the other side\\n4. Move two missionaries to the other side'), Document(metadata={}, page_content='4. Move two missionaries to the other side\\n5. Move one cannibal and one missionary to the other side\\nSo if we apply operator 5 to the state represented by 1, 1, 0, then we would\\nresult in state 2, 2, 1. One cannibal, one missionary, and the canoe have now\\nmoved over to the other side. Applying operator 3 to this state would lead\\nto an illegal state: 2, 1, 0.\\nWe consider rules such as this to be constraints, which limit the possible'), Document(metadata={}, page_content='operators that can be applied in each state. If we design our representation\\ncorrectly, the constraints are built in, meaning we do not ever need to\\nexamine illegal states.\\nWe need to have a test that can identify if we have reached the goal\\nstate—3, 3, 1.\\nWe will consider the cost of the path that is chosen to be the number of steps\\nthat are taken, or the number of times an operator is applied. In some cases,\\nas we will see later, it is desirable to find a solution that minimizes cost.'), Document(metadata={}, page_content='The first three levels of the search tree for the missionaries and cannibals\\nproblem is shown in Figure 3.6 (arcs are marked with which operator has\\nbeen applied).\\nNow, by extending this tree to include all possible paths, and the states\\nthose paths lead to, a solution can be found. A solution to the problem\\nwould be represented as a path from the root node to a goal node.'), Document(metadata={}, page_content='3.9 Search Trees 49\\nThis tree represents the presence of a cycle in the search space. Note that the\\nuse of search trees to represent the search space means that our representa-\\ntion never contains any cycles, even when a cyclical path is being followed\\nthrough the search space.\\nBy applying operator 1 (moving one cannibal to the other side) as the first\\naction, and then applying the same operator again, we return to the start'), Document(metadata={}, page_content='state. This is a perfectly valid way to try to solve the problem, but not a very\\nefficient one.\\n3.9.2 Improving the Representation\\nA more effective representation for the problem would be one that did not\\ninclude any cycles. Figure 3.7 is an extended version of the search tree for\\nthe problem that omits cycles and includes goal nodes.\\nNote that in this tree, we have omitted most repeated states. For example,\\nfrom the state 1,0,0, operator 2 is the only one shown. In fact, operators 1'), Document(metadata={}, page_content='and 3 can also be applied, leading to states 2,0,1 and 1,1,1 respectively. Nei-\\nther of these transitions is shown because those states have already\\nappeared in the tree.\\nAs well as avoiding cycles, we have thus removed suboptimal paths from\\nthe tree. If a path of length 2 reaches a particular state, s, and another path\\nof length 3 also reaches that state, it is not worth pursuing the longer path\\nbecause it cannot possibly lead to a shorter path to the goal node than the\\nfirst path.'), Document(metadata={}, page_content='first path.\\nHence, the two paths that can be followed in the tree in Figure 3.7 to the\\ngoal node are the shortest routes (the paths with the least cost) to the goal,\\nbut they are by no means the only paths. Many longer paths also exist.\\nBy choosing a suitable representation, we are thus able to improve the effi-\\nciency of our search method. Of course, in actual implementations, things\\nmay not be so simple. T o produce the search tree without repeated states, a'), Document(metadata={}, page_content='memory is required that can store states in order to avoid revisiting them. It\\nis likely that for most problems this memory requirement is a worthwhile\\ntradeoff for the saving in time, particularly if the search space being\\nexplored has many repeated states and cycles.\\nSolving the Missionaries and Cannibals problem involves searching the\\nsearch tree. As we will see, search is an extremely useful method for solving\\nproblems and is widely used in Artificial Intelligence.'), Document(metadata={}, page_content='50 CHAPTER 3 Knowledge Representation\\n0,0,0\\n1,1,11,0,1 2,0,1\\n2,2,1\\n1,3,1\\n3,3,13,3,1\\n0,3,0\\n2,3,1\\n2,2,01,3,0\\n2,0,0\\n3,0,1\\n1,0,0 3\\n1,0,0\\n1,1,0\\n2\\n1\\n2\\n2\\n25\\n1\\n1\\n4\\n4\\n5\\n1 5\\n13\\nFigure 3.7\\nSearch tree without cycles\\n3.9.3 Example 2: The Traveling Salesman\\nThe Traveling Salesman problem is another classic problem in Artificial\\nIntelligence and is NP-Complete, meaning that for large instances of the\\nproblem, it can be very difficult for a computer program to solve in a rea-'), Document(metadata={}, page_content='sonable period of time. A problem is defined as being in the class P if it can\\nbe solved in polynomial time. This means that as the size of the problem\\nincreases, the time it will take a deterministic computer to solve the prob-'), Document(metadata={}, page_content='3.9 Search Trees 51\\nlem will increase by some polynomial function of the size. Problems that\\nare NP can be solved nondeterministically in polynomial time. This means\\nthat if a possible solution to the problem is presented to the computer, it\\nwill be able to determine whether it is a solution or not in polynomial time.\\nThe hardest NP problems are termed NP-Complete. It was shown by\\nStephen Cook that a particular group of problems could be transformed'), Document(metadata={}, page_content='into the satisfiability problem (see Chapter 16). These problems are defined\\nas being NP-Complete. This means that if one can solve the satisfiability\\nproblem (for which solutions certainly do exist), then one can solve any\\nNP-Complete problem. It also means that NP-Complete problems take a\\ngreat deal of computation to solve.\\nThe Traveling Salesman problem is defined as follows:\\nA salesman must visit each of a set of cities and then return home. The aim of'), Document(metadata={}, page_content='the problem is to find the shortest path that lets the salesman visit each city.\\nLet us imagine that our salesman is touring the following American cities:\\nA Atlanta\\nB Boston\\nC Chicago\\nD Dallas\\nE El Paso\\nOur salesman lives in Atlanta and must visit all of the other four cities\\nbefore returning home. Let us imagine that our salesman is traveling by\\nplane and that the cost of each flight is directly proportional to distance'), Document(metadata={}, page_content='being traveled and that direct flights are possible between any pair of cities.\\nHence, the distances can be shown on a graph as in Figure 3.8.\\n(Note: The distances shown are not intended to accurately represent the\\ntrue locations of these cities but have been approximated for the purposes\\nof this illustration.)\\nThe graph in Figure 3.8 shows the relationships between the cities. We\\ncould use this graph to attempt to solve the problem. Certainly, we can use'), Document(metadata={}, page_content='it to find possible paths: One possible path is A,B,C,E,D,A, which has a\\nlength of 4500 miles.'), Document(metadata={}, page_content='52 CHAPTER 3 Knowledge Representation\\n800\\n1500\\n700\\n700\\n1700\\n1100\\n1000\\n600\\n600\\n900\\nA\\nBC\\nE\\nD\\nFigure 3.8\\nSimplified map showing\\nTraveling Salesman prob-\\nlem with five cities\\nT o solve the problem using search, a different representation would be\\nneeded, based on this graph. Figure 3.9 shows a part of the search tree that\\nrepresents the possible paths through the search space in this problem.\\nEach node is marked with a letter that represents the city that has been'), Document(metadata={}, page_content='reached by the path up to that point. Hence, in fact, each node represents\\nthe path from city A to the city named at that node. The root node of the\\ngraph thus represents the path of length 0, which consists simply of the city\\nA. As with the previous example, cyclical paths have been excluded from\\nthe tree, but unlike the tree for the missionaries and cannibals problem, the\\ntree does allow repeated states. This is because in this problem each state'), Document(metadata={}, page_content='must be visited once, and so a complete path must include all states. In the\\nMissionaries and Cannibals problem, the aim was to reach a particular\\nstate by the shortest path that could be found. Hence, including a path such\\nas A,B,C,D where a path A,D had already been found would be wasteful\\nbecause it could not possibly lead to a shorter path than A,D. With the\\nTraveling Salesman problem, this does not apply, and we need to examine'), Document(metadata={}, page_content='every possible path that includes each node once, with the start node at the\\nbeginning and the end.\\nFigure 3.9 is only a part of the search tree, but it shows two complete paths:\\nA,B,C,D,E,A and A,B,C,E,D,A. The total path costs of these two paths are\\n4000 miles and 4500 miles, respectively.\\nIn total there will be (n /H110021)! possible paths for a Traveling Salesman prob-\\nlem with n cities. This is because we are constrained in our starting city'), Document(metadata={}, page_content='3.9 Search Trees 53\\n1,000\\nA\\nB E\\nEC\\nE\\nE\\nAA\\nCCD\\nD\\nD\\nDBB\\nCD\\n900\\n600\\n800800 700\\n700\\n700 600\\n600600\\n7001000\\n6001500\\n1500\\n1700\\n700\\nFigure 3.9\\nPartial search tree for Traveling Salesman problem with five cities\\nand, thereafter, have a choice of any combination of (n /H110021) cities. In prob-\\nlems with small numbers of cities, such as 5 or even 10, this means that the\\ncomplete search tree can be evaluated by a computer program without'), Document(metadata={}, page_content='much difficulty; but if the problem consists of 40 cities, there would be 40!\\npaths, which is roughly 10\\n48, a ludicrously large number. As we see in the\\nnext chapter, methods that try to examine all of these paths are called\\nbrute-force search methods. T o solve search problems with large trees,\\nknowledge about the problem needs to be applied in the form of heuris-\\ntics, which enable us to find more efficient ways to solve the problem. A'), Document(metadata={}, page_content='heuristic is a rule or piece of information that is used to make search or\\nanother problem-solving method more effective or more efficient. The use\\nof heuristics for search is explained in more detail in Chapters 4 and 5.\\nFor example, a heuristic search approach to solving the Traveling Salesman\\nproblem might be: rather than examining every possible path, we simply\\nextend the path by moving to the city closest to our current position that'), Document(metadata={}, page_content='has not yet been examined. This is called the nearest neighbor heuristic.I n\\nour example above, this would lead to the path A,C,D,E,B,A, which has a\\ntotal cost of 4500 miles. This is certainly not the best possible path, as we'), Document(metadata={}, page_content='54 CHAPTER 3 Knowledge Representation\\n12 3 12 3\\nFigure 3.10\\nTwo states in the Towers of\\nHanoi problem\\nhave already seen one path (A,B,C,D,E,A) that has a cost of 4000 miles. This\\nillustrates the point that although heuristics may well make search more\\nefficient, they will not necessarily give the best results. We will see methods\\nin the next chapters that illustrate this and will also discuss ways of choos-\\ning heuristics that usually do give the best result.\\n3.9.4 Example 3: The Towers of Hanoi'), Document(metadata={}, page_content='3.9.4 Example 3: The Towers of Hanoi\\nThe T owers of Hanoi problem is defined as follows:\\nWe have three pegs and a number of disks of different sizes. The aim is to\\nmove from the starting state where all the disks are on the first peg, in size\\norder (smallest at the top) to the goal state where all the pegs are on the\\nthird peg, also in size order. We are allowed to move one disk at a time, as\\nlong as there are no disks on top of it, and as long as we do not move it on'), Document(metadata={}, page_content='top of a peg that is smaller than it.\\nFigure 3.10 shows the start state and a state after one disk has been moved\\nfrom peg 1 to peg 2 for a T owers of Hanoi problem with three disks.\\nNow that we know what our start state and goal state look like, we need to\\ncome up with a set of operators:\\nOp1 Move disk from peg 1 to peg 2\\nOp2 Move disk from peg 1 to peg 3\\nOp3 Move disk from peg 2 to peg 1\\nOp4 Move disk from peg 2 to peg 3\\nOp5 Move disk from peg 3 to peg 1\\nOp6 Move disk from peg 3 to peg 2'), Document(metadata={}, page_content='Op6 Move disk from peg 3 to peg 2\\nWe also need a way to represent each state. For this example, we will use\\nvectors of numbers where 1 represents the smallest peg and 3 the largest'), Document(metadata={}, page_content='3.9 Search Trees 55\\n(2,3)(1)(   )\\n(1,3)(  )(2) (1,3)(2)(  )\\n(1,3)(2)(  )\\n(2,3)(  )(1)\\n(3)(  )(1,2) (3)(1,2)(  )\\n(3)(1,2)(  )\\n(1,3)(  )(2)\\n(3)(  )(1,2)\\n(3)(1)(2) (3)(2)(1)\\n(1,2,3)(  )(   )\\nFigure 3.11\\nThe first five levels of the\\nsearch tree for the Towers\\nof Hanoi problem with\\nthree disks\\npeg. The first vector represents the first peg, and so on. Hence, the starting\\nstate is represented as\\n(1,2,3) () ()\\nThe second state shown in figure 3.10 is represented as\\n(2,3) (1) ()\\nand the goal state is'), Document(metadata={}, page_content='(2,3) (1) ()\\nand the goal state is\\n() () (1,2,3)\\nThe first few levels of the search tree for the T owers of Hanoi problem with\\nthree disks is shown in Figure 3.11. Again, we have ignored cyclical paths.\\nIn fact, with the T owers of Hanoi problem, at each step, we can always\\nchoose to reverse the previous action. For example, having applied opera-\\ntor Op1 to get from the start state to (2,3) (1) (), we can now apply opera-\\ntor Op3, which reverses this move and brings us back to the start state.'), Document(metadata={}, page_content='Clearly, this behavior will always lead to a cycle, and so we ignore such\\nchoices in our representation.\\nAs we see later in this book, search is not the only way to identify solutions\\nto problems like the T owers of Hanoi. A search method would find a solu-\\ntion by examining every possible set of actions until a path was found that\\nled from the start state to the goal state. A more intelligent system might be'), Document(metadata={}, page_content='56 CHAPTER 3 Knowledge Representation\\nPENGUIN KIWI\\nIS IT BLACK AND WHITE?\\nCAN IT FL Y?\\nNOYES\\nYES\\nYES NO\\nNO\\nDODO\\nIS IT EXTINCT?\\nFigure 3.12\\nSearch tree representation\\nused with Describe and\\nMatch to identify a \\npenguin\\ndeveloped that understood more about the problem and, in fact, under-\\nstood how to go about solving the problem without necessarily having to\\nexamine any alternative paths at all.\\n3.9.5 Example 4: Describe and Match'), Document(metadata={}, page_content='3.9.5 Example 4: Describe and Match\\nA method used in Artificial Intelligence to identify objects is to describe it\\nand then search for the same description in a database, which will identify\\nthe object.\\nAn example of Describe and Match is as follows:\\nAlice is looking out of her window and can see a bird in the garden. She\\ndoes not know much about birds but has a friend, Bob, who does. She calls\\nBob and describes the bird to him. From her description, he is able to tell'), Document(metadata={}, page_content='her that the bird is a penguin.\\nWe could represent Bob’s knowledge of birds in a search tree, where each\\nnode represents a question, and an arc represents an answer to the ques-\\ntion. A path through the tree describes various features of a bird, and a leaf\\nnode identifies the bird that is being described.\\nHence, Describe and Match enables us to use search in combination with\\nknowledge to answer questions about the world.'), Document(metadata={}, page_content='knowledge to answer questions about the world.\\nA portion of the search tree Bob used to identify the penguin outside Alice’s\\nwindow is shown in Figure 3.12.'), Document(metadata={}, page_content='3.11 Problem Reduction 57\\nFirst, the question at the top of the tree, in the root node, is asked. The\\nanswer determines which branch to follow from the root node. In this case,\\nif the answer is “yes, ” the left-hand branch is taken (this branch is not\\nshown in the diagram). If the answer is “no, ” then the right-hand branch is\\ntaken, which leads to the next question—“Is it extinct?”\\nIf the answer to this question is “yes, ” then a leaf node is reached, which'), Document(metadata={}, page_content='gives us the answer: the bird is a dodo. If the answer is “no, ” then we move\\non to the next question. The process continues until the algorithm reaches\\na leaf node, which it must eventually do because each step moves one level\\ndown the tree, and the tree does not have an infinite number of levels.\\nThis kind of tree is called a decision tree, and we learn more about them in\\nChapter 10, where we see how they are used in machine learning.\\n3.10 Combinatorial Explosion'), Document(metadata={}, page_content='3.10 Combinatorial Explosion\\nThe search tree for a Traveling Salesman problem becomes unmanageably\\nlarge as the number of cities increases. Many problems have the property\\nthat as the number of individual items being considered increases, the\\nnumber of possible paths in the search tree increases exponentially, mean-\\ning that as the problem gets larger, it becomes more and more unreason-\\nable to expect a computer program to be able to solve it. This problem is'), Document(metadata={}, page_content='known as combinatorial explosion because the amount of work that a\\nprogram needs to do to solve the problem seems to grow at an explosive\\nrate, due to the possible combinations it must consider.\\n3.11 Problem Reduction\\nIn many cases we find that a complex problem can be most effectively\\nsolved by breaking it down into several smaller problems. If we solve all of\\nthose smaller subproblems, then we have solved the main problem. This'), Document(metadata={}, page_content='approach to problem solving is often referred to as goal reduction because\\nit involves considering the ultimate goal of solving the problem in a way\\nthat involves generating subgoals for that goal.\\nFor example, to solve the T owers of Hanoi problem with n disks, it turns\\nout that the first step is to solve the smaller problem with n /H110021 disks.'), Document(metadata={}, page_content='58 CHAPTER 3 Knowledge Representation\\n123\\nFigure 3.13\\nThe starting state of the\\nTowers of Hanoi problem\\nwith four disks 123\\nFigure 3.14\\nTowers of Hanoi problem\\nof size 4 reduced to a prob-\\nlem of size 3 by first mov-\\ning the largest disk from\\npeg 1 to peg 3\\nFor example, let us examine the T owers of Hanoi with four disks, whose\\nstarting state is shown in Figure 3.13.\\nT o solve this problem, the first step is to move the largest block from peg 1 to'), Document(metadata={}, page_content='peg 3. This will then leave a T owers of Hanoi problem of size 3, as shown in\\nFigure 3.14, where the aim is to move the disks from peg 2 to peg 3. Because\\nthe disk that is on peg 3 is the largest disk, any other disk can be placed on\\ntop of it, and because it is in its final position, it can effectively be ignored.\\nIn this way, a T owers of Hanoi problem of any size n can be solved by first\\nmoving the largest disk to peg 3, and then applying the T owers of Hanoi'), Document(metadata={}, page_content='solution to the remaining disks, but swapping peg 1 and peg 2.\\nThe method for moving the largest disk is not difficult and is left as an exercise.\\n3.12 Goal Trees\\nA goal tree (also called an and-or tree) is a form of semantic tree used to\\nrepresent problems that can be broken down in this way. We say that the\\nsolution to the problem is the goal, and each individual step along the way\\nis a subgoal. In the case of the T owers of Hanoi, moving the largest disk to\\npeg 3 is a subgoal.'), Document(metadata={}, page_content='peg 3 is a subgoal.\\nEach node in a goal tree represents a subgoal, and that node’s children are\\nthe subgoals of that goal. Some goals can be achieved only by solving all of'), Document(metadata={}, page_content='3.12 Goal Trees 59\\nMOVE A, B, C, D FROM 1 TO 3\\nMOVE A, B, C, \\nFROM 2 TO 3\\nMOVE A, B, \\nFROM 1 TO 3\\nMOVE C \\nFROM 2 TO 3\\nMOVE B \\nFROM 1 TO 3\\nMOVE D \\nFROM 1 TO 3\\nMOVE A \\nFROM 2 TO 3\\nFigure 3.15\\nGoal tree for Towers of Hanoi problem with four disks\\nits subgoals. Such nodes on the goal tree are and-nodes, which represent\\nand-goals.\\nIn other cases, a goal can be achieved by achieving any one of its subgoals.\\nSuch goals are or-goals and are represented on the goal tree by or-nodes.'), Document(metadata={}, page_content='Goal trees are drawn in the same way as search trees and other semantic\\ntrees. An and-node is shown by drawing an arc across the arcs that join it to\\nits subgoals (children). Or-nodes are not marked in this way. The main dif-\\nference between goal trees and normal search trees is that in order to solve\\na problem using a goal tree, a number of subproblems (in some cases, all\\nsubproblems) must be solved for the main problem to be solved. Hence,'), Document(metadata={}, page_content='leaf nodes are called success nodes rather than goal nodes because each leaf\\nnode represents success at a small part of the problem.\\nSuccess nodes are always and-nodes. Leaf nodes that are or-nodes are\\nimpossible to solve and are called failure nodes.\\nA goal tree for the T owers of Hanoi problem with four disks is shown in\\nFigure 3.15. The root node represents the main goal, or root goal, of the\\nproblem, which is to move all four disks from peg 1 to peg 3. In this tree, we'), Document(metadata={}, page_content='have represented the four disks as A,B,C, and D, where A is the smallest\\ndisk, and D is the largest. The pegs are numbered from 1 to 3. All of the\\nnodes in this tree are and-nodes. This is true of most problems where there\\nis only one reasonable solution.'), Document(metadata={}, page_content='60 CHAPTER 3 Knowledge Representation\\nFigure 3.15 is somewhat of an oversimplification because it does not\\nexplain how to solve each of the subgoals that is presented. T o produce a\\nsystem that could solve the problem, a larger goal tree that included addi-\\ntional subgoals would be needed. This is left as an exercise.\\nBreaking down the problem in this way is extremely advantageous because it\\ncan be easily extended to solving T owers of Hanoi problems of all sizes. Once'), Document(metadata={}, page_content='we know how to solve the T owers of Hanoi with three disks, we then know\\nhow to solve it for four disks. Hence, we also know how to solve it for five\\ndisks, six disks, and so on. Computer programs can be developed easily that\\ncan solve the T owers of Hanoi problem with enormous numbers of disks.\\nAnother reason that reducing problems to subgoals in this way is of such\\ngreat interest in Artificial Intelligence research is that this is the way in'), Document(metadata={}, page_content='which humans often go about solving problems. If you want to cook a\\nfancy dinner for your friends, you probably have a number of subgoals to\\nsolve first:\\n■ find a recipe\\n■ go to the supermarket\\n■ buy ingredients\\n■ cook dinner\\n■ set the table\\nAnd so on. Solving the problem in this way is very logical for humans\\nbecause it treats a potentially complex problem as a set of smaller, simpler\\nproblems. Humans work very well in this way, and in many cases comput-\\ners do too.'), Document(metadata={}, page_content='ers do too.\\nOne area in which goal trees are often used is computer security. A threat\\ntree represents the possible threats to a computer system, such as a com-\\nputerized banking system. If the goal is “steal Edwin’s money from the\\nbank, ” you can (guess or convince me to divulge my PIN) and (steal or\\ncopy my card) and so on. The threat tree thus represents the possible paths\\nan attacker of the system might take and enables security experts to deter-\\nmine the weaknesses in the system.'), Document(metadata={}, page_content='mine the weaknesses in the system.\\n3.12.1 Top Down or Bottom Up?\\nThere are two main approaches to breaking down a problem into sub-\\ngoals—top down and bottom up.'), Document(metadata={}, page_content='3.12 Goal Trees 61\\nA top-down approach involves first breaking down the main problem into\\nsmaller goals and then recursively breaking down those goals into smaller\\ngoals, and so on, until leaf nodes, or success nodes, are reached, which can\\nbe solved.\\nA bottom-up approach involves first determining all of the subgoals that\\nare necessary to solve the entire problem, and then starting by solving the\\nsuccess nodes, and working up until the complete solution is found. As we'), Document(metadata={}, page_content='see elsewhere in this book, both of these approaches are valid, and the cor-\\nrect approach should be taken for each problem.\\nAgain, humans often think in these terms.\\nBusinesses often look at solving problems either from the top down or\\nfrom the bottom up. Solving a business problem from the top down means\\nlooking at the global picture and working out what subgoals are needed to\\nchange that big picture in a satisfactory way. This often means passing'), Document(metadata={}, page_content='those subgoals onto middle managers, who are given the task of solving\\nthem. Each middle manager will then break the problem down into smaller\\nsubproblems, each of which will be passed down the chain to subordinates.\\nIn this way, the overall problem is solved without the senior management\\never needing to know how it was actually solved. Individual staff members\\nsolve their small problems without ever knowing how that impacts on the\\noverall business.'), Document(metadata={}, page_content='overall business.\\nA bottom-up approach to solving business problems would mean looking\\nat individual problems within the organization and fixing those. Computer\\nsystems might need upgrading, and certain departments might need to\\nwork longer hours. The theory behind this approach is that if all the indi-\\nvidual units within the business are functioning well, then the business as a\\nwhole will be functioning well.\\n3.12.2 Uses of Goal Trees'), Document(metadata={}, page_content='3.12.2 Uses of Goal Trees\\nWe can use goal-driven search to search through a goal tree. As we describe\\nelsewhere in this book, this can be used to solve a number of problems in\\nArtificial Intelligence.\\n3.12.3 Example 1: Map Coloring\\nMap-coloring problems can be represented by goal trees. For example, Fig-\\nure 3.16 shows a goal tree that can be used to represent the map-coloring'), Document(metadata={}, page_content='62 CHAPTER 3 Knowledge Representation\\nrgby rgby rgby rgby rgbyrgby\\n123 4 56\\nFigure 3.16\\nGoal tree representing a map-coloring problem with six countries and four colors\\nproblem for six countries with four colors. The tree has just two levels. The\\ntop level consists of a single and-node, which represents the fact that all\\ncountries must be colored. The next level has an or-node for each country,\\nrepresenting the choice of colors that can be applied.'), Document(metadata={}, page_content='Of course, this tree alone does not represent the entire problem. Con-\\nstraints must be applied that specify that no two adjacent countries may\\nhave the same color. Solving the tree while applying these constraints solves\\nthe map-coloring problem. In fact, to apply a search method to this prob-\\nlem, the goal tree must be redrawn as a search tree because search methods\\ngenerally are not able to deal with and-nodes.\\nThis can be done by redrawing the tree as a search tree, where paths'), Document(metadata={}, page_content='through the tree represent plans rather than goals. Plans are discussed in\\nmore detail in Part 5 of this book. A plan consists of steps that can be taken\\nto solve the overall problem. A search tree can thus be devised where nodes\\nrepresent partial plans. The root node has no plan at all, and leaf nodes rep-\\nresent complete plans.\\nA part of the search tree for the map-coloring problem with six countries\\nand four colors is shown in Figure 3.17.'), Document(metadata={}, page_content='and four colors is shown in Figure 3.17.\\nOne of the search methods described in Chapter 4 or 5 can be applied to\\nthis search tree to find a solution. This may not be the most efficient way to\\nsolve the map-coloring problem, though.\\n3.12.4 Example 2: Proving Theorems\\nAs will be explained in Part 3 of this book, goal trees can be used to repre-\\nsent theorems that are to be proved. The root goal is the theorem that is to'), Document(metadata={}, page_content='3.12 Goal Trees 63\\nNO PLAN\\nSELECT A COLOR \\nFOR COUNTRY 1\\nCHOOSE\\nRED\\nCHOOSE\\nGREEN\\nCHOOSE\\nGREEN\\nCHOOSE\\nBLUE\\nSELECT A COLOR \\nFOR COUNTRY 2\\nSELECT A COLOR \\nFOR COUNTRY 2\\nFigure 3.17\\nPartial search tree for\\nmap-coloring problem\\nwith six countries and four\\ncolors\\nbe proved. It is an or-node because there may be several ways to prove the\\ntheorem. The next level down consists of and-nodes, which are lemmas\\nthat are to be proven. Each of these lemmas again may have several ways to'), Document(metadata={}, page_content='be proved so, therefore, is an or-node. The leaf-nodes of the tree represent\\naxioms that do not need to be proved.\\n3.12.5 Example 3: Parsing Sentences\\nAs is described in Chapter 20, a parser is a tool that can be used to analyze\\nthe structure of a sentence in the English language (or any other human\\nlanguage). Sentences can be broken down into phrases, and phrases can be\\nbroken down into nouns, verbs, adjectives, and so on. Clearly, this is ideally\\nsuited to being represented by goal trees.'), Document(metadata={}, page_content='suited to being represented by goal trees.\\n3.12.6 Example 4: Games\\nGame trees , which are described in more detail in Chapter 6, are goal\\ntrees that are used to represent the choices made by players when play-\\ning two-player games, such as chess, checkers, and Go. The root node of\\na game tree represents the current position, and this is an or-node\\nbecause I must choose one move to make. The next level down in the\\ngame tree represents the possible choices my opponent might make,'), Document(metadata={}, page_content='64 CHAPTER 3 Knowledge Representation\\nand because I need to consider all possible responses that I might make\\nto that move, this level consists of and-nodes. Eventually, the leaf nodes\\nrepresent final positions in the game, and a path through the tree repre-\\nsents a sequence of moves from start to finish, resulting in a win, loss,\\nor a draw.\\nThis kind of tree is a pure and-or tree because it has an or-node at the top,\\neach or-node has and-nodes as its direct successors, and each and-node has'), Document(metadata={}, page_content='or-nodes as its direct successors. Another condition of a pure and-or tree is\\nthat it does not have any constraints that affect which choices can be made.\\n3.13 Chapter Summary\\n■ Artificial Intelligence can be used to solve a wide range of prob-\\nlems, but for the methods to work effectively, the correct represen-\\ntation must be used.\\n■ Semantic nets use graphs to show relationships between objects.\\nFrame-based systems show the same information in frames.'), Document(metadata={}, page_content='■ Frame-based systems allow for inheritance, whereby one frame can\\ninherit features from another.\\n■ Frames often have procedures associated with them that enable a\\nsystem to carry out actions on the basis of data within the frames.\\n■ Search trees are a type of semantic tree. Search methods (several of\\nwhich are described in Chapters 4 and 5) are applied to search\\ntrees, with the aim of finding a goal.\\n■ Describe and Match is a method that can be used to identify an'), Document(metadata={}, page_content='object by searching a tree that represents knowledge about the uni-\\nverse of objects that are being considered.\\n■ Problems such as the T owers of Hanoi problem can be solved effec-\\ntively by breaking them down into smaller subproblems, thus\\nreducing an overall goal to a set of subgoals.\\n■ Goal trees (or and-or trees) are an effective representation for\\nproblems that can be broken down in this way.\\n■ Data-driven search (forward chaining) works from a start state'), Document(metadata={}, page_content='toward a goal. Goal-driven search (backward chaining) works in\\nthe other direction, starting from the goal.'), Document(metadata={}, page_content='Exercises 65\\n3.14 Review Questions\\n3.1 Why are representations so important in Artificial Intelligence?\\nWhat risks are inherent in using the wrong representation?\\n3.2 Explain the connection between frames and object-oriented struc-\\ntures in programming languages, such as Java and C++.\\n3.3 Explain the relationship between graphs, semantic nets, semantic\\ntrees, search spaces, and search trees.\\n3.4 Explain why goal trees are so useful to artificial intelligence'), Document(metadata={}, page_content='research. Give illustrations of how they are used.\\n3.5 Explain the connection between decision trees and the Describe\\nand Match algorithm. How efficient do you think this algorithm is?\\nCan you think of any ways to improve it?\\n3.6 Explain the problem of combinatorial explosion. What impact\\ndoes this have on the methods we use for solving large problems\\nusing search?\\n3.7 Explain why removing cycles from a search tree is a good idea.'), Document(metadata={}, page_content='3.8 Explain how and-or trees can be used to represent games. What\\nlimitations do you think a system that uses game trees to play chess\\nmight face? Would it face different limitations if it played tic-tac-\\ntoe? Or poker?\\n3.9 What is the difference between a top-down approach to solving a\\nproblem and a bottom-up approach? In what kinds of situations\\nmight each be more appropriate?\\n3.15 Exercises\\n3.10 Convert the following information into:\\na) a semantic net\\nb) a frame-based representation'), Document(metadata={}, page_content='a) a semantic net\\nb) a frame-based representation\\nA Ford is a type of car. Bob owns two cars. Bob parks his car at\\nhome. His house is in California, which is a state. Sacramento is the\\nstate capital of California. Cars drive on the freeway, such as Route\\n101 and Highway 81.'), Document(metadata={}, page_content='66 CHAPTER 3 Knowledge Representation\\n3.11 Design a decision tree that enables you to identify an item from a\\ncategory in which you are interested (e.g., cars, animals, pop\\nsingers, films, etc.).\\n3.12 Devise your own representation for the Missionaries and Canni-\\nbals problem and implement it either with pen and paper or in the\\nprogramming language of your choice. Use it to solve the problem.\\nHow efficient is your representation compared with that used in'), Document(metadata={}, page_content='Section 3.9.1 of this book? Does it come up with the same answer?\\nWhich approach is easier for an observer to quickly grasp? Which\\nwould you say is the better representation overall, and why?\\n3.13 Design a suitable representation and draw the complete search tree\\nfor the following problem:\\nA farmer is on one side of a river and wishes to cross the river with\\na wolf, a chicken, and a bag of grain. He can take only one item at a\\ntime in his boat with him. He can’t leave the chicken alone with the'), Document(metadata={}, page_content='grain, or it will eat the grain, and he can’t leave the wolf alone with\\nthe chicken, or the wolf will eat the chicken. How does he get all\\nthree safely across to the other side?\\n3.14 Write a program using the programming language of your choice\\nto implement the representation you designed for Review Ques-\\ntion 3.3. Have your program solve the problem, and have it show\\non the screen how it reaches the solution. Does it find the best pos-\\nsible solution? Does it find it as quickly as it might?'), Document(metadata={}, page_content='3.15 Write a program that solves either\\na) the T owers of Hanoi problem with up to 1000 disks, or,\\nb) the Traveling Salesman problem with up to 10 cities.\\nY ou may need to wait until you have read about some of the search\\ntechniques described in Chapter 4 before you can write this pro-\\ngram. For now, you can design a suitable representation and\\nimplement a suitable data structure for the problem in the lan-\\nguage of your choice.\\n3.16 Further Reading'), Document(metadata={}, page_content='guage of your choice.\\n3.16 Further Reading\\nAll Artificial Intelligence textbooks deal with the subject of representation.\\nA particularly good description in terms of search for problem solving is\\nfound in Russell and Norvig (1995).'), Document(metadata={}, page_content='Further Reading 67\\nWinston (1993) provides a good description in terms of semantics.\\nDromey (1982) provides an excellent description of the development of an\\nalgorithm for the T owers of Hanoi problem by problem reduction.\\nAnd-or trees and their uses are particularly well described by Luger (2002)\\nand Charniak and McDermott (1985).\\nFrames were introduced by Marvin Minsky in his 1975 paper,A framework\\nfor Representing Knowledge.'), Document(metadata={}, page_content='for Representing Knowledge.\\nKnowledge Representation, Reasoning and Declarative Problem Solving ,b y\\nChitta Baral (2003 – Cambridge University Press)\\nHow to Solve it by Computer, by R.G. Dromey (1982 – out of print)\\nKnowledge Representation and Defeasible Reasoning (Studies in Cognitive\\nSystems, Vol 5) , edited by Ronald P . Loui and Greg N. Carlson (1990 –\\nKluwer Academic Publishers)\\nA Framework for Representing Knowledge , by Marvin Minsky (1975 – in'), Document(metadata={}, page_content='Computation & Intelligence – Collected Readings, edited by George F. Luger,\\nThe MIT Press)\\nKnowledge Representation: Logical, Philosophical, and Computational Foun-\\ndations, by John F. Sowa and David Dietz (1999 – Brooks Cole)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='Search\\n2\\nIntroduction to Part 2\\nPart 2 is divided into three chapters.\\nSearch Methodologies\\nChapter 4 introduces a number of search methods, includ-\\ning depth-first search and breadth-first search. Metrics are\\npresented that enable analysis of search methods and pro-\\nvide a way to determine which search methods are most\\nsuitable for particular problems.\\nThis chapter also introduces the idea of heuristics for\\nsearch and presents a number of methods, such as best-first'), Document(metadata={}, page_content='search, that use heuristics to improve the performance of\\nsearch methods.\\nAdvanced Search\\nChapter 5 introduces a number of more complex search\\nmethods. In particular, it explains the way that search can\\nbe used to solve combinatorial optimization problems\\nusing local search and presents a number of local search\\nmethods, such as simulated annealing and tabu search. The\\nchapter also explains how search can be run in parallel and\\ndiscusses some of the complications that this introduces.'), Document(metadata={}, page_content='Game Playing\\nThis chapter explains the relationship between search and\\ngames, such as chess, checkers, and tic-tac-toe. It explains\\nthe Minimax algorithm and how alpha–beta pruning can\\nbe used to make it more efficient. It explains some of the\\nmore advanced techniques used in modern game-playing\\ncomputers and discusses why computers are currently\\nunable to beat humans at games such as Go.\\nPART\\n4\\nCHAPTER\\n5\\nCHAPTER\\n6\\nCHAPTER'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='4CHAPTER\\nSearch Methodologies\\nResearch is the process of going up alleys to see if they are blind.\\n—Marston Bates\\nWhen a thing is funny, search it carefully for a hidden truth.\\n—George Bernard Shaw\\nIf we do not find anything pleasant, at least we shall find something new.\\n—Voltaire,Candide\\nEveryone that asketh receiveth; and he that seeketh findeth.\\n—The Gospel according to St Matthew, Chapter 7, V erse 8\\n4.1 Introduction\\nIn Chapter 3, we introduced search trees and other methods and represen-'), Document(metadata={}, page_content='tations that are used for solving problems using Artificial Intelligence tech-\\nniques such as search. In Chapter 4, we introduce a number of methods\\nthat can be used to search, and we discuss how effective they are in different\\nsituations. Depth-first search and breadth-first search are the best-known\\nand widest-used search methods, and in this chapter we examine why this\\nis and how they are implemented. We also look at a number of properties'), Document(metadata={}, page_content='of search methods, including optimality and completeness, that can be\\nused to determine how useful a search method will be for solving a partic-\\nular problem.'), Document(metadata={}, page_content='72 CHAPTER 4 Search Methodologies\\nThe methods that are described in this chapter and Chapter 5 impact on\\nalmost every aspect of Artificial Intelligence. Because of the serial nature in\\nwhich computers tend to operate, search is a necessity to determine solu-\\ntions to an enormous range of problems.\\nThis chapter starts by discussing blind search methods and moves on to\\nexamine search methods that are more informed—these search methods\\nuse heuristics to examine a search space more efficiently.'), Document(metadata={}, page_content='4.2 Problem Solving as Search\\nProblem solving is an important aspect of Artificial Intelligence. A problem\\ncan be considered to consist of a goal and a set of actions that can be taken\\nto lead to the goal. At any given time, we consider the state of the search\\nspace to represent where we have reached as a result of the actions we have\\napplied so far.\\nFor example, consider the problem of looking for a contact lens on a foot-'), Document(metadata={}, page_content='ball field. The initial state is how we start out, which is to say we know that\\nthe lens is somewhere on the field, but we don’t know where. If we use the\\nrepresentation where we examine the field in units of one square foot, then\\nour first action might be to examine the square in the top-left corner of the\\nfield. If we do not find the lens there, we could consider the state now to be\\nthat we have examined the top-left square and have not found the lens.'), Document(metadata={}, page_content='After a number of actions, the state might be that we have examined 500\\nsquares, and we have now just found the lens in the last square we exam-\\nined. This is a goal state because it satisfies the goal that we had of finding\\na contact lens.\\nSearch is a method that can be used by computers to examine a problem\\nspace like this in order to find a goal. Often, we want to find the goal as\\nquickly as possible or without using too many resources. A problem space'), Document(metadata={}, page_content='can also be considered to be a search space because in order to solve the\\nproblem, we will search the space for a goal state. We will continue to use\\nthe term search space to describe this concept.\\nIn this chapter, we will look at a number of methods for examining a search\\nspace. These methods are called search methods.'), Document(metadata={}, page_content='4.3 Data-Driven or Goal-Driven Search 73\\n4.3 Data-Driven or Goal-Driven Search\\nThere are two main approaches to searching a search tree, which roughly\\ncorrespond to the top-down and bottom-up approaches discussed in Sec-\\ntion 3.12.1. Data-driven search starts from an initial state and uses actions\\nthat are allowed to move forward until a goal is reached. This approach is\\nalso known as forward chaining.\\nAlternatively, search can start at the goal and work back toward a start state,'), Document(metadata={}, page_content='by seeing what moves could have led to the goal state. This is goal-driven\\nsearch, also known as backward chaining.\\nMost of the search methods we will examine in this chapter and Chapter 5\\nare data-driven search: they start from an initial state (the root node in the\\nsearch tree) and work toward the goal node.\\nIn many circumstances, goal-driven search is preferable to data driven-\\nsearch, but for most of this part of the book, when we refer to “search, ” we\\nare talking about data-driven search.'), Document(metadata={}, page_content='are talking about data-driven search.\\nGoal-driven search and data-driven search will end up producing the same\\nresults, but depending on the nature of the problem being solved, in some\\ncases one can run more efficiently than the other—in particular, in some\\nsituations one method will involve examining more states than the other.\\nGoal-driven search is particularly useful in situations in which the goal can be\\nclearly specified (for example, a theorem that is to be proved or finding an exit'), Document(metadata={}, page_content='from a maze). It is also clearly the best choice in situations such as medical\\ndiagnosis where the goal (the condition to be diagnosed) is known, but the\\nrest of the data (in this case, the causes of the condition) need to be found.\\nData-driven search is most useful when the initial data are provided, and it is\\nnot clear what the goal is. For example, a system that analyzes astronomical\\ndata and thus makes deductions about the nature of stars and planets would'), Document(metadata={}, page_content='receive a great deal of data, but it would not necessarily be given any direct\\ngoals. Rather, it would be expected to analyze the data and determine conclu-\\nsions of its own. This kind of system has a huge number of possible goals that\\nit might locate. In this case, data-driven search is most appropriate.\\nIt is interesting to consider a maze that has been designed to be traversed\\nfrom a start point in order to reach a particular end point. It is nearly'), Document(metadata={}, page_content='always far easier to start from the end point and work back toward the start'), Document(metadata={}, page_content='74 CHAPTER 4 Search Methodologies\\npoint. This is because a number of dead end paths have been set up from\\nthe start (data) point, and only one path has been set up to the end (goal)\\npoint. As a result, working back from the goal to the start has only one pos-\\nsible path.\\n4.4 Generate and Test\\nThe simplest approach to search is called Generate and T est. This simply\\ninvolves generating each node in the search space and testing it to see if it is'), Document(metadata={}, page_content='a goal node. If it is, the search has succeeded and need not carry on. Other-\\nwise, the procedure moves on to the next node.\\nThis is the simplest form of brute-force search (also called exhaustive\\nsearch), so called because it assumes no additional knowledge other than\\nhow to traverse the search tree and how to identify leaf nodes and goal nodes,\\nand it will ultimately examine every node in the tree until it finds a goal.'), Document(metadata={}, page_content='T o successfully operate, Generate and T est needs to have a suitable Genera-\\ntor, which should satisfy three properties:\\n1. It must be complete: In other words, it must generate every possi-\\nble solution; otherwise it might miss a suitable solution.\\n2. It must be nonredundant: This means that it should not generate\\nthe same solution twice.\\n3. It must be well informed: This means that it should only propose\\nsuitable solutions and should not examine possible solutions that'), Document(metadata={}, page_content='do not match the search space.\\nThe Generate and T est method can be successfully applied to a number of\\nproblems and indeed is the manner in which people often solve problems\\nwhere there is no additional information about how to reach a solution.\\nFor example, if you know that a friend lives on a particular road, but you do\\nnot know which house, a Generate and T est approach might be necessary;\\nthis would involve ringing the doorbell of each house in turn until you'), Document(metadata={}, page_content='found your friend. Similarly, Generate and T est can be used to find solu-\\ntions to combinatorial problems such as the eight queens problem that is\\nintroduced in Chapter 5.\\nGenerate and T est is also sometimes referred to as a blind search technique\\nbecause of the way in which the search tree is searched without using any\\ninformation about the search space.'), Document(metadata={}, page_content='4.5 Depth-First Search 75\\n27\\n8\\n1\\nA\\nCB\\nFED\\nGH IJ K L\\n9\\n10\\n113\\n4\\n5\\n6\\n13\\n12 Figure 4.1\\nIllustrating depth-first\\nsearch\\nMore systematic examples of brute-force search are presented in this chap-\\nter, in particular, depth-first search and breadth-first search.\\nMore “intelligent” (or informed) search techniques are explored later in\\nthis chapter.\\n4.5 Depth-First Search\\nA commonly used search algorithm is depth-first search . Depth-first'), Document(metadata={}, page_content='search is so called because it follows each path to its greatest depth before\\nmoving on to the next path. The principle behind the depth-first approach\\nis illustrated in Figure 4.1. Assuming that we start from the left side and\\nwork toward the right, depth-first search involves working all the way down\\nthe left-most path in the tree until a leaf node is reached. If this is a goal\\nstate, the search is complete, and success is reported.'), Document(metadata={}, page_content='If the leaf node does not represent a goal state, search backtracks up to the\\nnext highest node that has an unexplored path. In Figure 4.1, after examining\\nnode G and discovering that it is not a leaf node, search will backtrack to\\nnode D and explore its other children. In this case, it only has one other child,\\nwhich is H. Once this node has been examined, search backtracks to the next\\nunexpanded node, which is A, because B has no unexplored children.'), Document(metadata={}, page_content='This process continues until either all the nodes have been examined, in\\nwhich case the search has failed, or until a goal state has been reached, in\\nwhich case the search has succeeded. In Figure 4.1, search stops at node J,\\nwhich is the goal node. As a result, nodes F, K, and L are never examined.'), Document(metadata={}, page_content='76 CHAPTER 4 Search Methodologies\\nA\\nCB\\nFED\\nGH I J K L\\n1\\n23\\n45 6\\n78 9 1 0\\nFigure 4.2\\nIllustrating breadth-first\\nsearch. The numbers indi-\\ncate the order in which the\\nnodes are examined.\\nDepth-first search uses a method called chronological backtracking to\\nmove back up the search tree once a dead end has been found. Chronolog-\\nical backtracking is so called because it undoes choices in reverse order of\\nthe time the decisions were originally made. We will see later in this chapter'), Document(metadata={}, page_content='that nonchronological backtracking, where choices are undone in a more\\nstructured order, can be helpful in solving certain problems.\\nDepth-first search is an example ofbrute-force search,o rexhaustive search.\\nDepth-first search is often used by computers for search problems such as\\nlocating files on a disk, or by search engines for spidering the Internet.\\nAs anyone who has used the find operation on their computer will know,'), Document(metadata={}, page_content='depth-first search can run into problems. In particular, if a branch of the\\nsearch tree is extremely large, or even infinite, then the search algorithm\\nwill spend an inordinate amount of time examining that branch, which\\nmight never lead to a goal state.\\n4.6 Breadth-First Search\\nAn alternative to depth-first search is breadth-first search. As its name sug-\\ngests, this approach involves traversing a tree by breadth rather than by'), Document(metadata={}, page_content='depth. As can be seen from Figure 4.2, the breadth-first algorithm starts by\\nexamining all nodes one level (sometimes called one ply) down from the\\nroot node.'), Document(metadata={}, page_content='4.6 Breadth-First Search 77\\nTable 4.1 Comparison of depth-first and breadth-first search\\nScenario Depth first Breadth first\\nSome paths are extremely long, or\\neven infinite\\nAll paths are of similar length\\nAll paths are of similar length, and all\\npaths lead to a goal state\\nHigh branching factor\\nPerforms badly\\nPerforms well\\nPerforms well\\nPerformance depends on other factors\\nPerforms well\\nPerforms well\\nWasteful of time and memory\\nPerforms poorly'), Document(metadata={}, page_content='Wasteful of time and memory\\nPerforms poorly\\nIf a goal state is reached here, success is reported. Otherwise, search contin-\\nues by expanding paths from all the nodes in the current level down to the\\nnext level. In this way, search continues examining nodes in a particular\\nlevel, reporting success when a goal node is found, and reporting failure if\\nall nodes have been examined and no goal node has been found.\\nBreadth-first search is a far better method to use in situations where the'), Document(metadata={}, page_content='tree may have very deep paths, and particularly where the goal node is in a\\nshallower part of the tree. Unfortunately, it does not perform so well where\\nthe branching factor of the tree is extremely high, such as when examining\\ngame trees for games like Go or Chess (see Chapter 6 for more details on\\ngame trees).\\nBreadth-first search is a poor idea in trees where all paths lead to a goal\\nnode with similar length paths. In situations such as this, depth-first search'), Document(metadata={}, page_content='would perform far better because it would identify a goal node when it\\nreached the bottom of the first path it examined.\\nThe comparative advantages of depth-first and breadth-first search are tab-\\nulated in Table 4.1.\\nAs will be seen in the next section, depth-first search is usually simpler to\\nimplement than breadth-first search, and it usually requires less memory\\nusage because it only needs to store information about the path it is currently'), Document(metadata={}, page_content='exploring, whereas breadth-first search needs to store information about all\\npaths that reach the current depth. This is one of the main reasons that\\ndepth-first search is used so widely to solve everyday computer problems.'), Document(metadata={}, page_content='78 CHAPTER 4 Search Methodologies\\nThe problem of infinite paths can be avoided in depth-first search by\\napplying a depth threshold. This means that paths will be considered to\\nhave terminated when they reach a specified depth. This has the disadvan-\\ntage that some goal states (or, in some cases, the only goal state) might be\\nmissed but ensures that all branches of the search tree will be explored in\\nreasonable time. As is seen in Chapter 6, this technique is often used when\\nexamining game trees.'), Document(metadata={}, page_content='examining game trees.\\n4.7 Properties of Search Methods\\nAs we see in this chapter, different search methods perform in different\\nways. There are several important properties that search methods should\\nhave in order to be most useful.\\nIn particular, we will look at the following properties:\\n■ complexity\\n■ completeness\\n■ optimality\\n■ admissibility\\n■ irrevocability\\nIn the following sections, we will explain what each of these properties'), Document(metadata={}, page_content='means and why they are useful. We will continue to refer to many of these\\nproperties (in particular, completeness and complexity) as we examine a\\nnumber of search methods in this chapter and in Chapter 5.\\n4.7.1 Complexity\\nIn discussing a search method, it is useful to describe how efficient that\\nmethod is, over time and space. The time complexity of a method is related\\nto the length of time that the method would take to find a goal state. The'), Document(metadata={}, page_content='space complexity is related to the amount of memory that the method\\nneeds to use.\\nIt is normal to use Big-O notation to describe the complexity of a method. For\\nexample, breadth-first search has a time complexity ofO(b\\nd), where b is the\\nbranching factor of the tree, andd is the depth of the goal node in the tree.'), Document(metadata={}, page_content='4.7 Properties of Search Methods 79\\nDepth-first search is very efficient in space because it only needs to store\\ninformation about the path it is currently examining, but it is not efficient\\nin time because it can end up examining very deep branches of the tree.\\nClearly, complexity is an important property to understand about a search\\nmethod. A search method that is very inefficient may perform reasonably\\nwell for a small test problem, but when faced with a large real-world prob-'), Document(metadata={}, page_content='lem, it might take an unacceptably long period of time. As we will see, there\\ncan be a great deal of difference between the performance of two search\\nmethods, and selecting the one that performs the most efficiently in a par-\\nticular situation can be very important.\\nThis complexity must often be weighed against the adequacy of the solu-\\ntion generated by the method. A very fast search method might not always\\nfind the best solution, whereas, for example, a search method that examines'), Document(metadata={}, page_content='every possible solution will guarantee to find the best solution, but it will be\\nvery inefficient.\\n4.7.2 Completeness\\nA search method is described as being complete if it is guaranteed to find a\\ngoal state if one exists. Breadth-first search is complete, but depth-first\\nsearch is not because it may explore a path of infinite length and never find\\na goal node that exists on another path.\\nCompleteness is usually a desirable property because running a search'), Document(metadata={}, page_content='method that never finds a solution is not often helpful. On the other hand,\\nit can be the case (as when searching a game tree, when playing a game, for\\nexample) that searching the entire search tree is not necessary, or simply\\nnot possible, in which case a method that searches enough of the tree might\\nbe good enough.\\nA method that is not complete has the disadvantage that it cannot neces-\\nsarily be believed if it reports that no solution exists.\\n4.7.3 Optimality'), Document(metadata={}, page_content='4.7.3 Optimality\\nA search method is optimal if it is guaranteed to find the best solution that\\nexists. In other words, it will find the path to a goal state that involves tak-\\ning the least number of steps.'), Document(metadata={}, page_content='80 CHAPTER 4 Search Methodologies\\nThis does not mean that the search method itself is efficient—it might take\\na great deal of time for an optimal search method to identify the optimal\\nsolution—but once it has found the solution, it is guaranteed to be the best\\none. This is fine if the process of searching for a solution is less time con-\\nsuming than actually implementing the solution. On the other hand, in\\nsome cases implementing the solution once it has been found is very sim-'), Document(metadata={}, page_content='ple, in which case it would be more beneficial to run a faster search method,\\nand not worry about whether it found the optimal solution or not.\\nBreadth-first search is an optimal search method, but depth-first search is\\nnot. Depth-first search returns the first solution it happens to find, which\\nmay be the worst solution that exists. Because breadth-first search examines\\nall nodes at a given depth before moving on to the next depth, if it finds a'), Document(metadata={}, page_content='solution, there cannot be another solution before it in the search tree.\\nIn some cases, the word optimal is used to describe an algorithm that finds\\na solution in the quickest possible time, in which case the concept of\\nadmissibility is used in place of optimality. An algorithm is then defined as\\nadmissible if it is guaranteed to find the best solution.\\n4.7.4 Irrevocability\\nMethods that use backtracking are described as tentative. Methods that do'), Document(metadata={}, page_content='not use backtracking, and which therefore examine just one path, are\\ndescribed as irrevocable. Depth-first search is an example of tentative\\nsearch. In Section 4.13 we look at hill climbing, a search method that is\\nirrevocable.\\nIrrevocable search methods will often find suboptimal solutions to prob-\\nlems because they tend to be fooled by local optima—solutions that look\\ngood locally but are less favorable when compared with other solutions\\nelsewhere in the search space.'), Document(metadata={}, page_content='elsewhere in the search space.\\n4.8 Why Humans Use Depth-First Search\\nBoth depth-first and breadth-first search are easy to implement, although\\ndepth-first search is somewhat easier. It is also somewhat easier for humans\\nto understand because it much more closely relates to the natural way in\\nwhich humans search for things, as we see in the following two examples.'), Document(metadata={}, page_content='4.8 Why Humans Use Depth-First Search 81\\n4.8.1 Example 1: Traversing a Maze\\nWhen traversing a maze, most people will wander randomly, hoping they\\nwill eventually find the exit (Figure 4.3). This approach will usually be suc-\\ncessful eventually but is not the most rational and often leads to what we\\ncall “going round in circles. ” This problem, of course, relates to search\\nspaces that contain loops, and it can be avoided by converting the search\\nspace into a search tree.'), Document(metadata={}, page_content='space into a search tree.\\nAn alternative method that many people know for traversing a maze is to\\nstart with your hand on the left side of the maze (or the right side, if you\\nprefer) and to follow the maze around, always keeping your left hand on\\nthe left edge of the maze wall. In this way, you are guaranteed to find the\\nexit. As can be seen in Figure 4.3, this is because this technique corresponds\\nexactly to depth-first search.'), Document(metadata={}, page_content='exactly to depth-first search.\\nIn Figure 4.3, certain special points in the maze have been labeled:\\n■ A is the entrance to the maze.\\n■ M is the exit from the maze.\\n■ C, E, F, G, H, J, L, and N are dead ends.\\n■ B, D, I, and K are points in the maze where a choice can be made as\\nto which direction to go next.\\nIn following the maze by running one’s hand along the left edge, the fol-\\nlowing path would be taken:\\nA, B, E, F, C, D, G, H, I, J, K, L, M'), Document(metadata={}, page_content='A, B, E, F, C, D, G, H, I, J, K, L, M\\nY ou should be able to see that following the search tree using depth-first\\nsearch takes the same path. This is only the case because the nodes of the\\nsearch tree have been ordered correctly. The ordering has been chosen so\\nthat each node has its left-most child first and its right-most child last.\\nUsing a different ordering would cause depth-first search to follow a differ-\\nent path through the maze.\\n4.8.2 Example 2: Searching for a Gift'), Document(metadata={}, page_content='4.8.2 Example 2: Searching for a Gift\\nWhen looking for a Christmas present for a relative in a number of shops,\\neach of which has several floors, and where each floor has several depart-\\nments, depth-first search might be a natural, if rather simplistic, approach.'), Document(metadata={}, page_content='82 CHAPTER 4 Search Methodologies\\nEF G H I\\nDC\\nA\\nB\\nJK\\nLM N\\nIN OUTAN M\\nE\\nD I\\nFL\\nB\\nH\\nC\\nJ\\nK\\nG\\nFigure 4.3\\nA maze and a search tree\\nrepresentation of the\\nmaze.\\nThis would involve visiting each floor in the first building before moving\\non to the next building. A breadth-first approach would mean examining\\nthe first department in each shop, and then going back to examine the sec-\\nond department in each shop, and so on. This way does not make sense due'), Document(metadata={}, page_content='to the spatial relationship between the departments, floors, and shops. For\\na computer, either approach would work equally well as long as a represen-\\ntation was used where moving from one building to another did not take\\nany computation time.'), Document(metadata={}, page_content='4.9 Implementing Depth-First and Breadth-First Search 83\\nA\\nBC\\nDE FG\\nONMLKJIH\\nFigure 4.4\\nA simple search tree with\\nfifteen nodes. The tree has\\na branching factor of two\\nand a depth of three.\\nIn both of the examples above, it can be seen that using breadth-first\\nsearch, although a perfectly reasonable approach for a computer system,\\nwould be rather strange for a human. This is probably because with depth-\\nfirst search, the approach is to explore each path fully before moving onto'), Document(metadata={}, page_content='another path, whereas with breadth-first search, the approach involves\\nrevisiting and extending particular paths many times.\\nDespite this, implementations in software of both algorithms are nearly\\nidentical, at least when expressed in pseudocode.\\n4.9 Implementing Depth-First and Breadth-First Search\\nA pseudocode implementation of depth-first search is given below.\\nThe variable state represents the current state at any given point in the'), Document(metadata={}, page_content='algorithm, and queue is a data structure that stores a number of states, in a\\nform that allows insertion and removal from either end. In this algorithm,\\nwe always insert at the front and remove from the front, which as we will\\nsee later on means that depth-first search can be easily implemented using\\na stack.\\nIn this implementation, we have used the function \\nsuccessors (state) ,\\nwhich simply returns all successors of a given state.'), Document(metadata={}, page_content='84 CHAPTER 4 Search Methodologies\\nFunction depth ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse add_to_front_of_queue (successors (state));\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nTable 4.2 shows the states that the variables queue and state take on when'), Document(metadata={}, page_content='running the depth-first search algorithm over a simple search tree, as\\nshown in Figure 4.3.\\nIn fact, depth-first search can be readily implemented on most computer\\nsystems using a stack, which is simply a “last in first out” queue (sometimes\\ncalled a LIFO). In this way, a recursive version of the algorithm given above\\ncan be used, as follows. Because this function is recursive, it needs to be\\ncalled with an argument:\\nrecursive_depth (root_node);\\nThe function is defined as follows:'), Document(metadata={}, page_content='The function is defined as follows:\\nFunction recursive_depth (state)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse\\n{\\nremove_from_stack (state);\\nadd_to_stack (successors (state))\\n}\\nwhile (stack != [])\\n{\\nif recursive_depth (stack [0]) == SUCCESS\\nthen return SUCCESS;\\nremove_first_item_from (stack);\\n}\\nreturn FAILURE;\\n}\\nIf you run through this algorithm on paper (or in a programming language\\nsuch as C++ or LISP), you will find that it follows the tree in the same way\\nas the previous algorithm,'), Document(metadata={}, page_content='as the previous algorithm,\\ndepth.'), Document(metadata={}, page_content='4.9 Implementing Depth-First and Breadth-First Search 85\\nTable 4.2 Analysis of depth-first search of tree shown in Figure 4.5\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\nA\\nA\\nB\\nB\\nD\\nD\\nH\\nI\\nE\\nE\\nJ\\nK\\nC\\nC\\nF\\nF\\nL\\n(empty)\\nB,C\\nC\\nD,E,C\\nE,C\\nH,I,E,C\\nI,E,C\\nE,C\\nC\\nJ,K,C\\nK,C\\nC\\n(empty)\\nF, G\\nG\\nL,M,G\\nM,G\\nThe queue starts out empty, and the initial state\\nis the root node, which is A.\\nThe successors of A are added to the queue.\\nThe successors of the current state, B, are added'), Document(metadata={}, page_content='The successors of the current state, B, are added\\nto the front of the queue.\\nH has no successors, so no new nodes are added\\nto the queue.\\nSimilarly, I has no successors.\\nAgain, J has no successors.\\nK has no successors. Now we have explored the\\nentire branch below B, which means we back-\\ntrack up to C.\\nThe queue is empty, but we are not at the point\\nin the algorithm where this would mean failing\\nbecause we are about to add successors of C to\\nthe queue.'), Document(metadata={}, page_content='the queue.\\nSUCCESS: the algorithm ends because a goal node\\nhas been located. In this case, it is the only goal\\nnode, but the algorithm does not know that and\\ndoes not know how many nodes were left to\\nexplore.'), Document(metadata={}, page_content='86 CHAPTER 4 Search Methodologies\\nAs was mentioned previously, depth-first search and breadth-first search\\ncan be implemented very similarly. The following is a pseudocode of a non-\\nrecursive implementation of breadth-first search, which should be com-\\npared with the implementation above of depth-first search:\\nFunction breadth ()\\n{\\nqueue = [];      // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)\\n{\\nif is_goal (state)\\nthen return SUCCESS'), Document(metadata={}, page_content='{\\nif is_goal (state)\\nthen return SUCCESS\\nelse add_to_back_of_queue (successors (state));\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nNotice that the only difference between depth and breadth is that where depth\\nadds successor states to the front of the queue, breadth adds them to the back\\nof the queue. So when applied to the search tree in Figure 4.4, breadth will fol-'), Document(metadata={}, page_content='low a rather different path from depth, as is shown in Table 4.3.\\nY ou will notice that in this particular case, depth-first search found the goal\\nin two fewer steps than breadth-first search. As has been suggested, depth-\\nfirst search will often find the goal quicker than breadth-first search if all\\nleaf nodes are the same depth below the root node. However, in search trees\\nwhere there is a very large subtree that does not contain a goal, breadth-'), Document(metadata={}, page_content='first search will nearly always perform better than depth-first search.\\nAnother important factor to note is that the queue gets much longer when\\nusing breadth-first search. For large trees, and in particular for trees with\\nhigh branching factors, this can make a significant difference because the\\ndepth-first search algorithm will never require a queue longer than the\\nmaximum depth of the tree, whereas breadth-first search in the worst case'), Document(metadata={}, page_content='will need a queue equal to the number of nodes at the level of the tree with\\nthe most nodes (eight in a tree of depth three with branching factor of two,\\nas in Figure 4.3). Hence, we say that depth-first search is usually more\\nmemory efficient than breadth-first search.'), Document(metadata={}, page_content='4.9 Implementing Depth-First and Breadth-First Search 87\\nTable 4.3 Analysis of breadth-first search of tree shown in Figure 4.4\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\nA\\nA\\nB\\nB\\nC\\nC\\nD\\nD\\nE\\nE\\nF\\nF\\nG\\nG\\nH\\nI\\nJ\\nK\\nL\\n(empty)\\nB,C\\nC\\nC,D,E\\nD,E\\nD,E,F,G\\nE,F ,G\\nE,F ,G,H,I\\nF ,G,H,I\\nF ,G,H,I,J,K\\nG,H,I,J,K\\nG,H,I,J,K,L,M\\nH,I,J,K,L,M\\nH,I,J,K,L,M,N,O\\nI,J,K,L,M,N,O\\nJ,K,L,M,N,O\\nK,L,M,N,O\\nL,M,N,O\\nM,N,O\\nThe queue starts out empty, and the initial\\nstate is the root node, which is A.'), Document(metadata={}, page_content='state is the root node, which is A.\\nThe two descendents of A are added to the\\nqueue.\\nThe two descendents of the current state, B, are\\nadded to the back of the queue.\\nH has no successors, so we have nothing to add\\nto the queue in this state, or in fact for any sub-\\nsequent states.\\nSUCCESS: A goal state has been reached.'), Document(metadata={}, page_content='88 CHAPTER 4 Search Methodologies\\nAs we have seen, however, depth-first search is neither optimal nor com-\\nplete, whereas breadth-first search is both. This means that depth-first\\nsearch may not find the best solution and, in fact, may not ever find a solu-\\ntion at all. In contrast, breadth-first search will always find the best solution.\\n4.10 Example: Web Spidering\\nAn example of the importance of choosing the right search strategy can be'), Document(metadata={}, page_content='seen in spidering the world wide web. The assumption is made that the\\nmajority of the web is connected, meaning that it is possible to get from\\none page to another by following a finite number of links, where a link con-\\nnects two pages together.\\nSome parts of the Internet have a very high branching factor, with many\\npages containing hundreds of links to other pages. On average though, the\\nbranching factor is reasonably low, and so it seems that breadth-first search'), Document(metadata={}, page_content='might be a sensible approach to spidering. In practice, however, the search\\ntree that represents the connected part of the Internet is huge, and search-\\ning it by pure breadth-first search would involve a prohibitive storage\\nrequirement. Depth-first search would also not be practical because some\\npaths might have almost infinite depth, particularly given that some pages\\non the Internet are generated automatically at the time they are accessed.'), Document(metadata={}, page_content='Hence, Internet spiders must use a combination of approaches, with a par-\\nticular emphasis placed on web pages that change frequently and pages that\\nare considered by some metric to be “important. ” Another important\\naspect of search engines is their ability to search in parallel. We discuss this\\nconcept in more detail in Chapter 5.\\n4.11 Depth-First Iterative Deepening\\nDepth-First Iterative Deepening, or DFID (also called Iterative Deepening'), Document(metadata={}, page_content='Search or IDS), is an exhaustive search technique that combines depth-first\\nwith breadth-first search. The DFID algorithm involves repeatedly carrying\\nout depth-first searches on the tree, starting with a depth-first search lim-\\nited to a depth of one, then a depth-first search of depth two, and so on,\\nuntil a goal node is found.\\nThis is an algorithm that appears to be somewhat wasteful in terms of the\\nnumber of steps that are required to find a solution. However, it has the'), Document(metadata={}, page_content='advantage of combining the efficiency of memory use of depth-first search'), Document(metadata={}, page_content='4.11 Depth-First Iterative Deepening 89\\nwith the advantage that branches of the search tree that are infinite or\\nextremely large will not sidetrack the search.\\nIt also shares the advantage of breadth-first search that it will always find\\nthe path that involves the fewest steps through the tree (although, as we will\\nsee, not necessarily the best path).\\nAlthough it appears that DFID would be an extremely inefficient way to'), Document(metadata={}, page_content='search a tree, it turns out to be almost as efficient as depth-first or breadth-\\nfirst search. This can be seen from the fact that for most trees, the majority\\nof nodes are in the deepest level, meaning that all three approaches spend\\nmost of their time examining these nodes.\\nFor a tree of depth d and with a branching factor of b, the total number\\nof nodes is\\n1 root node\\nb nodes in the first layer\\nb\\n2 nodes in the second layer\\n...\\nb\\nn nodes in the nth layer\\nHence, the total number of nodes is'), Document(metadata={}, page_content='Hence, the total number of nodes is\\n1 + b + b2 + b3 + . . . + bd\\nwhich is a geometric progression equal to\\nFor example, for a tree of depth 2 with a branching factor of 2, there are\\n= 7 nodes\\nUsing depth-first or breadth-first search, this means that the total number\\nof nodes to be examined is seven.\\nUsing DFID, nodes must be examined more than once, resulting in the fol-\\nlowing progression:\\n(d + 1) + b(d) + b2 (d /H110021) + b3(d /H110022) + . . . + bd\\n1 – 8/H50071 – 2\\n1 – bd+1\\n/H50071 – b'), Document(metadata={}, page_content='90 CHAPTER 4 Search Methodologies\\nHence, DFID has a time complexity of O(bd). It has the memory efficiency\\nof depth-first search because it only ever needs to store information about\\nthe current path. Hence, its space complexity is O(bd).\\nIn the case of the tree with depth of 2 and branching factor of 2, this means\\nexamining the following number of nodes:\\n(3 + 1) + 3 /H110032 + 4 /H110032 = 18\\nHence, for a small tree, DFID is far more inefficient in time than depth-first\\nor breadth-first search.'), Document(metadata={}, page_content='or breadth-first search.\\nHowever, if we compare the time needed for a larger tree with depth of 4\\nand branching factor of 10, the tree has the following number of nodes:\\n= 11,111 nodes\\nDFID will examine the following number of nodes:\\n(4 + 1) + 10 /H110034 + 100 /H110033 + 1,000 /H110032 + 10,000 = 12,345 nodes\\nHence, as the tree gets larger, we see that the majority of the nodes to be\\nexamined (in this case, 10,000 out of 12,345) are in the last row, which'), Document(metadata={}, page_content='needs to be examined only once in either case.\\nLike breadth-first search, DFID is optimal and complete. Because it also has\\ngood space efficiency, it is an extremely good search method to use where\\nthe search space may be very large and where the depth of the goal node is\\nnot known.\\n4.12 Using Heuristics for Search\\nDepth-first and breadth-first search were described as brute-force search\\nmethods. This is because they do not employ any special knowledge of the'), Document(metadata={}, page_content='search trees they are examining but simply examine every node in order\\nuntil they happen upon the goal. This can be likened to the human being\\nwho is traversing a maze by running a hand along the left side of the maze\\nwall.\\nIn some cases, this is the best that can be done because there is no addi-\\ntional information available that can be used to direct the search any better.\\nOften, however, such information does exist and can be used. Take the'), Document(metadata={}, page_content='example of looking for a suitable Christmas gift. V ery few people would\\n1 – 10\\n5\\n/H50071 – 10'), Document(metadata={}, page_content='4.12 Using Heuristics for Search 91\\nsimply walk into each shop as they came across it, looking in each\\ndepartment in turn until they happened upon a present. Most people\\nwould go straight to the shop that they considered to be most likely to\\nhave a suitable gift. If no gift was found in that shop, they would then\\nproceed to the shop they considered to be the next most likely to have a\\nsuitable gift.\\nThis kind of information is called a heuristic, and humans use them all the'), Document(metadata={}, page_content='time to solve all kinds of problems. Computers can also use heuristics, and\\nin many problems heuristics can reduce an otherwise impossible problem\\nto a relatively simple one.\\nA heuristic evaluation function is a function that when applied to a node\\ngives a value that represents a good estimate of the distance of the node\\nfrom the goal. For two nodes m and n, and a heuristic function f,i f f(m) <\\nf(n), then it should be the case that m is more likely to be on an optimal'), Document(metadata={}, page_content='path to the goal node than n. In other words, the lower the heuristic value\\nof a node, the more likely it is that it is on an optimal path to a goal and the\\nmore sensible it is for a search method to examine that node.\\nThe following sections provide details of a number of search methods that\\nuse heuristics and are thus thought of as heuristic search methods ,o r\\nheuristically informed search methods.\\nTypically, the heuristic used in search is one that provides an estimate of the'), Document(metadata={}, page_content='distance from any given node to a goal node. This estimate may or may not be\\naccurate, but it should at least provide better results than pure guesswork.\\n4.12.1 Informed and Uninformed Methods\\nA search method or heuristic is informed if it uses additional information\\nabout nodes that have not yet been explored to decide which nodes to\\nexamine next. If a method is not informed, it is uninformed,o r  blind.I n\\nother words, search methods that use heuristics are informed, and those'), Document(metadata={}, page_content='that do not are blind.\\nBest-first search is an example of informed search, whereas breadth-first\\nand depth-first search are uninformed or blind.\\nA heuristic h is said to be more informed than another heuristic, j,i f\\nh(node) ≤ j(node) for all nodes in the search space. (In fact, in order for h'), Document(metadata={}, page_content='92 CHAPTER 4 Search Methodologies\\n7\\n4\\n2\\n6\\n3\\n5\\n1\\n8\\n1\\n8\\n7\\n23\\n6\\n4\\n5\\nFigure 4.5\\nThe 8-puzzle, start state\\nand goal state\\nto be more informed than j, there must be some node where h(node) <\\nj(node). Otherwise they are as informed as each other.)\\nThe more informed a search method is, the more efficiently it will search.\\n4.12.2 Choosing a Good Heuristic\\nSome heuristics are better than others, and the better (more informed) the'), Document(metadata={}, page_content='heuristic is, the fewer nodes it needs to examine in the search tree to find a\\nsolution. Hence, like choosing the right representation, choosing the right\\nheuristic can make a significant difference in our ability to solve a problem.\\nIn choosing heuristics, we usually consider that a heuristic that reduces the\\nnumber of nodes that need to be examined in the search tree is a good\\nheuristic. It is also important to consider the efficiency of running the'), Document(metadata={}, page_content='heuristic itself. In other words, if it takes an hour to compute a heuristic\\nvalue for a given state, the fact that doing so saves a few minutes of total\\nsearch time is irrelevant. For most of this section, we will assume that\\nheuristic functions we choose are extremely simple to calculate and so do\\nnot impact on the overall efficiency of the search algorithm.\\n4.12.3 The 8-puzzle\\nT o illustrate the way in which heuristics are developed, we will use the 8-\\npuzzle, as illustrated in Figure 4.5.'), Document(metadata={}, page_content='puzzle, as illustrated in Figure 4.5.\\nThe puzzle consists of a 3 /H110033 grid, with the numbers 1 through 8 on tiles\\nwithin the grid and one blank square. Tiles can be slid about within the\\ngrid, but a tile can only be moved into the empty square if it is adjacent to\\nthe empty square. The start state of the puzzle is a random configuration,\\nand the goal state is as shown in the second picture in Figure 4.5, where the'), Document(metadata={}, page_content='4.12 Using Heuristics for Search 93\\nnumbers go from 1 to 8 clockwise around the empty middle square, with 1\\nin the top left.\\nTypically, it takes about 20 moves to get from a random start state to the\\ngoal state, so the search tree has a depth of around 20. The branching factor\\ndepends on where the blank square is. If it is in the middle of the grid, the\\nbranching factor is 4; if it is on an edge, the branching factor is 3, and if it is'), Document(metadata={}, page_content='in a corner, the branching factor is 2. Hence, the average branching factor\\nof the search tree is 3.\\nSo, an exhaustive search of the search tree would need to examine around\\n3\\n20 states, which is around 3.5 billion. Because there are only 9! or 362,880\\npossible states, the search tree could clearly be cut down significantly by\\navoiding repeated states.\\nIt is useful to find ways to reduce the search tree further, in order to devise'), Document(metadata={}, page_content='a way to solve the problem efficiently. A heuristic would help us to do this,\\nby telling us approximately how many moves a given state is from the goal\\nstate. We will examine a number of possible heuristics that could be used\\nwith the 8-puzzle.\\nT o be useful, our heuristic must never overestimate the cost of changing\\nfrom a given state to the goal state. Such a heuristic is defined as being\\nadmissible. As we will see, in many search methods it is essential that the'), Document(metadata={}, page_content='heuristics we use are admissible.\\nThe first heuristic we consider is to count how many tiles are in the wrong\\nplace. We will call this heuristic, h\\n1(node). In the case of the first state\\nshown in Figure 4.5, h1 (node) = 8 because all the tiles are in the wrong\\nplace. However, this is misleading because we could imagine a state with a\\nheuristic value of 8 but where each tile could be moved to its correct place\\nin one move. This heuristic is clearly admissible because if a tile is in the'), Document(metadata={}, page_content='wrong place, it must be moved at least once.\\nAn improved heuristic, h\\n2, takes into account how far each tile had to\\nmove to get to its correct state. This is achieved by summing the Manhat-\\ntan distances of each tile from its correct position. (Manhattan distance is\\nthe sum of the horizontal and vertical moves that need to be made to get\\nfrom one position to another, named after the grid system of roads used in\\nManhattan.)'), Document(metadata={}, page_content='94 CHAPTER 4 Search Methodologies\\nFor the first state in Figure 4.5, this heuristic would provide a value of\\nh2 (node) = 2 + 2 + 2 + 2 + 3 + 3 + 1 + 3 = 18\\nClearly, this is still an admissible heuristic because in order to solve the puz-\\nzle, each tile must be moved one square at a time from where it starts to\\nwhere it is in the goal state.\\nIt is worth noting that h\\n2 (node) ≥ h1 (node) for any node. This means that\\nh2 dominates h1, which means that a search method using heuristic h2 will'), Document(metadata={}, page_content='always perform more efficiently than the same search method using h1.\\nThis is because h2 is more informed than h1. Although a heuristic must\\nnever overestimate the cost, it is always better to choose the heuristic that\\ngives the highest possible underestimate of cost. The ideal heuristic would\\nthus be one that gave exactly accurate costs every time.\\nThis efficiency is best understood in terms of the effective branching fac-\\ntor, b*, of a search.'), Document(metadata={}, page_content='tor, b*, of a search.\\nIf a search method expands n nodes in solving a particular problem, and\\nthe goal node is at depth d, then b* is the branching factor of a uniform\\ntree that contains n nodes. Heuristics that give a lower effective branching\\nfactor perform better. A search method running with h\\n2 has a lower effec-\\ntive branching factor than the same search method running with h1 in solv-\\ning the 8-puzzle.\\nA third heuristic function, h3, takes into account the fact that there is extra'), Document(metadata={}, page_content='difficulty involved if two tiles have to move past each other because tiles\\ncannot jump over each other. This heuristic uses a function k(node), which\\nis equal to the number of direct swaps that need to be made between adja-\\ncent tiles to move them into the correct sequence.\\nh\\n3 (node) = h2 (node) + (2 /H11003k(node))\\nBecause k(node) must be at least 0, h3 (node) must be greater than h2\\n(node), meaning that h3 is a more informed heuristic than h2.'), Document(metadata={}, page_content='The heuristic functions h1, h2, and h3 are all admissible, meaning that using\\nthe A* algorithm (see Section 4.16.1) with any of these heuristics would\\nguarantee to find the quickest solution to the puzzle.\\nThere are a number of possible ways to generate useful heuristic functions.\\nFunctions like h1 and h2 can be generated by relaxing the 8-puzzle prob-'), Document(metadata={}, page_content='4.12 Using Heuristics for Search 95\\nlem. A relaxed problem is a version of a problem that has fewer con-\\nstraints. For example, a relaxed version of the 8-puzzle might be that a tile\\ncan be moved to an adjacent square regardless of whether that square is\\nempty or not. In that case, h\\n2 (node) would be exactly equal to the number\\nof moves needed to get from a node to the goal node.\\nIf the problem were relaxed further, we might say that a tile could move to'), Document(metadata={}, page_content='any square, even if that square is not adjacent to the square it is starting\\nfrom. In this case, h\\n1 (node) exactly equals the number of moves needed to\\nget from a node to the goal node.\\nHence, using an exact cost function for a relaxed version of a problem is\\noften a good way to generate a heuristic cost function for the main problem.\\nIt is clear that h3 is the best heuristic function to use of the three we gen-\\nerated because it dominates both h1 and h2. In some cases, a number of'), Document(metadata={}, page_content='heuristic functions may exist, none of which dominates the others. In\\nthat case, a new heuristic can be generated from the heuristics h\\n1... hn,a s\\nfollows:\\nh(node) = max (h1 [node], h2 [ n o d e ] ,...,hn [node])\\nBecause all of h1 to hn is admissible, h(node) must also be admissible. The\\nheuristic function h dominates all of the heuristics h1 ... hn and so is clearly\\nthe best one to use.\\nAs we see in Chapter 6, another way to find a heuristic is to take advantage'), Document(metadata={}, page_content='of features of the problem that is being modeled by the search tree. For\\nexample, in the case of playing checkers, computers are able to use heuris-\\ntics such as the fact that a player with more kings on the board is likely to\\nwin against a player with fewer kings.\\n4.12.4 Monotonicity\\nA search method is described as monotone if it always reaches a given node\\nby the shortest possible path.\\nSo, a search method that reaches a given node at different depths in the'), Document(metadata={}, page_content='search tree is not monotone. A monotone search method must be admissi-\\nble, provided there is only one goal state.\\nA monotonic heuristic is a heuristic that has this property.'), Document(metadata={}, page_content='96 CHAPTER 4 Search Methodologies\\n10\\n3 3\\n6\\n43\\n2\\n12\\nA\\nB\\nC\\nE\\nF\\nD\\nFigure 4.6\\nMap of five cities\\nAn admissible heuristic is a heuristic that never overestimates the true dis-\\ntance of a node from the goal. A monotonic heuristic is also admissible,\\nassuming there is only one goal state.\\n4.12.5 Example: The Modified Traveling Salesman Problem\\nIt is usual when examining heuristic search methods to relate the search\\nproblem to a real-world situation in order to derive suitable heuristics. For'), Document(metadata={}, page_content='this explanation, we will use the example of finding the best route between\\ntwo cities, a variation of the Traveling Salesman problem, as shown in Fig-\\nure 4.6.\\nIn this diagram, each node represents a town, and the vertices between\\nnodes represent roads that join towns together. A is the starting node, and F\\nis the goal node. Each vertex is labeled with a distance, which shows how\\nlong that road is. Clearly the diagram is not drawn to scale.'), Document(metadata={}, page_content='The aim of this problem is to find the shortest possible path from city A to\\ncity F. This is different from the traditional Traveling Salesman problem, in\\nwhich the problem is to find a way to travel around a group of cities and\\nfinally arrive back at the starting city.\\nWe can represent the search space of the map in Figure 4.6 as a search tree\\nby showing each possible path as a leaf node in the tree. In doing so, we\\nneed to be careful to remove repetitions of paths, or loops, because those'), Document(metadata={}, page_content='4.12 Using Heuristics for Search 97\\nE B\\nFF\\nFF\\nFF\\nDCB F E\\nEE DD\\nA\\nB C\\nD C\\nFigure 4.7\\nSearch tree for map in Figure 4.6\\nwould add redundancy to the graph and make searching it inefficient. The\\ntree for this search space is shown in Figure 4.7.\\nY ou will notice that this tree has nine leaf nodes, seven of which are goal\\nnodes. Two of the paths lead to cyclical paths and so are abandoned. There\\nare seven distinct paths that successfully lead from A to F. These seven paths'), Document(metadata={}, page_content='can be traced from the tree as follows:\\n1 A,B,D,E,F\\n2 A,B,D,F\\n3 A,B,C,E,D,F\\n4 A,B,C,E,F\\n5 A,C,E,F\\n6 A,C,B,D,E,F\\n7 A,C,B,D,F'), Document(metadata={}, page_content='98 CHAPTER 4 Search Methodologies\\nThe two cyclical paths are as follows:\\n1 A,B,D,E,C (which would then lead on to A or B)\\n2 A,C,E,D,B (which would then lead on to A or C)\\nA depth-first approach to this problem would provide path number 1,\\nA,B,D,E,F, which has a total distance of 29.\\nBreadth-first search always produces the path that has the least steps, but\\nnot necessarily the shortest path. In this case, it would yield path 2, which is'), Document(metadata={}, page_content='A,B,D,F and which has a length of 19. This is much shorter than the path\\nproduced by depth-first search but is not the shortest path (the shortest\\npath is path 5, A,C,E,F, which has a length of 17).\\nNow we introduce two new search methods that use heuristics to more effi-\\nciently identify search solutions.\\n4.13 Hill Climbing\\nHill climbing is an example of an informed search method because it uses\\ninformation about the search space to search in a reasonably efficient man-'), Document(metadata={}, page_content='ner. If you try to climb a mountain in fog with an altimeter but no map,\\nyou might use the hill climbing Generate and T est approach:\\nCheck the height 1 foot away from your current location in each direction:\\nnorth, south, east, and west.\\nAs soon as you find a position where the height is higher than your current\\nposition, move to that location and restart the algorithm.\\nIf all directions lead lower than your current position, then you stop and'), Document(metadata={}, page_content='assume you have reached the summit. As we see later, this might not neces-\\nsarily always be true.\\nIn examining a search tree, hill climbing will move to the first successor\\nnode that is “better” than the current node—in other words, the first\\nnode that it comes across with a heuristic value lower than that of the\\ncurrent node.\\n4.13.1 Steepest Ascent Hill Climbing\\nSteepest ascent hill climbing is similar to hill climbing, except that rather'), Document(metadata={}, page_content='than moving to the first position you find that is higher than the current'), Document(metadata={}, page_content='4.13 Hill Climbing 99\\n8\\nA\\nB\\nC\\nE\\nF\\nD\\n25\\n12\\n620\\nFigure 4.8\\nThe map of five cities\\nwhere the straight-line\\ndistance from each city to\\nthe goal city (F) is shown\\nposition, you always check around you in all four directions and choose the\\nposition that is highest.\\nSteepest ascent hill climbing can also be thought of as a variation on depth-\\nfirst search, which uses information about how far each node is from the\\ngoal node to choose which path to follow next at any given point.'), Document(metadata={}, page_content='For this method, we apply a heuristic to the search tree shown in Figure 4.7,\\nwhich is the straight-line distance from each town to the goal town. We are\\nusing this heuristic to approximate the actual distance from each town to\\nthe goal, which will of course be longer than the straight-line distance.\\nIn Figure 4.8, we can see the same search problem as presented in Figure\\n4.6, but instead of noting the lengths of vertices, we note how far each city'), Document(metadata={}, page_content='is (using a straight-line measurement) from the goal, city F.\\nNow hill climbing proceeds as with depth-first search, but at each step, the\\nnew nodes to be added to the queue are sorted into order of distance from\\nthe goal. Note that the only difference between this implementation and\\nthat given for depth-first search is that in hill climbing the successors of\\nstate are sorted according to their distance from the goal before being\\nadded to the queue:\\nFunction hill ()\\n{'), Document(metadata={}, page_content='added to the queue:\\nFunction hill ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)'), Document(metadata={}, page_content='100 CHAPTER 4 Search Methodologies\\nTable 4.4 Analysis of hill climbing\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nA\\nA\\nB\\nB\\nD\\nD\\nF\\n(empty)\\nB,C\\nC\\nD,C,C\\nC,C\\nF ,E,C,C\\nE,C,C\\nThe queue starts out empty, and the initial\\nstate is the root node, which is A.\\nThe successors of A are sorted and placed on\\nthe queue. B is placed before C on the queue\\nbecause it is closer to the goal state, F .\\nF is placed first on the queue because it is clos-\\nest to the goal. In fact, it is the goal, as will be'), Document(metadata={}, page_content='discovered in the next step.\\nSUCCESS: Path is reported as A,B,D,F .\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse\\n{\\nsort (successors (state));\\nadd_to_front_of_queue (successors (state));\\n}\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\nThis algorithm thus searches the tree in a depth-first manner, at each step\\nchoosing paths that appear to be most likely to lead to the goal.'), Document(metadata={}, page_content='The steps taken by a hill-climbing algorithm in solving the preceding prob-\\nlem are shown in Table 4.4:\\nIn this case, hill climbing has produced the same path as breadth-first\\nsearch, which is the path with the least steps, but not the shortest path. In'), Document(metadata={}, page_content='4.13 Hill Climbing 101\\nmany cases though, using this heuristic enables hill climbing to identify\\nshorter paths than would be identified by depth-first or breadth-first\\nsearch. Hill climbing uses heuristics to identify paths efficiently but does\\nnot necessarily identify the best path.\\nIf we ran the searches from right to left, instead of from left to right (or\\nordered the search tree the other way around), then we would find that'), Document(metadata={}, page_content='breadth-first search would produce a different path: A,C,E,F (which is in\\nfact the shortest path), but hill climbing would still produce the same\\npath, A,B,D,F. In other words, the particular ordering of nodes used\\naffects which result is produced by breadth-first and depth-first search\\nbut does not affect hill climbing in the same way. This can clearly be a\\nuseful property.\\n4.13.2 Foothills, Plateaus, and Ridges\\nAlthough we have been talking about using search techniques to tra-'), Document(metadata={}, page_content='verse search trees, they can also be used to solve search problems that\\nare represented in different ways. In particular, we often represent a\\nsearch problem as a three-dimensional space, where the x- and y-axes\\nare used to represent variables and the z-axis (or height) is used to rep-\\nresent the outcome.\\nThe goal is usually to maximize the outcome, and so search methods in\\nthese cases are aiming to find the highest point in the space.'), Document(metadata={}, page_content='Many such search spaces can be successfully traversed using hill climbing\\nand other heuristically informed search methods. Some search spaces,\\nhowever, will present particular difficulties for these techniques.\\nIn particular, hill climbing can be fooled by foothills, plateaus, and ridges.\\nFigure 4.9 has three illustrations, showing foothills, a plateau, and a ridge.\\nThis figure shows the search space represented as a three-dimensional ter-'), Document(metadata={}, page_content='rain. In this kind of terrain, the aim of search is to find the x and y values\\nthat give the highest possible value of z—in other words, the highest point\\nin the terrain. This is another way of looking at traditional search: search is\\nnormally aiming to maximize some function, which in this case is shown as\\nthe height of the terrain, but is traditionally a function that details the dis-\\ntance of a node from the goal node.\\nFoothills are often called local maxima by mathematicians. A local maxi-'), Document(metadata={}, page_content='mum is a part of the search space that appears to be preferable to the parts\\naround it, but which is in fact just a foothill of a larger hill. Hill-climbing'), Document(metadata={}, page_content='102 CHAPTER 4 Search Methodologies\\nz y\\nGLOBAL MAXIMUM\\nLOCAL MAXIMUM\\nMAXIMUM\\nPLATEAU\\nx\\nz y\\nx\\nz y\\nx\\nA\\nC\\nB\\n(a)\\n(b)\\n(c)\\nFigure 4.9\\n(a)  FOOTHILLS\\n(b)  PLATEAU\\n(c)  RIDGE\\ntechniques will reach this peak, where a more sophisticated technique\\nmight move on from there to look for the global maximum. Figure 4.9 (a)\\nshows a search space that has a single global maximum surrounded by a\\nnumber of foothills, or local maxima. Many search methods would reach'), Document(metadata={}, page_content='the top of one of these foothills and, because there was nowhere higher\\nnearby, would conclude that this was the best solution to the problem.'), Document(metadata={}, page_content='4.13 Hill Climbing 103\\nLater in this chapter and in Chapter 5, we see methods such as simulated\\nannealing that are good at avoiding being trapped by local maxima.\\nA plateau is a region in a search space where all the values are the same. In\\nthis case, although there may well be a suitable maximum value somewhere\\nnearby, there is no indication from the local terrain of which direction to\\ngo to find it. Hill climbing does not perform well in this situation. Figure'), Document(metadata={}, page_content='4.9 (b) shows a search space that consists of just one peak surrounded by a\\nplateau. A hill-climbing search method could well find itself stuck in the\\nplateau with no clear indication of where to go to find a good solution.\\nThe final problem for hill climbing is presented by ridges. A ridge is a long,\\nthin region of high land with low land on either side. When looking in one\\nof the four directions, north, south, east, and west from the ridge, a hill-'), Document(metadata={}, page_content='climbing algorithm would determine that any point on the top of the ridge\\nwas a maximum because the hill falls away in those four directions. The\\ncorrect direction is a very narrow one that leads up the top of the ridge, but\\nidentifying this direction using hill climbing could be very tricky.\\nFigure 4.9 (c) shows a ridge. The point marked A is lower than the point\\nmarked B, which is the global maximum. When a hill-climbing method'), Document(metadata={}, page_content='finds itself at point C, it might find it hard to get from there to B. The arrows\\non point C show that in moving north, south, east, or west, the method\\nwould find itself at a lower point. The correct direction is up the ridge.\\n4.14 Best-First Search\\nBest-first search employs a heuristic in a similar manner to hill climbing.\\nThe difference is that with best-first search, the entire queue is sorted after\\nnew paths have been added to it, rather than adding a set of sorted paths.'), Document(metadata={}, page_content='In practical terms, this means that best-first search follows the best path\\navailable from the current (partially developed) tree, rather than always fol-\\nlowing a depth-first style approach.\\nFunction best ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse'), Document(metadata={}, page_content='104 CHAPTER 4 Search Methodologies\\nTable 4.5 Analysis of best-first search of tree shown in Figure 4.4\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nA\\nA\\nA\\nB\\nB\\nB\\nD\\nD\\nD\\nF\\n(empty)\\nB,C\\nB,C\\nC\\nD,C,C\\nD,C,C\\nC,C\\nE,F ,C,C\\nF ,E,C,C\\nE,C,C\\nThe queue starts out empty, and the initial\\nstate is the root node, which is A.\\nThe successors of the current state, B and C, are\\nplaced in the queue.\\nThe queue is sorted, leaving B in front of C\\nbecause it is closer to the goal state, F .'), Document(metadata={}, page_content='because it is closer to the goal state, F .\\nThe children of node B are added to the front of\\nthe queue.\\nThe queue is sorted, leaving D at the front\\nbecause it is closer to the goal node than C.\\nNote that although the queue appears to con-\\ntain the same node twice, this is just an artifact\\nof the way the search tree was constructed. In\\nfact, those two nodes are distinct and represent\\ndifferent paths on our search tree.\\nThe children of D are added to the front of the\\nqueue.'), Document(metadata={}, page_content='queue.\\nThe queue is sorted, moving F to the front.\\nSUCCESS: Path is reported as A,B,D,F .\\n{\\nadd_to_front_of_queue (successors (state));\\nsort (queue);\\n}\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nThe path taken through the search tree shown in Figure 4.7 is shown in\\nTable 4.5.'), Document(metadata={}, page_content='4.14 Best-First Search 105\\nIt can be seen that, in this case, best-first search happens to produce the\\nsame path as hill climbing and breadth-first search, although the queue is\\nordered differently during the process. As with hill climbing, best-first\\nsearch will tend to provide a shorter path than depth first or breadth first,\\nbut not necessarily the shortest path.\\n4.15 Beam Search\\nBeam search is a form of breadth-first search that employs a heuristic, as'), Document(metadata={}, page_content='seen with hill climbing and best-first search. Beam search works using a\\nthreshold so that only the best few paths are followed downward at each\\nlevel. This method is very efficient in memory usage and would be particu-\\nlarly useful for exploring a search space that had a very high branching fac-\\ntor (such as in game trees for games, such as Go or Chess). It has the\\ndisadvantage of not exhaustively searching the entire tree and so may fail to\\never find a goal node.'), Document(metadata={}, page_content='ever find a goal node.\\nIn this implementation, the function call \\nselect_best_paths (queue, n)\\nremoves all but the best n paths from the queue.\\nFunction beam ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse\\n{\\nadd_to_back_of_queue (successors (state));\\nselect_best_paths (queue, n);\\n}\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue'), Document(metadata={}, page_content='state = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nIn this pseudocode, n is used to represent the width threshold, which is set\\nat the beginning of the procedure.'), Document(metadata={}, page_content='106 CHAPTER 4 Search Methodologies\\nTable 4.6 Analysis of beam search of tree shown in Figure 4.7\\nStep State Queue Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\nA\\nA\\nB\\nB\\nB\\nD\\nD\\nD\\nE\\nE\\nE\\nF\\n(empty)\\nB,C\\nC\\nC,D,C\\nD,C\\nC\\nC,E,F\\nE,F\\nF\\nF ,C,F\\nF, F\\nF\\nThe queue starts out empty, and the initial state\\nis the root node, which is A.\\nThe two children of the current node are added to\\nthe back of the queue.\\nThe two children of B are added to the back of\\nthe queue.\\nAll but the two best paths are discarded from the\\nqueue.'), Document(metadata={}, page_content='queue.\\nThe two children of the current node are added to\\nthe back of the queue.\\nAt this step, C is removed from the queue because\\nwe only require the two best paths.\\nThe two children of E are added to the back of the\\nqueue.\\nThe path that leads to C is discarded, in favor of\\nthe two better paths, both of which lead to F .\\nSUCCESS: Path is reported as A,B,D,E,F .\\nThe interesting aspect of this method is the choice of how to define the'), Document(metadata={}, page_content='“best” paths to include in the queue. Often, the path that involves the fewest\\nsteps is used or the path that has reached the point with the highest heuris-\\ntic value (in other words, the path that got closest to the goal).\\nIn Table 4.6, the value of state and queue are shown for the problem tree\\nshown in Figure 4.7, using beam search with a threshold of 2 (in other\\nwords, only two paths are extended down from each level). For this imple-'), Document(metadata={}, page_content='mentation, we have used the heuristic value of each node to determine'), Document(metadata={}, page_content='4.16 Identifying Optimal Paths 107\\nwhich path is the “best” path. So the “best” path will be the one that has\\nreached the closest to a goal node so far.\\n4.16 Identifying Optimal Paths\\nSeveral methods exist that do identify the optimal path through a search\\ntree. The optimal path is the one that has the lowest cost or involves travel-\\ning the shortest distance from start to goal node. The techniques described\\npreviously may find the optimal path by accident, but none of them are'), Document(metadata={}, page_content='guaranteed to find it.\\nThe simplest method for identifying the optimal path is called the British\\nMuseum procedure . This process involves examining every single path\\nthrough the search tree and returning via the best path that was found.\\nBecause every path is examined, the optimal path must be found. This\\nprocess is implemented as an extension of one of the exhaustive search\\ntechniques, such as depth-first or breadth-first search, but rather than stop-'), Document(metadata={}, page_content='ping when a solution is found, the solution is stored and the process con-\\ntinues until all paths have been explored. If an alternative solution is found,\\nits path is compared with the stored path, and if it has a lower cost, it\\nreplaces the stored path.\\nThe following more sophisticated techniques for identifying optimal paths\\nare outlined in this section:\\n■ A*\\n■ uniform cost search (Branch and Bound)\\n■ greedy search\\nThe British Museum procedure also has the property that it generates all'), Document(metadata={}, page_content='solutions. Most of the search methods we look at in this book stop when\\nthey find a solution. In some cases, this will be the best solution, and in\\nother cases it may even be the worst available solution (depth-first search\\nwill do this if the worst solution happens to be the left-most solution).\\nIn some cases, it may be necessary to identify all possible solutions, in\\nwhich case something like the British Museum procedure would be useful.'), Document(metadata={}, page_content='Assuming that none of the branches of the tree is infinitely deep, and that\\nno level has an infinite branching factor, then it does not matter which\\napproach is used (depth first or breadth first, for example) when running'), Document(metadata={}, page_content='108 CHAPTER 4 Search Methodologies\\nthe British Museum procedure: because the goal is to visit every node, the\\norder the nodes are visited probably does not matter.\\n4.16.1 A* Algorithms\\nA* algorithms are similar to best-first search but use a somewhat more\\ncomplex heuristic to select a path through the tree. The best-first algorithm\\nalways extends paths that involve moving to the node that appears to be\\nclosest to the goal, but it does not take into account the cost of the path to'), Document(metadata={}, page_content='that node so far.\\nThe A* algorithm operates in the same manner as best-first search but uses\\nthe following function to evaluate nodes:\\nf(node) = g(node) + h(node)\\ng(node) is the cost of the path so far leading up to the node, and h(node) is\\nan underestimate of the distance of the node from a goal state; f is called a\\npath-based evaluation function. When operating A*, f(node) is evaluated\\nfor successor nodes and paths extended using the nodes that have the low-\\nest values of f.'), Document(metadata={}, page_content='est values of f.\\nIf h(node) is always an underestimate of the distance of a node to a goal\\nnode, then the A* algorithm is optimal: it is guaranteed to find the shortest\\npath to a goal state. A* is described as being optimally efficient, in that in\\nfinding the path to the goal node, it will expand the fewest possible paths.\\nAgain, this property depends on h(node) always being an underestimate.\\nNote that running the A* algorithm on the search tree shown in Figure 4.4'), Document(metadata={}, page_content='would not be guaranteed to find the shortest solution because the esti-\\nmated values for h(node) are not all underestimates. In other words, the\\nheuristic that is being used is not admissible. If a nonadmissible heuristic\\nfor h(node) is used, then the algorithm is called A.\\nA* is the name given to the algorithm where the h(node) function is admis-\\nsible. In other words, it is guaranteed to provide an underestimate of the\\ntrue cost to the goal.'), Document(metadata={}, page_content='true cost to the goal.\\nA* is optimal and complete. In other words, it is guaranteed to find a solu-\\ntion, and that solution is guaranteed to be the best solution.\\nA* is in fact only complete if the tree it is searching has a finite branching\\nfactor and does not contain a path of finite cost, which has an infinite num-\\nber of nodes along it. Both of these conditions are likely to be met in all'), Document(metadata={}, page_content='4.16 Identifying Optimal Paths 109\\nreal-world situations, and so for simplicity we can state that A* is complete;\\nalthough, to be more accurate:\\nA* is complete if the graph it is searching is locally finite (that is, it has a\\nfinite branching factor) and if every arc between two nodes in the graph\\nhas a non-zero cost.\\nThat A* is optimal can be proved by considering a counter-example:\\nImagine we are applying the A* algorithm to a graph with two goals, G1'), Document(metadata={}, page_content='and G2. The path cost of G1 is f1 and the path cost of G2 is f2, where f2 >\\nf1. G1 is the goal with the lower cost, but let us imagine a scenario where\\nthe A* algorithm has reached G2 without having explored G1. In other\\nwords, we are imagining a scenario where the algorithm has not chosen the\\ngoal with the lesser cost.\\nIf we consider a node, n, that is on an optimal path from the root node to\\nG1, then because h is an admissible heuristic:\\nf1 ≥ f (n)'), Document(metadata={}, page_content='f1 ≥ f (n)\\nThe only reason the algorithm would not choose to expand n before it\\nreaches G2 would be if\\nf (n) > f (G2)\\nHence, by combining these two expressions together, we arrive at\\nf1 ≥ f(G2)\\nBecause G2 is a goal state, it must be the case that h(G2) = 0, and thus f(G2)\\n= g(G2). Thus we have\\nf1 ≥ g(G2)\\nThis, therefore, contradicts our original assumption that G2 had a higher\\npath cost than G1, which proves that A* can only ever choose the least cost\\npath to a goal.'), Document(metadata={}, page_content='path to a goal.\\nIt was mentioned that A* is similar to breadth-first search. In fact, breadth-\\nfirst search can be considered to be a special case of A*, where h(node) is\\nalways 0, so f(node) = g(node), and where every direct path between a node\\nand its immediate successor has a cost of 1.\\n4.16.2 Uniform Cost Search\\nUniform cost search (or Branch and Bound ) is a variation on best-first\\nsearch that uses the evaluation function g(node), which for a given node'), Document(metadata={}, page_content='110 CHAPTER 4 Search Methodologies\\nevaluates to the cost of the path leading to that node. In other words, this is\\nan A* algorithm but where h(node) is set to zero. At each stage, the path\\nthat has the lowest cost so far is extended. In this way, the path that is gen-\\nerated is likely to be the path with the lowest overall cost, but this is not\\nguaranteed. T o find the best path, the algorithm needs to continue running'), Document(metadata={}, page_content='after a solution is found, and if a preferable solution is found, it should be\\naccepted in place of the earlier solution.\\nUniform cost search is complete and is optimal, providing the cost of a\\npath increases monotonically. In other words, if for every node m that has a\\nsuccessor n, it is true that g(m) < g(n), then uniform cost is optimal. If it is\\npossible for the cost of a node to be less than the cost of its parent, then\\nuniform cost search may not find the best path.'), Document(metadata={}, page_content='uniform cost search may not find the best path.\\nUniform cost search was invented by Dijkstra in 1959 and is also known as\\nDijkstra’s algorithm.\\n4.16.3 Greedy Search\\nGreedy search is a variation of the A* algorithm, where g(node) is set to\\nzero, so that only h(node) is used to evaluate suitable paths. In this way, the\\nalgorithm always selects the path that has the lowest heuristic value or esti-\\nmated distance (or cost) to the goal.\\nGreedy search is an example of a best-first strategy.'), Document(metadata={}, page_content='Greedy-search methods tend to be reasonably efficient, although in the worst\\ncase, like depth-first search, it may never find a solution at all. Additionally,\\ngreedy search is not optimal and can be fooled into following extremely costly\\npaths. This can happen if the first step on the shortest path toward the goal is\\nlonger than the first step along another path, as is shown in Figure 4.10.\\n4.16.4 Example: The Knapsack Problem'), Document(metadata={}, page_content='4.16.4 Example: The Knapsack Problem\\nThe knapsack problem is an interesting illustration of the use of greedy-\\nsearch algorithms and their pitfalls. The fractional knapsack problem can\\nbe expressed as follows:\\nA man is packing items into his knapsack. He wants to take the most valu-\\nable items he can, but there is a limit on how much weight he can fit in his\\nknapsack. Each item has a weight w\\ni and is worth vi. He can only fit a total'), Document(metadata={}, page_content='i and is worth vi. He can only fit a total\\nweight of W in his knapsack. The items that he wants to take are things that\\ncan be broken up and still retain their value (like flour or milk), and he is'), Document(metadata={}, page_content='4.16 Identifying Optimal Paths 111\\n11 0\\n1000 10\\nFigure 4.10\\nA search tree where a\\ngreedy-search method will\\nnot find the best solution\\nable to take fractions of items. Hence, the problem is called the fractional\\nknapsack problem.\\nIn solving this problem, a greedy-search algorithm provides the best solution.\\nThe problem is solved by calculating the value per unit weight of each item:\\nv\\ni/wi, and then taking as much as he can carry of the item with the greatest'), Document(metadata={}, page_content='value per unit weight. If he still has room, he moves on to the item with the\\nnext highest value per unit weight, and so on.\\nThe 0-1 knapsack problem is the same as the fractional knapsack problem,\\nexcept that he cannot take parts of items. Each item is thus something like\\na television set or a laptop computer, which must be taken whole. In solving\\nthis problem, a greedy-search approach does not work, as can be seen from\\nthe following example:'), Document(metadata={}, page_content='the following example:\\nOur man has a knapsack that lets him carry a total of 100 pounds. His\\nitems are:\\n1 gold brick worth $1800 and weighing 50 pounds\\n1 platinum brick worth $1500 and weighing 30 pounds\\n1 laptop computer worth $2000 and weighing 50 pounds\\nHence, we have four items, whose values ofv and w are as follows:\\nv\\n1 = 1800 w1 = 50 v1/w1 = 36\\nv2 = 1500 w2 = 30 v2/w2 = 50\\nv3 = 2000 w3 = 50 v3/w3 = 40'), Document(metadata={}, page_content='112 CHAPTER 4 Search Methodologies\\nIn this case, a greedy-search strategy would pick item 2 first, and then\\nwould take item 3, giving a total weight of 80 pounds, and a total value of\\n$3500. In fact, the best solution is to take items 1 and 3 and to leave item 2\\nbehind giving a total weight of 100 pounds and a total value of $3800.\\n4.17 Chapter Summary\\n■ Generate and T est is an extremely simple example of a brute-force\\nor exhaustive search technique.'), Document(metadata={}, page_content='or exhaustive search technique.\\n■ Depth-first search and breadth-first search are extremely com-\\nmonly used and well understood exhaustive search methods.\\n■ In analyzing search methods, it is important to examine the com-\\nplexity (in time and space) of the method.\\n■ A search method is complete if it will always find a solution if one\\nexists. A search method is optimal (or admissible) if it always finds\\nthe best solution that exists.'), Document(metadata={}, page_content='the best solution that exists.\\n■ Depth-First Iterative Deepening (DFID) is a search method that\\nhas the low memory requirements of depth-first search and is opti-\\nmal and complete, like breadth-first search.\\n■ Heuristics can be used to make search methods more informed\\nabout the problem they are solving. A heuristic is a method that\\nprovides a better guess about the correct choice to make at any\\njunction that would be achieved by random guessing.'), Document(metadata={}, page_content='■ One heuristic is more informed than another heuristic if a search\\nmethod that uses it needs to examine fewer nodes to reach a goal.\\n■ Relaxing problems is one way to identify potentially useful heuristics.\\n■ Hill climbing is a heuristic search method that involves continually\\nmoving from one potential solution to a better potential solution\\nuntil no better solution can be found.\\n■ Hill climbing has problems in search spaces that have foothills,\\nplateaus, and ridges.'), Document(metadata={}, page_content='plateaus, and ridges.\\n■ A* is a heuristic search method that in most situations is optimal\\nand complete. It uses the path evaluation function to choose suit-\\nable paths through the search space.'), Document(metadata={}, page_content='Review Questions 113\\n■ Uniform cost search is similar to A* but uses a simpler evaluation\\nfunction, which is based just on the cost of reaching the node so far.\\n■ Greedy search involves always moving to the most immediately\\nattractive position on the next step. It can be used to solve the frac-\\ntional knapsack problem, but not the 1-0 knapsack problem.\\n4.18 Review Questions\\n4.1 Explain the idea behind Generate and T est. Why is this method\\ndescribed as being exhaustive ?'), Document(metadata={}, page_content='described as being exhaustive ?\\n4.2 Explain the differences and similarities between depth-first search\\nand breadth-first search. Give examples of the kinds of problems\\nwhere each would be appropriate.\\n4.3 Explain what is meant by the following terms in relation to search\\nmethods:\\n■ complexity\\n■ completeness\\n■ optimality\\n4.4 What is the complexity (in space and in time) of the following\\nsearch methods:\\n■ depth-first search\\n■ breadth-first search\\n■ best-first search\\n■ greedy search'), Document(metadata={}, page_content='■ best-first search\\n■ greedy search\\n4.5 What does it mean to say that a search method is monotonic? How\\ndesirable is this property? Which of the search methods described\\nin this chapter is monotonic?\\n4.6 Explain why Depth-First Search Iterative Deepening is reasonably\\nefficient. Why might it be preferable to use DFID rather than\\ndepth-first search?\\n4.7 Provide a definition of the word “heuristic. ” In what ways can\\nheuristics be useful in search? Name three ways in which you use'), Document(metadata={}, page_content='heuristics in your everyday life.'), Document(metadata={}, page_content='114 CHAPTER 4 Search Methodologies\\n4.8 Explain the components of the path evaluation function f(node)\\nused by A*. Do you think it is the best evaluation function that\\ncould be used? T o what kinds of problems might it be best suited?\\nAnd to what kinds of problems would it be worst suited?\\n4.9 Show that A* is optimal and complete in most circumstances.\\n4.10 Explain why a greedy method provides suboptimal solutions to the\\n0-1 knapsack problem but provides optimal solutions to the frac-'), Document(metadata={}, page_content='tional knapsack problem. Could there be a search tree for which\\ngreedy search found optimal solutions?\\n4.11 What effect does the ordering of a search tree have on the efficiency\\nof search? What effect does it have on the quality of the results?\\nHow would ordering affect the way that depth-first search or\\ngreedy search would perform when searching the search tree\\nshown in Figure 4.10?\\n4.19 Exercises\\n4.12 Implement a data structure that represents search trees in a pro-'), Document(metadata={}, page_content='gramming language of your choice. Have the program display the\\ntree on the screen, and provide functions that can select nodes and\\ndisplay paths.\\n4.13 Implement depth-first search in your program. Implement\\nbreadth-first search. Build a search tree of depth 10 and with a\\nbranching factor of 2. Which of your search methods finds a goal\\nthe most quickly? Can you change the tree so that the other\\nmethod finds the goal more quickly?'), Document(metadata={}, page_content='method finds the goal more quickly?\\n4.14 Add the concept of path cost to your implementation. Implement A*.\\nDoes it perform much better than depth-first or breadth-first search?\\nHow well does it do with the large tree you built in Exercise 4.8?\\n4.15 Implement a greedy-search algorithm. How well does it perform\\ncompared with the other methods you have implemented? Invent a\\n0-1 knapsack problem, and use your search tree implementation to'), Document(metadata={}, page_content='model this problem. Can you model the fractional knapsack prob-\\nlem using a search tree?\\n4.16 Investigate the file search facility on your computer. Which type of\\nsearch method do you think it uses? Why do you think this partic-'), Document(metadata={}, page_content='Further Reading 115\\nular search method was chosen? What problems could this\\napproach cause it? How well does it work when it is searching\\ndirectories with large numbers of files in them?\\n4.20 Further Reading\\nSearch is covered well by almost all artificial intelligence text books,\\nalthough the approaches taken vary.\\nA detailed description and analysis of Dijkstra’s algorithm (uniform cost\\nsearch) can be found in Cormen et al. (2001). Books such as this that cover'), Document(metadata={}, page_content='algorithms in more detail provide an interesting non-Artificial Intelligence\\nperspective on the subject.\\nMarvin Minsky’s 1961 paper,Steps Toward Artificial Intelligence, introduced\\nthe idea of hill climbing and discussed some of the difficulties faced by hill-\\nclimbing methods.\\nAllen Newell and Herbert A. Simon’s 1976 paper, Computer Science as\\nEmpirical Inquiry, contains an excellent discussion of heuristic search for\\nproblem solving.'), Document(metadata={}, page_content='problem solving.\\nA good description of the way that Prolog uses depth-first search for unifi-\\ncation is contained in Russell and Norvig (1995).\\nIntroduction to Algorithms , by Thomas H. Cormen, Charles E. Leiserson,\\nRonald L. Rivest, and Clifford Stein (2001 – MIT Press)\\nArtificial Intelligence: Strategies, Applications, and Models Through Search,b y\\nBenedict Du Boulay and Christopher James Thornton (1999 – AMACOM)\\nAlgorithmics: The Spirit of Computing, by David Harel (1987 – Addison Wesley)'), Document(metadata={}, page_content='Art of Computer Programming: Sorting and Searching , by Donald Knuth\\n(1973 – Pearson Addison Wesley)\\nSteps Towards Artificial Intelligence, by Marvin Minsky (1961 – in Computa-\\ntion & Intelligence—Collected Readings, edited by George F. Luger, MIT Press)\\nComputer Science as Empirical Enquiry: Symbols and Search , by Allen\\nNewell and Herbert A. Simon (1976 – in Computation & Intelligence—Col-\\nlected Readings, edited by George F. Luger, MIT Press)'), Document(metadata={}, page_content='Algorithms, by Robert Sedgewick (1988 – Addison Wesley)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='5CHAPTER\\nAdvanced Search\\nThe difficult we do immediately. The impossible takes a little longer.\\n—US Armed Forces slogan\\nHad I been present at the Creation, I would have given some useful hints for\\nthe better ordering of the universe.\\n—Alfonso ‘the wise’ , on studying the Ptolemaic system (13th centuryA.D.)\\nIf we value the pursuit of knowledge, we must be free to follow wherever that\\nsearch may lead us. The free mind is not a barking dog, to be tethered on a ten-\\nfoot chain.'), Document(metadata={}, page_content='foot chain.\\n—Adlai E. Stevenson Jr., speech at the University of Wisconsin, Madison,\\nOctober 8, 1952\\n5.1 Introduction\\nIn Chapter 4, we examined a range of methods that can be used to search a\\nproblem space. In Chapter 5, we introduce some more sophisticated methods.\\nFirst, we examine constraint satisfaction problems, such as the eight-\\nqueens problem, and search methods and heuristics that can be used to\\nsolve them.\\nWe also discuss local search methods , such as simulated annealing, that'), Document(metadata={}, page_content='attempt to find a solution to large combinatorial problems by moving\\nfrom one possible solution to another that is a little better. We also intro-\\nduce the idea of parallel search—using multiple processors (or multiple'), Document(metadata={}, page_content='118 CHAPTER 5 Advanced Search\\ncomputers) to deal with a single search problem to solve it more quickly.\\nMuch of the material in this chapter is introductory in nature, and refer-\\nences are given to books and papers where more information can be\\nlearned on the methods.\\n5.2 Constraint Satisfaction Search\\nSearch can be used to solve problems that are limited by constraints, such\\nas the eight-queens problem. Such problems are often known as Con-\\nstraint Satisfaction Problems,o r  CSPs.'), Document(metadata={}, page_content='straint Satisfaction Problems,o r  CSPs.\\nIn this problem, eight queens must be placed on a chess board in such a\\nway that no two queens are on the same diagonal, row, or column. If we use\\ntraditional chess board notation, we mark the columns with letters from a\\nto g and the rows with numbers from 1 to 8. So, a square can be referred to\\nby a letter and a number, such as a4 or g7.\\nThis kind of problem is known as a constraint satisfaction problem (CSP)'), Document(metadata={}, page_content='because a solution must be found that satisfies the constraints.\\nIn the case of the eight-queens problem, a search tree can be built that rep-\\nresents the possible positions of queens on the board.\\nOne way to represent this is to have a tree that is 8-ply deep, with a branch-\\ning factor of 64 for the first level, 63 for the next level, and so on, down to\\n57 for the eighth level.\\nA goal node in this tree is one that satisfies the constraints that no two'), Document(metadata={}, page_content='queens can be on the same diagonal, row, or column.\\nAn extremely simplistic approach to solving this problem would be to ana-\\nlyze every possible configuration until one was found that matched the\\nconstraints.\\nA more suitable approach to solving the eight-queens problem would be to\\nuse depth-first search on a search tree that represents the problem in the\\nfollowing manner:\\nThe first branch from the root node would represent the first choice of a'), Document(metadata={}, page_content='square for a queen. The next branch from these nodes would represent\\nchoices of where to place the second queen.\\nThe first level would have a branching factor of 64 because there are 64 pos-\\nsible squares on which to place the first queen. The next level would have a'), Document(metadata={}, page_content='5.2 Constraint Satisfaction Search 119\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nabcdef gh\\nFigure 5.1\\nThe eight-queens prob-\\nlem. Three queens have\\nbeen placed so far.\\nsomewhat lower branching factor because once a queen has been placed,\\nthe constraints can be used to determine possible squares upon which the\\nnext queen can be placed. The branching factor will decrease as the algo-\\nrithm searches down the tree. At some point, the tree will terminate\\nbecause the path being followed will lead to a position where no more'), Document(metadata={}, page_content='queens can be placed on legal squares on the board, and there are still some\\nqueens remaining.\\nIn fact, because each row and each column must contain exactly one queen,\\nthe branching factor can be significantly reduced by assuming that the first\\nqueen must be placed in row 1, the second in row 2, and so on. In this way,\\nthe first level will have a branching factor of 8 (a choice of eight squares on\\nwhich the first queen can be placed), the next 7, the next 6, and so on.'), Document(metadata={}, page_content='In fact, the search tree can be further simplified as each queen placed on the\\nboard “uses up” a diagonal, meaning that the branching factor is only 5 or 6\\nafter the first choice has been made, depending on whether the first queen\\nis placed on an edge of the board (columns a or h) or not. The next level\\nwill have a branching factor of about 4, and the next may have a branching\\nfactor of just 2, as shown in Figure 5.1.\\nThe arrows in Figure 5.1 show the squares to which each queen can move.'), Document(metadata={}, page_content='Note that no queen can move to a square that is already occupied by\\nanother queen.'), Document(metadata={}, page_content='120 CHAPTER 5 Advanced Search\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nabcdef gh\\nFigure 5.2\\nA solution to the eight-\\nqueens problem\\nIn Figure 5.1, the first queen was placed in column a of row 8, leaving six\\nchoices for the next row. The second queen was placed in column d of row\\n7, leaving four choices for row 6. The third queen was placed in column f in\\nrow 6, leaving just two choices (column c or column h) for row 5.\\nUsing knowledge like this about the problem that is being solved can help'), Document(metadata={}, page_content='to significantly reduce the size of the search tree and thus improve the effi-\\nciency of the search solution.\\nA solution will be found when the algorithm reaches depth 8 and success-\\nfully places the final queen on a legal square on the board. A goal node\\nwould be a path containing eight squares such that no two squares shared a\\ndiagonal, row, or column.\\nOne solution to the eight-queens problem is shown in Figure 5.2.\\nNote that in this solution, if we start by placing queens on squares e8, c7,'), Document(metadata={}, page_content='h6, and then d5, once the fourth queen has been placed, there are only two\\nchoices for placing the fifth queen (b4 or g4). If b4 is chosen, then this\\nleaves no squares that could be chosen for the final three queens to satisfy\\nthe constraints. If g4 is chosen for the fifth queen, as has been done in Fig-\\nure 5.2, only one square is available for the sixth queen (a3), and the final\\ntwo choices are similarly constrained. So, it can be seen that by applying the'), Document(metadata={}, page_content='constraints appropriately, the search tree can be significantly reduced for\\nthis problem.'), Document(metadata={}, page_content='5.4 Most-Constrained Variables 121\\nUsing chronological backtracking in solving the eight-queens problem\\nmight not be the most efficient way to identify a solution because it will\\nbacktrack over moves that did not necessarily directly lead to an error, as\\nwell as ones that did. In this case, nonchronological backtracking, or\\ndependency-directed backtracking (see Section 5.17) could be more use-\\nful because it could identify the steps earlier in the search tree that caused'), Document(metadata={}, page_content='the problem further down the tree.\\n5.3 Forward Checking\\nIn fact, backtracking can be augmented in solving problems like the eight-\\nqueens problem by using a method called forward checking . As each\\nqueen is placed on the board, a forward-checking mechanism is used to\\ndelete from the set of possible future choices any that have been rendered\\nimpossible by placing the queen on that square. For example, if a queen is\\nplaced on square a1, forward checking will remove all squares in row 1, all'), Document(metadata={}, page_content='squares in column a, and also squares b2, c3, d4, e5, f6, g7, and h8. In this\\nway, if placing a queen on the board results in removing all remaining\\nsquares, the system can immediately backtrack, without having to attempt\\nto place any more queens. This can often significantly improve the per-\\nformance of solutions for CSPs such as the eight-queens problem.\\n5.4 Most-Constrained Variables\\nA further improvement in performance can be achieved by using the most-'), Document(metadata={}, page_content='constrained variable heuristic. At each stage of the search, this heuristic\\ninvolves working with the variable that has the least possible number of\\nvalid choices. In the case of the eight-queens problem, this might be\\nachieved by considering the problem to be one of assigning a value to eight\\nvariables, a through h. Assigning value 1 to variable a means placing a\\nqueen in square a1. T o use the most constrained variable heuristic with this'), Document(metadata={}, page_content='representation means that at each move we assign a value to the variable\\nthat has the least choices available to it. Hence, after assigning a = 1, b = 3,\\nand c = 5, this leaves three choices for d, three choices for e, one choice for\\nf, three choices for g, and three choices for h. Hence, our next move is to\\nplace a queen in column f.\\nThis heuristic is perhaps more clearly understood in relation to the map-\\ncoloring problem. It makes sense that, in a situation where a particular'), Document(metadata={}, page_content='122 CHAPTER 5 Advanced Search\\ncountry can be given only one color due to the colors that have been\\nassigned to its neighbors, that country be colored next.\\nThe most-constraining variable heuristic is similar in that it involves\\nassigning a value next to the variable that places the greatest number of\\nconstraints on future variables.\\nThe least-constraining value heuristic is perhaps more intuitive than the\\ntwo already presented in this section. This heuristic involves assigning a'), Document(metadata={}, page_content='value to a variable that leaves the greatest number of choices for other vari-\\nables. This heuristic can be used to make n-queens problems with\\nextremely large values of n quite solvable.\\n5.5 Example: Cryptographic Problems\\nThe constraint satisfaction procedure is also a useful way to solve problems\\nsuch as cryptographic problems. For example:\\nFORTY\\n+ TEN\\n+ TEN\\nSIXTY\\nSolution:\\n29786\\n+ 850\\n+ 850\\n31486\\nThis cryptographic problem can be solved by using a Generate and T est'), Document(metadata={}, page_content='method, applying the following constraints:\\n■ Each letter represents exactly one number.\\n■ No two letters represent the same number.\\nAs explained in Chapter 4, Generate and T est is a brute-force method,\\nwhich in this case involves cycling through all possible assignments of\\nnumbers to letters until a set is found that meets the constraints and solves\\nthe problem.'), Document(metadata={}, page_content='5.6 Heuristic Repair 123\\nWithout using constraints, the method would first start by attempting to\\nassign 0 to all letters, resulting in the following sum:\\n00000\\n+ 000\\n+ 000\\n00000\\nAlthough this may appear to be a valid solution to the problem, it does not\\nmeet the constraints laid down that specify that each letter can be assigned\\nonly one number, and each number can be assigned only to one letter.\\nHence, constraints are necessary simply to find the correct solution to the'), Document(metadata={}, page_content='problem. They also enable us to reduce the size of the search tree. In this\\ncase, for example, it is not necessary to examine possible solutions where\\ntwo letters have been assigned the same number, which dramatically\\nreduces the possible solutions to be examined.\\nAs we see in the next section, there are more efficient methods than Gener-\\nate and T est to solve problems of this nature.\\n5.6 Heuristic Repair\\nHeuristics can be used to improve performance of solutions to con-'), Document(metadata={}, page_content='straint satisfaction problems. One way to do this is to use a heuristic\\nrepair method , which involves generating a possible solution (ran-\\ndomly, or using a heuristic to generate a position that is close to a solu-\\ntion) and then making changes that reduce the distance of the state\\nfrom the goal.\\nIn the case of the eight-queens problem, this could be done using the min-\\nconflicts heuristic. T o move from one state to another state that is likely to be'), Document(metadata={}, page_content='closer to a solution using the min-conflicts heuristic, select one queen that\\nconflicts with another queen (in other words, it is on the same row, column,\\nor diagonal as another queen). Now move that queen to a square where it\\nconflicts with as few queens as possible. Continue with another queen.\\nT o see how this method would work, consider the starting position shown\\nin Figure 5.3.'), Document(metadata={}, page_content='124 CHAPTER 5 Advanced Search\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nabcdef gh\\nFigure 5.3\\nAlmost a solution to the\\neight-queens problem\\nThis starting position has been generated by placing the queens such that there\\nare no conflicts on rows or columns. The only conflict here is that the queen in\\ncolumn 3 (on c7) is on a diagonal with the queen in column h (on h2).\\nT o move toward a solution, we choose to move the queen that is on column\\nh. We will only ever apply a move that keeps a queen on the same column'), Document(metadata={}, page_content='because we already know that we need to have one queen on each column.\\nEach square in column h has been marked with a number to show how\\nmany other queens that square conflicts with. Our first move will be to\\nmove the queen on column h up to row 6, where it will conflict only with\\none queen. Then we arrive at the position shown in Figure 5.4.\\nBecause we have created a new conflict with the queen on row 6 (on f6),\\nour next move must be to move this queen. In fact, we can move it to a'), Document(metadata={}, page_content='square where it has zero conflicts. This means the problem has been solved,\\nand there are no remaining conflicts.\\nThis method can be used not only to solve the eight-queens problem but\\nalso has been successfully applied to the n-queens problem for extremely\\nlarge values of n. It has been shown that, using this method, the 1,000,000-\\nqueens problem can be solved in an average of around 50 steps.\\nSolving the 1,000,000-queens problem using traditional search techniques'), Document(metadata={}, page_content='would be impossible because it would involve searching a tree with a\\nbranching factor of 10\\n12.'), Document(metadata={}, page_content='5.7 Combinatorial Optimization Problems 125\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nabcdef gh\\nFigure 5.4\\nAlmost a solution to the\\neight-queens problem;\\nposition after applying\\nmin-conflicts heuristic\\nonce to the position shown\\nin Figure 5.3\\n5.7 Combinatorial Optimization Problems\\nLocal search uses a range of techniques to solve large combinatorial opti-\\nmization problems . A combinatorial optimization problem is simply a\\nproblem that can be expressed in terms of finding the best possible set of'), Document(metadata={}, page_content='values for a group of variables.\\nAn example of a combinatorial optimization problem is the eight-queens\\nproblem presented in Chapter 4. The variables in this case can be consid-\\nered to be the eight queens, which can take on values that represent the\\nsquares on the board. The constraints of the problem make it harder than\\nsimply picking any eight values for the variables, and hence, as we have\\nseen, it is useful to find ways to restrict the number of choices that are avail-'), Document(metadata={}, page_content='able for each queen to avoid the problem of combinatorial explosion.\\nReal-world combinatorial optimization problems include allocating teach-\\ners to classrooms, scheduling machines and workers in factories, and select-\\ning the best routes for buses, taxis, and other vehicles. The traveling\\nsalesman problem is another such problem.\\nA relaxed optimization problem is a version of a problem where there are\\nmore possible solutions (the feasible region is larger), or where there are'), Document(metadata={}, page_content='fewer constraints applied to the possible values that the variables can take.\\nFor example, a relaxed (and trivial) version of the eight-queens problem'), Document(metadata={}, page_content='126 CHAPTER 5 Advanced Search\\nmight be that the eight queens must be placed on the board so that no two\\nqueens are on the same row or column. As we see in Section 5.2, finding\\nsolutions to relaxed problems can help to develop heuristics for more com-\\nplex problems.\\n5.8 Local Search and Metaheuristics\\nLocal search methods work by starting from some initial configuration\\n(usually random) and making small changes to the configuration until a'), Document(metadata={}, page_content='state is reached from which no better state can be achieved. Hill climbing is\\na good example of a local search technique. Local search techniques, used in\\nthis way, suffer from the same problems as hill climbing and, in particular,\\nare prone to finding local maxima that are not the best solution possible.\\nThe methods used by local search techniques are known as metaheuristics.\\nExamples of metaheuristics include simulated annealing (see Section 5.9),'), Document(metadata={}, page_content='tabu search (see Section 5.8.3), genetic algorithms (see Chapter 14), ant colony\\noptimization (see Section 5.8.4), and neural networks (see Chapter 11).\\nThis kind of search method is also known as local optimization because it\\nis attempting to optimize a set of values but will often find local maxima\\nrather than a global maximum.\\nA local search technique applied to the problem of allocating teachers to class-\\nrooms would start from a random position and make small changes until a'), Document(metadata={}, page_content='configuration was reached where no inappropriate allocations were made.\\n5.8.1 Exchanging Heuristics\\nThe simplest form of local search is to use an exchanging heuristic.A n\\nexchanging heuristic moves from one state to another by exchanging one or\\nmore variables by giving them different values. We saw this in solving the\\neight-queens problem as heuristic repair. Ak-exchange is considered to be a\\nmethod wherek variables have their values changed at each step. The heuris-'), Document(metadata={}, page_content='tic repair method we applied to the eight-queens problem was 2-exchange.\\nA k-exchange can be used to solve the traveling salesman problem. A tour\\n(a route through the cities that visits each city once, and returns to the\\nstart) is generated at random. Then, if we use 2-exchange, we remove two\\nedges from the tour and substitute them for two other edges. If this pro-'), Document(metadata={}, page_content='5.8 Local Search and Metaheuristics 127\\nduces a valid tour that is shorter than the previous one, we move on from\\nhere. Otherwise, we go back to the previous tour and try a different set of\\nsubstitutions.\\nIn fact, using k = 2 does not work well for the traveling salesman problem,\\nwhereas using k = 3 produces good results. Using larger numbers of k will\\ngive better and better results but will also require more and more iterations.'), Document(metadata={}, page_content='Using k = 3 gives reasonable results and can be implemented efficiently. It\\ndoes, of course, risk finding local maxima, as is often the case with local\\nsearch methods.\\n5.8.2 Iterated Local Search\\nIterated local search techniques attempt to overcome the problem of local\\nmaxima by running the optimization procedure repeatedly, from different\\ninitial states. If used with sufficient iterations, this kind of method will\\nalmost always find a global maximum.'), Document(metadata={}, page_content='almost always find a global maximum.\\nThe aim, of course, in running methods like this is to provide a very good\\nsolution without needing to exhaustively search the entire problem space.\\nIn problems such as the traveling salesman problem, where the search\\nspace grows extremely quickly as the number of cities increases, results\\ncan be generated that are good enough (i.e., a local maximum) without\\nusing many iterations, where a perfect solution would be impossible to'), Document(metadata={}, page_content='find (or at least it would be impossible to guarantee a perfect solution—\\neven one iteration of local search may happen upon the global maximum,\\nof course!).\\n5.8.3 Tabu Search\\nT abu search is a metaheuristic that uses a list of states that have already\\nbeen visited to attempt to avoid repeating paths. The tabu search meta-\\nheuristic is used in combination with another heuristic and operates on the\\nprinciple that it is worth going down a path that appears to be poor if it'), Document(metadata={}, page_content='avoids following a path that has already been visited. In this way, tabu\\nsearch is able to avoid local maxima.\\nT o quote from the www.tabusearch.net website: “a bad strategic choice can\\nyield more information than a good random choice. ”'), Document(metadata={}, page_content='128 CHAPTER 5 Advanced Search\\n5.8.4 Ant Colony Optimization\\nForaging ants leave a trail of pheromones so that they can lead other ants to\\nfind the food that they have found. The trail of pheromones is renewed reg-\\nularly, so that if another ant finds a better route, the pheromones along the\\nold route will gradually fade, and the new, superior route will become the\\nmost popular choice.\\nThe ant colony optimization (ACO) metaheuristic is based on this behav-'), Document(metadata={}, page_content='ior. For example, when attempting to solve the traveling salesman problem,\\na set of “artificial ants” is sent out along the routes, leaving trails of\\n“pheromones” that indicate how short the route they have taken is.\\nPheromones gradually fade, meaning that ants that follow later will take\\nthe route whose pheromones have been most recently updated, while\\nattempting to follow the pheromones that indicate the shortest path. ACO'), Document(metadata={}, page_content='has been successfully used to enable engineers to find the best way to route\\ncables through a communications network. Because the “ants” are continu-\\nally foraging through the network, this method is able to cope extremely\\nwell with changes in the environment, such as blockages and new routes.\\nWe will learn more about Artificial Intelligence methods based on biologi-\\ncal systems (artificial life) in Chapter 13.\\n5.9 Simulated Annealing'), Document(metadata={}, page_content='5.9 Simulated Annealing\\nAnnealing is a process of producing very strong glass or metal, which\\ninvolves heating the material to a very high temperature and then allowing\\nit to cool very slowly. In this way, the atoms are able to form the most stable\\nstructures, giving the material great strength.\\nSimulated annealing is a local search metaheuristic based on this method\\nand is an extension of a process calledmetropolis Monte Carlo simulation.'), Document(metadata={}, page_content='Simulated annealing is applied to a multi-value combinatorial problem\\nwhere values need to be chosen for many variables to produce a particular\\nvalue for some global function, dependent on all the variables in the sys-\\ntem. This value is thought of as the energy of the system, and in general the\\naim of simulated annealing is to find a minimum energy for a system.\\nSimple Monte Carlo simulationis a method of learning information (such'), Document(metadata={}, page_content='as shape) about the shape of a search space. The process involves randomly\\nselecting points within the search space. An example of its use is as follows:'), Document(metadata={}, page_content='5.9 Simulated Annealing 129\\nA square is partially contained within a circle. Simple Monte Carlo simula-\\ntion can be used to identify what proportion of the square is within the cir-\\ncle and what proportion is outside the circle. This is done by randomly\\nsampling points within the square and checking which ones are within the\\ncircle and which are not.\\nMetropolis Monte Carlo simulation extends this simple method as follows:\\nRather than selecting new states from the search space at random, a new'), Document(metadata={}, page_content='state is chosen by making a small change to the current state. If the new\\nstate means that the system as a whole has a lower energy than it did in the\\nprevious state, then it is accepted. If the energy is higher than for the previ-\\nous state, then a probability is applied to determine whether the new state\\nis accepted or not. This probability is called a Boltzmann acceptance crite-\\nrion and is calculated as follows:\\ne\\n(/H11002dE/T)'), Document(metadata={}, page_content='e\\n(/H11002dE/T)\\nwhere T is the current temperature of the system, and dE is the increase in\\nenergy that has been produced by moving from the previous state to the\\nnew state. The temperature in this context refers to the percentage of steps\\nthat can be taken that lead to a rise in energy: At a higher temperature, more\\nsteps will be accepted that lead to a rise in energy than at low temperature.\\nT o determine whether to move to a higher energy state or not, the proba-\\nbility e'), Document(metadata={}, page_content='bility e\\n(/H11002dE/T) is calculated, and a random number is generated between 0\\nand 1. If this random number is lower than the probability function, the\\nnew state is accepted. In cases where the increase in energy is very high, or\\nthe temperature is very low, this means that very few states will be accepted\\nthat involve an increase in energy, as e\\n(/H11002dE/T) approaches zero.\\nThe fact that some steps are allowed that increase the energy of the system'), Document(metadata={}, page_content='enables the process to escape from local minima, which means that simu-\\nlated annealing often can be an extremely powerful method for solving\\ncomplex problems with many local maxima.\\nNote: Some systems use e\\n(/H11002dE/kT) as the probability that the search will\\nprogress to a state with a higher energy, where k is Boltzmann’s constant\\n(Boltzmann’s constant is approximately 1.3807 /H1100310/H1100223 Joules per Kelvin).\\nSimulated annealing uses Monte Carlo simulation to identify the most stable'), Document(metadata={}, page_content='state (the state with the lowest energy) for a system. This is done by running'), Document(metadata={}, page_content='130 CHAPTER 5 Advanced Search\\n*VLSI — V ery Large-Scale Integration—a method used to get very large numbers of gates\\nonto silicon chips.\\nsuccessive iterations of metropolis Monte Carlo simulation, using progres-\\nsively lower temperatures. Hence, in successive iterations, fewer and fewer\\nsteps are allowed that lead to an overall increase in energy for the system.\\nA cooling schedule (or annealing schedule) is applied, which determines'), Document(metadata={}, page_content='the manner in which the temperature will be lowered for successive itera-\\ntions. Two popular cooling schedules are as follows:\\nT\\nnew = Told /H11002dT\\nTnew = C /H11003Told (where C < 1.0)\\nThe cooling schedule is extremely important, as is the choice of the number\\nof steps of metropolis Monte Carlo simulation that are applied in each iter-\\nation. These help to determine whether the system will be trapped by local\\nminima (known as quenching). The number of times the metropolis'), Document(metadata={}, page_content='Monte Carlo simulation is applied per iteration is for later iterations.\\nAlso important in determining the success of simulated annealing are the\\nchoice of the initial temperature of the system and the amount by which\\nthe temperature is decreased for each iteration. These values need to be\\nchosen carefully according to the nature of the problem being solved.\\nWhen the temperature, T, has reached zero, the system is frozen, and if the'), Document(metadata={}, page_content='simulated annealing process has been successful, it will have identified a\\nminimum for the total energy of the system.\\nSimulated annealing has a number of practical applications in solving prob-\\nlems with large numbers of interdependent variables, such as circuit design.\\nIt has also been successfully applied to the traveling salesman problem.\\n5.9.1 Uses of Simulated Annealing\\nSimulated annealing was invented in 1983 by Kirkpatrick, Gelatt, and V ec-\\nchi. It was first used for placing VLSI'), Document(metadata={}, page_content='chi. It was first used for placing VLSI\\n* components on a circuit board.\\nSimulated annealing has also been used to solve the traveling salesman\\nproblem, although this approach has proved to be less efficient than using\\nheuristic methods that know more about the problem. It has been used\\nmuch more successfully in scheduling problems and other large combina-'), Document(metadata={}, page_content='5.11 Real-Time A* 131\\ntorial problems where values need to be assigned to a large number of vari-\\nables to maximize (or minimize) some function of those variables.\\n5.10 Genetic Algorithms for Search\\nGenetic algorithms are discussed in much more detail in Chapter 14. This\\nsection provides a brief overview of the ways in which genetic algorithms\\ncan be used to solve search problems but does not assume any detailed\\nunderstanding of the mechanics of genetic algorithms.'), Document(metadata={}, page_content='Genetic algorithms involve finding solutions to complex problems using a\\nmethod based on the process of evolution that we see in nature. In much\\nthe same way as nature evolves creatures that are best designed to suit their\\nenvironments by selecting features that work (survival of the fittest),\\ngenetic algorithms work by combining potential solutions to a problem\\ntogether in a way that tends to produce better solutions over successive'), Document(metadata={}, page_content='generations. This is a form of local optimization, but where mutation and\\ncrossover are used to try to avoid local maxima.\\nAs is explained in Chapter 14, genetic algorithms are usually used to iden-\\ntify optimal solutions to complex problems. This can clearly be easily\\nmapped to search methods, which are aiming toward a similar goal.\\nGenetic algorithms can thus be used to search for solutions to multi-value\\nproblems where the closeness of any attempted solution to the actual solu-'), Document(metadata={}, page_content='tion (fitness) can be readily evaluated.\\nIn short, a population of possible solutions ( chromosomes) is generated,\\nand a fitness value for each chromosome is determined. This fitness is used\\nto determine the likelihood that a given chromosome will survive to the\\nnext generation, or reproduce. Reproduction is done by applying cross-\\nover to two (or more) chromosomes, whereby features ( genes) of each\\nchromosome are combined together. Mutation is also applied, which'), Document(metadata={}, page_content='involves making random changes to particular genes.\\n5.11 Real-Time A*\\nReal-time A* is a variation of A*, as presented in Chapter 4. Search contin-\\nues on the basis of choosing paths that have minimum values of f(node) =\\ng(node) + h(node). However,g(node) is the distance of the node from the\\ncurrent node, rather than from the root node. Hence, the algorithm will'), Document(metadata={}, page_content='132 CHAPTER 5 Advanced Search\\nbacktrack if the cost of doing so plus the estimated cost of solving the prob-\\nlem from the new node is less than the estimated cost of solving the prob-\\nlem from the current node.\\nImplementing real-time A* means maintaining a hash table of previously\\nvisited states with their h(node) values.\\n5.12 Iterative-Deepening A* (IDA*)\\nBy combining iterative-deepening with A*, we produce an algorithm that is'), Document(metadata={}, page_content='optimal and complete (like A*) and that has the low memory requirements\\nof depth-first search.\\nIDA* is a form of iterative-deepening search where successive iterations\\nimpose a greater limit on f(node) rather than on the depth of a node.\\nIDA* performs well in problems where the heuristic value f (node) has rel-\\natively few possible values. For example, using the Manhattan distance as a\\nheuristic in solving the eight-queens problem, the value of f (node) can'), Document(metadata={}, page_content='only have values 1, 2, 3, or 4. In this case, the IDA* algorithm only needs to\\nrun through a maximum of four iterations, and it has a time complexity\\nnot dissimilar from that of A*, but with a significantly improved space\\ncomplexity because it is effectively running depth-first search.\\nIn cases such as the traveling salesman problem where the value off (node)\\nis different for every state, the IDA* method has to expand 1 + 2 + 3 + . . .\\n+ n nodes = O(n\\n2) where A* would expand n nodes.'), Document(metadata={}, page_content='+ n nodes = O(n\\n2) where A* would expand n nodes.\\n5.13 Parallel Search\\nMany of the search methods that have been described in this book were\\ndeveloped in the 1960s, 1970s, and 1980s, when computers lacked the\\npower, memory, and storage space that they have today. Many of the issues\\nthat were thus of concern when those algorithms were developed are no\\nlonger important.\\nNowadays, computers have far more processing power and storage space and'), Document(metadata={}, page_content='so are able to run algorithms, such as search, a great deal faster. As we see in\\nChapter 6, this has helped to lead to a great improvement in the ability of\\nchess-playing computer programs. Another aspect of chess-playing com-\\nputer programs is that they tend to run parallel search. The names of many of'), Document(metadata={}, page_content='5.13 Parallel Search 133\\nthe best chess computers include the worddeep: Deep Thought, Deep Blue,\\nDeep Junior, and Deep Fritz, for example. The worddeep means parallel.\\nThe idea of parallel processing is that if a task can be broken down into a\\nnumber of sub-tasks, where those sub-tasks do not need to be run sequen-\\ntially, then they can be run in parallel, simultaneously on separate processors.\\nAs with much of Artificial Intelligence, there is a good basis for this idea:'), Document(metadata={}, page_content='the human brain. The human brain is massively parallel , which means\\nthat it is able to do millions of things simultaneously. Computers are much\\nfaster at raw processing than a human brain, but because the brain is able to\\ndo so many things simultaneously, it is able to operate at a much faster rate\\nthan a computer.\\nApplying this idea to search is clearly desirable because many search prob-\\nlems (such as playing chess) can be heavily time dependent.'), Document(metadata={}, page_content='One search method that can be simply parallelized is depth-first search. If\\nwe assume that we have two processors, we could simply divide the descen-\\ndants of the root node in half and assign half of them to one processor and\\nhalf to the other. The two processors would then run a series of depth-first\\nsearches on each of its nodes. The first processor to find a goal node would\\nreport success, and the whole computation would stop.\\nMore complex search methods such as alpha–beta pruning , which is'), Document(metadata={}, page_content='described in Chapter 6, are not so easy to implement in parallel. Alpha–beta\\npruning is a method that is used to eliminate portions of the search tree for\\nplaying games such as chess that can provide a great increase in perform-\\nance. It has been shown that running alpha–beta pruning searches in paral-\\nlel by simply dividing the search tree up between processors actually\\nprovides worse results than running it in serial (Fox et al. 1994).'), Document(metadata={}, page_content='T o develop a parallel version of an algorithm such as alpha–beta pruning,\\nmore care needs to be taken in how the tasks are split up so that perform-\\nance is not degraded.\\nOne area where parallel search can be readily applied is in solving con-\\nstraint satisfaction problems. In general, CSPs are not well solved by using\\nbrute-force search because this involves a combinatorial optimization\\nproblem. In situations where the search tree can be reduced somewhat, and'), Document(metadata={}, page_content='no better method can be found than blind search, the performance of the\\nsearch can be significantly improved by running it in a parallel fashion, by'), Document(metadata={}, page_content='134 CHAPTER 5 Advanced Search\\nsimply dividing the search tree between processors. In some cases, search\\nproblems can be divided between individual computers.\\nOf course, problems that can be solved using goal reduction are also often\\nsolved more efficiently using parallel search because the goal tree can be\\nbroken down into sub-goal trees, which can be worked on in parallel by\\nseparate processors.\\nWhen distributing work in this way, important concepts to consider are'), Document(metadata={}, page_content='task distribution (deciding which task to give to which processor), load\\nbalancing (ensuring that all processors have enough work to do and that\\nno single processor is overworked), and tree ordering (determining the\\ncorrect order to process the search tree).\\n5.13.1 Task Distribution\\nCook (1998) explores the process of implementing a parallel version of\\nIDA* search.\\nOne approach to distributing tasks for parallel implementations of IDA*'), Document(metadata={}, page_content='was to use parallel window search (PWS). This involves searching the dif-\\nferent depth-limited searches concurrently, rather than in series. For exam-\\nple, using three processors, the first processor might search with a depth\\nlimit of 1, the second with a depth limit of 2, and the third with a depth\\nlimit of 3. As soon as any processor completes a search, it is assigned a new\\nsearch with a depth that is deeper than any currently running. Unfortu-'), Document(metadata={}, page_content='nately, if there are too many processors (more than the number of itera-\\ntions needed to find an optimal solution) the PWS method can be very\\ninefficient because many processors will be idle for the entire search.\\nAnother approach used by Cook was distributed tree search (DTS). First,\\na breadth-first search is carried out until there are as many leaf nodes avail-\\nable as there are processors. Then, each of these nodes is assigned to a'), Document(metadata={}, page_content='processor for search. T o ensure that this method is optimal, when a proces-\\nsor finishes an iteration, it must wait for all other processors to finish their\\niterations before starting another. This means that there will often be idle\\nprocessors.\\nCook’s paper provides a very detailed analysis of both of these methods and\\ntheir respective advantages and disadvantages.'), Document(metadata={}, page_content='5.13 Parallel Search 135\\nIn their paper Randomized Parallel Algorithms for Backtrack Search and\\nBranch-and-Bound Computation, Richard Karp and Y anjun Zhang showed\\nthat distributing tasks between processors at random gives better results,\\nparticularly when running a variation of depth-first search called back-\\ntracking search . In backtracking search, when a node is discovered, the\\nprocessor passes one of the children of that node to an idle processor, if one'), Document(metadata={}, page_content='is available. The normal method of determining to which processor to pass\\nthis child node is fairly complex and can create a significant overhead. By\\npassing the child nodes randomly, this overhead can be eliminated, and the\\nsearch becomes much more efficient.\\n5.13.2 Tree Ordering\\nWhen running IDA* in parallel, the order of the tree can be very impor-\\ntant. Since the search tree is expanded in depth-first order from left to'), Document(metadata={}, page_content='right, if the optimal solution is on the left side of the tree, it will be found\\nmuch more quickly than if it is on the right side. Clearly, if a way could be\\nfound to ensure that the optimal solution was always on the left side of the\\ntree, then a search method would not be needed to find it. However, heuris-\\ntics can be used to attempt to examine the tree in a way that will increase\\nthe likelihood of finding an optimal solution quickly. These heuristics'), Document(metadata={}, page_content='operate in much the same way that the heuristics for best-first search and\\nother serial informed search methods use.\\n5.13.3 Search Engines\\nSearch engines are an excellent example of parallel search systems. One\\nproblem faced by search engines is the enormous size of the Internet (esti-\\nmated to be many billions of pages and growing continually). T o index a\\nreasonable percentage of these pages, search engines need to run in parallel.'), Document(metadata={}, page_content='Typically, search engines run their indexing on a number of indexing\\nservers. Pages or websites are distributed among the servers by a scheduling\\nprocess. Clearly, as well as getting the schedule right, it is important that the\\nsearch engines are able to communicate with each other. For example, if\\ntwo search engines both come across the same page, they need to be able to\\ndecide which one will search that page. Instead of crawling independently'), Document(metadata={}, page_content='like this, some search engine spiders simply have a list of links that they\\nneed to crawl. When a spider finds a page, it indexes the page and extracts'), Document(metadata={}, page_content='136 CHAPTER 5 Advanced Search\\nall the links from it. The spider places these links into a central database\\nand carries on with its own list of links. A central scheduling system then\\ndecides how to distribute the links in the central database to the servers. In\\nthis way, no two servers will ever duplicate work.\\n5.14 Bidirectional Search\\nBidirectional search (also known as wave search , due to the wave-like\\nnature in which paths are followed through the search space) is applied'), Document(metadata={}, page_content='when searching for the best path between a start node and a known goal\\nnode. This is somewhat different from most of the other search algorithms\\ndiscussed in this part, where the goal node is not known, and the purpose\\nof the algorithm is to find a path to a goal node without knowing where the\\ngoal node will be located in the tree.\\nBidirectional search involves simultaneously spreading out paths in a\\nbreadth-first fashion from both the start and goal nodes.'), Document(metadata={}, page_content='This requires that a predecessor function be available for each node, as well\\nas the successor function, so that paths can be extended backward from the\\ngoal node.\\nAs soon as the two paths meet, a complete path has been generated that\\nbegins at the start node, goes through the point where the two paths met,\\nand ends at the goal node. This path is guaranteed to be the shortest path\\n(or rather, the path involving the fewest steps).\\n5.15 Nondeterministic Search'), Document(metadata={}, page_content='5.15 Nondeterministic Search\\nNondeterministic search is a combination of depth-first and breadth-first\\nsearch, which avoids the problems of both but does not necessarily have the\\nadvantages of either.\\nWhen running a nondeterministic search, new paths are added to the\\nqueue at random positions. In the following pseudo-code implementation,\\nthe function call \\nadd_randomly_to_queue (successors (state)) adds the suc-\\ncessors of state to random positions in the queue:\\nFunction random ()\\n{'), Document(metadata={}, page_content='Function random ()\\n{\\nqueue = [];     // initialize an empty queue\\nstate = root_node;   // initialize the start state\\nwhile (true)'), Document(metadata={}, page_content='5.17 Nonchronological Backtracking 137\\n{\\nif is_goal (state)\\nthen return SUCCESS\\nelse add_randomly_to_queue (successors (state));\\nif queue == []\\nthen report FAILURE;\\nstate = queue [0]; // state = first item in queue\\nremove_first_item_from (queue);\\n}\\n}\\nThis method is useful in cases where very little information is available about\\nthe search space—for example, in a situation where there may be extremely\\nlong, or even infinite, paths and may also be an extremely large branching'), Document(metadata={}, page_content='factor. In situations like that, depth-first search might end up stuck down an\\ninfinitely long path, and breadth-first search could be extremely inefficient\\nin dealing with the large branching factor. A nondeterministic search will\\navoid these problems but will not necessarily find the best path.\\nNondeterministic search can also be used in combination with other search\\ntechniques. For example, by applying a nondeterministic search when a'), Document(metadata={}, page_content='maximum is found in hill climbing, the problems of local maxima (the\\nfoothill problem) can be avoided.\\n5.16 Island-Driven Search\\nIsland-driven search assumes that an island exists roughly half way\\nbetween the root node and a goal node. The method involves finding a path\\nbetween the root node and the island, and a path between the island and a\\ngoal node. If no path exists that goes through the island, the method reverts\\nto another search method that ignores the island.'), Document(metadata={}, page_content='to another search method that ignores the island.\\nThis method is useful in situations where it is extremely likely that the\\nisland actually does lie on a path to the goal, for example, if we are trying to\\nidentify a route between Milan and Naples, given the knowledge that all\\nroads lead to Rome.\\n5.17 Nonchronological Backtracking\\nNonchronological backtracking, or dependency-directed backtracking, is\\nan alternative to chronological backtracking, which we saw being used in'), Document(metadata={}, page_content='search methods such as depth-first search.'), Document(metadata={}, page_content='138 CHAPTER 5 Advanced Search\\nChronological backtracking operates as follows:\\nWhen a dead end in a tree is found (in other words, a leaf node that is not a\\ngoal node), move back up the search tree to the last point in the tree where\\na decision had to be made. Undo this decision, and all its consequences,\\nand choose the next option at this junction instead.\\nIn some cases, additional information is available about the search space'), Document(metadata={}, page_content='that can help to backtrack in a more efficient manner, undoing decisions\\nthat are more likely to lead to success, rather than just undoing each deci-\\nsion in chronological order. In these cases, we use nonchronological back-\\ntracking, which is also known as dependency-directed backtracking.\\nIt is particularly useful in solving constraint satisfaction problems, where\\nbacktracking can be applied by going back to the previous choice that\\ncaused a constraint to fail.\\n5.18 Chapter Summary'), Document(metadata={}, page_content='caused a constraint to fail.\\n5.18 Chapter Summary\\n■ Constraint satisfaction problems (CSPs) such as the eight-queens\\nproblem, can be solved using search.\\n■ Methods such as forward checking and heuristics such as the most-\\nconstrained variable heuristic and min-conflicts make it possible to\\nsolve extremely large CSPs (such as the 1,000,000-queens problem).\\n■ Large combinatorial optimization problems are best solved using\\nlocal search methods.'), Document(metadata={}, page_content='local search methods.\\n■ Local search methods (or metaheuristics) move from one potential\\nsolution to another by making small changes. When a local maxi-\\nmum is found, the search stops.\\n■ Iterating the local search from different random starting configu-\\nrations can avoid the problem of identifying local maxima and\\nignoring a global maximum.\\n■ Local search methods include tabu search, ant colony optimiza-\\ntion, and simulated annealing.'), Document(metadata={}, page_content='tion, and simulated annealing.\\n■ Simulated annealing is based on the way in which metals are hard-\\nened by being heated up and then slowly cooled, so that the crys-\\ntalline structure forms the strongest possible arrangement.'), Document(metadata={}, page_content='5.19 Review Questions 139\\n■ Variations on A* such as real-time A* and iterative-deepening A*\\ncan provide enhanced performance.\\n■ Parallel search methods can take advantage of modern parallel\\ncomputers. Issues such as task distribution, load balancing, and\\ntree ordering need to be considered.\\n5.19 Review Questions\\n5.1 Explain how search can be used to solve constraint satisfaction\\nproblems, such as the eight-queens problem. What difficulties arise'), Document(metadata={}, page_content='when such problems become extremely large (e.g., the 1,000,000-\\nqueens problem)? What kinds of methods can be applied to solve\\nsuch large problems efficiently?\\n5.2 Explain the idea behind the following heuristics:\\n■ most-constrained variable\\n■ most-constraining variable\\n■ least-constraining variable\\n■ min-conflicts\\n5.3 Why is local search more practical than depth-first search for solv-\\ning large combinatorial optimization problems? Explain what a\\nmetaheuristic is and why it is useful.'), Document(metadata={}, page_content='metaheuristic is and why it is useful.\\n5.4 How does iterated local search avoid the problem of local maxima?\\nWhy is this important?\\n5.5 Explain how ant colony optimization works. Why might it be use-\\nful for communications routing?\\n5.6 Describe in layman’s terms the idea behind simulated annealing and\\nwhy it works. What kinds of problems might it be useful for solving?\\n5.7 Explain the purpose of the temperature variable in simulated\\nannealing. How effective would the method be without it?'), Document(metadata={}, page_content='5.8 Explain why IDA* might be used instead of A*. In what kinds of\\nsituations might it be less useful?'), Document(metadata={}, page_content='140 CHAPTER 5 Advanced Search\\n5.9 Explain the importance of the following principles when running\\nparallel search methods:\\n■ task distribution\\n■ load balancing\\n■ tree ordering\\n5.10 How do search engines make use of search? Research a few of the\\nbest known search engines, and try to find out what kind of search\\nalgorithms they use. How efficient do you think they are at search-\\ning? Could you implement them better?\\n5.20 Exercises\\n5.1 Write a program in a programming language of your choice for'), Document(metadata={}, page_content='solving the n-queens problem. Run it with 8 queens, and then try it\\nwith 100 queens. How well does it perform? Could your program\\nfind a solution for 1,000,000 queens? If not, why not? If so, what\\noptimizations have you used to make that possible?\\n5.2 Write a program that can solve arbitrary cryptographic problems.\\nAdd heuristics to your implementation to make it more efficient.\\nWhat limitations does your program have?\\n5.3 Investigate tabu search. Write 1000 words explaining how it works'), Document(metadata={}, page_content='and what sorts of problems it is best suited to solving.\\n5.4 Write a program that uses simulated annealing to solve the traveling\\nsalesman problem of arbitrary size. Do you think that simulated\\nannealing is a good way to solve this problem? Explain your answer.\\n5.5 Implement a nondeterministic search algorithm. Build search trees\\nfor which it performs the following:\\na. better than depth-first search\\nb. worse than depth-first search\\nc. better than breadth-first search'), Document(metadata={}, page_content='c. better than breadth-first search\\nd. worse than breadth-first search'), Document(metadata={}, page_content='5.21 Further Reading 141\\n5.21 Further Reading\\nMost of the material covered in this chapter is covered well by the majority\\nof Artificial Intelligence textbooks. Material on local search is relatively\\nnew, and not so well covered by the older textbooks.\\nT abu Search by Glover and Laguna, the inventors of tabu search, provides a\\ngood insight into the tabu search metaheuristic.\\nThe min-conflicts heuristic was invented by Gu in 1989. Further informa-\\ntion on the method can be found in Minton (1992).'), Document(metadata={}, page_content='tion on the method can be found in Minton (1992).\\nPearl (1984) gives a good overview of search methods with a particular\\nfocus on heuristics.\\nRayward-Smith et al. (1996) gives excellent coverage of heuristics and\\nmetaheuristics in particular.\\nJansen (1997) reports on research that has been done using simulated\\nannealing in information retrieval to select a suitable ordering of results to\\nreturn to a user in response to a keyword text query.'), Document(metadata={}, page_content='Multiobjective Heuristic Search: An Introduction to Intelligent Search Meth-\\nods for Multicriteria Optimization by Pallab Dasgupta, P . P . Chakrabarti,\\nS. C. Desarkar (1999 - Friedrich Vieweg & Sohn)\\nAdaptive Parallel Iterative Deepening Searchby Diane J. Cook and R. Craig Var-\\nnell (1998 – inJournal of Artificial Intelligence Research, Vol. 9, pp. 139–166)\\nParallel Computing Works by G. C. Fox, R. D. Williams, and P . C. Messina\\n(1994 – Morgan Kaufmann)'), Document(metadata={}, page_content='(1994 – Morgan Kaufmann)\\nT abu Searchby Fred W. Glover, Manuel Laguna (1998 – Kluwer Academic\\nPublishers)\\nSimulated Annealing for Query Results Ranking by B. J. Jansen (1997 – in\\nACM Computer Science Education Conference)\\nLearning to Solve Problems by Searching for Macro-Operators (Research\\nNotes in Artificial Intelligence, Vol. 5) by Richard E. Korf (1985 – Longman\\nGroup United Kingdom)\\nSearch by Richard E. Korf (1987 – in Encyclopedia of Artificial Intelligence\\nedited by E. Shapiro – Wiley)'), Document(metadata={}, page_content='142 CHAPTER 5 Advanced Search\\nLearning Search Control Knowledge: An Explanation Based Approach by\\nStephen Minton (1988 – Kluwer Academic Publishers)\\nMinimizing Conflicts: A Heuristic Repair Method for Constraint Satisfaction\\nand Scheduling Problems by S. Minton, M. D. Johnson, A. B. Philips, and P .\\nLaird (1992 – Artificial Intelligence, Vol. 58)\\nHow to Solve It: Modern Heuristics by Zbigniew Michalewicz and David B.\\nFogel (1999 – Springer V erlag)'), Document(metadata={}, page_content='Fogel (1999 – Springer V erlag)\\nLocal Search for Planning and Scheduling: Ecai 2000 Workshop, Berlin, Ger-\\nmany, August 21, 2000: Revised Papers (Lecture Notes in Computer Science,\\n2148) edited by Alexander Nareyek (2001 – Springer V erlag)\\nCombinatorial Optimization: Algorithms and Complexity by Christos H.\\nPapadimitriou and Kenneth Steiglitz (1998 – Dover Publications)\\nHeuristics: Intelligent Search Strategies for Computer Problem Solving by\\nJudea Pearl (1984 – Addison Wesley)'), Document(metadata={}, page_content='Judea Pearl (1984 – Addison Wesley)\\nModern Heuristic Search Methods edited by V . J. Rayward-Smith, I. H.\\nOsman, Colin R. Reeves, and G. D. Smith (1996 – John Wiley & Sons)\\nThe Algorithm Design Manual by Steven S. Skiena (1997 – T elos)\\nSimulated Annealing: Theory and Applications by P . J. M. Van Laarhoven\\nand E. H. L. Aarts (1987 - D. Reidel Publishing Company – Out of Print)'), Document(metadata={}, page_content='6CHAPTER\\nGame Playing\\nAfter the other matches I felt hooked to be part of this competition because I\\nbelieve it is very important for the game of chess and the human race as a\\nwhole. Now I hope to use my experience to help set new standards and also\\nprove that human players are not hopeless.\\n—Garry Kasparov before his six-game chess match against Deep Junior\\nOne hundred years from now, the idea that humans could still beat computers'), Document(metadata={}, page_content='will seem quaint. It will be like men trying to race cars at the turn of the cen-\\ntury. Who’s better? Who cares? The technology is what matters. It’s improving,\\nand that’s what counts.\\n—Professor Jonathan Schaeffer discussing Kasparov’s chess match with\\nDeep Junior\\n‘The Game’ , said he, ‘is never lost till won. ’\\n—George Crabbe, Gretna Green\\nThe Game’s Afoot.\\n—William Shakespeare, Henry V\\n6.1 Introduction\\nOne of the most interesting and well publicized areas of Artificial Intelli-'), Document(metadata={}, page_content='gence research has been in the playing of games. With the success of Deep\\nBlue in 1997, a landmark was reached: a computer program that could\\ndefeat the best chess player in the world.'), Document(metadata={}, page_content='144 CHAPTER 6 Game Playing\\nGame-playing systems tend to rely heavily on the search techniques\\ndescribed in Chapters 4 and 5, in combination with a number of heuristics\\nand often a detailed database of knowledge about the game.\\nThis chapter explains the relationship between search and games such as\\nchess, checkers, and backgammon. It explains the concepts of alpha–beta\\npruning and Minimax. It uses Chinook, a checkers-playing computer sys-'), Document(metadata={}, page_content='tem, to explain some of the more advanced techniques used in modern\\ngame-playing computers and discusses why computers are currently\\nunable to beat humans at games such as Go.\\n6.2 Game Trees\\nMany two-player games can be efficiently represented using trees, called\\ngame trees. A game tree is an instance of a tree in which the root node rep-\\nresents the state before any moves have been made, the nodes in the tree\\nrepresent possible states of the game (or positions), and arcs in the tree'), Document(metadata={}, page_content='represent moves.\\nIt is usual to represent the two players’ moves on alternate levels of the\\ngame tree, so that all edges leading from the root node to the first level rep-\\nresent possible moves for the first player, and edges from the first level to\\nthe second represent moves for the second player, and so on.\\nLeaf nodes in the tree represent final states, where the game has been won,\\nlost, or drawn. In simple games, a goal node might represent a state in'), Document(metadata={}, page_content='which the computer has won, but for more complex games such as chess\\nand Go, the concept of a goal state is rarely of use.\\nOne approach to playing a game might be for the computer to use a tree\\nsearch algorithm such as depth-first or breadth-first search, looking for a\\ngoal state (i.e., a final state of the game where the computer has won).\\nUnfortunately, this approach does not work because there is another intel-\\nligence involved in the game. We will consider this to be a rational,'), Document(metadata={}, page_content='informed opponent who plays to win. Whether this opponent is human or\\nanother computer does not matter—or should not matter—but for the\\npurposes of this section of the book, we will refer to the opponent as being\\nhuman, to differentiate him or her from the computer.\\nConsider the game tree shown in Figure 6.1. This partial tree represents the\\ngame of tic-tac-toe, in which the computer is playing noughts, and the\\nhuman opponent is playing crosses. The branching factor of the root node'), Document(metadata={}, page_content='6.2 Game Trees 145\\no o o\\no o\\no\\nooo\\nox\\noox o\\no\\nx\\no\\no\\nxx\\no\\noo\\nxx o\\no\\no\\nxx\\no\\no\\nx\\nx\\no\\no\\nx\\nx\\no\\no\\nx\\nox o\\nx\\nFigure 6.1\\nA partial game tree for the game tic-tac-toe\\nis 9 because there are nine squares in which the computer can place its first\\nnought. The branching factor of the next level of the tree is 8, then 7 for the\\nnext level, and so on. The tree shown in Figure 6.1 is clearly just a part of\\nthat tree and has been pruned to enable it to fit comfortably on the page.'), Document(metadata={}, page_content='For a computer to use this tree to make decisions about moves in a game of\\ntic-tac-toe, it needs to use an evaluation function , which enables it to\\ndecide whether a given position in the game is good or bad. If we use\\nexhaustive search, then we only need a function that can recognize a win, a\\nloss, and a draw. Then, the computer can treat “win” states as goal nodes\\nand carry out search in the normal way.\\n6.2.1 Rationality, Zero Sum, and Other Assumptions'), Document(metadata={}, page_content='All the methods discussed in this chapter are designed for games with two\\nplayers. In most of the games, there is no element of chance (in other\\nwords, no dice are thrown, or cards drawn), and the players have complete'), Document(metadata={}, page_content='146 CHAPTER 6 Game Playing\\nknowledge of the state of the game, which means that the players do not\\nconceal information (apart from their strategies and plans) from their\\nopponents. This sets games such as chess and Go aside from games such as\\npoker, in which there is an element of chance, and it is also important that\\nplayers conceal information from each other.\\nMost of the games we will consider in this chapter are zero-sum games,'), Document(metadata={}, page_content='which means that if the overall score at the end of a game for each player\\ncan be 1 (a win), 0 (a draw), or /H110021 (a loss), then the total score for both\\nplayers for any game must always be 0. In other words, if one player wins,\\nthe other must lose. The only other alternative is that both players draw.\\nFor this reason, we consider the search techniques that are discussed here to\\nbe adversarial methods because each player is not only trying to win but to'), Document(metadata={}, page_content='cause the opponent to lose. In the algorithms such as Minimax and\\nalpha–beta that are discussed later, it is important that the computer can\\nassume that the opponent is rational and adversarial. In other words, the\\ncomputer needs to assume that the opponent will play to win.\\nIn discussing game trees, we use the concept of ply, which refers to the\\ndepth of the tree. In particular, we refer to the ply of lookahead. When a'), Document(metadata={}, page_content='computer evaluates a game tree to ply 5, it is examining the tree to a depth\\nof 5. The 4th ply in a game tree is the level at depth 4 below the root node.\\nBecause the games we are talking about involve two players, sequential plies\\nin the tree will alternately represent the two players. Hence, a game tree\\nwith a ply of 8 will represent a total of eight choices in the game, which cor-\\nresponds to four moves for each player. It is usual to use the word ply to'), Document(metadata={}, page_content='represent a single level of choice in the game tree, but for the word move to\\nrepresent two such choices—one for each player.\\n6.2.2 Evaluation Functions\\nEvaluation functions (also known as static evaluators because they are\\nused to evaluate a game from just one static position) are vital to most\\ngame-playing computer programs. This is because it is almost never possi-\\nble to search the game tree fully due to its size. Hence, a search will rarely'), Document(metadata={}, page_content='reach a leaf node in the tree at which the game is either won, lost, or drawn,\\nwhich means that the software needs to be able to cut off search and evalu-\\nate the position of the board at that node. Hence, an evaluation function is\\nused to examine a particular position of the board and estimate how well\\nthe computer is doing, or how likely it is to win from this position. Due to'), Document(metadata={}, page_content='6.2 Game Trees 147\\nthe enormous number of positions that must be evaluated in game playing,\\nthe evaluation function usually needs to be extremely efficient, to avoid\\nslowing down game play.\\nOne question is how the evaluation function will compare two positions. In\\nother words, given positions A and B, what relative values will it give those\\npositions? If A is a clearly better position than B, perhaps A should receive a\\nmuch higher score than B. In general, as we will see elsewhere, to be suc-'), Document(metadata={}, page_content='cessful, the evaluation function does not need to give values that linearly\\nrepresent the quality of positions: T o be effective, it just needs to give a\\nhigher score to a better position.\\nAn evaluation function for a chess game might look at the number of\\npieces, taking into account the relative values of particular pieces, and\\nmight also look at pawn development, control over the center of the board,\\nattack strength, and so on. Such evaluation functions can be extremely'), Document(metadata={}, page_content='complex and, as we will see, are essential to building successful chess-play-\\ning software.\\nEvaluation functions are usually weighted linear functions, meaning that a\\nnumber of different scores are determined for a given position and simply\\nadded together in a weighted fashion. So, a very simplistic evaluation func-\\ntion for chess might count the number of queens, the number of pawns,\\nthe number of bishops, and so on, and add them up using weights to indi-'), Document(metadata={}, page_content='cate the relative values of those pieces:\\nq = number of queens\\nr = number of rooks\\nn = number of knights\\nb = number of bishops\\np = number of pawns\\nscore = 9q + 5r + 3b + 3n + p\\nIf two computer programs were to compete with each other at a game such as\\ncheckers, and the two programs had equivalent processing capabilities and\\nspeeds, and used the same algorithms for examining the search tree, then the\\ngame would be decided by the quality of the programs’ evaluation functions.'), Document(metadata={}, page_content='In general, the evaluation functions for game-playing programs do not\\nneed to be perfect but need to give a good way of comparing two positions\\nto determine which is the better. Of course, in games as complex as chess'), Document(metadata={}, page_content='148 CHAPTER 6 Game Playing\\nand Go, this is not an easy question: two grandmasters will sometimes dif-\\nfer on the evaluation of a position.\\nAs we will see, one way to develop an accurate evaluation function is to\\nactually play games from each position and see who wins. If the play is per-\\nfect on both sides, then this will give a good indication of what the evalua-\\ntion of the starting position should be.\\nThis method has been used successfully for games such as checkers, but for'), Document(metadata={}, page_content='games such as chess and Go, the number of possible positions is so huge\\nthat evaluating even a small proportion of them is not feasible. Hence, it is\\nnecessary to develop an evaluation function that is dynamic and is able to\\naccurately evaluate positions it has never seen before.\\n6.2.3 Searching Game Trees\\nEven for the game of tic-tac-toe, a part of whose game tree is illustrated in\\nFigure 6.1, it can be inefficient for the computer to exhaustively search the'), Document(metadata={}, page_content='tree because it has a maximum depth of 9 and a maximum branching fac-\\ntor of 9, meaning there are approximately 9 /H110038 /H110037 /H11003... /H110032 /H110031 nodes\\nin the tree, which means more than 350,000 nodes to examine. Actually,\\nthis is a very small game tree compared with the trees used in games like\\nchess or Go, where there are many more possible moves at each step and\\nthe tree can potentially have infinite depth.'), Document(metadata={}, page_content='the tree can potentially have infinite depth.\\nIn fact, using exhaustive search on game trees is almost never a good idea\\nfor games with any degree of complexity. Typically, the tree will have very\\nhigh branching factors (e.g., a game tree representing chess has an average\\nbranching factor of 38) and often will be very deep. Exhaustively searching\\nsuch trees is just not possible using current computer technology, and so in'), Document(metadata={}, page_content='this chapter, we will explore methods that are used to prune the game tree\\nand heuristics that are used to evaluate positions.\\nThere is another problem with using exhaustive search to find goal nodes\\nin the game tree. When the computer has identified a goal state, it has sim-\\nply identified that it can win the game, but this might not be the case\\nbecause the opponent will be doing everything he or she can to stop the\\ncomputer from winning. In other words, the computer can choose one arc'), Document(metadata={}, page_content='in the game tree, but the opponent will choose the next one. It may be that\\ndepth-first search reveals a path to a leaf node where the computer wins,\\nbut the computer must also assume that the opponent will be attempting\\nto choose a different path, where the computer loses.'), Document(metadata={}, page_content='6.3 Minimax 149\\nSo, as we see later in this chapter, the computer can use methods like depth-\\nfirst or breadth-first search to identify the game tree, but more sophisti-\\ncated methods need to be used to choose the correct moves.\\n6.3 Minimax\\nWhen evaluating game trees, it is usual to assume that the computer is\\nattempting to maximize some score that the opponent is trying to minimize.\\nNormally we would consider this score to be the result of the evaluation'), Document(metadata={}, page_content='function for a given position, so we would usually have a high positive score\\nmean a good position for the computer, a score of 0 mean a neutral position,\\nand a high negative score mean a good position for the opponent.\\nThe Minimax algorithm is used to choose good moves. It is assumed that a\\nsuitable static evaluation function is available, which is able to give an over-\\nall score to a given position. In applying Minimax, the static evaluator will'), Document(metadata={}, page_content='only be used on leaf nodes, and the values of the leaf nodes will be filtered\\nup through the tree, to pick out the best path that the computer can achieve.\\nThis is done by assuming that the opponent will play rationally and will\\nalways play the move that is best for him or her, and thus worst for the\\ncomputer. The principle behind Minimax is that a path through the tree is\\nchosen by assuming that at its turn (a max node), the computer will choose'), Document(metadata={}, page_content='the move that will give the highest eventual static evaluation, and that at\\nthe human opponent’s turn (a min node), he or she will choose the move\\nthat will give the lowest static evaluation. So the computer’s aim is to max-\\nimize the lowest possible score that can be achieved.\\nFigure 6.2 shows how Minimax works on a very simple game tree. Note that\\nthe best result that max can achieve is a score of 6. If max chooses the left'), Document(metadata={}, page_content='branch as its first choice, then min will inevitably choose the right branch,\\nwhich leaves max a choice of 1 or 3. In this case, max will choose a score of\\n3. If max starts by choosing the right branch, min will have a choice between\\na path that leads to a score of 7 or a path that leads to a score of 6. It will\\ntherefore choose the left branch, leaving max a choice between 2 and 6.\\nFigure 6.2 shows how Minimax can use depth-first search to traverse the'), Document(metadata={}, page_content='game tree. The arrows start from the root node at the top and go down to\\nthe bottom of the left branch.\\nThis leads to a max node, which will get a score of 5. The value 5 is there-\\nfore passed up to the parent of this max node. Following the right path\\nfrom this min node leads to another max node, this time getting a score of'), Document(metadata={}, page_content='150 CHAPTER 6 Game Playing\\n6\\n6\\n6\\n6\\n6\\n62 0\\n0\\n7\\n73\\n3\\n3\\n3125\\n5\\n5\\nMIN\\nMAX\\nMAXSTART\\nFigure 6.2\\nIllustrating how minimax works on a very simple game tree. The arrows show the order in which the nodes are examined by\\nthe algorithm, and the values that are passed through the tree.\\n3. This comes back up to the min node, which now chooses the minimum\\nof 3 and 5, and selects 3. Eventually, having traversed the whole tree, the\\nbest result for max comes back up to the root node: 6.'), Document(metadata={}, page_content='The Minimax function provides a best available score for a given node\\nas follows:\\nFunction minimax (current_node)\\n{\\nif is_leaf (current_node)\\nthen return static_evaluation (current_node);\\nif is_min_node (current_node)\\nthen return min (minimax (children_of \\n(current_node)));\\nif is_max_node (current_node)\\nthen return max (minimax (children_of \\n(current_node)));\\n// this point will never be reached since\\n// every node must be a leaf node, a min node or a\\n// max node.\\n}'), Document(metadata={}, page_content='6.3 Minimax 151\\nThis is a recursive function because to evaluate the scores for the children\\nof the current node, the Minimax algorithm must be applied recursively to\\nthose children until a leaf node is reached.\\nMinimax can also be performed nonrecursively, starting at the leaf nodes\\nand working systematically up the tree, in a reverse breadth-first search.\\n6.3.1 Bounded Lookahead\\nMinimax, as we have defined it, is a very simple algorithm and is unsuitable'), Document(metadata={}, page_content='for use in many games, such as chess or Go, where the game tree is\\nextremely large. The problem is that in order to run Minimax, the entire\\ngame tree must be examined, and for games such as chess, this is not possi-\\nble due to the potential depth of the tree and the large branching factor.\\nIn such cases, bounded lookaheadis very commonly used and can be com-\\nbined with Minimax. The idea of bounded lookahead is that the search tree is'), Document(metadata={}, page_content='only examined to a particular depth. All nodes at this depth are considered to\\nbe leaf nodes and are evaluated using a static evaluation function. This corre-\\nsponds well to the way in which a human plays chess. Even the greatest grand-\\nmasters are not able to look forward to see every possible move that will occur\\nin a game. Chess players look forward a few moves, and good chess players\\nmay look forward a dozen or more moves. They are looking for a move that'), Document(metadata={}, page_content='leads to as favorable a position as they can find and are using their own static\\nevaluator to determine which positions are the most favorable.\\nHence, the Minimax algorithm with bounded lookahead is defined as follows:\\nFunction bounded_minimax (current_node, max_depth)\\n{\\nif is_leaf (current_node)\\nthen return static_evaluation (current_node);\\nif depth_of (current_node) == max_depth\\nthen return static_evaluation (current_node);\\nif is_min_node (current_node)\\nthen return min (minimax (children_of'), Document(metadata={}, page_content='then return min (minimax (children_of \\n(current_node)));\\nif is_max_node (current_node)\\nthen return max (minimax (children_of \\n(current_node)));\\n// this point will never be reached since\\n// every node must be a leaf node, a min node or a\\n// max node.\\n}'), Document(metadata={}, page_content='152 CHAPTER 6 Game Playing\\nFigure 6.3\\nChess position with black\\nto move\\nIn fact, it is not necessarily sensible to apply a fixed cut-off point for search.\\nThe reason for this can be seen from the chess position shown in Figure 6.3.\\nIf bounded Minimax search cut off search at this node, it might consider\\nthe position to be reasonably even because the two players have the same\\npieces, which are roughly equally well developed. In fact, although it is'), Document(metadata={}, page_content='black’s turn to move, white will almost certainly take black’s queen after\\nthis move, meaning that the position is extremely strong for white.\\nThis problem must be avoided if a computer program is to play chess or\\nany other game successfully. One way to avoid the problem is to only cut off\\nsearch at positions that are deemed to be quiescent. A quiescent position is\\none where the next move is unlikely to cause a large change in the relative'), Document(metadata={}, page_content='positions of the two players. So, a position where a piece can be captured\\nwithout a corresponding recapture is not quiescent.\\nAnother problem with bounded Minimax search is the horizon problem.\\nThis problem involves an extremely long sequence of moves that clearly lead\\nto a strong advantage for one player, but where the sequence of moves,\\nalthough potentially obvious to a human player, takes more moves than is\\nallowed by the bounded search. Hence, the significant end of the sequence'), Document(metadata={}, page_content='has been pushed over the horizon. This was a particular problem for Chi-\\nnook, the checkers-playing program that we learn more about in Section 6.5.\\nThere is no universal solution to the horizon problem, but one method to\\nminimize its effects is to always search a few ply deeper when a position is'), Document(metadata={}, page_content='6.4 Alpha–Beta Pruning 153\\nfound that appears to be particularly good. The singular-extension heuris-\\ntic is defined as follows: if a static evaluation of a move is much better than\\nthat of other moves being evaluated, continue searching.\\n6.4 Alpha–Beta Pruning\\nBounded lookahead can help to make smaller the part of the game tree that\\nneeds to be examined. In some cases, it is extremely useful to be able to\\nprune sections of the game tree. Using alpha–beta pruning, it is possible to'), Document(metadata={}, page_content='remove sections of the game tree that are not worth examining, to make\\nsearching for a good move more efficient.\\nThe principle behind alpha–beta pruning is that if a move is determined to\\nbe worse than another move that has already been examined, then further\\nexamining the possible consequences of that worse move is pointless.\\nConsider the partial game tree in Figure 6.4.\\nThis very simple game tree has five leaf nodes. The top arc represents a'), Document(metadata={}, page_content='choice by the computer, and so is a maximizing level (in other words, the\\ntop node is a max node). After calculating the static evaluation function for\\nthe first four leaf nodes, it becomes unnecessary to evaluate the score for\\nthe fifth. The reason for this can be understood as follows:\\nIn choosing the left-hand path from the root node, it is possible to achieve\\na score of 3 or 5. Because this level is a minimizing level, the opponent can'), Document(metadata={}, page_content='be expected to choose the move that leads to a score of 3. So, by choosing\\nthe left-hand arc from the root node, the computer can achieve a score of 3.\\nBy choosing the right-hand arc, the computer can achieve a score of 7 or 1,\\nor a mystery value. Because the opponent is aiming to minimize the score,\\n53 7 1\\nFigure 6.4\\nA partial game tree'), Document(metadata={}, page_content='154 CHAPTER 6 Game Playing\\nhe or she could choose the position with a score of 1, which is worse than\\nthe value the computer could achieve by choosing the left-hand path. So,\\nthe value of the rightmost leaf node doesn’t matter—the computer must\\nnot choose the right-hand arc because it definitely leads to a score ofat best\\n1 (assuming the opponent does not irrationally choose the 7 option).\\n6.4.1 The Effectiveness of Alpha–Beta Pruning'), Document(metadata={}, page_content='6.4.1 The Effectiveness of Alpha–Beta Pruning\\nIn this contrived example, alpha–beta pruning removes only one leaf node\\nfrom the tree, but in larger game trees, it can result in fairly valuable reduc-\\ntions in tree size. However, as Winston (1993) showed, it will not necessar-\\nily remove large portions of a game tree. In fact, in the worst case,\\nalpha–beta pruning will not prune any searches from the game tree, but\\neven in this case it will compute the same result as Minimax and will not'), Document(metadata={}, page_content='perform any less efficiently.\\nThe alpha–beta pruning method provides its best performance when the\\ngame tree is arranged such that the best choice at each level is the first one\\n(i.e., the left-most choice) to be examined by the algorithm. With such a\\ngame tree, a Minimax algorithm using alpha–beta cut-off will examine a\\ngame tree to double the depth that a Minimax algorithm without\\nalpha–beta pruning would examine in the same number of steps.\\nThis can be shown as follows:'), Document(metadata={}, page_content='This can be shown as follows:\\nIf a game tree is arranged optimally, then the number of nodes that must\\nbe examined to find the best move using alpha–beta pruning can be\\nderived as follows:\\nwhere\\nb = branching factor of game tree\\nd = depth of game tree\\ns = number of nodes that must be examined\\nThis means that approximately\\ns = 2b\\nd/2\\nWithout alpha–beta pruning, where all nodes must be examined:\\ns = bd\\nS b if d is even\\nb b if d is odd\\nd\\ndd= −\\n+−\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4 +() −()\\n21\\n1\\n2\\n12 12\\n/\\n//'), Document(metadata={}, page_content='6.4 Alpha–Beta Pruning 155\\nHence, we can consider that using alpha–beta pruning reduces the effective\\nbranching factor from b to /H20857b/H33526, meaning that in a fixed period of time,\\nMinimax with alpha–beta pruning can look twice as far in the game tree as\\nMinimax without pruning.\\nThis represents a significant improvement—for example, in chess it\\nreduces the effective branching factor from around 38 to around 6—but it\\nmust be remembered that this assumes that the game tree is arranged opti-'), Document(metadata={}, page_content='mally (such that the best choice is always the left-most choice). In reality, it\\nmight provide far less improvement.\\nIt was found that in implementing the Deep Blue chess computer (see Sec-\\ntion 6.6), use of the alpha–beta method did in fact reduce the average\\nbranching factor of the chess game tree from 38 to around 6.\\n6.4.2 Implementation\\nThe alpha-beta pruning algorithm is implemented as follows:\\n■ The game tree is traversed in depth-first order. At each non-leaf'), Document(metadata={}, page_content='node a value is stored. For max nodes, this value is called alpha,\\nand for min nodes, the value is beta.\\n■ An alpha value is the maximum (best) value found so far in the\\nmax node’s descendants.\\n■ A beta value is minimum (best) value found so far in the min\\nnode’s descendants.\\nIn the following pseudo-code implementation, we use the function call\\nbeta_value_of (min_ancestor_of (current_node)) , which returns the beta\\nvalue of some min node ancestor of the current node to see how it com-'), Document(metadata={}, page_content='pares with the alpha value of the current node. Similarly,\\nalpha_value_of\\n(max_ancestor_of (current_node)) returns the alpha value of some max\\nnode ancestor of the current node in order that it be compared with the\\nbeta value of the current node.\\nFunction alpha_beta (current_node)\\n{\\nif is_leaf (current_node)\\nthen return static_evaluation (current_node);\\nif is_max_node (current_node) and\\nalpha_value_of (current_node) >=\\nbeta_value_of (min_ancestor_of (current_node))'), Document(metadata={}, page_content='beta_value_of (min_ancestor_of (current_node))\\nthen cut_off_search_below (current_node);'), Document(metadata={}, page_content='156 CHAPTER 6 Game Playing\\nif is_min_node (current_node) and\\nbeta_value_of (current_node) <=\\nalpha_value_of (max_ancestor_of (current_node))\\nthen cut_off_search_below (current_node);\\n}\\nT o avoid searching back up the tree for ancestor values, values are propa-\\ngated down the tree as follows:\\n■ For each max node, the minimum beta value for all its min node\\nancestors is stored as beta.\\n■ For each min node, the maximum alpha value for all its max node\\nancestors is stored as alpha.'), Document(metadata={}, page_content='ancestors is stored as alpha.\\n■ Hence, each non-leaf node will have a beta value and an alpha\\nvalue stored.\\n■ Initially, the root node is assigned an alpha value of negative infin-\\nity and a beta value of infinity.\\nSo, the \\nalpha_beta function can be modified as follows. In the following\\npseudo-code implementation, the variable children is used to represent all\\nof the children of the current node, so the following line:\\nalpha = max (alpha, alpha_beta (children, alpha, beta));'), Document(metadata={}, page_content='means that alpha is set to the greatest of the current value of alpha, and the\\nvalues of the current node’s children, calculated by recursively calling\\nalpha_beta.\\nFunction alpha_beta (current_node, alpha, beta)\\n{\\nif is_root_node (current_node)\\nthen\\n{\\nalpha = -infinity\\nbeta = infinity\\n}\\nif is_leaf (current_node)\\nthen return static_evaluation (current_node);\\nif is_max_node (current_node)\\nthen\\n{\\nalpha = max (alpha, alpha_beta (children, alpha, beta));\\nif alpha >= beta'), Document(metadata={}, page_content='if alpha >= beta\\nthen cut_off_search_below (current_node);\\n}'), Document(metadata={}, page_content='6.4 Alpha–Beta Pruning 157\\n12 3 457 10 2615\\nde f g\\ncb\\na\\nMAX\\nMIN\\nMAX\\nFigure 6.5\\nA simple game tree\\nif is_min_node (current_node)\\nthen\\n{\\nbeta = min (beta, alpha_beta (children, alpha, beta));\\nif beta <= alpha\\nthen cut_off_search_below (current_node);\\n}\\n}\\nT o see how alpha–beta pruning works in practice, let us examine the game\\ntree shown in Figure 6.5.\\nThe non-leaf nodes in the tree are labeled from a to g, and the leaf nodes'), Document(metadata={}, page_content='have scores assigned to them by static evaluation: a is a max node; b and c\\nare min nodes; and d, e, f, and g are max nodes.\\nFollowing the tree by depth-first search, the first step is to follow the path\\na,b,d and then to the three children of d. This gives an alpha value for d of\\n3. This is passed up to b, which now has a beta value of 3, and an alpha\\nvalue that has been passed down from a of negative infinity.\\nNow, the first child of e is examined and has a score of 4. In this case, clearly'), Document(metadata={}, page_content='there is no need to examine the other children of e because the minimizing\\nchoice at node b will definitely do worse by choosing e rather than d. So cut-\\noff is applied here, and the nodes with scores of 5 and 7 are never examined.\\nThe full analysis of the tree is shown in Table 6.1, which shows how the\\nscores move through the tree from step to step of the process.'), Document(metadata={}, page_content='158 CHAPTER 6 Game Playing\\nTABLE 6.1 Analysis of alpha–beta pruning for the game tree in Figure 6.5\\nStep Node Alpha Beta Notes\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\na\\nb\\nd\\nd\\nd\\nd\\nb\\ne\\ne\\na\\nc\\nf\\nc\\n/H11002/H11009\\n/H11002/H11009\\n/H11002/H11009\\n1\\n2\\n3\\n/H11002/H11009\\n/H11002/H11009\\n4\\n3\\n3\\n3\\n3\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n3\\n3\\n3\\n/H11009\\n/H11009\\n/H11009\\n3\\nAlpha starts as /H11002/H11009and beta starts as /H11009.\\nAt this stage, we have examined the three children of d'), Document(metadata={}, page_content='and have obtained an alpha value of 3, which is passed\\nback up to node b.\\nAt this min node, we can clearly achieve a score of 3 or\\nbetter (lower). Now we need to examine the children of\\ne to see if we can get a lower score.\\nCUT-OFF . A score of 4 can be obtained from the first child\\nof e. Min clearly will do better to choose d rather than e\\nbecause if he chooses e, max can get at least 4, which is\\nworse for min than 3. Hence, we can now ignore the\\nother children of e.'), Document(metadata={}, page_content='other children of e.\\nThe value of 3 has been passed back up to the root node,\\na. Hence, max now knows that he can score at least 3. He\\nnow needs to see if he can do better.\\nWe now examine the three children of f and find that\\nnone of them is better than 3. So, we pass back a value of\\n3 to c.\\nCUT-OFF . Max has already found that by taking the left-\\nhand branch, he can achieve a score of 3. Now it seems\\nthat if he chooses the right-hand branch, min can choose'), Document(metadata={}, page_content='f, which will mean he can only achieve a score of 2. So\\ncut-off can now occur because there is no need to exam-\\nine g or its children.'), Document(metadata={}, page_content='6.5 Checkers 159\\n43 87 21 6 5\\nMAX\\nMIN\\nMAX\\nFigure 6.6\\nA game tree optimized for alpha–beta search\\n1Draughts is another name for the common variety of checkers, which is played on an 8/H110038\\nboard. International checkers is another variety, which is played on a 10/H1100310 board.\\nHence, out of the 12 leaf nodes in the tree, the algorithm has needed to\\nexamine only 7 to conclude that the best move for max to make is b, in\\nwhich case min will choose d and max will choose the right hand node,'), Document(metadata={}, page_content='ending with a static evaluation of 3.\\nAt this stage, we can easily see that this is the right answer because if max\\nchooses c, then min will clearly choose f, resulting in max being able to\\nachieve a score of only 2.\\nAn ideal tree for alpha–beta pruning is shown in Figure 6.6.\\nIn this case, the Minimax algorithm with alpha–beta cut-off will need to\\nexamine only five of the eight leaf nodes.\\n6.5 Checkers\\nThe game of checkers (or draughts 1) has proved to be an excellent chal-'), Document(metadata={}, page_content='lenge for the methods of Artificial Intelligence and one that has been met\\nwith a reasonable degree of success.'), Document(metadata={}, page_content='160 CHAPTER 6 Game Playing\\nIn his paper of 1959, Some Studies in Machine Learning Using the Game of\\nCheckers, Arthur Samuel described a computer system that could play\\ncheckers to a reasonable level, using Minimax with alpha–beta pruning.\\nThis system used a weighted linear function of a variety of heuristics that\\nmeasured how strong a particular position was.\\nIf a particular strategy was played and found to lose, the system would'), Document(metadata={}, page_content='adjust its weights to avoid making such a mistake in the future. In this way,\\nby playing many games it was able to learn to play checkers to a fairly high\\nlevel. Samuel’s system was a significant milestone in the Artificial Intelli-\\ngence research. Unfortunately, it was widely reported by the media that\\nSamuel’s system had solved the game of checkers and was able to beat a\\ntop-ranked human player. Neither of these claims was true, but it caused'), Document(metadata={}, page_content='the Artificial Intelligence community to believe that there was nothing\\nmore to be learned about checkers, and so until Chinook, very little further\\nresearch was done on the game.\\n6.5.1 Chinook\\nChinook is a checkers-playing computer that was developed by a team led\\nby Dr. Jonathan Schaeffer of the University of Alberta in Canada.\\nChinook uses Minimax search with alpha–beta pruning. It also applies iter-\\native deepening (see Section 4.11) and a number of heuristics to maximize'), Document(metadata={}, page_content='the efficiency of the search and to minimize the size of the game tree that the\\nprogram needs to examine for each move. Chinook also has a database of\\nendgames consisting of hundreds of billions of possible positions. Such a\\ndatabase would not be practical in chess, due to the enormous number of\\npossible positions, but is possible with checkers because there are fewer legal\\nsquares on the board (32 instead of 64), fewer piece types (2 as opposed to'), Document(metadata={}, page_content='6), and a smaller average branching factor (8 compared with 38).\\nChinook also has a large amount of knowledge about the game and built-in\\nheuristics to help it evaluate positions and choose better moves.\\nOn average, Chinook examines the game tree to a depth of 20 ply and is\\nable to examine around 1000 positions per second per MIPS (millions of\\ninstructions per second) of processing power.\\nIn 1990, Chinook was beaten by the world champion checkers player,'), Document(metadata={}, page_content='Marion Tinsley, by a margin of 7.5 to 6.5. Tinsley had been world cham-'), Document(metadata={}, page_content='6.5 Checkers 161\\npion for 40 years and is commonly recognized as the greatest checkers\\nplayer of all time.\\nIn 1992, Tinsley beat Chinook in the World Checkers Championship. Forty\\ngames were played; Chinook won two and Tinsley won four. Thirty-three\\ngames were drawn. This was the first World Championship match of any\\ngame in which a computer had taken part. In a rematch in 1994, Tinsley\\nand Chinook played six games, all of which were drawn, after which Tins-'), Document(metadata={}, page_content='ley resigned due to ill health, surrendering the world championship title to\\nChinook. Sadly, Marion Tinsley succumbed to cancer in 1995 before a fur-\\nther rematch could be played. Chinook did beat Don Lafferty, who was\\nthen the world’s second-best human player, in 1995 to retain the title. The\\ncurrent world champion, Ron King, has been beaten many times by Chi-\\nnook, but never in a championship match.\\nThe version of Chinook that played in 1994 was a significantly more pow-'), Document(metadata={}, page_content='erful player (running on a much more powerful computer and using\\nmore sophisticated algorithms). One might imagine that with increasing\\ncomputer power, a checkers computer could be developed that would\\n“solve” the game of checkers—that is, a computer program that could\\nexamine the entire game tree for checkers and determine the outcome\\nbefore a game was started. This is a difficult challenge and one that does\\nnot seem likely to be achieved in the near future. The main problem is'), Document(metadata={}, page_content='that as the depth of analysis increases, the improvement in play increases\\nless than linearly, meaning that a much deeper analysis is needed to pro-\\nvide small improvements (compare with chess—Section 6.6). Addition-\\nally, although the complete game tree for checkers would be vastly smaller\\nthan that for chess, it would still be extremely large (around 10\\n20 possible\\nmoves to examine).\\n6.5.2 Chinook’ s Databases\\nOne of the secrets of Chinook’s success at playing checkers lies in its data-'), Document(metadata={}, page_content='bases of endgame positions and opening moves. Chinook uses the opening\\nbook of a commercial checkers program, Colossus. This book had been\\ndeveloped over years, mainly from the published literature on the game.\\nChinook spent a number of months examining each position in the book\\nto a depth of at least 19 ply to ensure that the opening moves were correct.\\nThis process was also used to identify unusual opening moves that might\\nsurprise an experienced player such as Tinsley.'), Document(metadata={}, page_content='162 CHAPTER 6 Game Playing\\nIn fact, to make Chinook play more imaginatively, the developers chose to\\navoid the use of the opening book in most situations. Instead, they built an\\nanti-book that contains moves that Chinook should avoid, and in most\\nother cases, Chinook uses search to decide on good moves to make. Schaef-\\nfer and his team have found that this actually tends to lead to better play\\nthan following the opening book.'), Document(metadata={}, page_content='than following the opening book.\\nThe main power of Chinook lies in its databases of endgame positions. In\\n1992, Chinook’s database contained an evaluation (win, lose, or draw) for\\nevery possible position involving seven pieces—around 40 billion positions.\\nBy 1994, the databases had been extended to cover all positions involving\\neight pieces—another 440 billion positions.\\nIn most games, within a few moves from the start, Chinook’s search has'), Document(metadata={}, page_content='reached a position stored in its database, meaning that it can usually deter-\\nmine the outcome of a game within 10 moves of the start. Most possible\\ngames lead to a draw, but Chinook has been programmed to look for draws\\nthat involve complicated strategies that the opponent is likely to miss.\\nHence, in evaluating the game tree, Chinook does not give all draws a score\\nof zero, but gives them a score depending on how likely it is that the oppo-\\nnent will make a mistake and lose the game.'), Document(metadata={}, page_content='nent will make a mistake and lose the game.\\n6.5.3 Chinook’ s Evaluation Function\\nChinook uses a linear weighted static evaluation function, based on a num-\\nber of heuristics. These heuristics include piece count, king count, balance\\n(the distribution of pieces between the left and right sides of the board), the\\nnumber of trapped kings, and so on.\\nChinook divides the game up into four phases, which are identified by the\\nnumber of pieces left on the board.\\nPhase Number of pieces\\n1 20–24\\n2 14–19'), Document(metadata={}, page_content='Phase Number of pieces\\n1 20–24\\n2 14–19\\n3 10–13\\n4 less than 10'), Document(metadata={}, page_content='6.5 Checkers 163\\nChinook has different sets of weights for its 25 heuristic values for each\\nphase of the game, so that, for example, it places more importance on kings\\nin later phases of the game than in the first phase.\\n6.5.4 Forward Pruning\\nAnother technique used by Chinook, and other game-playing systems, is\\nforward pruning. The idea behind forward pruning is that if a line of play\\n(i.e., a path through the game tree) is being examined in which a number of'), Document(metadata={}, page_content='pieces are lost without any way to recapture, then examination of the path\\nis terminated and the tree is pruned at this point.\\nHence, unlike alpha–beta pruning, no static evaluation is needed. It is pos-\\nsible, but unlikely, that such pruning could miss a useful strategy, but this is\\nless likely with checkers than with chess, where sacrifices of important\\npieces can often lead to a win.\\n6.5.5 Limitations of Minimax\\nSchaeffer’s work with Chinook has revealed some serious limitations with'), Document(metadata={}, page_content='alpha–beta search.\\nIn some cases, Minimax with alpha–beta pruning might show that a num-\\nber of different paths lead to a draw. As was mentioned in the previous sec-\\ntion, some draws can be harder to achieve than others. In particular, one\\nposition might lead to a draw no matter what moves a player makes,\\nwhereas another position might lead to a draw only if a player makes the\\ncorrect choice on each move for 40 moves. Clearly the latter draw is much'), Document(metadata={}, page_content='harder to achieve. Chinook was programmed to take advantage of this,\\nafter it drew a game that it could probably have won because it chose a draw\\nthat was easy for its opponent to achieve and neglected a draw that looked\\nvery much like a win to most human players.\\nSimilarly, in one game against Marion Tinsley, Chinook discovered a long,\\ncomplicated win for Tinsley, and made a sacrifice to try to avoid this. As a\\nresult, it lost the game easily. In fact, it is quite possible that Tinsley would'), Document(metadata={}, page_content='have missed the win without the sacrifice.\\nA modified version of Minimax would take advantage of the difficulty of\\nparticular paths through the game tree, as well as the final outcome.'), Document(metadata={}, page_content='164 CHAPTER 6 Game Playing\\n6.5.6 Blondie 24\\nIn 2000, a new kind of checkers program was developed, called Blondie 24.\\nUnlike Chinook, Blondie 24 does not have any knowledge of the game\\n(other than the basic rules) built in. The program, developed by Dr. David\\nFogel of Natural Selection Inc. based in San Diego, uses evolutionary tech-\\nniques (described in Chapter 13) to develop neural networks (see Chapter\\n11) that can learn how to play checkers, and how to win.'), Document(metadata={}, page_content='In fact, Blondie uses standard Minimax evaluation to search the game tree.\\nThe evolutionary method was used to develop the static evaluation function.\\nThe static evaluation function is evaluated by neural networks that were\\ndeveloped by starting with programs that would play the game randomly\\nand breeding together the ones that played the most successfully. Repeating\\nthis over many generations, and allowing random mutations, led to the'), Document(metadata={}, page_content='final version of the software, which proved able to beat many human play-\\ners. This program is not nearly at the level of Chinook, which currently is\\nthe checkers world champion, but it does represent a fascinating example\\nof a combination of artificial life techniques with Artificial Intelligence,\\ngenerating a solution that performs extremely well, without ever having\\nhad any help from humans.\\n6.6 Chess\\nOne of the best-known applications of Artificial Intelligence is the develop-'), Document(metadata={}, page_content='ment of computer programs that can play chess against human players.\\nChess programs typically use Minimax algorithms with alpha–beta prun-\\ning and are almost always programmed with large libraries of opening\\nmoves (similar to the databases used in Chinook for endgame moves).\\nIt has been shown that there is a more or less linear relationship between\\nthe depth to which a program can examine the game tree for chess and its\\nskill in playing the game. In other words, a system that is able to examine'), Document(metadata={}, page_content='the tree to 12 ply is likely to beat a system that can only examine the tree to\\n10 ply (see Newborn 2002, pages 291–294). As mentioned in Section 6.5,\\nthis is not true for checkers, where a law of diminishing returns applies. In\\nfact, Schaeffer (1991) claims that this relationship is the same for chess and\\ncheckers, but that the relationship is more or less linear up to a depth of\\naround 15, tailing off after this. Because the best chess programs tend to'), Document(metadata={}, page_content='6.7 Go 165\\nanalyze to a depth of around 12, they have not yet reached the nonlinear\\nstage of the graph.\\nAs a result of this relationship, great advances in the ability of computers to\\nplay chess have been made simply as a result of improvement in speed of\\ncomputers and in the use of parallel computing techniques.\\nIn 1997, a chess-playing computer system developed by IBM called Deep\\nBlue beat world champion Garry Kasparov, who is generally considered to be'), Document(metadata={}, page_content='the strongest chess player of all time. The final score after six games was 3.5 to\\n2.5. Kasparov won one game, Deep Blue won two, and three were drawn.\\nIn 2002, Vladimir Kramnik, one of the highest-ranking chess players in the\\nworld, played a match against a German computer program, Deep Fritz.\\nThe eight-game match ended in a draw. There are a number of other chess\\ncomputers that can play at a comparable level, and at the time of writing'), Document(metadata={}, page_content='this book, the competition between human and computer chess players is\\nvery close. In January 2003, Garry Kasparov played a six-game match\\nagainst Deep Junior, an Israeli computer program. The match ended in a\\ndraw, with Kasparov and Deep Junior each winning one game, and the\\nother four games being drawn.\\nIt is certainly not the case as it is with games such as Othello and checkers\\nthat the best computers are unbeatable by humans, and neither is it the case'), Document(metadata={}, page_content='as with Go or bridge that computers cannot beat the best humans.\\nIt seems likely that given the linear relationship between depth of search\\nand quality of play that with the improvement in computer power, it will\\nsoon be the case that the best computers are unbeatable by even the very\\nbest human players.\\n6.7 Go\\nGo is an ancient Japanese game, which is considered by many to be the final\\nfrontier for research in game-playing computers. It is certainly more com-'), Document(metadata={}, page_content='plex than chess: Go is played on a 19/H1100319 board, with far greater freedom of\\nchoice in playing most moves than chess, resulting in an enormous branch-\\ning factor (on average around 360, compared with around 38 for chess).\\nIt has thus been impossible to develop a system that searches the game tree\\nfor Go in the same way as for chess or checkers. Systems have been developed\\nthat play Go, but the best of these can compete only at the level of a weak club'), Document(metadata={}, page_content='166 CHAPTER 6 Game Playing\\nplayer. None have shown any possibility yet of approaching the level of the\\nbest human players. Methods usually involve extremely selective search—\\nusing constraints to eliminate many options at each stage of the game tree.\\nSome success has also been had with pattern recognition techniques.\\nA Taiwanese business man offered one million dollars to the first person\\nwho could write a computer program that could beat a professional Go'), Document(metadata={}, page_content='player. Although the business man died in 1997, his bet looks as though it\\nwill remain safe for a while yet.\\n6.7.1 Go-Moku\\nGo-moku is a far simpler version of Go, usually played on a 15 /H1100315 board.\\nThis game is often used for teaching and illustrating Artificial Intelligence\\nbecause it is fairly simple. Alternate players place stones on the board, and\\nthe first player to place five stones in a row wins the game.\\nGo-Moku belongs to a group of games including Connect-4 and tic-tac-toe'), Document(metadata={}, page_content='(noughts and crosses) that have been solved. That is to say, the complete\\ngame tree has been evaluated such that the outcome of the game can be\\ndetermined from the start. Assuming both players play correctly, the player\\nwho starts the game will always win.\\n6.8 Othello (Reversi)\\nOthello is a simpler game than chess, and typical Othello computer pro-\\ngrams can now examine the search tree to a depth of around 50 ply. The best'), Document(metadata={}, page_content='human players are now unable to beat this level of play. In 1997, the human\\nworld champion Takeshi Murakami of Japan was beaten 6–0 by an American\\nOthello computer program, developed by Michael Buro, called Logistello.\\n6.9 Games of Chance\\nUnlike the games we have considered so far, many games involve an ele-\\nment of chance—often introduced by a dice roll or a draw of cards. With\\nthe exception of simple games like Chutes and Ladders and Ludo, most'), Document(metadata={}, page_content='games of chance still involve a reasonable degree of skill because the chance\\nelement merely restricts the choices that can be made.'), Document(metadata={}, page_content='6.10 Chapter Summary 167\\nGames such as backgammon, scrabble, and bridge are popular games that\\ninvolve chance. Computer programs have been developed that can play\\nbackgammon and Scrabble at a level where they can beat all but the best\\nhuman players in the world. Bridge is rather more complex, with its bidding\\nsystem presenting real problems for Artificial Intelligence system develop-\\ners. Bridge systems have been developed that can play at an intermediate'), Document(metadata={}, page_content='level but are not yet close to playing at the level of the best human players.\\n6.9.1 Expectiminimax\\nExpectiminimax is a version of the Minimax algorithm that has been extended\\nto take into account the probability of each successor node being reached. In\\ngames that involve the throw of a single die, the successor nodes at each ply will\\nall have equal probabilities (one-sixth), but in more complex games, the prob-\\nabilities are not so straightforward. For example, in backgammon, where two'), Document(metadata={}, page_content='dice are rolled for each move (or rather, for each ply in the game tree), the like-\\nlihood of achieving a double (1 and 1, 2 and 2, 3 and 3, 4 and 4, 5 and 5, or 6\\nand 6) is 1 in 36, whereas the likelihood of rolling any other pair is 1 in 18.\\nRather than giving each position in the game tree a particular Minimax\\nvalue, the Expectiminimax algorithm, which is described in more detail in\\nRussell and Norvig (1995) assigns an expected value to each node, which is'), Document(metadata={}, page_content='the average value that could be obtained on the node, taking into account\\nthe probabilities of each possible outcome.\\n6.10 Chapter Summary\\n■ Game trees can be used to represent two-player games.\\n■ Searching game trees is very hard for all but the simplest games.\\n■ Minimax is an algorithm that identifies the best move to make in a\\ntwo-player game with perfect knowledge, assuming the entire tree\\ncan be examined.\\n■ When the entire tree cannot be examined, a static evaluator is'), Document(metadata={}, page_content='needed that can assign a score to any given position according to\\nhow well each player is doing and how likely each player is to win\\nthe game from that position.\\n■ Alpha–beta pruning enables Minimax to run more efficiently\\nbecause it removes unnecessary branches from the game tree.'), Document(metadata={}, page_content='168 CHAPTER 6 Game Playing\\n■ In the best case, alpha–beta pruning enables Minimax to search the\\ngame tree to double the depth that it would be able to search with-\\nout pruning.\\n■ Games such as Go-moku and tic-tac-toe have been solved, mean-\\ning that the result of a game is known from the start.\\n■ Computers are able to beat the best players in the world at games\\nsuch as Othello and checkers.\\n■ Computers cannot compete with humans at games such as Go\\nand bridge.'), Document(metadata={}, page_content='and bridge.\\n■ Although Deep Blue beat Garry Kasparov at chess in 1997, comput-\\ners and humans are fairly evenly matched at chess at the moment.\\n6.11 Review Questions\\n6.1 Discuss the current state of the art of game-playing computer sys-\\ntems in relation to the following games: chess, checkers, Go, bridge,\\nOthello, tic-tac-toe. What advances are likely in the near future?\\nDo you believe there are any fundamental limitations?\\n6.2 Discuss the approach you would take to building a system for play-'), Document(metadata={}, page_content='ing Scrabble or another word game of the sort. What limitations\\ndoes your system have? How likely do you think it is that your sys-\\ntem would be able to beat the best human players in the world?\\n6.3 What problems did the developers of Chinook face? What new\\ntechniques did they add to simple Minimax with alpha–beta prun-\\ning? Would these techniques extend well to other games?\\n6.4 Explain why the alpha–beta procedure will always generate the'), Document(metadata={}, page_content='same answer as Minimax without pruning. Why is it useful?\\n6.5 Show the steps that would be taken in running the Minimax algo-\\nrithm on the game tree in Figure 6.7. Now run through the same\\ntree using alpha–beta pruning. How do the two compare?\\n6.6 Why might it be particularly difficult to program a computer to\\nsuccessfully play card games like bridge or poker? What sort of\\nalgorithms might you use to play these games?\\n6.7 What does it mean to say that a game has been solved? How likely is'), Document(metadata={}, page_content='it that games like Go and chess will ever be solved? Is it always the'), Document(metadata={}, page_content='6.12 Exercises 169\\n54 10 89 67 37 28 19 02\\na\\nbc\\ngfed\\nhi j k l m no\\ncase that the player who goes first will win a game that has been\\nsolved? Even if both players play correctly?\\n6.8 Most commercially available chess programs for home users are\\ndesigned to play at a range of levels from beginner up to grand-\\nmaster. Consider the additional difficulties involved in program-\\nming a computer to play suboptimally. Would alpha–beta pruning\\nstill be appropriate? What methods might a programmer use to'), Document(metadata={}, page_content='program the computer to play over such a range of abilities?\\n6.12 Exercises\\n6.1 For a game tree of depth d, and branching factor b, show that iter-\\native deepening does not increase by a great deal the number of\\nstatic evaluations needed to examine the tree.\\n6.2 Write an algorithm in pseudo-code, or a programming language of\\nyour choice, that evaluates a position in tic-tac-toe. Y our static\\nFigure 6.7\\nGame tree for question 6.5'), Document(metadata={}, page_content='170 CHAPTER 6 Game Playing\\nevaluator should give 0 for a drawn position, 1 for a won position\\nfor crosses, /H110021 for won position for noughts.\\n6.3 Extend the algorithm you designed for Exercise 6.2 so that it is able\\nto evaluate positions that are nonterminal—in other words, posi-\\ntions where the game has not yet finished. Y our score should be\\npositive for an advantage to crosses, and negative for an advantage\\nto noughts.\\n6.4 Implement a Minimax algorithm using your static evaluator for'), Document(metadata={}, page_content='tic-tac-toe, and write a simple program that plays the game. Have\\nthe program output how many nodes in the game tree it had to\\nexamine as well as its choice of move.\\n6.5 Add alpha–beta pruning to your program, and see what difference\\n(if any) it makes to the number of nodes the program has to exam-\\nine when playing a game.\\n6.6 Implement an Expectiminimax algorithm for a game of chance\\n(you might use backgammon, or another dice game).'), Document(metadata={}, page_content='(you might use backgammon, or another dice game).\\n6.7 Is it possible to add alpha–beta pruning to your Expectiminimax\\nprogram? If so, do so. If not, can you find another way of pruning\\nthe tree that improves the performance of the program? How can\\nyou tell if it is improving the performance?\\n6.13 Further Reading\\nThere is a great deal of fascinating literature on the subject of game playing\\nusing Artificial Intelligence. The Chinook website is well worth visiting and'), Document(metadata={}, page_content='contains a great deal of insight into the way game-playing systems are\\ndeveloped and improved. It can be found using any search engine.\\nArthur Samuel’s articles on his checkers-playing system are also worth reading.\\nA number of books and articles have been published on the subject of Deep\\nBlue and other chess computers. Monty Newborn’s book does not contain\\na great deal of computer science but does make a fascinating read, particu-\\nlarly for anyone interested in the game of chess.'), Document(metadata={}, page_content='larly for anyone interested in the game of chess.\\nBlondie 24: Playing at the Edge of AI by David B. Fogel (2001 – Morgan\\nKaufmann)\\nBehind Deep Blue: Building the Computer That Defeated the World Chess\\nChampion by Feng-Hsiung Hsu (2002 – Princeton University Press)'), Document(metadata={}, page_content='6.13 Further Reading 171\\nAn analysis of alpha beta pruning , by Donald Knuth and R. W. Moore \\n(1975 - in Artificial Intelligence, Vol. 6(4), pp. 293–326)\\nDeep Blue: An Artificial Intelligence Milestone by Monty Newborn (2003 –\\nSpringer V erlag)\\nKasparov Versus Deep Blue: Computer Chess Comes of Age by Monty New-\\nborn (1997 – Springer V erlag)\\nKasparov and Deep Blue by Bruce Pandolfini (1997 – Fireside)\\nSome Studies in Machine Learning Using the Game of Checkers by Arthur'), Document(metadata={}, page_content='Samuel (1959–in Computation & Intelligence – Collected Readings edited by\\nGeorge F. Luger - MIT Press)\\nOne Jump Ahead: Challenging Human Supremacy in Checkers by Jonathan\\nSchaeffer (1997 – Springer V erlag)\\nA Re-examination of Brute-force Search by Jonathan Schaeffer, Paul Lu,\\nDuane Szafron, and Robert Lake (1993 – in Games: Planning and Learning,\\nAAAI 1993 Fall Symposium, Report FS9302, pp. 51–58)\\nA World Championship Caliber Checkers Program by Jonathan Schaeffer,'), Document(metadata={}, page_content='Joseph Culberson, Norman Treloar, Brent Knight, Paul Lu, and Duane\\nSzafron – (1992 – in Artificial Intelligence, Vol. 53(2–3), pp. 273–290)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='Knowledge Representation\\nand Automated Reasoning\\n3\\nIntroduction to Part 3\\nPart 3 is divided into three chapters:\\nPropositional and Predicate Logic\\nIn Chapter 7, the basic concepts behind propositional calculus\\nand predicate calculus are introduced. Truth tables and the ideas\\nbehind proofs by deduction are explained. The concept of tau-\\ntology is introduced, as is satisfiability and logical equivalence.\\nProperties of logical systems such as soundness, completeness,'), Document(metadata={}, page_content='and decidability are discussed. Logical systems other than\\nthose of classical logic are briefly introduced.\\nInference and Resolution for Problem Solving\\nIn Chapter 8, we introduce in detail the ideas behind proof by\\nrefutation and resolution for automated theorem proving.\\nThe chapter explains the steps needed to automate resolution,\\nincluding: converting expressions to conjunctive normal\\nform, Skolemization, and unification. The use of resolution'), Document(metadata={}, page_content='and Horn Clauses in Prolog is discussed, as are other practical\\napplications of resolution, such as for solving combinatorial\\nsearch problems.\\nRules and Expert Systems\\nChapter 9 discusses how rules and frames are used to build\\nexpert systems and discusses the practicalities of implement-\\ning such systems. Methods such as forward and backward\\nchaining and conflict resolution are discussed, as is the Rete\\nAlgorithm for a more efficient rule-based approach.'), Document(metadata={}, page_content='The ideas behind inheritance and multiple inheritance are dis-\\ncussed in relation to frames, and the relationship between\\nframes and object-oriented programming languages such as\\nC++ and Java is explored.\\nPART\\n7\\nCHAPTER\\n8\\nCHAPTER\\n9\\nCHAPTER'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='7CHAPTER\\nPropositional and\\nPredicate Logic\\nIf, dear Reader, you will faithfully observe these Rules, and so give my little book\\na really fair trial, I promise you, most confidently, that you will find Symbolic\\nLogic to be one of the most, if not the most, fascinating of mental recreations!\\n—Lewis Carroll, from the Introduction to Symbolic Logic\\nIf it was so, it might be; and if it were so, it would be: but as it isn’t, it ain’t.\\nThat’s logic.\\n—Lewis Carroll, from Through The Looking Glass'), Document(metadata={}, page_content='—Lewis Carroll, from Through The Looking Glass\\nOf science and logic he chatters\\nAs fine and as fast as he can;\\nThough I am no judge of such matters,\\nI’m sure he’s a talented man.\\n—Winthrop Mackworth Praed, from The T alented Man\\n7.1 Introduction\\nIn this chapter, we introduce propositional calculus and first-order predi-\\ncate calculus, the languages of logic. We introduce methods that can be\\nused to carry out deductions and prove whether or not a conclusion fol-\\nlows from a set of premises.'), Document(metadata={}, page_content='lows from a set of premises.\\nWe introduce the ideas of logical equivalence, tautologies, and satisfiability.\\nThis chapter also discusses some important properties of logical systems,\\nincluding soundness, completeness, monotonicity, and decidability.'), Document(metadata={}, page_content='176 CHAPTER 7 Propositional and Predicate Logic\\nThis chapter assumes no previous knowledge of logic, so readers who are\\nalready familiar with the ideas of propositional logic and the predicate cal-\\nculus may wish to skim this chapter.\\n7.2 What Is Logic?\\nLogic is concerned with reasoning and the validity of arguments. In gen-\\neral, in logic, we are not concerned with the truth of statements, but rather\\nwith their validity. That is to say, although the following argument is'), Document(metadata={}, page_content='clearly logical, it is not something that we would consider to be true:\\nAll lemons are blue\\nMary is a lemon\\nTherefore, Mary is blue\\nThis set of statements is considered to be valid because the conclusion\\n(Mary is blue) follows logically from the other two statements, which we\\noften call the premises.\\nThe reason that validity and truth can be separated in this way is simple: a\\npiece of a reasoning is considered to be valid if its conclusion is true in cases'), Document(metadata={}, page_content='where its premises are also true. Hence, a valid set of statements such as the\\nones above can give a false conclusion, provided one or more of the prem-\\nises are also false.\\nWe can say:a piece of reasoning is valid if it leads to a true conclusion in every\\nsituation where the premises are true.\\nLogic is concerned with truth values. The possible truth values are true and\\nfalse. These can be considered to be the fundamental units of logic, and'), Document(metadata={}, page_content='almost all logic is ultimately concerned with these truth values.\\n7.3 Why Logic Is Used in Artificial Intelligence\\nLogic is widely used in computer science, and particularly in Artificial\\nIntelligence. Logic is widely used as a representational method for Artificial\\nIntelligence. Unlike some other representations (such as frames, which are\\ndescribed in detail in Chapter 3), logic allows us to easily reason about neg-\\natives (such as, “this book is not red”) and disjunctions (“or”—such as,'), Document(metadata={}, page_content='“He’s either a soldier or a sailor”).'), Document(metadata={}, page_content='7.4 Logical Operators 177\\nLogic is also often used as a representational method for communicating\\nconcepts and theories within the Artificial Intelligence community. In\\naddition, logic is used to represent language in systems that are able to\\nunderstand and analyze human language.\\nAs we will see, one of the main weaknesses of traditional logic is its inabil-\\nity to deal with uncertainty. Logical statements must be expressed in terms'), Document(metadata={}, page_content='of truth or falsehood—it is not possible to reason, in classical logic, about\\npossibilities. We will see different versions of logic such as modal logics that\\nprovide some ability to reason about possibilities, and also probabilistic\\nmethods and fuzzy logic that provide much more rigorous ways to reason\\nin uncertain situations.\\n7.4 Logical Operators\\nIn reasoning about truth values, we need to use a number of operators,\\nwhich can be applied to truth values. We are familiar with several of these'), Document(metadata={}, page_content='operators from everyday language:\\nI like apples and oranges.\\nY ou can have an ice cream or a cake.\\nIf you come from France, then you speak French.\\nI am not stupid!\\nHere we see the four most basic logical operators being used in everyday\\nlanguage. The operators are:\\n■ and\\n■ or\\n■ not\\n■ if . . . then . . . (usually called implies)\\nThese operators work more or less as we expect them to. One important\\npoint to note is that or is slightly different from the way we usually use it. In'), Document(metadata={}, page_content='the sentence, “Y ou can have an icecream or a cake, ” the mother is usually\\nsuggesting to her child that he can only have one of the items, but not both.\\nThis is referred to as an exclusive-or in logic because the case where both\\nare allowed is excluded. The version of or that is used in logic is called\\ninclusive-or and allows the case with both options.'), Document(metadata={}, page_content='178 CHAPTER 7 Propositional and Predicate Logic\\nThe operators are usually written using the following symbols, although\\nother symbols are sometimes used, according to the context:\\nand ∧\\nor ∨\\nnot ¬\\nimplies →\\niff ↔\\nIff is an abbreviation that is commonly used to mean “if and only if. ” We see\\nlater that this is a stronger form of implies that holds true if one thing\\nimplies another, and also the second thing implies the first.'), Document(metadata={}, page_content='For example, “you can have an ice-cream if and only if you eat your din-\\nner. ” It may not be immediately apparent why this is different from “you\\ncan have an icecream if you eat your dinner. ” This is because most mothers\\nreally mean iff when they use if in this way.\\n7.5 Translating between English and Logic Notation\\nT o use logic, it is first necessary to convert facts and rules about the real\\nworld into logical expressions using the logical operators described in Sec-'), Document(metadata={}, page_content='tion 7.4. Without a reasonable amount of experience at this translation, it\\ncan seem quite a daunting task in some cases.\\nLet us examine some examples.\\nFirst, we will consider the simple operators,\\n∧, ∨, and ¬.\\nSentences that use the word and in English to express more than one con-\\ncept, all of which is true at once, can be easily translated into logic using the\\nAND operator,\\n∧. For example:\\n“It is raining and it is Tuesday. ”\\nmight be expressed as:\\nR ∧ T'), Document(metadata={}, page_content='might be expressed as:\\nR ∧ T\\nWhere R means “it is raining” and T means “it is Tuesday. ” Note that we\\nhave been fairly arbitrary in our choice of these terms. This is all right, as\\nlong as the terms are chosen in such a way that they represent the problem\\nadequately. For example, if it is not necessary to discuss where it is raining,\\nR is probably enough. If we need to write expressions such as “it is raining'), Document(metadata={}, page_content='7.5 Translating between English and Logic Notation 179\\nin New Y ork” or “it is raining heavily” or even “it rained for 30 minutes on\\nThursday, ” then R will probably not suffice.\\nT o express more complex concepts like these, we usually use predicates.\\nHence, for example, we might translate “it is raining in New Y ork” as:\\nN(R)\\nWe might equally well choose to write it as:\\nR(N)\\nThis depends on whether we consider the rain to be a property of New'), Document(metadata={}, page_content='Y ork, or vice versa. In other words, when we write N(R), we are saying that\\na property of the rain is that it is in New Y ork, whereas with R(N) we are\\nsaying that a property of New Y ork is that it is raining.\\nWhich we use depends on the problem we are solving. It is likely that if we\\nare solving a problem about New Y ork, we would use R(N), whereas if we\\nare solving a problem about the location of various types of weather, we\\nmight use N(R).'), Document(metadata={}, page_content='might use N(R).\\nLet us return now to the logical operators. The expression “it is raining in New\\nY ork, and I’m either getting sick or just very tired” can be expressed as follows:\\nR(N)\\n∧ (S(I) ∨ T(I))\\nHere we have used both the ∧ operator, and the ∨ operator to express a col-\\nlection of statements. The statement can be broken down into two sections,\\nwhich is indicated by the use of parentheses. The section in the parentheses\\nis S(I)'), Document(metadata={}, page_content='is S(I)\\n∨ T(I), which means “I’m either getting sick OR I’m very tired” . This\\nexpression is “AND’ed” with the part outside the parentheses, which isR(N).\\nFinally, the ¬ operator is applied exactly as you would expect—to express\\nnegation. For example,\\nIt is not raining in New Y ork,\\nmight be expressed as\\n¬R(N)\\nIt is important to get the ¬ in the right place. For example: “I’m either not\\nwell or just very tired” would be translated as\\n¬W(I) ∨ T(I)'), Document(metadata={}, page_content='180 CHAPTER 7 Propositional and Predicate Logic\\nThe position of the ¬ here indicates that it is bound to W(I) and does not\\nplay any role in affecting T(I). This idea of precedence is explained further\\nin Section 7.7.\\nNow let us see how the → operator is used. Often when dealing with logic\\nwe are discussing rules, which express concepts such as “if it is raining then\\nI will get wet. ”\\nThis sentence might be translated into logic as\\nR → W(I)'), Document(metadata={}, page_content='R → W(I)\\nThis is read “R implies W(I)” or “IF R THEN W(I)” . By replacing the sym-\\nbols R and W(I) with their respective English language equivalents, we can\\nsee that this sentence can be read as\\n“raining implies I’ll get wet”\\nor “IF it’s raining THEN I’ll get wet. ”\\nImplication can be used to express much more complex concepts than this.\\nFor example, “Whenever he eats sandwiches that have pickles in them, he\\nends up either asleep at his desk or singing loud songs” might be translated as\\nS(y)'), Document(metadata={}, page_content='S(y)\\n∧ E(x, y) ∧ P(y) → A(x) ∨ (S(x, z) ∧ L(z))\\nHere we have used the following symbol translations:\\nS(y) means that y is a sandwich.\\nE(x, y) means that x (the man) eats y (the sandwich).\\nP(y) means that y (the sandwich) has pickles in it.\\nA(x) means that x ends up asleep at his desk.\\nS(x, z) means that x (the man) sings z (songs).\\nL(z) means that z (the songs) are loud.\\nIn fact, there are better ways to express this kind of sentence, as we will see'), Document(metadata={}, page_content='when we examine the quantifiers ∃ and ∀ in Section 7.13.\\nThe important thing to realize is that the choice of variables and predicates\\nis important, but that you can choose any variables and predicates that map\\nwell to your problem and that help you to solve the problem. For example, in\\nthe example we have just looked at, we could perfectly well have used instead\\nS → A\\n∨ L\\nwhere S means “he eats a sandwich which has pickles in it, ” A means “he'), Document(metadata={}, page_content='ends up asleep at his desk, ” and L means “he sings loud songs. ”'), Document(metadata={}, page_content='7.6 Truth Tables 181\\nThe choice of granularity is important, but there is no right or wrong way\\nto make this choice. In this simpler logical expression, we have chosen to\\nexpress a simple relationship between three variables, which makes sense if\\nthose variables are all that we care about—in other words, we don’t need to\\nknow anything else about the sandwich, or the songs, or the man, and the\\nfacts we examine are simply whether or not he eats a sandwich with pickles,'), Document(metadata={}, page_content='sleeps at his desk, and sings loud songs. The first translation we gave is\\nmore appropriate if we need to examine these concepts in more detail and\\nreason more deeply about the entities involved.\\nNote that we have thus far tended to use single letters to represent logical\\nvariables. It is also perfectly acceptable to use longer variable names, and\\nthus to write expressions such as the following:\\nFish (x) \\n∧ living (x) → has_scales (x)'), Document(metadata={}, page_content='Fish (x) \\n∧ living (x) → has_scales (x)\\nThis kind of notation is obviously more useful when writing logical expres-\\nsions that are intended to be read by humans but when manipulated by a\\ncomputer do not add any value.\\n7.6 Truth Tables\\nWe can use variables to represent possible truth values, in much the same\\nway that variables are used in algebra to represent possible numerical val-\\nues. We can then apply logical operators to these variables and can reason\\nabout the way in which they behave.'), Document(metadata={}, page_content='about the way in which they behave.\\nIt is usual to represent the behavior of these logical operators using truth\\ntables. A truth table shows the possible values that can be generated by\\napplying an operator to truth values.\\n7.6.1 Not\\nFirst of all, we will look at the truth table for not, ¬.\\nNot is a unary operator, which means it is applied only to one variable. Its\\nbehavior is very simple:\\n¬ true is equal to false\\n¬ false is equal to true\\nIf variable A has value true, then ¬A has value false.'), Document(metadata={}, page_content='If variable B has value false, then ¬B has value true.'), Document(metadata={}, page_content='182 CHAPTER 7 Propositional and Predicate Logic\\nThese can be represented by a truth table,\\nA ¬ A\\ntrue false\\nfalse true\\n7.6.2 And\\nNow, let us examine the truth table for our first binary operator—one\\nwhich acts on two variables:\\nABA  ∧ B\\nfalse false false\\nfalse true false\\ntrue false false\\ntrue true true\\n∧ is also called theconjunctive operator. A ∧ B is theconjunctionof A and B.\\nY ou can see that the only entry in the truth table for which A ∧ B is true is'), Document(metadata={}, page_content='the one where A is true and B is true.I f A is false, or if B is false, then A ∧ B\\nis false. If both A and B are false, then A ∧ B is also false.\\nWhat do A and B mean? They can represent any statement, or proposition,\\nthat can take on a truth value. For example, A might represent “It’s sunny,”\\nand B might represent “It’s warm outside. ” In this case,A ∧ B would mean\\n“It is sunny and it’s warm outside, ” which clearly is true only if the two'), Document(metadata={}, page_content='component parts are true (i.e., if it is true that it is sunny and it is true that\\nit is warm outside).\\n7.6.3 Or\\nThe truth table for the or operator, ∨, should need little explanation.\\nABA  ∨ B\\nfalse false false\\nfalse true true\\ntrue false true\\ntrue true true'), Document(metadata={}, page_content='7.6 Truth Tables 183\\n∨ is also called thedisjunctive operator. A ∨ B is the disjunction of A and B.\\nClearly A ∨ B is true for any situation except when both A and B are false.I f\\nA is true, or if B is true, or if both A and B are true, A ∨ B is true.\\nY ou should notice that this table represents the inclusive-or operator. A\\ntable to represent exclusive-or would have false in the final row. In other\\nwords, while A ∨ B is true if A and B are both true, A EOR B (A exclusive-or'), Document(metadata={}, page_content='B) is false if A and B are both true.\\nY ou may also notice a pleasing symmetry between the truth tables for ∧\\nand ∨. This will become useful later, as will a number of other symmetrical\\nrelationships.\\n7.6.4 Implies\\nThe truth table for implies (→) is a little less intuitive.\\nABA  → B\\nfalse false true\\nfalse true true\\ntrue false false\\ntrue true true\\n(This form of implication is also known as material implication.)\\nIn the statement A → B, A is the antecedent, and B is the consequent.'), Document(metadata={}, page_content='The bottom two lines of the table should be obvious. If A is true and B is\\ntrue, then A → B seems to be a reasonable thing to believe. For example, if\\nA means “you live in France” and B means “Y ou speak French, ” then A → B\\ncorresponds to the statement “if you live in France, then you speak French. ”\\nClearly, this statement is true (A → B is true) if I live in France and I speak\\nFrench (A is true and B is true).\\nSimilarly, if I live in France, but I don’t speak French ( A is true,b u t  B is'), Document(metadata={}, page_content='false), then it is clear that A → B is not true.\\nThe situations where A is false are a little less clear. If I do not live in France\\n(A is not true), then the truth table tells us that regardless of whether I\\nspeak French or not (the value of B), the statement A → B is true.\\nA → B is usually read as “A implies B” but can also be read as “If A then B”\\nor “If A is true then B is true. ” Hence, ifA is false, the statement is not really'), Document(metadata={}, page_content='184 CHAPTER 7 Propositional and Predicate Logic\\nsaying anything about the value of B, so B is free to take on any value (as\\nlong as it is true or false, of course!).\\nThis can lead to some statements being valid that might at first glance\\nappear absurd. All of the following statements are valid:\\n52 = 25 → 4 = 4 (true → true)\\n9 /H110039 = 123 → 8 > 3 (false → true)\\n52 = 25 → 0 = 2 (false → false)\\nIn fact, in the second and third examples, the consequent could be given'), Document(metadata={}, page_content='any meaning, and the statement would still be true. For example, the fol-\\nlowing statement is valid:\\n52 = 25 → Logic is weird\\nNotice that when looking at simple logical statements like these, there does\\nnot need to be any real-world relationship between the antecedent and the\\nconsequent. For logic to be useful, though, we tend to want the relation-\\nships being expressed to be meaningful as well as being logically true.\\n7.6.5 iff\\nThe truth table for iff (if and only if {↔}) is as follows:'), Document(metadata={}, page_content='ABA  ↔ B\\nfalse false true\\nfalse true false\\ntrue false false\\ntrue true true\\nIt can be seen that A\\n↔ B is true as long as A and B have the same value. In\\nother words, if one is true and the other false, then A ↔ B is false. Other-\\nwise, if A and B have the same value, A ↔ B is true.\\n7.7 Complex Truth Tables\\nTruth tables are not limited to showing the values for single operators. For\\nexample, a truth table can be used to display the possible values forA ∧(B ∨C).'), Document(metadata={}, page_content='7.7 Complex Truth Tables 185\\nABCA  ∧ (B ∨ C)\\nfalse false false false\\nfalse false true false\\nfalse true false false\\nfalse true true false\\ntrue false false false\\ntrue false true true\\ntrue true false true\\ntrue true true true\\nNote that for two variables, the truth table has four lines, and for three vari-\\nables, it has eight. In general, a truth table for n variables will have 2\\nn lines.\\nThe use of brackets in this expression is important. A ∧ (B ∨ C) is not the\\nsame as (A ∧ B) ∨ C.'), Document(metadata={}, page_content='same as (A ∧ B) ∨ C.\\nT o avoid ambiguity, the logical operators are assigned precedence, as with\\nmathematical operators. The order of precedence that is used is as follows:\\n¬, ∧, ∨, →, ↔\\nHence, in a statement such as\\n¬A ∨¬ B ∧ C\\nthe ¬ operator has the greatest precedence, meaning that it is most closely\\ntied to its symbols.∧ has a greater precedence than ∨, which means that the\\nsentence above can be expressed as\\n(¬A) ∨ ((¬B) ∧ C)\\nSimilarly, when we write\\n¬A ∨ B\\nthis is the same as\\n(¬A) ∨ B'), Document(metadata={}, page_content='¬A ∨ B\\nthis is the same as\\n(¬A) ∨ B\\nrather than\\n¬(A ∨ B)'), Document(metadata={}, page_content='186 CHAPTER 7 Propositional and Predicate Logic\\nIn general, it is a good idea to use brackets whenever an expression might\\notherwise be ambiguous.\\n7.8 Tautology\\nConsider the following truth table:\\nAA  ∨ ¬ A\\nfalse true\\ntrue true\\nThis truth table has a property that we have not seen before: the value of\\nthe expression A\\n∨¬ A is true regardless of the value ofA. An expression like\\nthis that is always true is called a tautology.\\nIf A is a tautology, we write:\\n|=A'), Document(metadata={}, page_content='If A is a tautology, we write:\\n|=A\\nTautologies may seem like rather uninteresting entities, but in fact they are\\nextremely useful for logic, as we see later.\\nA logical expression that is a tautology is often described as being valid.A\\nvalid expression is defined as being one that is true under any interpreta-\\ntion. In other words, no matter what meanings and values we assign to the\\nvariables in a valid expression, it will still be true. For example, the follow-\\ning sentences are all valid:'), Document(metadata={}, page_content='ing sentences are all valid:\\nIf wibble is true, then wibble is true.\\nEither wibble is true, or wibble is not true.\\nIn the language of logic, we can replace wibble with the symbol A, in which\\ncase these two statements can be rewritten as\\nA → A\\nA\\n∨¬ A\\nIf an expression is false in any interpretation, it is described as being con-\\ntradictory. The following expressions are contradictory:\\nA ∧¬ A\\n(A ∨¬ A) → (A ∧¬ A)'), Document(metadata={}, page_content='7.9 Equivalence 187\\nIt doesn’t matter whatA means in these expressions, the result cannot betrue.\\nSome expressions are satisfiable, but not valid. This means that they are\\ntrue under some interpretation, but not under all interpretations. The fol-\\nlowing expressions are satisfiable:\\nA\\n∨ B\\n(A ∧ B ∨¬ C) → (D ∧ E)\\nA contradictory expression is clearly not satisfiable and so is described as\\nbeing unsatisfiable.\\n7.9 Equivalence\\nConsider the following two expressions:\\nA ∧ B\\nB ∧ A'), Document(metadata={}, page_content='A ∧ B\\nB ∧ A\\nIt should be fairly clear that these two expressions will always have the same\\nvalue for a given pair of values forA and B. In other words, we say that the first\\nexpression islogically equivalentto the second expression. We write this as\\nA ∧ B /H11013B ∧ A\\nThis means that the ∧ operator is commutative.\\nNote that this is not the same as implication:\\nA ∧ B → B ∧ A\\nalthough this second statement is also true. The difference is that if for two\\nexpressions e1 and e2:\\ne1 /H11013e2'), Document(metadata={}, page_content='expressions e1 and e2:\\ne1 /H11013e2\\nthen e1 will always have the same value as e2 for a given set of variables. On\\nthe other hand, as we have seen, e1 → e2 is true if e1 is false and e2 is true.\\nThere are a number of logical equivalences that are extremely useful. The\\nfollowing is a list of a few of the most common:\\nA ∨ A /H11013A\\nA ∧ A /H11013A\\nA ∧ (B ∧ C) /H11013(A ∧ B) ∧ C( ∧ is associative)'), Document(metadata={}, page_content='188 CHAPTER 7 Propositional and Predicate Logic\\nA ∨ (B ∨ C) /H11013(A ∨ B) ∨ C( ∨ is associative)\\nA ∧ (B ∨ C) /H11013(A ∧ B) ∨ (A ∧ C) ( ∧ is distributive over ∨)\\nA ∧ (A ∨ B) /H11013A\\nA ∨ (A ∧ B) /H11013A\\nA ∧ true /H11013A\\nA ∧ false /H11013false\\nA ∨ true /H11013true\\nA ∨ false /H11013A\\nAll of these equivalences can be proved by drawing up the truth tables for\\neach side of the equivalence and seeing if the two tables are the same. Y ou'), Document(metadata={}, page_content='may want to try this to satisfy yourself that all of the equivalences are cor-\\nrect, particularly for some of the less intuitive ones.\\nThe following is a very important equivalence:\\nA → B /H11013\\n¬A ∨ B\\nY ou can verify this by checking the truth tables. The reason that this is use-\\nful is that it means we do not need to use the → symbol at all—we can\\nreplace it with a combination of ¬ and ∨. Similarly, the following equiva-\\nlences mean we do not need to use ∧ or ↔:\\nA ∧ B /H11013¬(¬A ∨¬ B)'), Document(metadata={}, page_content='A ∧ B /H11013¬(¬A ∨¬ B)\\nA ↔ B /H11013¬(¬(¬A ∨ B) ∨¬ (¬B ∨ A))\\nIn fact, any binary logical operator can be expressed using ¬ and ∨. This is a\\nfact that is employed in electronic circuits, wherenor gates, based on an oper-\\nator callednor,a r eu s e d .Nor is represented by ↓, and is defined as follows:\\nA ↓ B /H11013¬(A ∨ B)\\nFinally, the following equivalences are known as DeMorgan’s Lawsand will\\nbe used later in this chapter:\\nA ∧ B /H11013¬(¬A ∨¬ B)\\nA ∨ B /H11013¬(¬A ∧¬ B)'), Document(metadata={}, page_content='A ∧ B /H11013¬(¬A ∨¬ B)\\nA ∨ B /H11013¬(¬A ∧¬ B)\\nBy using these and other equivalences, logical expressions can be simpli-\\nfied. For example,\\n(C ∧ D) ∨ ((C ∧ D) ∧ E)'), Document(metadata={}, page_content='7.10 Propositional Logic 189\\ncan be simplified using the following rule:\\nA ∨ (A ∧ B) /H11013A\\nhence,\\n(C ∧ D) ∨ ((C ∧ D) ∧ E) /H11013C ∧ D\\nIn this way, it is possible to eliminate subexpressions that do not contribute\\nto the overall value of the expression.\\n7.10 Propositional Logic\\nThere are a number of possible systems of logic. The system we have been\\nexamining so far in this chapter is called propositional logic. The language'), Document(metadata={}, page_content='that is used to express propositional logic is called the propositional calcu-\\nlus (although in practice, many people use the expressions logic and calcu-\\nlus interchangeably in this context).\\nA logical system can be defined in terms of its syntax (the alphabet of\\nsymbols and how they can be combined), its semantics (what the sym-\\nbols mean), and a set of rules of deduction that enable us to derive one\\nexpression from a set of other expressions and thus make arguments\\nand proofs.\\n7.10.1 Syntax'), Document(metadata={}, page_content='and proofs.\\n7.10.1 Syntax\\nWe have already examined the syntax of propositional calculus. The alpha-\\nbet of symbols, /H9018is defined as follows\\n/H9018= {true, false,\\n¬, →,( ,) , ∧, ∨, ↔,p 1,p 2,p 3,...,p n,...}\\nHere we have used set notation to define the possible values that are con-\\ntained within the alphabet /H9018. Note that we allow an infinite number of\\nproposition letters,o r  propositional symbols , p1, p2, p3,..., and so on.'), Document(metadata={}, page_content='More usually, we will represent these by capital letters P, Q, R, and so on,\\nalthough if we need to represent a very large number of them, we will use\\nthe subscript notation (e.g., p\\n1).\\nAn expression is referred to as a well-formed formula (often abbreviated as\\nwff) or a sentence if it is constructed correctly, according to the rules of the\\nsyntax of propositional calculus, which are defined as follows. In these'), Document(metadata={}, page_content='190 CHAPTER 7 Propositional and Predicate Logic\\nrules, we use A, B, C to represent sentences. In other words, we define a sen-\\ntence recursively, in terms of other sentences. The following are well-\\nformed sentences:\\nP ,Q ,R ...\\ntrue, false\\n(A)\\n¬A\\nA ∧ B\\nA ∨ B\\nA → B\\nA ↔ B\\nHence, we can see that the following is an example of a wff:\\nP ∧ Q ∨ (B ∧¬ C) → A ∧ B ∨ D ∧ (¬E)\\nThis is not to make any claims about the validity or otherwise of the expres-'), Document(metadata={}, page_content='sion, simply that it is allowed within the syntax of propositional calculus.\\n7.10.2 Semantics\\nThe semantics of the operators of propositional calculus can be defined in\\nterms of truth tables. As we have seen, the meaning of P\\n∧ Q is defined as\\n“true when P is true and Q is also true.”\\nThe meaning of symbols such as P and Q is arbitrary and could be ignored\\naltogether if we were reasoning about pure logic. In other words, reasoning\\nabout sentences such as P'), Document(metadata={}, page_content='about sentences such as P\\n∨ Q ∧¬ R is possible without considering what P,\\nQ, and R mean.\\nBecause we are using logic as a representational method for artificial intel-\\nligence, however, it is often the case that when using propositional logic, the\\nmeanings of these symbols are very important. The beauty of this represen-\\ntation is that it is possible for a computer to reason about them in a very\\ngeneral way, without needing to know much about the real world.'), Document(metadata={}, page_content='In other words, if we tell a computer, “I like ice cream, and I like chocolate, ”\\nit might represent this statement as A\\n∧ B, which it could then use to reason\\nwith, and, as we will see, it can use this to make deductions.'), Document(metadata={}, page_content='7.11 Deduction 191\\n7.11 Deduction\\nIf we have a set of assumptions { A1, A2,..., An}, and from those assump-\\ntions we are able to derive a conclusion, C, then we say that we have\\ndeduced C from the assumptions, which is written\\n{A1,A 2,...,A n} ⊢C\\nIf C can be concluded without any assumptions, then we write\\n⊢C\\nT o derive a conclusion from a set of assumptions, we apply a set of infer-\\nence rules. T o distinguish an inference rule from a sentence, we often write\\nA ⊢B as follows:'), Document(metadata={}, page_content='A ⊢B as follows:\\nSome of the most useful inference rules for propositional logic are as fol-\\nlows. In these rules, A, B, and C stand for any logical expressions.\\n7.11.1\\nThis rule is very straightforward. It says: Given A and B, we can deduce A ∧\\nB. This follows from the definition of ∧.\\n7.11.2\\nSimilarly,\\nThese rules say that given A ∧ B, we can deduce A and we can also deduce B\\nseparately. Again, these follow from the definition of∧.\\nAB\\nB\\n∧\\nAB\\nA\\n∧\\n-Elimination\\nA   B\\nAB∧\\n-Introduction\\nA/H5007B'), Document(metadata={}, page_content='192 CHAPTER 7 Propositional and Predicate Logic\\n7.11.3 Or-Introduction\\nThese rules say that from A we can deduce the disjunction of A with any\\nexpression. For example, from the statement “I like logic, ” we can deduce\\nexpressions such as “I like logic or I like cheese, ”“I like logic or I do not like\\nlogic, ” “I like logic or fish can sing, ” “I like logic or 2 + 2 = 123, ” and so on.\\nThis follows because true\\n∨ B is true for any value of B.\\n7.11.4 → Elimination'), Document(metadata={}, page_content='7.11.4 → Elimination\\nThis rule is usually known as modus ponens and is one of the most com-\\nmonly used rules in logical deduction. It is expressed as follows:\\nIn other words, ifA is true and A implies B is true, then we know thatB is true.\\nFor example, if we replace A with “it is raining” and B with “I need an\\numbrella, ” then we produce the following:\\nIt is raining. If it’s raining, I need an umbrella. Therefore, I need\\nan umbrella.\\nThis kind of reasoning is clearly valid.'), Document(metadata={}, page_content='This kind of reasoning is clearly valid.\\n7.11.5 Reductio Ad Absurdum\\nWe need to introduce a new notation for this rule:\\nThe symbol ⊥ is called falsum, which is used to indicate an absurdity, or a\\ncontradiction. For example, ⊥ can be deduced from A ∧¬ A.\\n¬\\n⊥\\nA\\nA\\nM\\nA    A B\\nB\\n→\\nB\\nAB∨\\nA\\nAB∨'), Document(metadata={}, page_content='7.11 Deduction 193\\nThe reductio ad absurdum rule simply says that if we assume that A is false\\n(¬A) and this leads to a contradiction ( ⊥), then we can deduce that A is\\ntrue. This is known as proof by contradiction.\\nAs we will see, this is an extremely powerful concept and is widely used in\\nlogical systems.\\n7.11.6 → Introduction\\nThis rule shows that if in carrying out a proof we start from an assumption\\nA and derive a conclusion C, then we can conclude that A → C.\\n7.11.7 ¬¬ Elimination'), Document(metadata={}, page_content='7.11.7 ¬¬ Elimination\\nThis rule states that if we have a sentence that is negated twice, we can con-\\nclude the sentence itself, without the negation. Clearly, this rule follows\\nfrom the definition of\\n¬.\\n7.11.8 Example 1\\nT o carry out a proof that one set of sentences follows logically from\\nanother, we selectively apply the rules presented above to the assumptions\\nuntil we arrive at the conclusions.\\nFor example, it would be useful to prove the following:\\n{A,\\n¬A} ⊢⊥'), Document(metadata={}, page_content='{A,\\n¬A} ⊢⊥\\nIn other words, if we start from the set of assumptions A and ¬A, we can\\nconclude falsum.\\nFirst, note that\\n¬A /H11013A → ⊥\\nThis can be seen by comparing the truth tables for ¬A and for A → ⊥.\\n¬¬A\\nA\\nA\\nC\\nAC\\nM\\n→'), Document(metadata={}, page_content='194 CHAPTER 7 Propositional and Predicate Logic\\nHence, we can take as our set of assumptions\\n{A, A → ⊥}\\nThus, our proof using modus ponens (the → ELIMINATION rule pre-\\nsented in Section 7.11.2) is as follows:\\n7.11.9 Example 2\\nLet us prove the following:\\n{A ∧ B} ⊢A ∨ B\\nThe proof is as follows:\\nA ∧ B assumption\\nAb y  ∧ elimination\\nA ∨ Bb y  ∨ introduction\\n7.11.10 Example 3\\nWe will use reductio ad absurdum to prove the following:\\n⊢(¬A → B) → (¬B → A)'), Document(metadata={}, page_content='⊢(¬A → B) → (¬B → A)\\nThe usual method for carrying out such proofs is based on the idea that in\\norder to prove something of the form A → B, it is a good idea to start by\\nassuming A.\\nWe will start with two assumptions: ¬A and (¬A → B). After the first step,\\nwhich uses modus ponens, on our original assumptions to prove B, we\\nintroduce a new assumption, which is ¬B. The proof is as follows:\\n¬A ¬A → B assumptions\\nB ¬B modus ponens\\nBB  →⊥ rewriting ¬B\\n⊥ modus ponens\\nA reductio ad absurdum'), Document(metadata={}, page_content='⊥ modus ponens\\nA reductio ad absurdum\\n¬B → A → introduction\\n(¬A → B) → (¬B → A) → introduction\\nA  A →⊥\\n⊥'), Document(metadata={}, page_content='7.12 The Deduction Theorem 195\\nIn carrying out this proof, we have used the relationship between ¬B and B\\n→ ⊥ as we did in Example 1. We have also used reductio absurdum to show\\nthat if we start by assuming ¬A, we end up with a contradiction ( ⊥), and\\ntherefore our initial assumption,¬A, was false.H e n c e ,A must be true.\\n7.11.11 Example 4\\nLet us now aim to prove the following:\\n⊢(A → B) → ((B → C) → ((C → D) → (A → D)))'), Document(metadata={}, page_content='⊢(A → B) → ((B → C) → ((C → D) → (A → D)))\\nT o prove this, we will need to make a series of assumptions. We will start by\\nmaking two assumptions, A and A → B. Hence, our proof is as follows:\\nAA  → B assumptions\\nBB  → C modus ponens\\nCC  → D modus ponens\\nD modus ponens\\nA → D → introduction\\n(C → D) → (A → D) → introduction\\n(B → C) → ((C → D) → (A → D)) → introduction\\n(A → B) → ((B → C) → ((C → D) → (A → D))) → introduction\\n7.12 The Deduction Theorem'), Document(metadata={}, page_content='7.12 The Deduction Theorem\\nA useful rule known as the deduction theorem provides us with a way to\\nmake propositional logic proofs easier. The rule is as follows:\\nif A ⋃ {B} ⊢C then A ⊢(B → C)\\nHere A is a set of wff’s, which makes up our assumptions. Note that this\\nrule is true even if A is the empty set. A ⋃ {B} means the union of the set A\\nwith the set consisting of one element, B.\\nThe rule also holds in reverse:\\nif A ⊢(B → C) then A ⋃ {B} ⊢C'), Document(metadata={}, page_content='if A ⊢(B → C) then A ⋃ {B} ⊢C\\nLet us see an example of a proof using the deduction theorem.\\nOur aim is to prove the following:\\n{A → B}\\n⊢A → (C → B)'), Document(metadata={}, page_content='196 CHAPTER 7 Propositional and Predicate Logic\\nRecall the axiom that was presented earlier:\\nA → (B → A)\\nBecause propositional logic is monotonic (see Section 7.18), we can add in\\nan additional assumption, that A is true:\\nA\\nNow, by applying modus ponens to this assumption and our hypothesis, A\\n→ B, we arrive at\\nB\\nWe can now apply our axiom\\nB → (C → B)\\nAnd by modus ponens on the above two lines, we get\\nC → B\\nHence, we have shown that\\n{A → B} ⋃ A ⊢(C → B)\\nAnd, therefore, by the deduction theorem'), Document(metadata={}, page_content='And, therefore, by the deduction theorem\\n{A → B} ⊢A → (C → B)\\n7.13 Predicate Calculus\\n7.13.1 Syntax\\nPredicate calculus allows us to reason about properties of objects and rela-\\ntionships between objects. In propositional calculus, we could express the\\nEnglish statement “I like cheese” by A. This enables us to create constructs\\nsuch as \\n¬A, which means “I do not like cheese, ” but it does not allow us to\\nextract any information about the cheese, or me, or other things that I like.'), Document(metadata={}, page_content='In predicate calculus, we use predicates to express properties of objects. So\\nthe sentence “I like cheese” might be expressed as\\nL(me, cheese)\\nwhere L is a predicate that represents the idea of “liking. ” Note that as well\\nas expressing a property of me, this statement also expresses a relationship\\nbetween me and cheese. This can be useful, as we will see, in describing\\nenvironments for robots and other agents. For example, a simple agent may'), Document(metadata={}, page_content='7.13 Predicate Calculus 197\\nbe concerned with the location of various blocks, and a statement about\\nthe world might be\\nT(A,B)\\nwhich could mean: Block A is on top of Block B.\\nThus far we have expressed ideas about specific objects. It is also possible to\\nmake more general statements using the predicate calculus. For example, to\\nexpress the idea that everyone likes cheese, we might say\\n(∀x)(P(x) → L(x, C))\\nThe symbol ∀ is read “for all, ” so the statement above could be read as “for'), Document(metadata={}, page_content='every x it is true that if property P holds for x, then the relationship L holds\\nbetween x and C, ” or in plainer English: “every x that is a person likes\\ncheese. ” (Here we are interpreting P(x) as meaning “x is a person” or, more\\nprecisely, “x has property P.” )\\nNote that we have used brackets rather carefully in the statement above.\\nThis statement can also be written with fewer brackets:\\n∀x P(x) → L(x, C)\\n∀ is called the universal quantifier.'), Document(metadata={}, page_content='∀ is called the universal quantifier.\\nThe quantifier ∃ can be used to express the notion that some values do have\\na certain property, but not necessarily all of them:\\n(∃x)(L(x,C))\\nThis statement can be read “there exists an x such that x likes cheese. ” This\\ndoes not make any claims about the possible values of x, so x could be a\\nperson, or a dog, or an item of furniture. When we use the existential quan-\\ntifier in this way, we are simply saying that there is at least one value of x for'), Document(metadata={}, page_content='which L(x,C) holds.\\nNote, therefore, that the following is true:\\n(∀x)(L(x,C)) → (∃x)(L(x,C))\\nbut the following is not:\\n(∃x)(L(x,C)) → (∀x)(L(x,C))\\n7.13.2 Relationships between ∀ and ∃\\nIt is also possible to combine the universal and existential quantifiers, such\\nas in the following statement:'), Document(metadata={}, page_content='198 CHAPTER 7 Propositional and Predicate Logic\\n(∀x) (∃y) (L(x,y))\\nThis statement can be read “for all x, there exists a y such that L holds for x\\nand y, ” which we might interpret as “everyone likes something. ”\\nA useful relationship exists between ∀ and ∃. Consider the statement “not\\neveryone likes cheese. ” We could write this as\\n¬(∀x)(P(x) → L(x,C)) (1)\\nAs we have already seen, A → B is equivalent to ¬A ∨ B. Using DeMorgan’s'), Document(metadata={}, page_content='laws, we can see that this is equivalent to ¬(A ∧¬ B). Hence, the statement\\n(1) above, can be rewritten:\\n¬(∀x)¬(P(x) ∧¬ L(x,C)) (2)\\nThis can be read as “It is not true that for allx the following is not true: x is\\na person and x does not like cheese. ” If you examine this rather convoluted\\nsentence carefully, you will see that it is in fact the same as “there exists anx\\nsuch that x is a person andx does not like cheese. ” Hence we can rewrite it as\\n(∃x)(P(x) ∧¬ L(x,C)) (3)'), Document(metadata={}, page_content='(∃x)(P(x) ∧¬ L(x,C)) (3)\\nIn making this transition from statement (2) to statement (3), we have uti-\\nlized the following equivalence:\\n∃x /H11013¬(∀x)¬\\nIn an expression of the form ( ∀x)(P(x, y)), the variable x is said to be\\nbound, whereas y is said to be free. This can be understood as meaning that\\nthe variable y could be replaced by any other variable because it is free, and\\nthe expression would still have the same meaning, whereas if the variable x'), Document(metadata={}, page_content='were to be replaced by some other variable in P(x,y), then the meaning of\\nthe expression would be changed:\\n(∀x)(P(y, z))\\nis not equivalent to (∀x)(P(x, y)), whereas (∀x)(P(x, z)) is. Note that a vari-\\nable can occur both bound and free in an expression, as in\\n(∀x)(P(x,y,z) → (∃y)(Q(y,z)))\\nIn this expression, x is bound throughout, and z is free throughout; y is free\\nin its first occurrence but is bound in (∃y)(Q(y,z)). (Note that both occur-\\nrences of y are bound here.)'), Document(metadata={}, page_content='rences of y are bound here.)\\nMaking this kind of change is known as substitution. Substitution is\\nallowed of any free variable for another free variable.'), Document(metadata={}, page_content='7.14 First-Order Predicate Logic 199\\n7.13.3 Functions\\nIn much the same way that functions can be used in mathematics, we can\\nexpress an object that relates to another object in a specific way using\\nfunctions. For example, to represent the statement “my mother likes\\ncheese, ” we might use\\nL(m(me),cheese)\\nHere the function m(x) means the mother of x. Functions can take more\\nthan one argument, and in general a function with n arguments is rep-\\nresented as\\nf(x\\n1,x 2,x 3,...,x n)'), Document(metadata={}, page_content='resented as\\nf(x\\n1,x 2,x 3,...,x n)\\n7.14 First-Order Predicate Logic\\nThe type of predicate calculus that we have been referring to is also calledfirst-\\norder predicate logic (FOPL). A first-order logic is one in which the quantifiers\\n∀ and ∃ can be applied to objects orterms, but not to predicates or functions.\\nSo we can define the syntax of FOPL as follows. First, we define a term:\\nA constant is a term.\\nA variable is a term.\\nf(x\\n1,x 2,x 3,...,x n) is a term if x1,x 2,x 3,...,x n are all terms.'), Document(metadata={}, page_content='Anything that does not meet the above description cannot be a term. For\\nexample, the following is not a term:∀xP (x). This kind of construction we\\ncall a sentence or a well-formed formula (wff), which is defined as follows.\\nIn these definitions, P is a predicate,x\\n1, x2, x3,..., xn are terms, and A,B are\\nwff’s. The following are the acceptable forms for wff’s:\\nP(x1,x 2,x 3,...,x n)\\n¬A\\nA ∧ B\\nA ∨ B\\nA → B\\nA ↔ B\\n(∀x)A\\n(∃x)A\\nAn atomic formula is a wff of the form P(x1, x2, x3,..., xn).'), Document(metadata={}, page_content='200 CHAPTER 7 Propositional and Predicate Logic\\nHigher order logics exist in which quantifiers can be applied to predicates\\nand functions, and where the following expression is an example of a wff:\\n(∀P)(∃x)P(x)\\nIn this book, we will stick with first-order logics, in which quantifiers can\\nonly be applied to variables, not predicates or functions.\\n7.15 Soundness\\nWe have seen that a logical system such as propositional logic consists of a'), Document(metadata={}, page_content='syntax, a semantics, and a set of rules of deduction. A logical system also\\nhas a set of fundamental truths, which are known as axioms. The axioms\\nare the basic rules that are known to be true and from which all other theo-\\nrems within the system can be proved.\\nAn axiom of propositional logic, for example, is\\nA → (B → A)\\nA theorem of a logical system is a statement that can be proved by applying\\nthe rules of deduction to the axioms in the system.\\nIf A is a theorem, then we write\\n⊢A'), Document(metadata={}, page_content='If A is a theorem, then we write\\n⊢A\\nA logical system is described as being sound if every theorem is logically\\nvalid, or a tautology.\\nIt can be proved by induction that both propositional logic and FOPL\\nare sound.\\n7.16 Completeness\\nA logical system is complete if every tautology is a theorem—in other\\nwords, if every valid statement in the logic can be proved by applying the\\nrules of deduction to the axioms. Both propositional logic and FOPL are'), Document(metadata={}, page_content='complete. The proofs that these systems are complete are rather complex.\\n7.17 Decidability\\nA logical system is decidable if it is possible to produce an algorithm that\\nwill determine whether any wff is a theorem. In other words, if a logical\\nsystem is decidable, then a computer can be used to determine whether\\nlogical expressions in that system are valid or not.'), Document(metadata={}, page_content='7.19 Abduction and Inductive Reasoning 201\\nWe can prove that propositional logic is decidable by using the fact that it is\\ncomplete. Thanks to the completeness of propositional logic, we can prove\\nthat a wff A is a theorem by showing that it is a tautology. T o show if a wff\\nis a tautology, we simply need to draw up a truth table for that wff and\\nshow that all the lines have true as the result. This can clearly be done algo-\\nrithmically because we know that a truth table for n values has 2'), Document(metadata={}, page_content='n lines and\\nis therefore finite, for a finite number of variables.\\nFOPL, on the other hand, is not decidable. This is due to the fact that it is\\nnot possible to develop an algorithm that will determine whether an arbi-\\ntrary wff in FOPL is logically valid.\\n7.18 Monotonicity\\nA logical system is described as being monotonic if a valid proof in the sys-\\ntem cannot be made invalid by adding additional premises or assumptions.\\nIn other words, if we find that we can prove a conclusion C by applying'), Document(metadata={}, page_content='rules of deduction to a premise B with assumptions A, then adding addi-\\ntional assumptions A/H11032and B/H11032will not stop us from being able to deduce C.\\nBoth propositional logic and FOPL are monotonic. Elsewhere in this book,\\nwe learn about probability theory, which is not a monotonic system.\\nMonotonicity of a logical system can be expressed as follows:\\nIf we can prove {A, B} \\n⊢C,\\nthen we can also prove: {A, B, A/H11032,B /H11032} ⊢C.'), Document(metadata={}, page_content='Note that A/H11032and B/H11032can be anything, including ¬A and ¬B. In other words,\\neven adding contradictory assumptions does not stop us from making the\\nproof in a monotonic system. In fact, it turns out that adding contradictory\\nassumptions allows us to prove anything, including invalid conclusions.\\nThis makes sense if we recall the line in the truth table for →, which shows\\nthat false → true. By adding a contradictory assumption, we make our'), Document(metadata={}, page_content='assumptions false and can thus prove any conclusion.\\n7.19 Abduction and Inductive Reasoning\\nThe kind of reasoning that we have seen so far in this chapter has been\\ndeductive reasoning, which in general is based on the use of modus ponens\\nand the other deductive rules of reasoning. This kind of reasoning assumes'), Document(metadata={}, page_content='202 CHAPTER 7 Propositional and Predicate Logic\\nthat we are dealing with certainties and does not allow us to reason about\\nthings of which we are not certain. As we see elsewhere in this book, there is\\nanother kind of reasoning, inductive reasoning, which does not have the\\nsame logical basis but can be extremely powerful for dealing with situations\\nin which we lack certainty.\\nStrangely, another form of reasoning, abduction, is based on a common\\nfallacy, which can be expressed as'), Document(metadata={}, page_content='fallacy, which can be expressed as\\nNote that abduction is very similar to modus ponens but is not logically\\nsound. A typical example of using this rule might be “When Jack is sick, he\\ndoesn’t come to work. Jack is not at work today. Therefore Jack is sick.”\\nIn fact, Jack may be having a holiday, or attending a funeral, or it may be\\nSunday or Christmas Day.\\nGiven that this type of reasoning is invalid, why are we discussing it here?'), Document(metadata={}, page_content='It turns out that although abduction does not provide a logically sound\\nmodel for reasoning, it does provide a model that works reasonably well\\nin the real world because it allows us to observe a phenomenon and pro-\\npose a possible explanation or cause for that phenomenon without com-\\nplete knowledge. Abductive reasoning is discussed in more detail in\\nChapter 17.\\nInductive reasoning enables us to make predictions about what will hap-'), Document(metadata={}, page_content='pen, based on what has happened in the past. Humans use inductive rea-\\nsoning all the time without realizing it. In fact, our entire lives are based\\naround inductive reasoning, for example, “the sun came up yesterday and\\nthe day before, and every day I know about before that, so it will come up\\nagain tomorrow. ” It’s possible it won’t, but it seems fairly unlikely. This kind\\nof reasoning becomes more powerful when we apply probabilities to it, as'), Document(metadata={}, page_content='in “I’ve noticed that nearly every bird I see is a swallow. Therefore, it’s quite\\nlikely that that bird is a swallow. ”\\nAs we will see, these kinds of reasoning are extremely useful for dealing\\nwith uncertainty and are the basis of most of the learning techniques used\\nin Artificial Intelligence.\\nB      A B\\nA\\n→'), Document(metadata={}, page_content='7.20 Modal Logics and Possible Worlds 203\\n7.20 Modal Logics and Possible Worlds\\nThe forms of logic that we have dealt with so far deal with facts and prop-\\nerties of objects that are either true or false. In these classical logics,w e  d o\\nnot consider the possibility that things change or that things might not\\nalways be as they are now.\\nModal logics are an extension of classical logic that allow us to reason\\nabout possibilities and certainties. In other words, using a modal logic, we'), Document(metadata={}, page_content='can express ideas such as “although the sky is usually blue, it isn’t always”\\n(for example, at night). In this way, we can reason about possible worlds. A\\npossible world is a universe or scenario that could logically come about.\\nThe following statements may not be true in our world, but they are possible,\\nin the sense that they are not illogical, and could be true in a possible world:\\nTrees are all blue.\\nDogs can fly.\\nPeople have no legs.'), Document(metadata={}, page_content='Dogs can fly.\\nPeople have no legs.\\nIt is possible that some of these statements will become true in the future,\\nor even that they were true in the past. It is also possible to imagine an\\nalternative universe in which these statements are true now. The following\\nstatements, on the other hand, cannot be true in any possible world:\\nA\\n∧¬ A\\n(x > y) ∧ (y > z) ∧ (z > x)\\nThe first of these illustrates the law of the excluded middle , which simply'), Document(metadata={}, page_content='states that a fact must be either true or false: it cannot be both true and\\nfalse. It also cannot be the case that a fact is neither true nor false. This is a\\nlaw of classical logic, and as we see in Chapter 18, it is possible to have a log-\\nical system without the law of the excluded middle, and in which a fact can\\nbe both true and false.\\nThe second statement cannot be true by the laws of mathematics. We are\\nnot interested in possible worlds in which the laws of logic and mathemat-'), Document(metadata={}, page_content='ics do not hold.\\nA statement that may be true or false, depending on the situation, is called\\ncontingent. A statement that must always have the same truth value,'), Document(metadata={}, page_content='204 CHAPTER 7 Propositional and Predicate Logic\\nregardless of which possible world we consider, is noncontingent.H e n c e ,\\nthe following statements are contingent:\\nA ∧ B\\nA ∨ B\\nI like ice cream.\\nThe sky is blue.\\nThe following statements are noncontingent:\\nA\\n∨¬ A\\nA ∧¬ A\\nIf you like all ice cream, then you like this ice cream.\\nClearly, a noncontingent statement can be either true or false, but the fact\\nthat it is noncontingent means it will always have that same truth value.'), Document(metadata={}, page_content='If a statement A is contingent, then we say that A is possibly true, which\\nis written\\n/H17003A\\nIf A is noncontingent, then it is necessarily true, which is written\\nA\\n7.20.1 Reasoning in Modal Logic\\nIt is not possible to draw up a truth table for the operators /H17003and . (Con-\\nsider the four possible truth tables for unary operators—it should be clear\\nthat none of these matches these operators.) It is possible, however, to rea-\\nson about them.'), Document(metadata={}, page_content='son about them.\\nThe following rules are examples of the axioms that can be used to reason\\nin this kind of modal logic:\\nA → /H17003A\\n¬A → ¬/H17003A\\n/H17003A → ¬ A\\nAlthough truth tables cannot be drawn up to prove these rules, you should\\nbe able to reason about them using your understanding of the meaning of\\nthe and /H17003operators.\\nModal logic, and other nonclassical logics, are discussed in Chapter 17.'), Document(metadata={}, page_content='7.23 Review Questions 205\\n7.21 Dealing with Change\\nAs we have seen, classical logics do not deal well with change. They assume\\nthat if an object has a property, then it will always have that property and\\nalways has had it. Of course, this is not true of very many things in the real\\nworld, and a logical system that allows things to change is needed. The sit-\\nuation and event calculi are covered in more detail in Chapters 17 and 19.\\n7.22 Chapter Summary'), Document(metadata={}, page_content='7.22 Chapter Summary\\n■ Logic is primarily concerned with the logical validity of state-\\nments, rather than with truth.\\n■ Logic is widely used in Artificial Intelligence as a representa-\\ntional method.\\n■ Abduction and inductive reasoning are good at dealing with\\nuncertainty, unlike classical logic.\\n■ The main operators of propositional logic are ∧, ∨, ¬, →, and ↔\\n(and, or, not, implies, and iff).\\n■ The behavior of these logical operators can be expressed in truth'), Document(metadata={}, page_content='tables. Truth tables can also be used to solve complex problems.\\n■ Propositional logic deals with simple propositions such as “I like\\ncheese. ” First-order predicate logic allows us to reason about more\\ncomplex statements such as “All people who eat cheese like cats, ”\\nusing the quantifiers ∀ and ∃ (“for all” , and “there exists”).\\n■ A statement that is always true in any situation is called a tautology.\\nA ∨¬ A is an example of a tautology.'), Document(metadata={}, page_content='A ∨¬ A is an example of a tautology.\\n■ Two statements are logically equivalent if they have the same\\ntruth tables.\\n■ First-order predicate logic is sound and complete, but not decid-\\nable. Propositional logic is sound, complete, and decidable.\\n■ Modal logics allow us to reason about certainty.\\n7.23 Review Questions\\n7.1 Explain the meanings of the following terms in the context of logic:\\na. truth\\nb. validity'), Document(metadata={}, page_content='206 CHAPTER 7 Propositional and Predicate Logic\\nc. equivalent\\nd. uncertainty\\ne. tautology\\nf. satisfiable\\ng. sound\\nh. complete\\ni. decidable\\nj. modal logic\\n7.2 “Inductive reasoning is a reasonable way to think about everyday\\nlife, but it does not provide the logical structure that propositional\\nlogic does. ” Discuss.\\n7.3 Explain what is meant by the following: “Classical logics are not\\ngood at dealing with uncertainty. ”\\n7.4 Explain why the addition of the quantifiers ∀ and ∃ makes predi-'), Document(metadata={}, page_content='cate calculus so powerful.\\n7.5 Explain the rule of modus ponens. Explain how it is used in every-\\nday life.\\n7.6 Explain in layman’s terms what the law of the excluded middle\\nmeans. What difficulties might you encounter in logical deduction\\nif you ignored the law of the excluded middle?\\n7.7 Assume the law of the excluded middle is not true, and use this to\\nprove the equality 1 = 0.\\n7.8 What does it mean to say that a logic is monotonic? Is proposi-'), Document(metadata={}, page_content='tional logic monotonic? What complexities do you think nonmo-\\nnotonicity would add to the process of logical deduction? Would\\nmodus ponens still hold in a nonmonotonic logic?\\n7.24 Exercises\\n7.1 Translate the following sentences into logical statements, using\\neither propositional or predicate logic as appropriate:\\na. I like apples and pears.\\nb. When I eat apples and pears, I usually like to have a walk.\\nc. Every apple that I have ever eaten has been delicious.'), Document(metadata={}, page_content='d. The fact that some pears are not delicious will not stop me eat-\\ning them.'), Document(metadata={}, page_content='7.24 Exercises 207\\ne. I can only eat an apple if I have first eaten a pear, and I can only\\neat a pear if I eat an apple immediately afterward.\\nf. There exists a book that includes details of every book.\\ng. There exists somewhere in the world a book that lists every\\nsingle person who doesn’t appear in any other book.\\nh. If you haven’t read the book that lists all other books, then you\\nhaven’t read any book, unless you’ve read the book that lists'), Document(metadata={}, page_content='books that do not exist, in which case you’ve read every book.\\n7.2 Draw a truth table for the following expression:\\n¬A ∧ (A ∨ B) ∧ (B ∨ C)\\n7.3 (Hard) Prove that propositional logic and first-order predicate\\nlogic are sound and complete.\\n7.4 Write expressions in propositional calculus to represent the follow-\\ning statements:\\na. If you go to Mexico, you will be far away.\\nb. I cannot hear you when you are far away.\\nc. When I can’t hear you, I forget what you look like.'), Document(metadata={}, page_content='d. If I come to Mexico, and I don’t know what you look like, I\\nwon’t be able to find you.\\ne. Therefore, if you go to Mexico, and I follow you, I won’t be\\nable to find you.\\nProve whether the conclusion follows from the premises or not.\\n7.5 Write expressions in first-order predicate logic to represent the fol-\\nlowing statements, and prove whether the conclusion follows from\\nthe premises or not:\\na. All dancers love to dance.\\nb. Everyone who sings and plays an instrument loves to dance.'), Document(metadata={}, page_content='c. Therefore, all dancers sing and play an instrument.\\n7.6 Prove the following:\\na.\\n⊢A → A\\nb. ⊢(( ¬A → ¬B) → A) → ((¬B → ¬A) → ¬B)\\nc. ⊢(¬¬¬ A → ¬¬¬ B) → (¬A → ¬B)'), Document(metadata={}, page_content='208 CHAPTER 7 Propositional and Predicate Logic\\n7.25 Further Reading\\nMost textbooks on Artificial Intelligence provide good coverage of logic.\\nAn excellent, short introduction to logic that provides more detail on most\\nof the subject than has been provided here is Kelly (1997). Lewis Carroll’s\\nwork, though over 100 years old, still makes for an interesting and relevant\\nread on the subject of logic and reasoning, although his approach was\\nrather different from that usually found today.'), Document(metadata={}, page_content='rather different from that usually found today.\\nThe idea of abduction was introduced by C. S. Peirce, in his 1878 paper\\nHow to Make Our Ideas Clear, published in Popular Science Monthly.\\nFrancis Bacon introduced the idea of inductive reasoning in 1620. His writ-\\nings on the subject can be found in The New Organon, and Related Writ-\\nings, published in 1960.\\nFrancis Bacon: The New Organon by Francis Bacon, edited by Lisa Jardine\\nand Michael Silverthorne (2002 – Cambridge University Press)'), Document(metadata={}, page_content='Propositional Logic: Deduction and Algorithms by Hans Kleine Büning and\\nTheodor Lettmann (1999 – Cambridge University Press)\\nSymbolic Logic and Game of Logic by Lewis Carroll (published in one vol-\\nume – 1958 – Dover Books).\\nPredicate Logic: The Semantic Foundations of Logic by Richard L. Epstein\\n(2000 – Wadsworth Publishing)\\nPropositional Logics: The Semantic Foundations of Logic by Richard L.\\nEpstein (2000 – Wadsworth Publishing)\\nThe Essence of Logic by John Kelly (1997 – Prentice Hall)'), Document(metadata={}, page_content='Introduction to Logic: Propositional Logic by Howard Pospesel (1999 – Pren-\\ntice Hall)\\nLogic for Computer Science by Steve Reeves and Michael Clarke (1993 –\\nAddison Wesley)\\nLogical Forms: An Introduction to Philosophical Logic by Mark Sainsbury\\n(1991 – Blackwell)\\nLogic and Prolog by Richard Spencer-Smith (1991 – Harvester Wheatsheaf)'), Document(metadata={}, page_content='8CHAPTER\\nInference and Resolution for\\nProblem Solving\\nEarly work in theorem proving programs for quantified logics culminated in 1965\\nwith Alan Robinson’s development of machine-oriented formulation of first-\\norder logic called Resolution (Robinson, 1965). There followed an immensely pro-\\nductive period of exploration of resolution-based theorem-proving.\\n—Alan Newell, The Knowledge Level\\nWhen you have eliminated the impossible, whatever remains, no matter how\\nimprobable, must be the truth.'), Document(metadata={}, page_content='improbable, must be the truth.\\n—Sir Arthur Conan Doyle, The Sign of Four\\nAt thirty a man suspects himself a fool;\\nKnows it at forty, and reforms his plan;\\nAt fifty chides his infamous delay,\\nPushes his prudent purpose to resolve;\\nIn all the magnanimity of thought\\nResolves; and re-resolves; then dies the same.\\n—Edward Y oung,Night Thoughts\\n8.1 Introduction\\nThis chapter introduces the main ideas behind automated reasoning, or theo-'), Document(metadata={}, page_content='rem proving. The method of resolution is discussed in some detail because it\\npertains to both propositional logic and first-order predicate logic (FOPL).\\nT o explain resolution, ideas such as unification, normal forms, and Her-\\nbrand universes are introduced. This chapter is somewhat more advanced'), Document(metadata={}, page_content='210 CHAPTER 8 Inference and Resolution for Problem Solving\\nthan Chapter 7 and assumes an understanding of propositional calculus\\nand first-order predicate calculus.\\nThis chapter also briefly explains how PROLOG uses resolution to process\\ndata and to provide solutions to problems. In fact, resolution is fundamen-\\ntal to the way that PROLOG works. Resolution is an important part of Arti-\\nficial Intelligence research and provides a common method for systems to'), Document(metadata={}, page_content='reason logically and to prove theorems in an automated manner. This has\\nadvantages over other theorem-proving methods that depend more on\\nintuition and experience and are thus best applied by humans. In this chap-\\nter, we show how resolution can be entirely automated and an algorithm\\ngenerated for using resolution to prove theorems.\\nThe representational methods discussed in this chapter and Chapter 7 pro-\\nvide a powerful tool for reasoning logically and, in particular, for enabling'), Document(metadata={}, page_content='computer systems to automatically reason about a database of facts. This\\nrepresentational method, though, is not suitable for all problems, and in\\nmany situations it is entirely inadequate.\\nIn particular, predicate logic does not have a mechanism for dealing with\\nchange or with time. We will discuss a number of alternative representations in\\nChapters 17 and 18 that overcome some of these difficulties. As we see in these'), Document(metadata={}, page_content='chapters, and in Chapter 19, intelligent agents and other Artificial Intelligence\\nsystems often need to reason about events, situations, and other time-based\\nfactors. The logic we are looking at here is better suited to static environments,\\nwhich is certainly appropriate for solving a number of problems.\\n8.2 Resolution in Propositional Logic\\nIn Chapter 7, we introduced a method of deductive reasoning in order to\\nmake proofs in predicate and propositional logic. It is not clear how this'), Document(metadata={}, page_content='process might be automated because at each stage an amount of initiative\\nwas required to choose the right next step. We now introduce a proof\\nmethod, resolution, which can be automated because it involves a fixed set\\nof steps. Before we examine resolution, we must introduce some key ideas.\\n8.2.1 Normal Forms\\nA sentence or well-formed formula (wff) is inconjunctive normal formif it\\nis of the following form:\\nA1 ∧ A2 ∧ A3 ∧ ... ∧ An'), Document(metadata={}, page_content='8.2 Resolution in Propositional Logic 211\\nwhere each clause, Ai, is of the form\\nB1 ∨ B2 ∨ B3 ∨ ... ∨ Bn\\nEach Bi is a literal, where a literal is a basic symbol of propositional logic. In\\nfact, a literal can be more accurately defined as an atom or an atom that is\\nnegated, where an atom is one of the basic object symbols in propositional\\nlogic. Hence, in the following expression:\\nA\\n∧ B ∨ (¬C ∧ D)\\nA is an atom, as are B, C, and D. The literals are A, B, ¬C, and D.'), Document(metadata={}, page_content='So, an expression is in conjunctive normal form (often written CNF) if it\\nconsists of a set of or phrases anded together, such as:\\nA ∧ (B ∨ C) ∧ (¬A ∨¬ B ∨¬ C ∨ D)\\nTrivially, a literal is also in CNF.\\nA sentence is in disjunctive normal form (DNF) if it consists of a set of\\nand phrases ored together, as in\\nA\\n∨ (B ∧ C) ∨ (¬A ∧¬ B ∧¬ C ∧ D)\\nAny wff can be converted to CNF by using the following equivalences,\\nwhich we have encountered previously:\\n1. A ↔ B /H11013(A → B) ∧ (B → A)'), Document(metadata={}, page_content='1. A ↔ B /H11013(A → B) ∧ (B → A)\\n2. A → B /H11013¬A ∨ B\\n3. ¬(A ∧ B) /H11013¬A ∨¬ B\\n4. ¬(A ∨ B) /H11013¬A ∧¬ B\\n5. ¬¬A /H11013A\\n6. A ∨ (B ∧ C) /H11013(A ∨ B) ∧ (A ∨ C)\\nFor example, we will convert (A → B) → C to CNF:\\n(A → B) → C\\n¬(A → B) ∨ C (2)\\n¬(¬A ∨ B) ∨ C (3)\\n(A ∧¬ B) ∨ C (4)\\n(A ∨ C) ∧ (¬B ∨ C) (6)'), Document(metadata={}, page_content='212 CHAPTER 8 Inference and Resolution for Problem Solving\\nA further example follows:\\nA ↔ (B ∧ C)\\n(A → (B ∧ C)) ∧ ((B ∧ C) → A) (1)\\n(¬A ∨ (B ∧ C)) ∧ (¬(B ∧ C) ∨ A) (2)\\n(¬A ∨ (B ∧ C)) ∧ (¬B ∨¬ C ∨ A) (3)\\n(¬A ∨ B) ∧ (¬A ∨ C) ∧ (¬B ∨¬ C ∨ A) (6)\\nNote that this process can be automated, as the equivalences can always be\\napplied in the order they were listed, replacing symbols and constructions\\nas they are encountered. As we will see, this is a useful fact: an algorithm'), Document(metadata={}, page_content='can be expressed that will convert any wff into CNF.\\nHaving converted a wff into CNF, we can now express it as a set of clauses.\\nSo our expression above\\n(\\n¬A ∨ B) ∧ (¬A ∨ C) ∧ (¬B ∨¬ C ∨ A)\\nwould be represented in clause form as\\n{(¬A, B), (¬A, C), (¬B, ¬C, A)}\\n8.2.2 The Resolution Rule\\nNow we introduce a new rule to sit alongside the rules presented in Section\\n7.10, which is called the resolution rule:\\nThis rule is not as immediately obvious as the rules in Section 7.10, but it'), Document(metadata={}, page_content='does prove to be extremely useful. It can also be written as follows:\\nIn this form, the rule can be seen to be saying that implication is transitive,\\nor in other words, if A implies B and B implies C, then A implies C.\\nThis can be applied to wff’s in clause form, as follows:\\nIf a wff contains a clause that contains literal L and another clause that con-\\ntains literal \\n¬L, then these two clauses can be combined together, and L and\\n¬L can be removed from those clauses. For example,\\n¬→ →\\n¬→'), Document(metadata={}, page_content='¬→ →\\n¬→\\nA B       B C\\nAC\\nA B       B C\\nAC\\n∨¬ ∨\\n∨'), Document(metadata={}, page_content='8.2 Resolution in Propositional Logic 213\\n{(A, B), (¬B, C)}\\ncan be resolved to give\\n{(A, C)}\\nSimilarly,\\n{(A, B, C), D, (¬A, D, E), (¬D, F)}\\ncan be resolved to give\\n{ ( B ,C ,D ,E ) ,D ,(¬D, F)}\\nwhich can be further resolved to give either\\n{ ( B ,C ,D ,E ) ,F }\\nor\\n{ ( B ,C ,E ,F ) ,D }\\nNote that at the first step, we also had a choice and could have resolved to\\n{(A, B, C), D, (¬A, E, F)}\\nwhich can be further resolved to give\\n{ ( B ,C ,E ,F ) ,D }\\nNow, if wff P resolves to give wff Q, we write'), Document(metadata={}, page_content='Now, if wff P resolves to give wff Q, we write\\nP |= Q\\nFor example, we can resolve (A ∨ B) ∧ (¬A ∨ C) ∧ (¬B ∨ C) as follows:\\n{(A, B), (¬A, C), (¬B, C)}\\n{(B, C), (¬B, C)}\\n{C}\\nWe can express this as\\n(A ∨ B) ∧ (¬A ∨ C) ∧ (¬B ∨ C) |= C\\nIf we resolve two clauses, we produce the resolvent of those clauses. The\\nresolvent is a logical consequence of the two clauses.\\n8.2.3 Resolution Refutation\\nNow let us resolve the following clauses:\\n{(¬A, B), (¬A, ¬B, C), A,¬C}'), Document(metadata={}, page_content='214 CHAPTER 8 Inference and Resolution for Problem Solving\\nWe begin by resolving the first clause with the second clause, thus eliminat-\\ning B and ¬B:\\n{(¬A, C), A,¬C}\\n{C, ¬C}\\n⊥\\nThe fact that this resolution has resulted in falsum means that the original\\nclauses were inconsistent. We have refuted the original clauses, using reso-\\nlution refutation. We can write\\n{(¬A, B), (¬A, ¬B, C), A,¬C} |= ⊥\\nThe idea behind resolution refutation is explained in more detail in Sec-\\ntion 8.2.4.'), Document(metadata={}, page_content='tion 8.2.4.\\n8.2.4 Proof by Refutation\\nProof by refutation (also known as proof by contradiction ), as used in\\nresolution refutation, is a powerful method for solving problems. For\\nexample, let us imagine that we want to determine whether the following\\nlogical argument is valid:\\nIf it rains and I don’t have an umbrella, then I will get wet.\\nIt is raining, and I don’t have an umbrella.\\nTherefore, I will get wet.\\nWe can rewrite this in propositional calculus as follows:\\n(A\\n∧¬ B) → C\\nA ∧¬ B\\n∴C'), Document(metadata={}, page_content='(A\\n∧¬ B) → C\\nA ∧¬ B\\n∴C\\nT o prove this by refutation, we first negate the conclusion and convert the\\nexpressions into clause form. The first expression is the only one that is not\\nalready in CNF, so first we convert this to CNF as follows:\\n(A\\n∧¬ B) → C\\n/H11013¬(A ∧¬ B) ∨ C\\n/H11013¬A ∨ B ∨ C'), Document(metadata={}, page_content='8.2 Resolution in Propositional Logic 215\\nNow, to prove that our conclusion is valid, we need to show that\\n{(¬A, B, C), A,¬B, ¬C} |= ⊥\\nWe resolve these clauses as follows:\\n{(B, C), ¬B, ¬C}\\n{C, ¬C}\\n⊥\\nHence, in showing that by negating our conclusion we lead to a contradic-\\ntion, we have shown that our original conclusion must have been true.\\nIf this process leads to a situation where some clauses are unresolved, and\\nfalsum cannot be reached, we have shown that the clauses with the negated'), Document(metadata={}, page_content='conclusion are not contradictory and that therefore the original conclusion\\nwas not valid.\\nBecause following resolution explanations in this way can be confusing, it is\\noften preferable to present a resolution proof in the form of a tree, where pairs\\nof resolved clauses are connected together, as shown in the following proof:\\nA → B\\nB → C\\nC → D\\nD → E\\n∨ F\\n∴ A → F\\nFirst we negate the conclusion, to give: ¬(A → F). Next we convert to\\nclause form:\\nD → E ∨ F\\n/H11013¬D ∨ (E ∨ F)\\nand\\n¬(A → F)'), Document(metadata={}, page_content='D → E ∨ F\\n/H11013¬D ∨ (E ∨ F)\\nand\\n¬(A → F)\\n/H11013¬(¬A ∨ F)\\n/H11013A ∧¬ F\\nSo, our clauses are\\n{(¬A, B), (¬B, C), (¬C, D), (¬D, E, F), A,¬F)}'), Document(metadata={}, page_content='216 CHAPTER 8 Inference and Resolution for Problem Solving\\nOur proof in tree form is as follows:\\nWe have not been able to reach falsum because we are left with a single\\nclause {E}, which cannot be resolved with anything. Hence, we can con-\\nclude that our original conclusion was not valid. Y ou can prove this for\\nyourself using a truth table.\\n8.3 Applications of Resolution\\nClearly, resolution can be used to automate the process of proving whether'), Document(metadata={}, page_content='a conclusion can be derived in a valid way from a set of premises or not.\\nThis is certainly useful, but resolution is not limited in its applications to\\njust proving logical arguments.\\nA combinatorial search problem is a problem where there are a number of\\nvariables, each of which can be assigned a particular value. For example, a\\njigsaw puzzle can be seen as a problem where each piece is represented by a\\nvariable, and the position that each piece is placed in is the value assigned'), Document(metadata={}, page_content='to that variable. Of course, the point of the puzzle is that there is only one\\ncorrect way to place the pieces, so correspondingly, there would only be one\\nset of assignments of values to variables that would be correct. We have\\nalready examined combinatorial search problems and seen ways to solve\\nthem in Chapter 5. Although resolution cannot help us to solve such prob-\\nlems, it can help by telling us whether a solution exists or not.'), Document(metadata={}, page_content='An example of a combinatorial search problem is the three-coloring prob-\\nlem: Given a map, is it possible to color the countries using three colors\\nsuch that no two countries that are next to each other have the same color?\\n(¬ A, B)\\n(¬ A, C)\\n(¬ A, D)\\n(¬ B, C) ( ¬ C, D) ( ¬ D, E, F)\\n(¬ A, E, F)\\n(E, F)\\n¬ FA\\nE'), Document(metadata={}, page_content='8.3 Applications of Resolution 217\\nA\\nBC D\\nFigure 8.1\\nA graph representing a\\nthree-coloring problem\\nA\\nB\\nC\\nD\\nFigure 8.2\\nGraph that cannot be\\nthree-colored\\nA slightly more general version of the three-coloring problem for maps is\\nto represent the countries in the three-coloring problem as nodes on a\\ngraph, as in Figure 8.1. The problem now is to assign values from a set of\\nthree possible values to each of the nodes in the graph, such that no two'), Document(metadata={}, page_content='nodes that are joined by an edge have been assigned the same value.\\nFor example, in the graph in Figure 8.1, the following assignments would\\nbe a suitable three-coloring:\\nA = red\\nB = green\\nC = blue\\nD = green\\nNote that in the graph shown in Figure 8.2, no suitable three-coloring exists.\\nAlso note that some graphs do not represent maps of countries, as in the\\ncase of a graph with five nodes where every pair of nodes is connected by'), Document(metadata={}, page_content='an edge. However, any map of countries can be represented by a graph, and\\nsolving the three-coloring problem for that graph is equivalent to solving\\nthe three-coloring problem for the map.\\nThe three-coloring problem for graphs can be solved using resolution and\\npropositional logic, by representing the graph as a set of clauses and deter-\\nmining whether the clauses can be satisfied.'), Document(metadata={}, page_content='218 CHAPTER 8 Inference and Resolution for Problem Solving\\nThe representation is generated as follows:\\nFirst we can represent which color has been assigned to a vertex, as follows:\\nAr means that vertex A has been colored red.\\nAg means that vertex A has been colored green.\\nAb means that vertex A has been colored blue.\\nAnd so on for all vertices.\\nHence, for each vertex, we can generate the following set of clauses to repre-'), Document(metadata={}, page_content='sent that it must be given a color, but cannot be given more than one color:\\nAr ∨ Ag ∨ Ab\\n¬Ar ∨¬ Ag (/H11013Ar → ¬Ag)\\n¬Ag ∨¬ Ab\\n¬Ab ∨¬ Ar\\nSimilarly, for each edge, we can represent the fact that the two nodes at\\neither end of the edge must have different colors:\\nIf (A,B) is an edge, then:\\n¬Ar ∨¬ Br\\n¬Ab ∨¬ Bb\\n¬Ag ∨¬ Bg\\nNow, if we ∧ these sets of clauses together, and apply resolution to the\\nresult, we can see if the set is satisfiable. If so, then a three-coloring solution'), Document(metadata={}, page_content='does exist for the graph. If not, then it does not.\\nIn the same way, any instance of a combinatorial search problem can be\\nrepresented as a set of clauses, and their satisfiability can be tested using\\nresolution. This method tells us if a solution exists but does not tell us what\\nthat solution is. We have already seen in Part 2 of this book how solutions\\nto such problems can be found using search.\\n8.4 Resolution in Predicate Logic'), Document(metadata={}, page_content='8.4 Resolution in Predicate Logic\\nResolution as applied to propositional logic can also be applied to FOPL by\\nreducing FOPL expressions to a suitable normal form. The methods used\\nto carry out this process are described in the following sections.'), Document(metadata={}, page_content='8.5 Normal Forms for Predicate Logic 219\\n8.5 Normal Forms for Predicate Logic\\nT o apply resolution to FOPL expressions, we first need to deal with the\\npresence of the quantifiers ∀ and ∃. The method that is used is to move\\nthese quantifiers to the beginning of the expression, resulting in an expres-\\nsion that is in prenex normal form.\\nIn converting a wff to prenex normal form, we use the same rules as we\\nused to convert a wff to CNF:\\n1. A ↔ B /H11013(A → B)\\n∧ (B → A)\\n2. A → B /H11013¬A ∨ B'), Document(metadata={}, page_content='∧ (B → A)\\n2. A → B /H11013¬A ∨ B\\n3. ¬(A ∧ B) /H11013¬A ∨¬ B\\n4. ¬(A ∨ B) /H11013¬A ∧¬ B\\n5. ¬¬A /H11013A\\n6. A ∨ (B ∧ C) /H11013(A ∨ B) ∧ (A ∨ C)\\nIn addition, we use the following rules to move the quantifiers to the front:\\n7. ¬(∀x)A(x) /H11013(∃x)¬A(x)\\n8. ¬(∃x)A(x) /H11013(∀x)¬A(x)\\n9. ( ∀x)A(x) ∧ B /H11013(∀x)(A(x) ∧ B)\\n10. ( ∀x)A(x) ∨ B /H11013(∀x)(A(x) ∨ B)\\n11. ( ∃x)A(x) ∧ B /H11013(∃x)(A(x) ∧ B)\\n12. ( ∃x)A(x) ∨ B /H11013(∃x)(A(x) ∨ B)\\n13. ( ∀x)A(x) ∧ (∀y)B(y) /H11013(∀x)(∀y)(A(x) ∧ B(y))'), Document(metadata={}, page_content='14. ( ∀x)A(x) ∧ (∃ y)B(y) /H11013(∀x)(∃y)(A(x) ∧ B(y))\\n15. ( ∃x)A(x) ∧ (∀y)B(y) /H11013(∃x)(∀y)(A(x) ∧ B(y))\\n16. ( ∃x)A(x) ∧ (∃y)B(y) /H11013(∃x)(∃y)(A(x) ∧ B(y))\\nNote that rules 9, 10, 11, and 12 can be used only if x does not occur in B.\\nLet us briefly examine why some of these rules make logical sense. Rule 7,\\nfor example, can be stated in layman’s terms as “if it is not true that all x’s\\nhave property A, then there must exist some x for which A is not true. ” This\\nis clearly valid.'), Document(metadata={}, page_content='220 CHAPTER 8 Inference and Resolution for Problem Solving\\nSimilarly, rule 8 states “if there does not exist an x for which A is true, then\\nA is not true for any x.”\\nRules 9 through 12 take advantage of the fact that x is not present in B.\\nHence, if B is true, then it is also true for all x:\\nB →∀ xB\\nprovided x is not free in B.\\nRules 13 through 16 similarly take advantage of the fact that x is not pres-\\nent in B(y) and that y is not present in A(x).'), Document(metadata={}, page_content='ent in B(y) and that y is not present in A(x).\\nA further set of rules (17–20) can be generated by replacing ∧ with ∨ in\\nrules 13 through 16.\\nFor example, let us convert the following wff to prenex normal form:\\n(∀x)(A(x) → B(x)) → (∃y)(A(y) ∧ B(y))\\n¬(∀x)(A(x) → B(x)) ∨ (∃y)(A(y) ∧ B(y)) (2)\\n¬(∀x)(¬A(x) ∨ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (2)\\n(∃x)¬(¬A(x) ∨ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (7)\\n(∃x)(¬¬A(x) ∧¬ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (4)\\n(∃x)(A(x) ∧¬ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (5)'), Document(metadata={}, page_content='(∃x)(A(x) ∧¬ B(x)) ∨ (∃y)(A(y) ∧ B(y)) (5)\\n(∃x)(∃y)((A(x) ∧¬ B(x)) ∨ (A(y) ∧ B(y))) (19)\\n(∃x)(∃y)(((A(x) ∨ A(y)) ∧ (¬B(x) ∨ A(y)) ∧ (A(x) ∨ B(y)) ∧\\n(¬B(x) ∨ B(y)))) (6)\\n8.6 Skolemization\\nBefore resolution can be carried out on a wff, we need to eliminate all the\\nexistential quantifiers, ∃, from the wff.\\nThis is done by replacing a variable that is existentially quantified by a con-\\nstant, as in the following case:\\n∃(x) P(x)\\nwould be converted to\\nP(c)'), Document(metadata={}, page_content='8.6 Skolemization 221\\nwhere c is a constant that has not been used elsewhere in the wff . Although\\nP(c) is not logically equivalent to ∃(x) P(x), we are able to make this substi-\\ntution in the process of resolution because we are interested in seeing\\nwhether a solution exists. If there exists some x for which P(x) holds, then\\nwe may as well select such an x and name it c. This process is called skolem-\\nization, and the variable c is called a skolem constant.'), Document(metadata={}, page_content='The variable c can be thought of as an example of a suitable value for x. It is\\nextremely important that c not appear anywhere else in the expression\\nbecause that would create a conflict. Imagine, for example, replacing x with\\nb in the following expression:\\n∃x(x\\n∨ b)\\nThis would leave us with\\nb ∨ b\\nwhich reduces to\\nb\\nThis clearly is not the same expression, and, in fact, it should be skolemized\\nusing a different constant, such as:\\nc ∨ b'), Document(metadata={}, page_content='using a different constant, such as:\\nc ∨ b\\nSkolemization must proceed slightly differently in cases where the ∃ follows\\na ∀ quantifier, as in the following example:\\n(∀x)(∃y)(P(x,y))\\nIn this case, rather than replacing y with a skolem constant, we must replace\\nit with a skolem function, such as in the following:\\n(∀x)(P(x,f(x))\\nNote that the skolem function is a function of the variable that is univer-\\nsally quantified, in this case, x.'), Document(metadata={}, page_content='sally quantified, in this case, x.\\nHaving removed the existential quantifiers in this way, the wff is said to be\\nin skolem normal form, and has been skolemized.\\n8.6.1 Example of Skolemization\\nThe following expression\\n(∀x)(∃y)(∀z)(P(x) ∧ Q(y, z))'), Document(metadata={}, page_content='222 CHAPTER 8 Inference and Resolution for Problem Solving\\nwould be skolemized as follows:\\n(∀x)(∀z)(P(x) ∧ Q(f(x), z)\\nNote that y has been replaced by f(x) because ∃y occurred after ∀x.\\n8.6.2 Second Example of Skolemization\\nThe following expression:\\n(∀w)(∀x)(∃y)(∀z)(P(x) ∧ Q(w, y, z))\\nwould be skolemized as follows:\\n(∀w)(∀x)(∀z)(P(x) ∧ Q(w, f(w,x), z)\\nHere y has been replaced by a function of both w and x: f(w,x) because ∃y\\noccurred after ∀w and ∀x.'), Document(metadata={}, page_content='occurred after ∀w and ∀x.\\nT o proceed with resolution, this wff must now be represented as set of\\nclauses. T o do this, we first drop the universal quantifiers. Hence, our\\nexpression above will be represented as the following set of two clauses:\\n{(P(x)), (Q(w, f(w,x), z))}\\n8.6.3 Unification\\nT o carry out resolution on wff’s in FOPL, we need to carry out one final\\nstage, which is to make substitutions.\\nFor example, if we had the following set of clauses:\\n{(P(w,x)), (¬P(y,z))}'), Document(metadata={}, page_content='{(P(w,x)), (¬P(y,z))}\\nIt seems clear that we should be able to resolve P with ¬P, but they are not\\ncurrently identical because they have different arguments. These clauses\\ncan be resolved by making a substitution. We will replace w with y and x\\nwith z, to result in the following clauses:\\n{(P(y,z)), (\\n¬P(y,z))}\\nThese can now clearly be resolved to give falsum.\\nThe substitution that was made here can be written as\\n{y/w, z/x}'), Document(metadata={}, page_content='{y/w, z/x}\\nIn this case, it was easy to see which substitution to make, but with more\\ncomplex clauses, it can be harder. A formal process exists for determining'), Document(metadata={}, page_content='8.6 Skolemization 223\\nhow to make these substitutions, which of course means the process can be\\nautomated.\\nIn general, we use the symbol /H9268to indicate a substitution, and we can write\\n/H9268= {y/w, z/x}\\nA = P(w,x)\\nB = \\n¬P(y,z)\\nA/H9268= P(y,z)\\nB/H9268= ¬P(y,z)\\nIn general, if a substitution can be applied to a set of clauses { A, B, C,\\n...}s u c h that\\nA/H9268= B/H9268= C/H9268=  ...\\nThen /H9268is called a unifier for the set {A, B, C,...} .'), Document(metadata={}, page_content='In some cases, more than one substitution needs to be applied to produce a\\nform that can be resolved. The operator, o, can be applied to two substitu-\\ntions to provide thecomposition of those two substitutions, which produces\\na third substitution that is effectively the same as applying both substitutions.\\nLet us take two substitutions /H92681 and /H92682, defined as follows:\\n/H92681 = {a\\n1/x1,a 2/x2,...,a m/xm}\\n/H92682 = {b1/y1,b 2/y2,...,b n/yn}'), Document(metadata={}, page_content='/H92682 = {b1/y1,b 2/y2,...,b n/yn}\\nNow we define the composition of these two substitutions as follows:\\n/H92681 o /H92682 = {a1/H92682/x1,a 2/H92682/x2,...,a n/H92682/xn, b1/y1,b 2/y2,...,b n/yn}\\nCertain elements on the composite set can be eliminated:\\nIf yi = xj then bi/yi can be eliminated.\\nIf ai/H92682 = xi then ai/H92682/xi can be eliminated.\\nFor example, let us form the composition of the following two substitutions:\\n/H92681 = {a/x, x/y, f(a)/z}\\n/H92682 = {y/x, f(z)/y}'), Document(metadata={}, page_content='/H92682 = {y/x, f(z)/y}\\n/H92681 o /H92682 = {a/H92682/x, x/H92682/y, f(a)/H92682/z, y/x, f(z)/y}\\n= {a/x, y/y, f(a)/z, y/x, f(z)/y}\\n= {a/x, f(a)/z, f(z)/y}'), Document(metadata={}, page_content='224 CHAPTER 8 Inference and Resolution for Problem Solving\\nNotice that y/y is removed because this is a form of ai/H92682 = xi and that y/x is\\nremoved because this matches the elimination rule where yi = xj.\\nNow let us find /H92682 o /H92681:\\n/H92681 = {a/x, x/y, f(a)/z}\\n/H92682 = {y/x, f(z)/y}\\n/H92682 o /H92681 = {y/H92681/x, f(z)/H92681/y, a/x, x/y, f(a)/z}\\n= {x/x, f(f(a))/y, a/x, x/y, f(a)/z}\\n= {f(f(a))/y, f(a)/z}'), Document(metadata={}, page_content='= {f(f(a))/y, f(a)/z}\\nHence, the o operator is not commutative, as /H92681 o /H92682 ≠ /H92682 o /H92681.\\nSome rules determine the way in which unifiers can be applied:\\n1. A constant, such as a, cannot be replaced by a variable.\\n2. A variable x cannot be replaced by a term that contains x.\\nHence, for example, the following substitutions are not valid unifiers:\\n{P(x)/x}\\n{x/a}\\n8.6.4 Most General Unifiers\\nA unifier u1 is called a most general unifier for a set S = {A, B, C,...}  i f'), Document(metadata={}, page_content='any other unifier u2 can be expressed as the composition of u 1 with some\\nother substitution (i.e., u2 = u1 o u3).\\nA most general unifier ( mgu) is a unique unifier that provides the most\\ngeneral set of substitutions to unify a set of clauses. By most general, we\\nmean where possible variables are used in place of constants because con-\\nstants are specific and variables are general.\\n8.6.5 Unification Algorithm\\nT o automate the process of resolution in FOPL, we need an algorithm for'), Document(metadata={}, page_content='generating a most general unifier, in order to put clauses in a form that can\\nbe resolved.'), Document(metadata={}, page_content='8.6 Skolemization 225\\nThe unification algorithm is expressed as follows:\\nT o unify a set of clauses,S0:\\nInitialize: /H92680 = {}\\ni = 0\\nLoop: If Si has only one element, terminate and report that /H9268i is\\nthe mgu for S0.\\nIf Si has more than one element, find the disagreement set\\nDi of Si (i.e., the substitutions that need to be made).\\nIf Di contains a variable x and a term t where x is not con-\\ntained within t, then we say:\\n/H9268i/H110011= /H9268i o {t/x}\\nSi/H110011 = Si {t/x}'), Document(metadata={}, page_content='Si/H110011 = Si {t/x}\\nIncrement i, and repeat the loop.\\n8.6.6 Unification Example\\nLet us find the mgu for the following set of clauses:\\nS0 = {P(a, x, y, z), P (x, y, z, w)}\\n(a is a constant, and x, y, z are variables).\\nWe initialize /H92680 = {} and i = 0.\\nNow we proceed as follows:\\nD0 = {a,x}\\n/H92681= /H9268o o {a/x} = {a/x}\\nS1 = So {a/x} = {P(a, a, y, z), P (a, y, z, w)}\\nD1 = {a,y}\\n/H92682 = {a/x} o {a/y} = {a/x, a/y}\\nS2 = S1 {a/x, a/y} = {P(a, a, a, z), P (a, a, z, w)}\\nD2 = {a, z}'), Document(metadata={}, page_content='D2 = {a, z}\\n/H92683 = {a/x, a/y} o {a/z} = {a/x, a/y, a/z}\\nS3 = S2 {a/x, a/y, a/z} = {P(a, a, a, a), P(a, a, a, w)}'), Document(metadata={}, page_content='226 CHAPTER 8 Inference and Resolution for Problem Solving\\nD3 = {a, w}\\n/H92684 = {a/x, a/y, a/z} o {a/w} = {a/x, a/y, a/z, a/w}\\nS4 = S3 {a/x, a/y, a/z, a/w} = {P(a, a, a, a), P(a, a, a, a)}\\n= {P(a, a, a, a)}\\nNow we stop because S4 has just one element, and we have found a mgu,/H92684,\\nwhich is {a/x, a/y, a/z, a/w}.\\nThe following is also a unifier of S0:\\n/H92685 = {x/a, x/y, x/z, x/w}\\nHowever, this is not a mgu because we can find another substitution /H92686'), Document(metadata={}, page_content='such that the composition /H92685 o /H92686 gives the mgu, /H92684:\\n{x/a, x/y, x/z, x/w} o {a/x}\\ngives\\n{x{a/x}/a, x{a/x}/y, x{a/x}/z, x{a/x}/w}\\nwhich in turn leads to\\n{a/a, a/x, a/y, a/z, a/w}\\nWe can eliminate a/a, which gives us\\n{a/x, a/y, a/z, a/w}\\nwhich is the mgu /H92684. because /H92684 is a mgu, we could find another substitu-\\ntion /H9268x for any other unifier /H9268u, such that /H9268u o /H9268x = /H92684.\\n8.7 Resolution Algorithm'), Document(metadata={}, page_content='8.7 Resolution Algorithm\\nNow we have all the tools we need to produce an automated system for gen-\\nerating proofs using resolution on FOPL expressions. Given a set of\\nassumptions and a conclusion, we can prove whether the assumption logi-\\ncally follows from the assumptions as follows:\\n■ First, negate the conclusion and add it to the list of assumptions.\\n■ Now convert the assumptions into prenex normal form.\\n■ Next, skolemize the resulting expression.'), Document(metadata={}, page_content='■ Next, skolemize the resulting expression.\\n■ Now convert the expression into a set of clauses.\\nNow we resolve the clauses using the following method:'), Document(metadata={}, page_content='8.8 Horn Clauses and PROLOG 227\\nIf our clauses are {A, B, C,...}  a n d A has a literal LA and B has a literal LB\\nsuch that LA and ¬LB have a mgu /H9268, then we can resolve A and B to give the\\nresolvent of these clauses, which is:\\n(A/H9268/H11002LA/H9268) ⋃ (B/H9268/H11002LB/H9268)\\nwhere ∪ is the set union operator, and /H11002is the set difference operator. For\\nexample, in resolving the following clauses:\\n{A(x,y), B(f(y)), C(x, y, z)}\\n{A(f(x), z),\\n¬B(z), C(f(a), b, z)}'), Document(metadata={}, page_content='{A(f(x), z),\\n¬B(z), C(f(a), b, z)}\\nwe note that B(f(y)) and B(z) have a mgu that is { f(y)/z}. Hence, we apply\\nthis unifier to the two clauses to give:\\n{A(x,y), B(f(y)), C(x, y, z)}{f(y)/z} /H11002B(f(y)){f(y)/z}\\n⋃ (A(f(x), z), ¬B(z), C(f(a), b, z)} /H11002¬B(z){f(y)/z}\\n= {A(x,y), C(x, y, z)} U {A(f(x), z), C(f(a), b, z)}\\n= {A(x,y), C(x, y, z), A(f(x), z), C(f(a), b, z)}\\nIn this way, we have removed the two literals \\n¬B(z) and B(f(y)), and by con-'), Document(metadata={}, page_content='¬B(z) and B(f(y)), and by con-\\ntinuing this process until no further literals can be resolved, a set of clauses\\ncan be tested for contradiction. Note that each stage of this process can be\\nexpressed as an algorithm, and so the process of resolution can be used by\\ncomputers to prove the validity of deductions in FOPL.\\n8.8 Horn Clauses and PROLOG\\nA Horn clause,o r  Horn sentence, is a clause that has, at most, one positive\\nliteral. Hence, the following Horn clause takes the following form:'), Document(metadata={}, page_content='A ∨¬ B ∨¬ C ∨¬ D ∨¬ E  ...\\nwhere A, B, C, D, E, and so on are positive literals.\\nThis Horn clause can also be written as an implication:\\nB ∧ C ∧ D ∧ E → A\\nIn the programming language PROLOG, this would be written in reverse,\\nwith the conclusion on the left, as follows:\\nA :− B, C, D, E\\nHorn clauses can take three forms. The type we have seen above, where there is\\none positive literal, and one or more negative literals is called arule relation.'), Document(metadata={}, page_content='228 CHAPTER 8 Inference and Resolution for Problem Solving\\nA clause with no negative literals is called a fact:\\nA :−\\nFinally, a clause with no positive literal is called a goal, or a headless clause:\\n:− B, C, D, E\\nUnfortunately, not all expressions can be represented as Horn clauses (e.g.,A ∨\\nB). However, this representation does have the benefit of efficiency, and it also\\nmeans that if a set of clauses has a contradiction, then resolution bydepth-first'), Document(metadata={}, page_content='search(see Chapter 4) is guaranteed to result in the empty clause\\n:−\\nthus proving the original assertion.\\nA program in PROLOG consists of a set of Horn clauses. PROLOG applies\\nunification and resolution to attempt to derive conclusions from a set of\\nrules that are defined in the language.\\nIn fact, the programmer is able to define rules and facts, and to set up a\\ngoal that the PROLOG system must attempt to prove, using unification and\\nresolution.'), Document(metadata={}, page_content='resolution.\\nRules and their uses in programming are briefly mentioned in Chapter 3\\nand are examined in more detail in Chapter 9.\\nThe rule for resolution in PROLOG can be expressed as follows:\\nHence, for example, a PROLOG programmer might start by establishing\\nthe following facts:\\nspeaks (Bob, French).\\nspoken_in (French, France).\\nlocated_in (Task1, France).\\nNext, the programmer writes a rule:\\nassign_task (X, P) \\n:− located_in (X, C),\\nspoken_in (L, C),\\nspeaks (P , L)'), Document(metadata={}, page_content='spoken_in (L, C),\\nspeaks (P , L)\\nD :— A,  B,          G:— D,  E,  \\nG :— A,  B,  E,  \\nKK\\nK'), Document(metadata={}, page_content='8.9 Herbrand Universes 229\\nFinally, the goal is set up:\\nassign_task (Task1, Bob)\\nThis is simply asking, should Task1 be assigned to Bob?\\nFirst, the system needs to use unification to be able to carry out resolution.\\nClearly, our goal could be resolved with the rule if we made the following\\nsubstitution:\\n{Task1 / X, Bob / P}\\nAfter applying this substitution and resolution, we are left with a new goal:\\n:− located_in (Task1, C),\\nspoken_in (L, C),\\nspeaks (Bob, L)\\nNow we apply a further substitution:'), Document(metadata={}, page_content='Now we apply a further substitution:\\n{France / C, French / L}\\nwhich enables us to resolve this new goal with the three established facts to\\nresult in the empty clause:\\n:−\\nHence, the goal has been proved.\\n8.9 Herbrand Universes\\nFor a set of clauses,S, the Herbrand universe, HS, is defined as being the set\\nof constants that are contained within S, and the set of functions in S\\napplied to those constants. These constants and functions are known as'), Document(metadata={}, page_content='ground terms because they do not contain variables.\\nFor example,\\nS is {{A(x), B(y, a), C(z)}, {D(x, a, b),\\n¬E(y, c, b)}}\\nthen HS, the Herbrand universe of S, is defined as follows:\\nHS = {a, b, c}\\nBecause there are no functions in S, the Herbrand universe consists just of\\nthe constants in S, which are a, b, and c.'), Document(metadata={}, page_content='230 CHAPTER 8 Inference and Resolution for Problem Solving\\nA further example follows:\\nS is {{A(x), B(y, a), C(z)}, {D(x, a, b),¬E(y, f(x, y))}}\\nIn this case, where S contains a function, the Herbrand universe is infinite:\\nHS = {a, b, c, f(a, a), f(a, b), f (b, a), f(b, b), f(f(a), a), f(f(a), f(a)) ...}\\nIn the following case, S contains no functions or constants:\\nS is {{A(x), B(y, z), C(z)}, {D(x, y, z), ¬E(y, x)}}\\nIn such cases, we define the Herbrand universe to contain the constant a:'), Document(metadata={}, page_content='HS = {a}\\nA ground instance of a clause in S is a version of that clause in which any\\nvariables it contains have been replaced by ground terms from HS.\\nFor instance, given the following definition of S:\\nS is {{A(x), B(y, a), C(z)}, {D(x, a, b),¬E(y, c, b)}}\\na ground instance of the first clause could be\\n{A(a), B(b, a), C(a)}\\nAnother ground instance of the same clause is\\n{A(c), B(a, a), C(c)}\\n8.9.1 The Herbrand Base\\nThe Herbrand base of S, HS(S), is defined as the set of ground atoms that'), Document(metadata={}, page_content='can be obtained by replacing variables in S by members of HS. A ground\\natom is a formula that contains no variables, only ground terms.\\nFor example,\\nS is {{A(x), B(y, a), C(z)}, {D(x, a, b), ¬E(y, c, b)}}\\nthen\\nHS = {a, b, c}\\nand\\nHS(S) = {{A(a), B(a, a), C(a), D(a, a, b), ¬E(a, c, b),\\nA(b), B(b, a), C(b), D(b, a, b), ¬E(b, c, b),\\n... }'), Document(metadata={}, page_content='8.9 Herbrand Universes 231\\nIn this case, where there are no functions in S, the Herbrand base is finite\\n(although it will consist of a large number of clauses). As with the Her-\\nbrand universe, if the clauses contain functions, then the Herbrand base\\nwill be infinite.\\n8.9.2 Herbrand Interpretations\\nA Herbrand interpretation for S is defined as a set of assignments of true\\nand false to the elements of the Herbrand base, HS(S).\\nFor example,\\nS is {{A(x, y), B(a, x)}, {¬C(z, a), D(a, z)}\\nHS = {a}'), Document(metadata={}, page_content='HS = {a}\\nHS(S) = {A(a, a), B(a, a), C(a, a), D(a, a)}\\nThen two possible Herbrand interpretations of S are\\n{¬A(a, a), B(a, a), C(a, a), D(a, a)}\\n{A(a, a), ¬B(a, a), C(a, a),¬D(a, a)}\\nIn the first interpretation, all the elements of HS(S) have been assigned the\\nvalue true, apart from A(a, a), which has been assigned false. In general, if\\nwe assign a value true to A, we write A, and if we assign the value false to A,\\nwe write ¬A. In the second interpretation, the second and fourth elements'), Document(metadata={}, page_content='of HS(S) have been assigned the value false, and the first and third have\\nbeen assigned the value true.\\nFor this set, S, there will be 16 possible Herbrand interpretations because\\nthere are four elements in the Herbrand base. In general, if there are n ele-\\nments in the Herbrand base, there will be 2n Herbrand interpretations.\\nNow we come back to the idea of satisfiability. In some cases, a given Her-\\nbrand interpretation of a set S will be set to satisfy S.'), Document(metadata={}, page_content='Let us use our previous example again:\\nS is {{A(x, y), B(a, x)}, {¬C(z, a), D(a, z)}\\nWe presented two Herbrand interpretations for this S:\\n{¬A(a, a), B(a, a), C(a, a), D(a, a)}\\n{A(a, a), ¬B(a, a), C(a, a),¬D(a, a)}\\nNow we say that a given interpretation for S satisfies S if the assignments\\nfrom the interpretation make each clause in S true.'), Document(metadata={}, page_content='232 CHAPTER 8 Inference and Resolution for Problem Solving\\nFor example, replacing the variables in S with the constants from the Her-\\nbrand universe, we get the following:\\n{{A(a, a), B(a, a)}, {¬C(a, a), D(a, a)}\\nNow, if we use the first interpretation where A(a, a) was the only element\\nthat had been assigned the value false, we see that the clauses become\\n{{false, true}, {false, true}}\\nNow, recall that clauses are a set of conjunctions of disjunctions, and so we\\ncan rewrite this as'), Document(metadata={}, page_content='can rewrite this as\\n(false ∨ true) ∧ (false ∨ true)\\nwhich clearly is true. Hence, this interpretation satisfies S.\\nUsing the second interpretation, the clauses become\\n{{true, false}, {false, false}}\\nwhich is not true. So the second interpretation does not satisfy S.\\nIt can be shown that if no Herbrand interpretation exists for a set of clauses\\nS that satisfies S, then S is not satisfiable. This is, in fact, the basis for reso-'), Document(metadata={}, page_content='lution because it can further be shown that if a set of clauses is unsatisfi-\\nable, then resolving that set of clauses will lead to falsum.\\n8.9.3 Example\\nWe have seen that the satisfiability of a set of clauses can be proved or dis-\\nproved by examining the Herbrand interpretations for the set. We now\\npresent an example:\\nS = {{\\n¬A(y, a), B(y)}, {¬B(x)}, {B(a), A(x, a)}}\\nThen we can define the Herbrand universe as\\nHS = {a}\\nNext, we define the Herbrand base:\\nHS(S) = {A(a, a), B(a)}'), Document(metadata={}, page_content='8.10 Resolution for Problem Solving 233\\nThus, there are four possible Herbrand interpretations for S:\\n1. {A(a, a), B(a)}\\n2. {A(a, a),\\n¬B(a)}\\n3. { ¬A(a, a), B (a)}\\n4. { ¬A(a, a), ¬B(a)}\\nY ou should be able to see that none of these interpretations satisfies S.\\nHence, we have proved that S is unsatisfiable.\\n8.10 Resolution for Problem Solving\\nAlthough it seems complex, resolution is made up a series of simple steps.\\nBecause each of those steps can be readily automated, resolution is widely'), Document(metadata={}, page_content='used. As has already been explained, PROLOG systems use resolution to\\nsolve problems.\\nLet us now see how resolution can be used to solve a simple logic problem.\\nConsider the following set of premises:\\n1. Some children will eat any food.\\n2. No children will eat food that is green.\\n3. All children like food made by Cadbury’s.\\nWe now wish to prove that the following conclusion follows from these\\npremises:\\nNo food made by Cadbury’s is green.'), Document(metadata={}, page_content='premises:\\nNo food made by Cadbury’s is green.\\nFirst, we need to represent the premises and the conclusion in predicate\\ncalculus. We will use the following symbols:\\nC(x) means “x is a child. ”\\nF(x) means “x is food. ”\\nL(x, y) means “x likes y. ”\\nG(x) means “x is green. ”\\nM(x, y) means “x makes y. ”\\nc means “Cadbury’s. ”'), Document(metadata={}, page_content='234 CHAPTER 8 Inference and Resolution for Problem Solving\\nSo our premises can be represented as:\\n1. ( ∃x)(C(x) ∧ (∀y)(F(y) → L(x,y)))\\n2. ( ∀x)(C(x) → (∀y)((F(y) ∧ G(y)) → ¬L(x,y)))\\n3. ( ∀x)((F(x) ∧ M(c,x)) → (∀y)(C(y) → L(y,x)))\\nOur conclusion can be represented as follows:\\n(∀x)((F(x) ∧ M(c,x)) → ¬G(x))\\nFirst, we must negate the conclusion and add it to the set of premises,\\nwhich means we must now prove that the following expression cannot be\\nsatisfied:\\n(∃x)(C(x)\\n∧ (∀y)(F(y) → L(x,y)))'), Document(metadata={}, page_content='satisfied:\\n(∃x)(C(x)\\n∧ (∀y)(F(y) → L(x,y)))\\n∧ (∀x)(C(x) → (∀y)((F(y) ∧ G(y)) → ¬L(x,y)))\\n∧ (∀x)((F(x) ∧ M(c,x)) → (∀y)(C(y) → L(y,x)))\\n∧¬ ((∀x)((F(x) ∧ M(c,x)) → ¬G(x)))\\nWe will convert this into a set of clauses, starting with expression 1:\\n(∃x)(C(x) ∧ (∀y)(F(y) → L(x,y)))\\n→ must be eliminated first:\\n(∃x)(C(x) ∧ (∀y)(¬F(y) ∨ L(x,y)))\\nNext, we bring the quantifiers to the front of the expression:\\n(∃x)(∀y)(C(x) ∧ (¬F(y) ∨ L(x,y)))'), Document(metadata={}, page_content='(∃x)(∀y)(C(x) ∧ (¬F(y) ∨ L(x,y)))\\nNow we skolemize this expression, to eliminate the existential quantifier:\\n(∀y)(C(a) ∧ (¬F(y) ∨ L(a,y)))\\nThis can be expressed as the following clauses:\\n{C(a)}, {¬F(y), L(a,y)}\\nNext, we deal with expression 2 in the same way:\\n(∀x)(C(x) → (∀y)((F(y) ∧ G(y)) → ¬L(x,y)))\\n→ is eliminated first:\\n(∀x)(¬C(x) ∨ (∀y)(¬(F(y) ∧ G(y)) ∨¬ L(x,y)))'), Document(metadata={}, page_content='8.10 Resolution for Problem Solving 235\\nNow DeMorgan’s law is applied:\\n(∀x)(¬C(x) ∨ (∀y)(¬F(y) ∨¬ G(y) ∨¬ L(x,y)))\\nQuantifiers are moved to the front:\\n(∀x)(∀y)(¬C(x) ∨¬ F(y) ∨¬ G(y) ∨¬ L(x,y))\\nThis can be written as the following single clause:\\n{¬C(x), ¬F(y), ¬(G(y), ¬L(x,y)}\\nNow, for expression 3:\\n(∀x)((F(x) ∧ M(c,x)) → (∀y)(C(y) → L(y,x)))\\nWe first eliminate →:\\n(∀x)(¬(F(x) ∧ M(c,x)) ∨ (∀y)(¬C(y) ∨ L(y,x)))\\nNext, we apply DeMorgan’s law:\\n(∀x)(¬F(x) ∨¬ M(c,x) ∨ (∀y)(¬C(y) ∨ L(y,x)))'), Document(metadata={}, page_content='(∀x)(¬F(x) ∨¬ M(c,x) ∨ (∀y)(¬C(y) ∨ L(y,x)))\\nNow we bring the quantifiers to the front of the expression:\\n(∀x)(∀y)(¬F(x) ∨¬ M(c,x) ∨¬ C(y) ∨ L(y,x))\\nThis can be expressed as the following single clause:\\n{¬F(x), ¬M(c,x), ¬C(y), L(y,x)}\\nNow we deal with the conclusion, which has been negated:\\n¬(∀x)((F(x) ∧ M(c,x)) → ¬G(x))\\nFirst, we eliminate →:\\n¬(∀x)(¬(F(x) ∧ M(c,x)) ∨¬ G(x))\\nNow we apply the quantifier equivalence to move the ¬ from the front of\\nthe expression:\\n(∃x)¬(¬(F(x) ∧ M(c,x)) ∨¬ G(x))'), Document(metadata={}, page_content='the expression:\\n(∃x)¬(¬(F(x) ∧ M(c,x)) ∨¬ G(x))\\nDeMorgan’s law can now be applied:\\n(∃x)(¬¬(F(x) ∧ M(c,x)) ∧¬ ¬G(x))\\nWe can now remove ¬¬:\\n(∃x)(F(x) ∧ M(c,x) ∧ G(x))'), Document(metadata={}, page_content='236 CHAPTER 8 Inference and Resolution for Problem Solving\\nThis expression is now skolemized:\\nF(b) ∧ M(c,b) ∧ G(b))\\nThis can be expressed as the following set of clauses:\\n{{F(b)}, {M(c,b)}, {G(b)}}\\nNow we have arrived at a set of clauses, upon which resolution can be\\napplied. The clauses we have are the following:\\n1. {C(a)}\\n2. { ¬F(y), L(a,y)}\\n3. { ¬C(x), ¬F(y), ¬(G(y), ¬L(x,y)}\\n4. { ¬F(x), ¬M(c,x), ¬C(y), L(y,x)}\\n5. {F(b)}\\n6. {M(c,b)}\\n7. {G(b)}\\nWe now apply resolution as follows:'), Document(metadata={}, page_content='7. {G(b)}\\nWe now apply resolution as follows:\\nFirst, we unify lines 1 and 3 using {a/x} and resolve these two, to give\\n8. {\\n¬F(y), ¬(G(y), ¬L(a,y)}\\nSimilarly, the unifier {b/y} can be applied to lines 2 and 5, which are then\\nresolved to give\\n9. {L(a,b)}\\nNow we apply {b/y} to resolve line 5 with line 8 to give\\n10. { ¬(G(b), ¬L(a,b)}\\nNow lines 9 and 10 can be resolved to give\\n11. { ¬(G(b)}\\nFinally, line 7 can be resolved with line 11 to give\\n12. ⊥'), Document(metadata={}, page_content='12. ⊥\\nHence, we have proven that the set of clauses derived from the premises 1,\\n2, and 3, and the negation of the conclusion, are unsatisfiable. Thus we have\\nsuccessfully proved that the conclusion does indeed follow from the prem-\\nises, and so the argument is a valid one.'), Document(metadata={}, page_content='8.11 Chapter Summary 237\\n8.11 Chapter Summary\\n■ Any well-formed formula (wff) can be expressed in conjunctive\\nnormal form (CNF) or disjunctive normal form (DNF). An\\nexpression in CNF is a conjunction of disjunctions, and an expres-\\nsion in DNF is a disjunction of conjunctions.\\n■ An algorithm can be generated that will convert any wff into CNF.\\n■ The resolution rule says that if you know ( A ∨ B) and you know\\n(¬B ∨ C), then you can eliminate the instances of B from these two'), Document(metadata={}, page_content='expressions, to produce (A ∨ C).\\n■ By negating a conclusion, and proving that the resultant set of\\nexpressions is unsatisfiable, one can prove that an argument is\\nvalid. Such reasoning is called proof by refutation (or proof by\\ncontradiction). The traditional method for using resolution is to\\nprove arguments valid by refutation.\\n■ Any combinatorial problem can be represented as a set of clauses,\\nand the existence of a solution to the problem can be determined\\nusing resolution on those clauses.'), Document(metadata={}, page_content='using resolution on those clauses.\\n■ T o apply resolution in first-order predicate logic, an expression\\nneeds to be first converted to prenex normal form (where the\\nquantifiers are at the beginning) and then skolemized, which\\ninvolves removing variables that are existentially quantified by\\nreplacing them with constants and functions.\\n■ Unification involves using a unifier to resolve two similar clauses\\nthat do not have the same variables. An algorithm can be generated'), Document(metadata={}, page_content='to unify any sets of clauses that can be unified.\\n■ A unifier u1 is a most general unifier if any other unifier, u2, can\\nbe expressed as the composition of u1 with another unifier, u3: u2\\n= u1 o u3.\\n■ Resolution can be applied to a set of clauses that have been skolem-\\nized. This process can be automated because each step can be\\nexpressed algorithmically.\\n■ A Horn clause is one with, at most, one positive literal. PROLOG\\nuses resolution on Horn clauses to solve problems.'), Document(metadata={}, page_content='■ If no Herbrand interpretation exists for a set of clauses that satis-\\nfies that set, then the clauses are not satisfiable.'), Document(metadata={}, page_content='238 CHAPTER 8 Inference and Resolution for Problem Solving\\n8.12 Review Questions\\n8.1 Explain the concept of proof by refutation.\\n8.2 Explain how and to what extent combinatorial problems can be\\nsolved using resolution.\\n8.3 Explain what is meant by prenex normal form and skolem normal\\nform.\\n8.4 Explain each step of the algorithm for resolution in first-order\\npredicate logic.\\n8.5 Explain the following terms:\\n■ Herbrand universe\\n■ Herbrand base\\n■ Herbrand interpretation\\n8.13 Exercises'), Document(metadata={}, page_content='■ Herbrand interpretation\\n8.13 Exercises\\n8.1 Convert the following expression to CNF and to DNF:\\nA ∨ (B ∧ C) ∨ (D ∧ E ∧¬ (A ∨ B))\\n8.2 Convert the following expressions to a set of clauses:\\n(∀x)(P(x) → (A(x) ∧ B(x) ∨¬ C(x, a))\\n(∃y)(Q(y, a) ∧ ((∀z) A(z) → ¬ B(y))).\\n8.3 Prove that the resolution rule is valid.\\n8.4 Generate the full set of clauses for the map-coloring graph in Fig-\\nure 8.1. Resolve these clauses to prove that a three-coloring solu-\\ntion does exist for the graph.'), Document(metadata={}, page_content='tion does exist for the graph.\\n8.5 Use resolution to determine whether the following is valid:\\n(((∀x)(A (x) → B(x)))\\n∧ (¬ B(x))) → ¬ A(x)\\n8.6 Use resolution to determine whether the following is valid:\\n(∀x)(∃y) (((A (x) ∧ B(y)) → (A (y) ∧ B(x))) → (A(x) → B(x)))\\n8.7 Use resolution to prove that the following logical argument,\\ndevised by Lewis Carroll, is valid:\\nNo shark ever doubts that it is well fitted out'), Document(metadata={}, page_content='8.14 Further Reading 239\\nA fish, that cannot dance a minuet, is contemptible\\nNo fish is quite certain that it is well fitted out, unless it has three rows\\nof teeth\\nAll fishes, except sharks, are kind to children\\nNo heavy fish can dance a minuet\\nA fish with three rows of teeth is not to be despised\\nConclusion:\\nNo heavy fish is unkind to children\\n8.14 Further Reading\\nResolution is covered by most of the standard Artificial Intelligence texts,'), Document(metadata={}, page_content='but you may need to look in the specialized books such as Chang and Lee\\n(1973) to find deeper coverage of the subject. Reeves (1990) provides a\\ngood introduction to resolution.\\nA Resolution Principle for a Logic With Restricted Quantifiers by H. J. Burck-\\nert (1992 – Springer V erlag)\\nThe Resolution Calculus by Alexander Leitsch (1997 – Springer V erlag)\\nAutomated Theorem Proving: A Logical Basis by Donald W. Loveland (1978\\n– Elsevier Science – out of print)'), Document(metadata={}, page_content='– Elsevier Science – out of print)\\nAutomated Theorem Proving: Theory and Practice by Monty Newborn\\n(2001 – Springer V erlag)\\nLogic, Form and Function: The Mechanization of Deductive Reasoning by\\nJohn Alan Robinson (1980 – Elsevier Science)\\nUsing Sophisticated Models in Resolution Theorem Proving (Lecture Notes in\\nComputer Science, Vol. 90) by David M. Sandford (1981 – Springer V erlag)\\nResolution Proof Systems: An Algebraic Theory (Automated Reasoning Series,'), Document(metadata={}, page_content='Vol. 4)by Zbigniew Stachniak (1996 – Kluwer Academic Publishers)\\nSymbolic Logic and Mechanical Theorem Proving by Chin-Liang Chang and\\nRichard Char-Tung Lee (1973 – Academic Press)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='9CHAPTER\\nRules and Expert Systems\\nAny problem that can be solved by your in-house expert in a 10–30 minute\\ntelephone call can be developed as an expert system.\\n—M. Firebaugh, Artificial Intelligence: A Knowledge-Based Approach\\n‘Rule Forty-two. All persons more than a mile high to leave the court. ’\\n‘That’s not a regular rule: you invented it just now. ’\\n‘It’s the oldest rule in the book, ’ said the King.\\n‘Then it ought to be number one, ’ said Alice.'), Document(metadata={}, page_content='‘Then it ought to be number one, ’ said Alice.\\n—Lewis Carroll, Alice’s Adventures in Wonderland\\nThese so-called expert systems were often right, in the specific areas for which\\nthey had been built, but they were extremely brittle. Given even a simple prob-\\nlem just slightly beyond their expertise, they would usually get a wrong\\nanswer. Ask a medical program about a rusty old car, and it might blithely\\ndiagnose measles.\\n—Douglas B. Lenat, Programming Artificial Intelligence\\n9.1 Introduction'), Document(metadata={}, page_content='9.1 Introduction\\nIn this chapter, we introduce the ideas behind production systems, or\\nexpert systems, and explain how they can be built using rule-based systems,\\nframes, or a combination of the two.'), Document(metadata={}, page_content='242 CHAPTER 9 Rules and Expert Systems\\nThis chapter explains techniques such as forward and backward chaining,\\nconflict resolution, and the Rete algorithm. It also explains the architecture\\nof an expert system and describes the roles of the individuals who are\\ninvolved in designing, building, and using expert systems.\\n9.2 Rules for Knowledge Representation\\nOne way to represent knowledge is by using rules that express what must\\nhappen or what does happen when certain conditions are met. Rules are'), Document(metadata={}, page_content='usually expressed in the form of IF . . . THEN . . . statements, such as:\\nIF A THEN B\\nThis can be considered to have a similar logical meaning as the following:\\nA → B\\nAs we saw in Chapter 7, A is called the antecedent and B is the consequent\\nin this statement. In expressing rules, the consequent usually takes the form\\nof an action or a conclusion. In other words, the purpose of a rule is usu-\\nally to tell a system (such as an expert system) what to do in certain cir-'), Document(metadata={}, page_content='cumstances, or what conclusions to draw from a set of inputs about the\\ncurrent situation.\\nIn general, a rule can have more than one antecedent, usually combined\\neither by AND or by OR (logically the same as the operators \\n∧ and ∨ we\\nsaw in Chapter 7). Similarly, a rule may have more than one consequent,\\nwhich usually suggests that there are multiple actions to be taken.\\nIn general, the antecedent of a rule compares anobject with a possiblevalue,'), Document(metadata={}, page_content='using anoperator. For example, suitable antecedents in a rule might be\\nIF x > 3\\nIF name is “Bob”\\nIF weather is cold\\nHere, the objects being considered are x, name, and weather; the operators\\nare “>” and “is” , and the values are 3, “Bob, ” and cold. Note that an object is\\nnot necessarily an object in the real-world sense—the weather is not a real-\\nworld object, but rather a state or condition of the world. An object in this'), Document(metadata={}, page_content='sense is simply a variable that represents some physical object or state in the\\nreal world.\\nAn example of a rule might be'), Document(metadata={}, page_content='9.3 Rule-Based Systems 243\\nIF name is “Bob”\\nAND weather is cold\\nTHEN tell Bob ‘Wear a coat’\\nThis is an example of a recommendation rule, which takes a set of inputs\\nand gives advice as a result. The conclusion of the rule is actually an action,\\nand the action takes the form of a recommendation to Bob that he should\\nwear a coat. In some cases, the rules provide more definite actions such as\\n“move left” or “close door, ” in which case the rules are being used to repre-\\nsent directives.'), Document(metadata={}, page_content='sent directives.\\nRules can also be used to represent relations such as:\\nIF temperature is below 0\\nTHEN weather is cold\\n9.3 Rule-Based Systems\\nRule-based systems or production systems are computer systems that use\\nrules to provide recommendations or diagnoses, or to determine a course\\nof action in a particular situation or to solve a particular problem.\\nA rule-based system consists of a number of components:\\n■ a database of rules (also called a knowledge base)\\n■ a database of facts'), Document(metadata={}, page_content='■ a database of facts\\n■ an interpreter,o r  inference engine\\nIn a rule-based system, the knowledge base consists of a set of rules that rep-\\nresent the knowledge that the system has. The database of facts represents\\ninputs to the system that are used to derive conclusions, or to cause actions.\\nThe interpreter, or inference engine, is the part of the system that controls\\nthe process of deriving conclusions. It uses the rules and facts, and com-\\nbines them together to draw conclusions.'), Document(metadata={}, page_content='bines them together to draw conclusions.\\nAs we will see, these conclusions are often derived using deduction,\\nalthough there are other possible approaches. Using deduction to reach a\\nconclusion from a set of antecedents is called forward chaining. An alter-\\nnative method, backward chaining, starts from a conclusion and tries to\\nshow it by following a logical path backward from the conclusion to a set of\\nantecedents that are in the database of facts.'), Document(metadata={}, page_content='244 CHAPTER 9 Rules and Expert Systems\\n9.3.1 Forward Chaining\\nForward chaining employs the same deduction method that we saw in\\nChapter 7. In other words, the system starts from a set of facts, and a set of\\nrules, and tries to find a way of using those rules and facts to deduce a con-\\nclusion or come up with a suitable course of action.\\nThis is known as data-driven reasoning because the reasoning starts from\\na set of data and ends up at the goal, which is the conclusion.'), Document(metadata={}, page_content='When applying forward chaining, the first step is to take the facts in the fact\\ndatabase and see if any combination of these matches all the antecedents of\\none of the rules in the rule database. When all the antecedents of a rule are\\nmatched by facts in the database, then this rule is triggered. Usually, when\\na rule is triggered, it is then fired, which means its conclusion is added to\\nthe facts database. If the conclusion of the rule that has fired is an action or'), Document(metadata={}, page_content='a recommendation, then the system may cause that action to take place or\\nthe recommendation to be made.\\nFor example, consider the following set of rules that is used to control an\\nelevator in a three-story building:\\nRule 1\\nIF on first floor and button is pressed on first floor\\nTHEN open door\\nRule 2\\nIF on first floor\\nAND button is pressed on second floor\\nTHEN go to second floor\\nRule 3\\nIF on first floor\\nAND button is pressed on third floor\\nTHEN go to third floor\\nRule 4\\nIF on second floor'), Document(metadata={}, page_content='THEN go to third floor\\nRule 4\\nIF on second floor\\nAND button is pressed on first floor'), Document(metadata={}, page_content='9.3 Rule-Based Systems 245\\nAND already going to third floor\\nTHEN remember to go to first floor later\\nThis represents just a subset of the rules that would be needed, but we can\\nuse it to illustrate how forward chaining works.\\nLet us imagine that we start with the following facts in our database:\\nFact 1\\nAt first floor\\nFact 2\\nButton pressed on third floor\\nFact 3\\nT oday is Tuesday\\nNow the system examines the rules and finds that Facts 1 and 2 match the'), Document(metadata={}, page_content='antecedents of Rule 3. Hence, Rule 3 fires, and its conclusion\\nGo to third floor\\nis added to the database of facts. Presumably, this results in the elevator\\nheading toward the third floor. Note that Fact 3 was ignored altogether\\nbecause it did not match the antecedents of any of the rules.\\nNow let us imagine that the elevator is on its way to the third floor and has\\nreached the second floor, when the button is pressed on the first floor. The fact\\nButton pressed on first floor'), Document(metadata={}, page_content='Button pressed on first floor\\nIs now added to the database, which results in Rule 4 firing. Now let us imag-\\nine that later in the day the facts database contains the following information:\\nFact 1\\nAt first floor\\nFact 2\\nButton pressed on second floor\\nFact 3\\nButton pressed on third floor\\nIn this case, two rules are triggered—Rules 2 and 3. In such cases where\\nthere is more than one possible conclusion,conflict resolution needs to be\\napplied to decide which rule to fire.'), Document(metadata={}, page_content='246 CHAPTER 9 Rules and Expert Systems\\n9.3.2 Conflict Resolution\\nIn a situation where more than one conclusion can be deduced from a set\\nof facts, there are a number of possible ways to decide which rule to fire\\n(i.e., which conclusion to use or which course of action to take).\\nFor example, consider the following set of rules:\\nIF it is cold\\nTHEN wear a coat\\nIF it is cold\\nTHEN stay at home\\nIF it is cold\\nTHEN turn on the heat'), Document(metadata={}, page_content='IF it is cold\\nTHEN turn on the heat\\nIf there is a single fact in the fact database, which is “it is cold, ” then clearly\\nthere are three conclusions that can be derived. In some cases, it might be\\nfine to follow all three conclusions, but in many cases the conclusions are\\nincompatible (for example, when prescribing medicines to patients).\\nIn one conflict resolution method, rules are given priority levels, and when'), Document(metadata={}, page_content='a conflict occurs, the rule that has the highest priority is fired, as in the fol-\\nlowing example:\\nIF patient has pain\\nTHEN prescribe painkillers priority 10\\nIF patient has chest pain\\nTHEN treat for heart disease priority 100\\nHere, it is clear that treating possible heart problems is more important\\nthan just curing the pain.\\nAn alternative method is thelongest-matching strategy. This method involves\\nfiring the conclusion that was derived from the longest rule. For example:\\nIF patient has pain'), Document(metadata={}, page_content='IF patient has pain\\nTHEN prescribe painkiller\\nIF patient has chest pain'), Document(metadata={}, page_content='9.3 Rule-Based Systems 247\\nAND patient is over 60\\nAND patient has history of heart conditions\\nTHEN take to emergency room\\nHere, if all the antecedents of the second rule match, then this rule’s con-\\nclusion should be fired rather than the conclusion of the first rule because\\nit is a more specific match.\\nA further method for conflict resolution is to fire the rule that has matched\\nthe facts most recently added to the database.'), Document(metadata={}, page_content='the facts most recently added to the database.\\nIn each case, it may be that the system fires one rule and then stops (as in\\nmedical diagnosis), but in many cases, the system simply needs to choose a\\nsuitable ordering for the rules (as when controlling an elevator) because\\neach rule that matches the facts needs to be fired at some point.\\n9.3.3 Meta Rules\\nIn designing an expert system, it is necessary to select the conflict resolu-'), Document(metadata={}, page_content='tion method that will be used, and quite possibly it will be necessary to use\\ndifferent methods to resolve different types of conflicts. For example, in\\nsome situations it may make most sense to use the method that involves fir-\\ning the most recently added rules. This method makes most sense in situa-\\ntions in which the timeliness of data is important. It might be, for example,\\nthat as research in a particular field of medicine develops, new rules are'), Document(metadata={}, page_content='added to the system that contradict some of the older rules. It might make\\nmost sense for the system to assume that these newer rules are more accu-\\nrate than the older rules.\\nIt might also be the case, however, that the new rules have been added by an\\nexpert whose opinion is less trusted than that of the expert who added the\\nearlier rules. In this case, it clearly makes more sense to allow the earlier\\nrules priority.\\nThis kind of knowledge is called meta knowledge —knowledge about'), Document(metadata={}, page_content='knowledge. The rules that define how conflict resolution will be used, and\\nhow other aspects of the system itself will run, are called meta rules.\\nThe knowledge engineer who builds the expert system is responsible for\\nbuilding appropriate meta knowledge into the system (such as “expert A is'), Document(metadata={}, page_content='248 CHAPTER 9 Rules and Expert Systems\\nto be trusted more than expert B” or “any rule that involves drug X is not to\\nbe trusted as much as rules that do not involve X”).\\nMeta rules are treated by the expert system as if they were ordinary rules\\nbut are given greater priority than the normal rules that make up the\\nexpert system. In this way, the meta rules are able to override the normal\\nrules, if necessary, and are certainly able to control the conflict resolu-\\ntion process.'), Document(metadata={}, page_content='tion process.\\n9.3.4 Backward Chaining\\nForward chaining applies a set of rules and facts to deduce whatever con-\\nclusions can be derived, which is useful when a set of facts are present, but\\nyou do not know what conclusions you are trying to prove. In some cases,\\nforward chaining can be inefficient because it may end up proving a num-\\nber of conclusions that are not currently interesting. In such cases, where a\\nsingle specific conclusion is to be proved, backward chaining is more\\nappropriate.'), Document(metadata={}, page_content='appropriate.\\nIn backward chaining, we start from a conclusion, which is the hypothesis\\nwe wish to prove, and we aim to show how that conclusion can be reached\\nfrom the rules and facts in the database.\\nThe conclusion we are aiming to prove is called a goal, and so reasoning in\\nthis way is known as goal-driven reasoning.\\nAs we see in Chapter 16, backward chaining is often used in formulating\\nplans. A plan is a sequence of actions that a program (such as an intelli-'), Document(metadata={}, page_content='gent agent) decides to take to solve a particular problem. Backward chain-\\ning can make the process of formulating a plan more efficient than\\nforward chaining.\\nBackward chaining in this way starts with the goal state, which is the set of\\nconditions the agent wishes to achieve in carrying out its plan. It now\\nexamines this state and sees what actions could lead to it. For example, if\\nthe goal state involves a block being on a table, then one possible action'), Document(metadata={}, page_content='would be to place that block on the table. This action might not be possible\\nfrom the start state, and so further actions need to be added before this\\naction in order to reach it from the start state. In this way, a plan can be for-\\nmulated starting from the goal and working back toward the start state.'), Document(metadata={}, page_content='9.3 Rule-Based Systems 249\\nThe benefit in this method is particularly clear in situations where the first\\nstate allows a very large number of possible actions. In this kind of situa-\\ntion, it can be very inefficient to attempt to formulate a plan using forward\\nchaining because it involves examining every possible action, without pay-\\ning any attention to which action might be the best one to lead to the goal\\nstate. Backward chaining ensures that each action that is taken is one that'), Document(metadata={}, page_content='will definitely lead to the goal, and in many cases this will make the plan-\\nning process far more efficient.\\n9.3.5 Comparing Forward and Backward Chaining\\nLet us use an example to compare forward and backward chaining. In this\\ncase, we will revert to our use of symbols for logical statements, in order to\\nclarify the explanation, but we could equally well be using rules about ele-\\nvators or the weather.\\nRules:\\nRule 1 A ∧ B → C\\nRule 2 A → D\\nRule 3 C ∧ D → E\\nRule 4 B ∧ E ∧ F → G\\nRule 5 A ∧ E → H'), Document(metadata={}, page_content='Rule 4 B ∧ E ∧ F → G\\nRule 5 A ∧ E → H\\nRule 6 D ∧ E ∧ H → I\\nFacts:\\nFact 1 A\\nFact 2 B\\nFact 3 F\\nGoal:\\nOur goal is to prove H.\\nFirst let us use forward chaining. As our conflict resolution strategy, we will\\nfire rules in the order they appear in the database, starting from Rule 1.\\nIn the initial state, Rules 1 and 2 are both triggered. We will start by firing\\nRule 1, which means we add C to our fact database. Next, Rule 2 is fired,\\nmeaning we add D to our fact database.'), Document(metadata={}, page_content='meaning we add D to our fact database.\\nWe now have the facts A, B, C, D, F, but we have not yet reached our goal,\\nwhich is G.'), Document(metadata={}, page_content='250 CHAPTER 9 Rules and Expert Systems\\nNow Rule 3 is triggered and fired, meaning that fact E is added to the data-\\nbase. As a result, Rules 4 and 5 are triggered. Rule 4 is fired first, resulting in\\nFact G being added to the database, and then Rule 5 is fired, and Fact H is\\nadded to the database. We have now proved our goal and do not need to go\\non any further.\\nThis deduction is presented in the following table:\\nFacts Rules triggered Rule fired\\nA, B, F 1, 2 1\\nA, B, C, F 2 2\\nA, B, C, D, F 3 3'), Document(metadata={}, page_content='A, B, F 1, 2 1\\nA, B, C, F 2 2\\nA, B, C, D, F 3 3\\nA, B, C, D, E, F 4, 5 4\\nA, B, C, D, E, F , G 5 5\\nA, B, C, D, E, F , G, H 6 STOP\\nNow we will consider the same problem using backward chaining. T o do so,\\nwe will use a goals database in addition to the rule and fact databases. In\\nthis case, the goals database starts with just the conclusion, H, which we\\nwant to prove. We will now see which rules would need to fire to lead to this'), Document(metadata={}, page_content='conclusion. Rule 5 is the only one that has H as a conclusion, so to prove H,\\nwe must prove the antecedents of Rule 5, which are A and E.\\nFact A is already in the database, so we only need to prove the other\\nantecedent, E. Therefore, E is added to the goal database. Once we have\\nproved E, we now know that this is sufficient to prove H, so we can remove\\nH from the goals database.\\nSo now we attempt to prove Fact E. Rule 3 has E as its conclusion, so to'), Document(metadata={}, page_content='prove E, we must prove the antecedents of Rule 3, which are C and D. Nei-\\nther of these facts is in the fact database, so we need to prove both of them.\\nThey are both therefore added to the goals database. D is the conclusion of\\nRule 2 and Rule 2’s antecedent, A, is already in the fact database, so we can\\nconclude D and add it to the fact database.\\nSimilarly, C is the conclusion of Rule 1, and Rule 1’s antecedents, A and B,'), Document(metadata={}, page_content='are both in the fact database. So, we have now proved all the goals in the\\ngoal database and have therefore proved H and can stop.'), Document(metadata={}, page_content='9.4 Rule-Based Expert Systems 251\\nThis process is represented in the table below:\\nFacts Goals Matching rules\\nA, B, F H 5\\nA, B, F E 3\\nA, B, F C, D 1\\nA, B, C, F D 2\\nA, B, C, D, F STOP\\nIn this case, backward chaining needed to use one fewer rule. If the rule data-\\nbase had had a large number of other rules that had A, B, and F as their\\nantecedents, then forward chaining might well have been even more inefficient.\\nIn many situations, forward chaining is more appropriate, particularly in'), Document(metadata={}, page_content='a situation where a set of facts is available, but the conclusion is not\\nalready known.\\nIn general, backward chaining is appropriate in cases where there are few\\npossible conclusions (or even just one) and many possible facts, not very\\nmany of which are necessarily relevant to the conclusion. Forward chaining\\nis more appropriate when there are many possible conclusions.\\nThe way in which forward or backward chaining is usually chosen is to'), Document(metadata={}, page_content='consider which way an expert would solve the problem. This is particularly\\nappropriate because rule-based reasoning is often used in expert systems.\\n9.4 Rule-Based Expert Systems\\nAn expert system is one designed to model the behavior of an expert in\\nsome field, such as medicine or geology. Rule-based expert systems are\\ndesigned to be able to use the same rules that the expert would use to draw\\nconclusions from a set of facts that are presented to the system.'), Document(metadata={}, page_content='9.4.1 The People Involved in an Expert System\\nThe design, development, and use of expert systems involves a number of\\npeople. The end-user of the system is the person who has the need for the\\nsystem. In the case of a medical diagnosis system, this may be a doctor, or it\\nmay be an individual who has a complaint that they wish to diagnose.'), Document(metadata={}, page_content='252 CHAPTER 9 Rules and Expert Systems\\nFact\\nDatabase\\nKnowledge\\nBase\\nInference\\nEngine\\nExplanation\\nSystem\\nKnowledge\\nBase Editor\\nExpert\\nSystem\\nShell\\nUser Interface\\nUser\\nFigure 9.1\\nArchitecture of an expert\\nsystem\\nThe knowledge engineer is the person who designs the rules for the sys-\\ntem, based on either observing the expert at work or by asking the expert\\nquestions about how he or she works.\\nThe domain expert is very important to the design of an expert system. In'), Document(metadata={}, page_content='the case of a medical diagnosis system, the expert needs to be able to\\nexplain to the knowledge engineer how he or she goes about diagnosing\\nillnesses.\\n9.4.2 Architecture of an Expert System\\nA typical expert system architecture is shown in Figure 9.1.\\nThe knowledge base contains the specific domain knowledge that is used by\\nan expert to derive conclusions from facts. In the case of a rule-based expert\\nsystem, this domain knowledge is expressed in the form of a series of rules.'), Document(metadata={}, page_content='The explanation system provides information to the user about how the\\ninference engine arrived at its conclusions. This can often be essential, par-\\nticularly if the advice being given is of a critical nature, such as with a med-\\nical diagnosis system. If the system has used faulty reasoning to arrive at its'), Document(metadata={}, page_content='9.4 Rule-Based Expert Systems 253\\nconclusions, then the user may be able to see this by examining the data\\ngiven by the explanation system.\\nThe fact database contains the case-specific data that are to be used in a\\nparticular case to derive a conclusion. In the case of a medical expert sys-\\ntem, this would contain information that had been obtained about the\\npatient’s condition.\\nThe user of the expert system interfaces with it through a user interface,'), Document(metadata={}, page_content='which provides access to the inference engine, the explanation system, and\\nthe knowledge-base editor. The inference engine is the part of the system\\nthat uses the rules and facts to derive conclusions. The inference engine will\\nuse forward chaining, backward chaining, or a combination of the two to\\nmake inferences from the data that are available to it.\\nThe knowledge-base editor allows the user to edit the information that is'), Document(metadata={}, page_content='contained in the knowledge base. The knowledge-base editor is not usually\\nmade available to the end user of the system but is used by the knowledge\\nengineer or the expert to provide and update the knowledge that is con-\\ntained within the system.\\n9.4.3 The Expert System Shell\\nNote that in Figure 9.1, the parts of the expert system that do not contain\\ndomain-specific or case-specific information are contained within the'), Document(metadata={}, page_content='expert system shell. This shell is a general toolkit that can be used to build\\na number of different expert systems, depending on which knowledge base\\nis added to the shell.\\nAn example of such a shell is CLIPS (C Language Integrated Production\\nSystem), which is described in more detail in Section 9.4. Other examples\\nin common use include OPS5, ART, JESS, and Eclipse.\\n9.4.4 The Rete Algorithm\\nOne potential problem with expert systems is the number of comparisons'), Document(metadata={}, page_content='that need to be made between rules and facts in the database. In some cases,\\nwhere there are hundreds or even thousands of rules, running comparisons\\nagainst each rule can be impractical.\\nThe Rete Algorithm is an efficient method for solving this problem and is\\nused by a number of expert system tools, including OPS5 and Eclipse.'), Document(metadata={}, page_content='254 CHAPTER 9 Rules and Expert Systems\\nThe Rete is a directed, acyclic, rooted graph (or a search tree, which was\\ndiscussed in great detail in Chapters 3 and 4).\\nEach path from the root node to a leaf in the tree represents the left-hand\\nside of a rule. Each node stores details of which facts have been matched by\\nthe rules at that point in the path.\\nAs facts are changed, the new facts are propagated through the Rete from\\nthe root node to the leaves, changing the information stored at nodes'), Document(metadata={}, page_content='appropriately. This could mean adding a new fact, or changing information\\nabout an old fact, or deleting an old fact.\\nIn this way, the system only needs to test each new fact against the rules,\\nand only against those rules to which the new fact is relevant, instead of\\nchecking each fact against each rule.\\nThe Rete algorithm depends on the principle that in general, when using\\nforward chaining in expert systems, the values of objects change relatively'), Document(metadata={}, page_content='infrequently, meaning that relatively few changes need to be made to the\\nRete. In such cases, the Rete algorithm can provide a significant improve-\\nment in performance over other methods, although it is less efficient in\\ncases where objects are continually changing.\\n9.4.5 Knowledge Engineering\\nKnowledge engineering is a vital part of the development of any expert sys-\\ntem. The knowledge engineer does not need to have expert domain knowl-'), Document(metadata={}, page_content='edge but does need to know how to convert such expertise into the rules\\nthat the system will use, preferably in an efficient manner. Hence, the\\nknowledge engineer’s main task is communicating with the expert, in order\\nto understand fully how the expert goes about evaluating evidence and\\nwhat methods he or she uses to derive conclusions.\\nHaving built up a good understanding of the rules the expert uses to draw\\nconclusions, the knowledge engineer must encode these rules in the expert'), Document(metadata={}, page_content='system shell language that is being used for the task.\\nIn some cases, the knowledge engineer will have freedom to choose the\\nmost appropriate expert system shell for the task. In other cases, this deci-\\nsion will have already been made, and the knowledge engineer must work\\nwith what he is given.'), Document(metadata={}, page_content='9.5 CLIPS (C Language Integrated Production System) 255\\n9.5 CLIPS (C Language Integrated Production System)\\nCLIPS is a freely available expert system shell that has been implemented in\\nC. It provides a language for expressing rules and mainly uses forward\\nchaining to derive conclusions from a set of facts and rules.\\nThe notation used by CLIPS is very similar to that used by LISP . The fol-\\nlowing is an example of a rule specified using CLIPS:\\n(defrule birthday\\n(firstname ?r1 John)'), Document(metadata={}, page_content='(defrule birthday\\n(firstname ?r1 John)\\n(surname ?r1 Smith)\\n(haircolor ?r1 Red)\\n=>\\n(assert (is-boss ?r1)))\\n?r1 is used to represent a variable, which in this case is a person. Assert is\\nused to add facts to the database, and in this case the rule is used to draw a\\nconclusion from three facts about the person: If the person has the first\\nname John, has the surname Smith, and has red hair, then he is the boss.\\nThis can be tried in the following way:\\n(assert (firstname x John))'), Document(metadata={}, page_content='(assert (firstname x John))\\n(assert (surname x Smith))\\n(assert (haircolor x Red))\\n(run)\\nAt this point, the command (facts) can be entered to see the facts that are\\ncontained in the database:\\nCLIPS> (facts)\\nf-0 (firstname x John)\\nf-1 (surname x Smith)\\nf-2 (haircolor x Red)\\nf-3 (is-boss x)\\nSo CLIPS has taken the three facts that were entered into the system and\\nused the rule to draw a conclusion, which is that x is the boss. Although this'), Document(metadata={}, page_content='is a simple example, CLIPS, like other expert system shells, can be used to\\nbuild extremely sophisticated and powerful tools.\\nFor example, MYCIN is a well-known medical expert system that was\\ndeveloped at Stanford University in 1984. MYCIN was designed to assist\\ndoctors to prescribe antimicrobial drugs for blood infections. In this way,\\nexperts in antimicrobial drugs are able to provide their expertise to other\\ndoctors who are not so expert in that field. By asking the doctor a series of'), Document(metadata={}, page_content='256 CHAPTER 9 Rules and Expert Systems\\nquestions, MYCIN is able to recommend a course of treatment for the\\npatient. Importantly, MYCIN is also able to explain to the doctor which\\nrules fired and therefore is able to explain why it produced the diagnosis\\nand recommended treatment that it did.\\nMYCIN has proved successful: for example, it has been proven to be able to\\nprovide more accurate diagnoses of meningitis in patients than most doctors.'), Document(metadata={}, page_content='MYCIN was developed using LISP , and its rules are expressed as LISP\\nexpressions. The following is an example of the kind of rule used by\\nMYCIN, translated into English:\\nIF the infection is primary-bacteria\\nAND the site of the culture is one of the sterile sites\\nAND the suspected portal of entry is the gastrointestinal tract\\nTHEN there is suggestive evidence (0.7) that infection is bacteroid\\nIn Chapter 17, we learn more about how MYCIN uses certainty factors to\\naid its diagnosis process.'), Document(metadata={}, page_content='aid its diagnosis process.\\nThe following is a very simple example of a CLIPS session where rules are\\ndefined to operate an elevator:\\nCLIPS> (defrule rule1\\n(elevator ?floor_now)\\n(button ?floor_now)\\n=>\\n(assert (open_door)))\\nCLIPS> (defrule rule2\\n(elevator ?floor_now)\\n(button ?other_floor)\\n=>\\n(assert (goto ?other_floor)))\\nCLIPS> (assert (elevator floor1))\\n==> f-0 (elevator floor1)\\n<Fact-0>\\nCLIPS> (assert (button floor3))\\n==> f-1 (button floor3)\\n<Fact-1>\\n<CLIPS> (run)\\n==>f-2 (goto floor3)'), Document(metadata={}, page_content='<Fact-1>\\n<CLIPS> (run)\\n==>f-2 (goto floor3)\\nThe segments in bold are inputs by the knowledge engineer, and the plain\\ntext sections are CLIPS.\\nNote that ?floor_now is an example of a variable within CLIPS, which\\nmeans that any object can match it for the rule to trigger and fire. In our'), Document(metadata={}, page_content='9.6 Backward Chaining in Rule-Based Expert Systems 257\\nexample, the first rule simply says: If the elevator is on a floor, and the but-\\nton is pressed on the same floor, then open the door. The second rule says:\\nIf the elevator is on one floor, and the button is pressed on a different floor,\\nthen go to that floor.\\nAfter the rules, two facts are inserted into the database. The first fact says\\nthat the elevator is on floor 1, and the second fact says that the button has\\nbeen pressed on floor 3.'), Document(metadata={}, page_content='been pressed on floor 3.\\nWhen the (run) command is issued to the system, it inserts a new fact into\\nthe database, which is a command to the elevator to go to floor 3.\\n9.6 Backward Chaining in Rule-Based Expert Systems\\nA common method for building expert systems is to use a rule-based sys-\\ntem with backward chaining. Typically, a user enters a set of facts into the\\nsystem, and the system tries to see if it can prove any of the possible'), Document(metadata={}, page_content='hypotheses using these facts. In some cases, it will need additional facts, in\\nwhich case the expert system will often ask the user questions, to ascertain\\nfacts that could enable further rules to fire.\\nThe algorithm is applied as follows:\\nT o prove a conclusion, we must prove a set of hypotheses, one of which is\\nthe conclusion. For each hypothesis, H:\\nIf H is in the facts database, it is proved.\\nOtherwise, if H can be determined by asking a question, then enter'), Document(metadata={}, page_content='the user’s answer in the facts database. Hence, it can be determined\\nwhether H is true or false, according to the user’s answer.\\nOtherwise, find a rule whose conclusion is H. Now apply this algo-\\nrithm to try to prove this rule’s antecedents.\\nIf none of the above applies, we have failed to prove H.\\nTypically, backward chaining is used in combination with forward chain-\\ning. Whenever a new fact is added to the database, forward chaining is'), Document(metadata={}, page_content='applied to see if any further facts can be derived. Backward chaining is then\\nused to try to prove each possible hypothesis.\\nLet us imagine a simple medical expert system that is designed to diagnose\\nand treat patients’ diseases. The rules might be as follows:\\nRule 1: If headache then prescribe painkiller'), Document(metadata={}, page_content='258 CHAPTER 9 Rules and Expert Systems\\nRule 2: If headache and sore throat and coughing then diagnose flu\\nRule 3: If tired and headache then diagnose glandular fever\\nRule 4: If tired and sore throat then diagnose tonsillitis\\nRule 5: If tired then diagnose stress\\nNow we might imagine a session between the user (the patient) and the\\nexpert system as follows:\\nPatient: I have a headache\\nSystem: I suggest you take a painkiller. Do you have a sore throat?\\nPatient: Y es, I do.'), Document(metadata={}, page_content='Patient: Y es, I do.\\nSystem: Have you been coughing?\\nPatient: No, why do you ask?\\nSystem: If you had a headache and a sore throat and had been\\ncoughing, I would have concluded you might have flu. It\\ndoesn’t sound as though you have flu. Have you been feel-\\ning tired recently?\\nPatient: Y es, I have.\\nSystem: It sounds as though you may have glandular fever or tonsillitis.\\nThis was a rather simplistic example, but it illustrates the way that the sys-'), Document(metadata={}, page_content='tem uses forward chaining to derive new facts and asks questions in order\\nto carry out its backward chaining analysis. For example, having been told\\nthat the patient has a headache, the system first applies forward chaining\\nwith Rule 1 and, as a result, adds “prescribe painkiller” to its database of\\nfacts. This fact has an action, or a recommendation, associated with it, and\\nso the system advises the patient that she should take a painkiller.'), Document(metadata={}, page_content='Next, the system tries to see if it can prove any other hypotheses. The possi-\\nble hypotheses are flu, tonsillitis, glandular fever, and stress. First, the sys-\\ntem uses backward chaining to try to prove the hypothesis that the patient\\nhas the flu.\\nT o prove this hypothesis, the antecedents of Rule 2 must be proved: that the\\npatient has a headache and a sore throat and has been coughing. The\\npatient has already said that she has a headache, so this fact is already in the'), Document(metadata={}, page_content='fact database. Next, the system must establish whether the patient has a\\nsore throat. She says that she does, so this fact is added to the fact database.'), Document(metadata={}, page_content='9.7 CYC 259\\nShe has not been coughing, though, so the system concludes that she does\\nnot have flu.\\nAt this point also note that the patient asks why the system asked the last\\nquestion. The system is able to use its explanation facility to provide an\\nexplanation for why it asked the question and what conclusion it was able\\nto draw from the answer.\\nFinally, the patient says that she has been feeling tired, and as a result of this'), Document(metadata={}, page_content='fact being added to the database, Rules 3, 4, and 5 are all triggered. In this\\ncase, conflict resolution has been applied in a rather simplistic way, such\\nthat Rules 3 and 4 both fire, but 5 does not. In a real medical expert system,\\nit is likely that further questions would be asked, and more sophisticated\\nrules applied to decide which condition the patient really had.\\n9.7 CYC\\nCYC is an example of a frame-based representational system of knowledge,'), Document(metadata={}, page_content='which is, in a way, the opposite of an expert system. Whereas an expert sys-\\ntem has detailed knowledge of a very narrow domain, the developers of CYC\\nhave fed it information on over 100,000 different concepts from all fields of\\nhuman knowledge. CYC also has information of over 1,000,000 different\\npieces of “common sense” knowledge about those concepts. The system has\\nover 4000 different types of links that can exist between concepts, such as'), Document(metadata={}, page_content='inheritance, and the “is–a” relationship that we have already looked at.\\nThe idea behind CYC was that humans function in the world mainly on the\\nbasis of a large base of knowledge built up over our lifetimes and our ances-\\ntors’ lifetimes. By giving CYC access to this knowledge, and the ability to\\nreason about it, they felt they would be able to come up with a system with\\ncommon sense. Ultimately, they predict, the system will be built into word'), Document(metadata={}, page_content='processors. Then word processors will not just correct your spelling and\\ngrammar, but will also point out inconsistencies in your document. For\\nexample, if you promise to discuss a particular subject later in your docu-\\nment, and then forget to do so, the system will point this out to you. They\\nalso predict that search engines and other information retrieval systems\\n(see Chapter 20) will be able to find documents even though they do not\\ncontain any of the words you entered as your query.'), Document(metadata={}, page_content='CYC’s knowledge is segmented into hundreds of different contexts to avoid\\nthe problem of many pieces of knowledge in the system contradicting each'), Document(metadata={}, page_content='260 CHAPTER 9 Rules and Expert Systems\\nother. In this way, CYC is able to know facts about Dracula and to reason\\nabout him, while also knowing that Dracula does not really exist.\\nCYC is able to understand analogies, and even to discover new analogies for\\nitself, by examining the similarities in structure and content between differ-\\nent frames and groups of frames. CYC’s developers claim, for example, that\\nit discovered an analogy between the concept of “family” and the concept of\\n“country. ”'), Document(metadata={}, page_content='“country. ”\\n9.8 Chapter Summary\\n■ IF . . . THEN . . . rules can be used to represent knowledge\\nabout objects.\\n■ Rule-based systems, or production systems, use rules to attempt to\\nderive diagnoses or to provide instructions.\\n■ Rule systems can work using forward chaining, backward chaining,\\nor both. Forward chaining works from a set of initial facts, and\\nworks toward a conclusion. Backward chaining starts with a\\nhypothesis and tries to prove it using the facts and rules that are\\navailable.'), Document(metadata={}, page_content='available.\\n■ Conflict resolution methods are used to determine what to do\\nwhen more than one solution is provided by a rule-based system.\\n■ Knowledge from a domain expert is translated by a knowledge\\nengineer into a form suitable for an expert system, which is then\\nable to help an end-user solve problems that would normally\\nrequire a human expert.\\n■ In many cases, expert systems use backward chaining and ask ques-\\ntions of the end user to attempt to prove a hypothesis.'), Document(metadata={}, page_content='■ Expert systems are usually built on an expert system shell (such as\\nCLIPS), which provides a generic toolkit for building expert systems.\\n■ The Rete algorithm provides an efficient method for chaining in\\nrule-based systems where there are many rules and where facts do\\nnot change frequently.\\n■ Semantic nets and frame-based systems are also used as the basis\\nfor expert systems.'), Document(metadata={}, page_content='9.11 Further Reading 261\\n■ CYC is a system built with knowledge of over 100,000 objects and\\nis able to make complex deductions about the real world.\\n9.9 Review Questions\\n9.1 Explain why expert systems are so called.\\n9.2 Explain the difference between forward chaining and backward\\nchaining. Explain the advantages and disadvantages of each method.\\n9.3 Explain the various methods of conflict resolution that can be used\\nin rule-based expert systems. For each of these, give an example of'), Document(metadata={}, page_content='a scenario where using it would not give the correct result.\\n9.4 What is the purpose of meta rules? Would an expert system have\\nany advantages if it knew the difference between meta rules and\\nnormal rules?\\n9.5 Describe the architecture of an expert system, and describe the\\nroles of the various people involved in it.\\n9.6 What is the purpose of the Rete algorithm? Describe how it works.\\n9.7 Explain the relationships between rules, logic, semantic nets, and'), Document(metadata={}, page_content='frames. Give an example of a situation where you might use each of\\nthem. Which system has the greatest representational adequacy?\\nWhich has the least? Why?\\n9.10 Exercises\\n9.1 Extend the CLIPS rules given in Section 9.5 to produce a more use-\\nful system for running an elevator on a building with five floors.\\n9.2 Implement a rule-based or frame-based expert system shell in the\\nprogramming language of your choice. Implement an expert sys-'), Document(metadata={}, page_content='tem in your expert system shell to solve problems in an area in\\nwhich you have expertise (for example, solving computer science\\nproblems, or identifying films or songs).\\n9.11 Further Reading\\nNegnevitsky (2002) provides an excellent overview of rule- and frame-\\nbased expert systems.'), Document(metadata={}, page_content='262 CHAPTER 9 Rules and Expert Systems\\nFor a more detailed description of the Rete algorithm, see Rete: A Fast Algo-\\nrithm for the Many Pattern / Many Object Pattern Match Problem by C. L.\\nForgy from Artificial Intelligence, Vol. 19, pp. 17–37, 1982.\\nWinston (1993) also covers the Rete algorithm.\\nA brief description of the CYC expert system, and some of the difficulties\\nfaced in building it can be found in Douglas B. Lenat’s article,Programming'), Document(metadata={}, page_content='Artificial Intelligence , which was first published in Scientific American in\\nSeptember 1995 and can also be found in Fritz (2002).\\nDeeper coverage of CYC can be found in Lenat and Guha (1990).\\nDiscussion of productions systems and an example of an expert system\\nimplementation in PROLOG can be found in Logic and Prolog by Richard\\nSpencer-Smith (1991).\\nAn interesting discussion of some of the limitations of MYCIN can be\\nfound in McCarthy (1983). In particular, he points out that the system is'), Document(metadata={}, page_content='not aware of its own limitations and so is liable to make confident recom-\\nmendations that are potentially dangerous.\\nFundamentals of Expert System Technology: Principles and Concepts by\\nSamuel J. Biondo (1990 – Intellect)\\nRule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuris-\\ntic Programming Project by B. G. Buchanan and E. H. Shortliffe (1984 –\\nAddison Wesley)\\nProlog Programming for Students: With Expert Systems and Artificial Intelli-'), Document(metadata={}, page_content='gence Topics by David Callear (2001 – Continuum)\\nArtificial Intelligence: A Knowledge-Based Approach by Morris W. Firebaugh\\n(1988 – Boyd & Fraser Publishing Company – out of print)\\nUnderstanding Artificial Intelligence (Science Made Accessible) compiled by\\nSandy Fritz (2002 – Warner Books)\\nExpert Systems: Principles and Programming by Joseph C. Giarratano (1998\\n– Brooks Cole)\\nManaging Uncertainty in Expert Systems by Jerzy W. Grzymala-Busse (1991\\n– Kluwer Academic Publishers)'), Document(metadata={}, page_content='– Kluwer Academic Publishers)\\nExpert Systems: Artificial Intelligence in Business by Paul Harmon (1985 –\\nJohn Wiley & Sons – out of print)'), Document(metadata={}, page_content='9.11 Further Reading 263\\nIntroduction to Expert Systems by Peter Jackson (1999 – Addison Wesley)\\nKnowledge Acquisition for Expert Systems: A Practical Handbookby Alison L.\\nKidd (1987 – Plenum Publishing Corporation)\\nBuilding Large Knowledge-Based Systems: Representation and Inference in the\\nCYC Project by Douglas B. Lenat and R. V . Guha (1990 – Addison Wesley)\\nThe Logic of Knowledge Bases by Hector J. Levesque and Gerhard Lakemeyer\\n(2001 – MIT Press)'), Document(metadata={}, page_content='(2001 – MIT Press)\\nSome Expert Systems Need Common Sense by John McCarthy (1983 – in\\nComputer Culture: The Scientific, Intellectual and Social Impact of the Com-\\nputer edited by Heinz Pagels, Vol. 426)\\nBuilding Expert Systems in Prologby Dennis Merritt (1995 – Springer V erlag)\\nArtificial Intelligence: A Guide to Intelligent Systems by Michael Negnevitsky\\n(2002 – Addison Wesley)\\nComputer Based Medical Consultations: Mycinby Edward Shortliffe (1976 –\\nElsevier Science, out of print)'), Document(metadata={}, page_content='Elsevier Science, out of print)\\nLogic and Prolog by Richard Spencer-Smith (1991 – Harvester Wheatsheaf)\\nManaging Expert Systems edited by Efraim Turban and Jay Liebowitz (1992\\n– Idea Group Publishing)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='Machine Learning\\n4\\nIntroduction to Part 4\\nPart 4 is divided into five chapters:\\nIntroduction to Machine Learning\\nThis chapter introduces a number of techniques for\\nmachine learning, such as ID3 for learning decision trees,\\nversion spaces, and the nearest neighbor algorithm. It also\\nintroduces the ideas behind neural networks, which are\\ncovered in more detail in Chapter 11.\\nThis chapter explains the idea of inductive bias and why it\\nis important in machine learning.\\nNeural Networks'), Document(metadata={}, page_content='is important in machine learning.\\nNeural Networks\\nThis chapter expands on the ideas introduced in Chapter 10\\nand gives a more detailed coverage of neural networks. It\\nexplains the relationship between artificial neurons and\\nbiological neurons, and introduces perceptions. The chap-\\nter then explains multilayer networks and introduces back-\\npropagation as a way to train multilayer networks. It also\\nintroduces recurrent networks, such as Hopfield networks.'), Document(metadata={}, page_content='This chapter explains unsupervised neural networks (such\\nas Kohonen maps) as well as supervised ones.\\nFinally, this chapter briefly introduces the idea of evolving\\nneural networks, combining ideas from this chapter with\\nideas from Chapters 13 and 14.\\nPART\\n10\\nCHAPTER\\n11\\nCHAPTER'), Document(metadata={}, page_content='Probabilistic Reasoning and Bayesian Belief Networks\\nThis chapter introduces probabilistic reasoning and\\nexplains how it can be used to deal with situations in which\\nthere is uncertainty about some variables. The chapter\\nexplains the notation that is used and introduces condi-\\ntional probability. It explains Bayes’ theorem and shows\\nhow it can be used with some practical examples. It then\\nintroduces methods that use Bayesian reasoning to learn,'), Document(metadata={}, page_content='including the optimal Bayes’ classifier, which provides the\\nbest possible classification of unseen data.\\nArtificial Life: Learning through Emergent Behavior\\nChapter 13 provides a broad overview of the subject of arti-\\nficial life. It starts with a question: What is life? It then\\nexplains how artificial life techniques model nature. A\\nnumber of techniques and experimental findings are dis-\\ncussed, including cellular automata, genetic programming,\\nevolutionary programming, and L-systems.'), Document(metadata={}, page_content='evolutionary programming, and L-systems.\\nThe chapter also discusses the idea of emergent behavior\\nand the reason that evolution is such an important concept.\\nSome concepts relating to genetic algorithms are intro-\\nduced in this chapter and explored further in Chapter 14.\\nGenetic Algorithms\\nChapter 14 builds on the ideas introduced in Chapter 13,\\nby explaining in more detail the ideas behind genetic algo-\\nrithms. The chapter explains the methods used in building'), Document(metadata={}, page_content='genetic algorithms, such as crossover and mutation, and\\nattempts to provide a basis for understanding why genetic\\nalgorithms work.\\nA concrete example is given of how a genetic algorithm could\\nbe built to play a simple game—the Prisoner’s Dilemma.\\n12\\nCHAPTER\\n13\\nCHAPTER\\n14\\nCHAPTER'), Document(metadata={}, page_content='10CHAPTER\\nIntroduction to \\nMachine Learning\\nO, what learning is!\\n—William Shakespeare, Romeo and Juliet\\nMuch learning doth make thee mad.\\n—The Acts of the Apostles, Chapter 26, V erse 24\\nWhence is thy learning? Hath thy toil\\nO’er books consumed the midnight oil?\\n—John Gay,Fables\\nLearning and intelligence are intimately related to each other. It is usually\\nagreed that a system capable of learning deserves to be called intelligent; and'), Document(metadata={}, page_content='conversely, a system being considered as intelligent is, among other things,\\nusually expected to be able to learn. Learning always has to do with the self-\\nimprovement of future behaviour based on past experience.\\n—Sandip Sen and Gerhard Weiss,Learning in Multiagent Systems\\n10.1 Introduction\\nMachine learning is an extremely important part of Artificial Intelligence.\\nThis chapter provides a brief overview of some of the main methods and'), Document(metadata={}, page_content='ideas that are used in machine learning and also provides a very brief intro-\\nduction to neural networks, which are covered in more detail in Chapter 11.'), Document(metadata={}, page_content='268 CHAPTER 10 Introduction to Machine Learning\\nIn this chapter, concept learning methods are explored, which are able to\\ngeneralize from a set of training data to be able to correctly classify data\\nthat has not been seen before. Decision-tree learning is examined, and the\\nID3 algorithm is explained.\\n10.2 Training\\nIn most learning problems, the task is to learn to classify inputs according\\nto a finite (or sometimes infinite) set of classifications. Typically, a learning'), Document(metadata={}, page_content='system is provided with a set of training data, which have been classified by\\nhand. The system then attempts to learn from these training data how to\\nclassify the same data (usually a relatively easy task) and also how to classify\\nnew data that it has not seen.\\nLearning to classify unseen data clearly assumes that there is some relation-\\nship between the data and the classifications—in other words, some function\\nf can be generated such that if a piece of datax belongs in classificationy, then'), Document(metadata={}, page_content='f(x) = y\\nFor example, if the equality function were used, the learning task would be\\nrelatively simple because each datum would be classified as itself. Clearly\\nmost real-world problems are not so simple, and producing a function that\\napproximates the correct mapping is one of the main challenges of\\nmachine learning.\\nIn fact, in most learning problems, the input data consist of more than\\none variable.\\nFor example, let us consider a system that is to learn how to evaluate static\\nchess positions.'), Document(metadata={}, page_content='chess positions.\\nFirst, we will consider a number of variables:\\nx\\n1: Number of white pieces on the board\\nx2: Number of black pieces on the board\\nx3: Number of black pieces threatened by white pieces\\nx4: Number of white pieces threatened by black pieces\\nx5: Can white checkmate on the next go?'), Document(metadata={}, page_content='10.2 Training 269\\nx6: Can black checkmate on the next go?\\nx7: Number of different moves white can make\\nx8: Number of different moves black can make\\nClearly, this is an oversimplification because a real chess system would need\\nto use a much more complex set of variables to evaluate a position.\\nNote that the variables are not all of the same type: most of the variables are\\nnumeric, but two of them are Boolean (can each side achieve checkmate on'), Document(metadata={}, page_content='the next go). Many learning problems will involve data of a number of dif-\\nferent types.\\nThe evaluation of each position is to be calculated as a high positive in the\\nevent that white has the better position and a high negative if black has the\\nbetter position. A value of 0 indicates a level position, and a score of /H11006100\\nindicates that one side has won the game, or is about to win.\\nIt seems probable that a simple linear weighted function of these variables'), Document(metadata={}, page_content='will suffice: We will write our evaluation function f as follows:\\nf(x\\n1,x 2,x 3,x 4,x 5,x 6,x 7,x 8) =\\nw1x1 + w2x2 + w3x3 + w4x4 + w5x5 + w6x6 + w7x7 + w8x8\\nwhere w1 to w8 are the weights associated with the eight variables. The aim\\nof the system is to determine suitable values for these weights, based on the\\ntraining data that are provided.\\nAn item of training data might be\\nf(10, 2, 1, 0, true, false, 10, 1) = 100\\nThis suggests that the position described by the training data is a definite'), Document(metadata={}, page_content='win for white.\\nClearly, there are an extraordinarily large number of possible sets of train-\\ning data for this function, and it may not even be the case that a suitable\\nfunction exists for this representation. A superior representation, for which\\na suitable function certainly exists, would be to map the positions of all 32\\npieces to the 64 squares on the board. In this case, a system could certainly\\nbe trained to determine whether any given position was better for white or'), Document(metadata={}, page_content='270 CHAPTER 10 Introduction to Machine Learning\\nfor black, but the enormous number of possible input data makes the prob-\\nlem somewhat harder.\\nIn Chapter 11, we see how artificial neural networks can be used to provide\\nextremely accurate mappings from input data to classifications for prob-\\nlems such as this.\\nIn this chapter, we will look at methods that are primarily used to learn\\nsomewhat simpler mappings, although these methods can certainly be\\nextended to work with more complex sets of data.'), Document(metadata={}, page_content='extended to work with more complex sets of data.\\n10.3 Rote Learning\\nThe simplest way for a computer to learn from experience is simply to learn\\nby rote. Training involves storing each piece of training data and its classi-\\nfication. Thereafter, a new item of data is classified by looking to see if it is\\nstored in memory. If it is, then the classification that was stored with that\\nitem is returned. Otherwise, the method fails.'), Document(metadata={}, page_content='item is returned. Otherwise, the method fails.\\nHence, a rote learner is able to classify only data that it has already seen,\\nand no attempt is made to approximate the mapping function, which is a\\nmajor weakness.\\n10.4 Learning Concepts\\nWe will now look at a number of methods that can be used to learn con-\\ncepts. Concept learning involves determining a mapping from a set of\\ninput variables to a Boolean value.\\nThe methods described here are known as inductive-learning methods .'), Document(metadata={}, page_content='These methods are based on the principle that if a function is found that\\ncorrectly maps a large set of training data to classifications, then it will also\\ncorrectly map unseen data. In doing so, a learner is able to generalize from\\na set of training data.\\nT o illustrate these methods, we will use a simple toy problem, as follows:\\nOur learning task will be to determine whether driving in a particular man-\\nner in particular road conditions is safe or not. We will use the following\\nattributes:'), Document(metadata={}, page_content='10.4 Learning Concepts 271\\nAttribute Possible values\\nSpeed slow, medium, fast\\nWeather wind, rain, snow, sun\\nDistance from car in front 10ft, 20ft, 30ft, 40ft, 50ft, 60ft\\nUnits of alcohol driver has drunk 0, 1, 2, 3, 4, 5\\nTime of day morning, afternoon, evening, night\\nTemperature cold, warm, hot\\nWe will consider a hypothesis to be a vector of values for these attributes. A\\npossible hypothesis is\\nh\\n1 = <slow, wind, 30ft, 0, evening, cold>'), Document(metadata={}, page_content='h\\n1 = <slow, wind, 30ft, 0, evening, cold>\\nWe also want to represent in a hypothesis that we do not care what value an\\nattribute takes. This is represented by “?” , as in the following hypothesis:\\nh2 = <fast, rain, 10ft, 2, ?, ?>\\nh2 represents the hypothesis that driving quickly in rainy weather, close to\\nthe car in front after having drunk two units of alcohol is safe, regardless of\\nthe time of day or the temperature. Clearly, this hypothesis is untrue and'), Document(metadata={}, page_content='would be considered by the learner to be a negative training example.\\nIn other cases, we need to represent a hypothesis that no value of a particu-\\nlar attribute will provide a positive example. We write this as “∅” , as in the\\nfollowing hypothesis:\\nh\\n3 = <fast, rain, 10ft, 2, ∅, ∅>\\nh3 states the opposite of h2—that driving quickly in rainy weather, close to\\nthe car in front after having drunk two units of alcohol cannot be safe,\\nregardless of the time of day or the temperature.'), Document(metadata={}, page_content='regardless of the time of day or the temperature.\\nThe task of the concept learner is to examine a set of positive and negative\\ntraining data and to use these to determine a hypothesis that matches all\\nthe training data, and which can then be used to classify instances that have\\nnot previously been seen.\\nConcept learning can be thought of as search through a search space that\\nconsists of all possible hypotheses, where the goal is the hypothesis that\\nmost closely represents the correct mapping.'), Document(metadata={}, page_content='272 CHAPTER 10 Introduction to Machine Learning\\n10.5 General-to-Specific Ordering\\nConsider the following two hypotheses:\\nhg = <?, ?, ?, ?, ?, ?>\\nhs = <∅, ∅, ∅, ∅, ∅, ∅>\\nhg is the hypothesis that it is safe to drive regardless of the conditions—this\\nis the most general hypothesis.\\nhs is the most specific hypothesis, which states that it is never safe to drive,\\nunder any circumstances.\\nThese hypotheses represent two extremes, and clearly a useful hypothesis'), Document(metadata={}, page_content='that accurately represents the mapping from attribute values to a Boolean\\nvalue will be somewhere in between these two.\\nOne method for concept learning is based on the idea that a partial order\\nexists over the space of hypotheses. This partial order is represented by the\\nrelationship “more general than”:\\n≥\\ng\\nWe w r ite\\nh1 ≥g h2\\nwhich states that h1 is more general than (or as general as) h2. Similarly, we\\ncould write\\nh1 >g h2\\nin the case where h1 is certainly more general than h2.'), Document(metadata={}, page_content='≥g defines a partial order over the hypothesis space, rather than a total\\norder, because some hypotheses are neither more specific nor more general\\nthan other hypotheses. For example, consider the following hypotheses:\\nh\\n1 = <?, ?, ?, ?, evening, cold>\\nh2 = <medium, snow, ?, ?, ?, ?>\\nWe cannot express any relationship betweenh1 and h2 in terms of generality.\\nOne hypothesis is more general than (or equally general as) another'), Document(metadata={}, page_content='hypothesis if every instance that is matched by the second hypothesis is also\\nmatched by the first hypothesis.'), Document(metadata={}, page_content='10.5 General-to-Specific Ordering 273\\nFor example,\\n<slow, ?, ?, ?, ?, ?> ≥ g <slow, ?, ?, ?, ?, cold>\\nIt should be clear that a more general hypothesis matches more instances\\nthan a less general hypothesis.\\n10.5.1 A Simple Learning Algorithm\\nThe following algorithm uses the general-to-specific ordering of hypotheses to\\nsearch the hypothesis space for a suitable hypothesis. The method is as follows:\\nStart with the most specific hypothesis. In our example above, this would'), Document(metadata={}, page_content='be <∅, ∅, ∅, ∅, ∅, ∅, ∅, ∅>.\\nNow, for each positive training example, determine whether each attribute in\\nthe example is matched by the current hypothesis. If it is not, replace the\\nattributes in the hypothesis with the next more general value that does match.\\nFor example, let us consider the following set of positive training data:\\n<slow, wind, 30ft, 0, evening, cold>\\n<slow, rain, 20ft, 0, evening, warm>\\n<slow, snow, 30ft, 0, afternoon, cold>'), Document(metadata={}, page_content='<slow, snow, 30ft, 0, afternoon, cold>\\nFirst, let us compare the first item of training data with the current hypoth-\\nesis, which is <∅, ∅, ∅, ∅, ∅, ∅, ∅, ∅>. Clearly, none of the attributes are\\nmatched by this hypothesis. The next most general value for each attribute\\nthan ∅ that matches the training data is the value contained in the training\\ndata. So we replace our hypothesis with the following hypothesis:\\n<slow, wind, 30ft, 0, evening, cold>'), Document(metadata={}, page_content='<slow, wind, 30ft, 0, evening, cold>\\nClearly, the hypothesis <?, ?, ?, ?, ?, ?, ?, ?> would have been more general than\\nthe initial hypothesis, but the method we are using is to select thenext more\\ngeneral value for each attribute. In this way, we move from a hypothesis that\\nis too specific to one that is general enough to match all the training data.\\nWe will now consider the second item of training data:\\n<slow, rain, 20ft, 0, evening, warm>'), Document(metadata={}, page_content='<slow, rain, 20ft, 0, evening, warm>\\nNow we compare each attribute value with the corresponding value in our\\ncurrent hypothesis. Where the values match, we do not need to make any\\nchange. Where they do not match, we need to replace the value with “?” so'), Document(metadata={}, page_content='274 CHAPTER 10 Introduction to Machine Learning\\nthat the hypothesis matches both items of training data. Hence, our new\\nhypothesis is\\n<slow, ?, ?, 0, evening, ?>\\nBy comparing with our final item of training data, we arrive at the follow-\\ning hypothesis:\\n<slow, ?, ?, 0, ?, ?>\\nThis hypothesis states that it is only safe to drive if one drives slowly and\\nhas not drunk any alcohol, and that this is true regardless of the road or\\nweather conditions.'), Document(metadata={}, page_content='weather conditions.\\nThis hypothesis is consistent with the training examples, which means that\\nit maps each of them to the correct classification.\\nThis algorithm will generate the most specific hypothesis that matches\\nall of the training data. There are a number of problems with this algo-\\nrithm: first of all, it may not be desirable to identify the most specific\\nhypothesis—it may be that the most general hypothesis that matches the'), Document(metadata={}, page_content='training data provides a better solution. Secondly, the most specific\\nhypothesis identified by the algorithm may not be the only solution—\\nthere may be other most specific hypotheses that match the data, one of\\nwhich may be a preferable solution. Additionally, this algorithm does\\nnot make any use of negative examples. As we will see, most useful\\nlearning methods are able to make use of negative as well as positive\\ntraining examples.'), Document(metadata={}, page_content='training examples.\\nFinally, the method does not deal well with inconsistent or incorrect train-\\ning data. In real-world problems, an ability to deal with such errors is vital,\\nas we see later in this part of the book.\\n10.6 Version Spaces\\nGiven a set of training examples (positive and negative), the set of hypothe-\\nses that correctly map each of the training examples to its classification is\\ncalled the version space.\\nOne method for learning from a set of data is thus to start from a complete'), Document(metadata={}, page_content='version space that contains all hypotheses and systematically remove all\\nthe hypotheses that do not correctly classify each training example.\\nAlthough this method might work well on small problems, for problems of'), Document(metadata={}, page_content='10.7 Candidate Elimination 275\\nany reasonable size, the task of enumerating all hypotheses would be\\nimpractical.\\n10.7 Candidate Elimination\\nWe now explore another method that uses version spaces to learn. The\\naim of these methods is to identify a single hypothesis, if possible, that\\ncorrectly describes the problem. The more training data that are avail-\\nable, the fewer hypotheses are contained in the version space. If all the'), Document(metadata={}, page_content='training data have been used, and the version space contains just a single\\nhypothesis, then this matches all the training data and should also match\\nunseen data.\\nThe candidate elimination learning method operates in a similar manner\\nto the simple algorithm presented in Section 10.5.1. Unlike the earlier sim-\\nple method, the candidate elimination method stores not just a single\\nhypothesis, but two sets of hypotheses. In addition to maintaining a set of'), Document(metadata={}, page_content='most specific hypotheses that match the training data, this method also\\nmaintains a set of hypotheses that starts out as a set with the single item \\n<?, ?, ?, ?, ?, ?, ?, ?> and ends up being a set of the most general hypotheses\\nthat match all the training data. This algorithm is thus able to make use of\\nnegative training data as well as positive training data.\\nThe method operates as follows: Two sets are maintained of hypotheses, h\\ns'), Document(metadata={}, page_content='s\\nand hg: hs is initialized as {<∅, ∅, ∅, ∅, ∅, ∅, ∅, ∅>} and hg is initialized\\nas {<?, ?, ?, ?, ?, ?, ?, ?>}.\\nWhen a positive training example is encountered, it is compared with the\\nhypotheses contained in hg. If any of these hypotheses does not match the\\ntraining example, it is removed from hg. The positive training data are then\\ncompared with the hypotheses contained in hs. If one of these hypotheses\\ndoes not match the training data, it is replaced by the set of slightly more'), Document(metadata={}, page_content='general hypotheses that are consistent with the data, and such that there is\\nat least one hypothesis in h\\ng that is more general.\\nThis method is applied in reverse for negative training data. By applying\\nthis method to each item of training data, the sets hg and hs move closer to\\neach other and eventually between them contain the full version space of\\nhypotheses that match all the training data.'), Document(metadata={}, page_content='276 CHAPTER 10 Introduction to Machine Learning\\n10.8 Inductive Bias\\nAll learning methods have an inductive bias . Inductive bias refers to the\\nrestrictions that are imposed by the assumptions made in the learning\\nmethod. For example, in the above discussions we have been assuming\\nthat the solution to the problem of road safety can be expressed as a con-\\njunction of a set of eight concepts. This does not allow for more complex\\nexpressions that cannot be expressed as a conjunction. This inductive'), Document(metadata={}, page_content='bias means that there are some potential solutions that we cannot\\nexplore, and which are, therefore, not contained within the version space\\nwe examine.\\nThis may seem like an unfortunate limitation, but in fact inductive bias is\\nessential for learning. In order to have an unbiased learner, the version\\nspace would have to contain every possible hypothesis that could possibly\\nbe expressed. This would impose a severe limitation: the solution that the'), Document(metadata={}, page_content='learner produced could never be any more general than the complete set of\\ntraining data. In other words, it would be able to classify data that it had\\npreviously seen (as the rote learner could) but would be unable to general-\\nize in order to classify new, unseen data.\\nThe inductive bias of the candidate elimination algorithm is that it is only\\nable to classify a new piece of data if all the hypotheses contained within its'), Document(metadata={}, page_content='version space give the data the same classification. Hence, the inductive bias\\ndoes impose a limitation on the learning method.\\nIn the 14th century, William of Occam proposed his famous “ Occam’s\\nrazor, ” which simply states that it is best to choose the simplest hypothesis\\nto explain any phenomenon. We can consider this to be a form of inductive\\nbias, which states that the best hypothesis to fit a set of training data is the'), Document(metadata={}, page_content='simplest hypothesis. We will see later how this inductive bias can be useful\\nin learning decision trees.\\n10.9 Decision-Tree Induction\\nIn Chapter 3, we see a tree that was used to determine which species a par-\\nticular bird belonged to, based on various observed features of the bird. A\\nvariation of this kind of tree, where the leaf nodes are all Boolean values is'), Document(metadata={}, page_content='10.9 Decision-Tree Induction 277\\nCountry of\\nOrigin\\nUSA Europe Rest of World\\nComedy Romance\\nScience FictionYe s N o\\nGenreBig Star\\nfalsefalsetruefalsetrue\\nfalse\\nFigure 10.1\\nA simple decision tree \\nfor determining whether\\nor not a film will be a \\nbox-office success\\ncalled a decision tree. A decision tree takes in a set of attribute values and\\noutputs a Boolean decision.\\nAn example of a decision tree is shown in Figure 10.1. This decision tree can be'), Document(metadata={}, page_content='used to determine whether or not a given film will be a success at the box office.\\nT o use the decision tree, we start at the top and apply the question to the\\nfilm. If the film is made in the United States, we move down the first branch\\nof the tree; if it is made in Europe the second; and if elsewhere then we\\nexplore the third branch. The final boxes represent the Boolean value, true\\nor false, which expresses whether a film is a success or not.'), Document(metadata={}, page_content='According to this extremely simplistic (and possibly somewhat con-\\ntentious) decision tree, a film can only be a box-office success if it is made\\nin the United States and has a big star, or if it is a European comedy.\\nWhereas version spaces are able to represent expressions that consist solely\\nof conjunctions, decision trees can represent more complex expressions,\\ninvolving disjunctions and conjunctions. For example, the decision tree in\\nFigure 10.1 represents the following expression:'), Document(metadata={}, page_content='Figure 10.1 represents the following expression:\\n((Country = USA) ∧ (Big Star = yes)) ∨ ((Country = Europe) ∧\\n(Genre = comedy))\\nDecision-tree induction (or decision-tree learning) involves using a set of\\ntraining data to generate a decision tree that correctly classifies the training\\ndata. If the learning has worked, this decision tree will then correctly clas-\\nsify new input data as well.'), Document(metadata={}, page_content='278 CHAPTER 10 Introduction to Machine Learning\\nThe best-known decision tree induction algorithm is ID3, which was devel-\\noped by Quinlan in the 1980s.\\nThe ID3 algorithm builds a decision tree from the top down. The nodes are\\nselected by choosing features of the training data set that provide the most\\ninformation about the data and turning those features into questions. For\\nexample, in the above example, the first feature to be noted might be that'), Document(metadata={}, page_content='the country of origin is a significant determinant of whether a film will be\\na success or not. Hence, the first question to be placed into the decision tree\\nis “what is the film’s country of origin?” .\\nThe most important feature of ID3 is how the features are chosen. It would\\nbe possible to produce a decision tree by selecting the features in an arbi-\\ntrary order, but this would not necessarily produce the most efficient deci-'), Document(metadata={}, page_content='sion tree. The ID3 algorithm finds the shortest possible decision tree that\\ncorrectly classifies the training data.\\n10.9.1 Information Gain\\nThe method used by ID3 to determine which features to use at each stage of\\nthe decision tree is to select, at each stage, the feature that provides the\\ngreatest information gain. Information gain is defined as the reduction in\\nentropy. The entropy of a set of training data, S, is defined as\\nH(S) = /H11002p\\n1 log2 p1 /H11002p0 log2 p0'), Document(metadata={}, page_content='H(S) = /H11002p\\n1 log2 p1 /H11002p0 log2 p0\\nwhere p1 is defined as the proportion of the training data that includes\\npositive examples, and p0 is defined as the proportion that includes neg-\\native examples. The entropy of S is zero when all the examples are posi-\\ntive, or when all the examples are negative. The entropy reaches its\\nmaximum value of 1 when exactly half of the examples are positive and\\nhalf are negative.'), Document(metadata={}, page_content='half are negative.\\nThe information gain of a particular feature tells us how closely that feature\\nrepresents the entire target function, and so at each stage, the feature that\\ngives the highest information gain is chosen to turn into a question.'), Document(metadata={}, page_content='10.9 Decision-Tree Induction 279\\n10.9.2 Example\\nWe will start with the training data given below:\\nFilm Country of origin Big star Genre Success\\nFilm 1 United States yes Science Fiction true\\nFilm 2 United States no Comedy false\\nFilm 3 United States yes Comedy true\\nFilm 4 Europe no Comedy true\\nFilm 5 Europe yes Science fiction false\\nFilm 6 Europe yes Romance false\\nFilm 7 Rest of World yes Comedy false\\nFilm 8 Rest of World no Science fiction false\\nFilm 9 Europe yes Comedy true'), Document(metadata={}, page_content='Film 9 Europe yes Comedy true\\nFilm 10 United States yes Comedy true\\nWe will now calculate the information gain for the three different attributes\\nof the films, to select which one to use at the top of the tree.\\nFirst, let us calculate the information gain of the attribute “country of ori-\\ngin. ” Our collection of training data consists of five positive examples and\\nfive negative examples, so currently it has an entropy value of 1.'), Document(metadata={}, page_content='Four of the training data are from the United States, four from Europe, and\\nthe remaining two from the rest of the world.\\nThe information gain of this attribute is the reduction in entropy that it\\nbrings to the data. This can be calculated as follows:\\nFirst, we calculate the entropy of each subset of the training data as broken\\nup by this attribute. In other words, we calculate the entropy of the items\\nthat are from the United States, the entropy of the items from Europe, and'), Document(metadata={}, page_content='the entropy of the items from the rest of the world.'), Document(metadata={}, page_content='280 CHAPTER 10 Introduction to Machine Learning\\nOf the films from the United States, three were successes and one was not.\\nHence, the entropy of this attribute is\\nH(USA) = /H11002(3/4) log2 (3/4) /H11002(1/4) log2 (1/4)\\n= 0.311 + 0.5\\n= 0.811\\nSimilarly, we calculate the entropies of the other two subsets as divided by\\nthis attribute:\\nH(Europe) = 1\\n(since half of the European films were successes, and half were not).\\nH(Rest of world) = 0\\n(since none of these films were successes).'), Document(metadata={}, page_content='(since none of these films were successes).\\nThe total information gain is now defined as the original entropy of the set\\nminus the weighted sum of these entropies, where the weight applied to\\neach entropy value is the proportion of the training data that fell into that\\ncategory. For example, four-tenths of the training data were from the\\nUnited States, so the weight applied to H(USA) is 4/10 = 0.4.\\nThe information gain is defined as:'), Document(metadata={}, page_content='The information gain is defined as:\\nGain = 1 /H11002(0.4 /H110030.811) /H11002(0.4 /H110031) /H11002(0.2 /H110030)\\n= 1 /H110020.3244 /H110020.4 /H110020\\n= 0.2756\\nHence, at this stage, the information gain for the “country of origin” attrib-\\nute is 0.2756.\\nFor the “Big star” attribute\\nH(yes) = 0.9852\\nH(no) = 1'), Document(metadata={}, page_content='10.9 Decision-Tree Induction 281\\nso, the information gain for this attribute is\\nGain = 1 /H11002(0.7 /H110030.9852) /H11002(0.3 /H110031)\\n= 1 /H110020.68964 /H110020.3\\n= 0.01\\nFor the “Genre” attribute\\nH(science fiction) = 0.918296\\nH(comedy) = 0.918296\\nH(romance) = 0\\n(note that we treat 0 /H11003log\\n20 as 0)\\nhence, the information gain for this attribute is\\nGain = 1 /H11002(0.3 /H110030.918296) /H11002(0.6 /H110030.918296) /H11002(0.1 /H110030)\\n= 1 /H110020.2754888 /H110020.5509776 /H110020'), Document(metadata={}, page_content='= 1 /H110020.2754888 /H110020.5509776 /H110020\\n= 0.17\\nHence, at this stage, the category “Country of origin” provides the greatest\\nentropy gain and so is placed at the top of the decision tree. This method is\\nthen applied recursively to the sub-branches of the tree, examining the\\nentropy gain achieved by subdividing the training data further.\\n10.9.3 Inductive Bias of ID3\\nID3’s inductive bias is that it tends to produce the shortest decision tree'), Document(metadata={}, page_content='that will correctly classify all of the training data. This fits very well with\\nOccam’s razor, which was briefly introduced in Section 10.8. It is not the\\ncase that Occam’s razor can be applied in all situations to provide the opti-\\nmal solution: it is, however, the case that ID3 tends to produce adequate\\nresults. Additionally, a smaller decision tree is clearly easier for humans to\\nunderstand, which in some circumstances can be very useful, for example if'), Document(metadata={}, page_content='the need arises to debug the learner and find out why it makes a mistake on\\na particular piece of unseen data.'), Document(metadata={}, page_content='282 CHAPTER 10 Introduction to Machine Learning\\nFigure 10.2\\nIllustration of the problem\\nof overfitting\\n10.10 The Problem of Overfitting\\nIn some situations, decision trees (and other learning methods) can run\\ninto the problem of overfitting. Overfitting usually occurs when there is\\nnoise in the training data, or when the training data do not adequately rep-\\nresent the entire space of possible data. In such situations, it can be possible'), Document(metadata={}, page_content='for one decision tree to correctly classify all the training data, but to per-\\nform less well at classifying unseen data than some other decision tree that\\nperforms poorly at classifying the training data. In other words, if the train-\\ning data do not adequately and accurately represent the entire data set, the\\ndecision tree that is learned from it may not match unseen data.\\nThis problem does not just apply to decision trees, but also to other learn-'), Document(metadata={}, page_content='ing methods. It can best be understood by examining the illustration in\\nFigure 10.2.\\nIn the first diagram in Figure 10.2, black dots are positive training data, and\\nwhite dots are negative training data. The two lines represent two hypothe-\\nses that have been developed to distinguish the training data. The thin line\\nis a relatively simple hypothesis, which incorrectly classifies some of the\\ntraining data—it should have all positive examples below it and all negative'), Document(metadata={}, page_content='examples above it. The thicker line correctly classifies all the training data,\\nusing a more complex hypothesis, which is somewhat warped by noise in\\nthe data. In the next diagram, the thin line is shown to map reasonably\\neffectively the full set of data. It does make some errors, but it reasonably'), Document(metadata={}, page_content='10.11 The Nearest Neighbor Algorithm 283\\nclosely represents the trend in the data. The third diagram, however, shows\\nthat the more complex solution does not at all represent the full set of data.\\nThis hypothesis has been overfitted to the training data, allowing itself to\\nbe warped by noise in the training data.\\nOverfitting is, perhaps, a good illustration of why Occam’s razor can some-\\ntimes be a useful inductive bias: selecting a complex solution to accommo-'), Document(metadata={}, page_content='date all of the training data can be a bad idea when the training data\\ncontain errors.\\n10.11 The Nearest Neighbor Algorithm\\nThe nearest neighbor algorithm is an example of instance-based learning.\\nInstance-based learning methods do not attempt to generalize from train-\\ning data to produce a hypothesis to match all input data, instead, they store\\nthe training data and use these data to determine a classification for each\\nnew piece of data as it is encountered.'), Document(metadata={}, page_content='new piece of data as it is encountered.\\nThe nearest neighbor algorithm operates in situations where each instance\\ncan be defined by an n-dimensional vector, where n is the number of attrib-\\nutes used to describe each instance, and where the classifications are dis-\\ncrete numerical values. The training data are stored, and when a new\\ninstance is encountered it is compared with the training data to find its\\nnearest neighbors. This is done by computing the Euclidean distance'), Document(metadata={}, page_content='between the instances in n-dimensional space. In two-dimensional space,\\nfor example, the distance between <x\\n1, y1> and <x2, y2> is\\nTypically, the nearest neighbor algorithm obtains the classifications of the\\nnearest k neighbors to the instance that is to be classified and assigns it the\\nclassification that is most commonly returned by those neighbors.\\nAn alternative approach is to weight the contribution of each of the neigh-'), Document(metadata={}, page_content='bors according to how far it is from the instance that is to be classified. In\\nthis way, it is possible to allow every instance of training data to contribute\\nto the classification of a new instance. When used in this way, the algorithm\\nis known as Shepard’s method (see Shepard 1968).\\nUnlike decision-tree learning, the nearest neighbor algorithm performs\\nvery well with noisy input data. Its inductive bias is to assume that\\nxx yy12\\n2\\n12\\n2−( ) +−( )( )'), Document(metadata={}, page_content='284 CHAPTER 10 Introduction to Machine Learning\\ninstances that are close to each other in terms of Euclidean distance will\\nhave similar classifications. In some cases, this can be an erroneous\\nassumption; for example, in a situation where 10 attributes are used to\\ndefine each instance, but only 3 of those attributes play any part in deter-\\nmining the classification of the instance. In this situation, instances can be\\nvery far apart from each other in 10-dimensional space and yet have the'), Document(metadata={}, page_content='same classification. This problem can be avoided to some extent by neglect-\\ning to include unimportant attributes from the calculations.\\n10.12 Learning Neural Networks\\nAn artificial neural network is a network of simple processing nodes,\\nwhich is roughly modeled on the human brain. The human brain is a mas-\\nsively parallel computation device, which achieves its power through the\\nenormous connectivity between its neurons. Each neuron is a very simple'), Document(metadata={}, page_content='device that can either fire or not fire, but by combining billions of these\\nneurons together, the brain is able to achieve levels of complexity as yet\\nunattainable by machines.\\nThe word artificial is often used to describe neural networks to differentiate\\nthem from the biological neural networks that make up the human brain,\\nbut in this book we shall simply refer to them as neural networks because it\\nshould be clear from the context which type of network we are referring to.'), Document(metadata={}, page_content='Neural networks consist of a number of nodes, each of which can be\\nthought of as representing a neuron. Typically, these neurons are arranged\\ninto layers, and the neurons from one layer are connected to the neurons in\\nthe two layers on either side of it.\\nTypically, the network is arranged such that one layer is the input layer,\\nwhich receives inputs that are to be classified. These inputs cause some of the\\nneurons in the input layer to fire, and these neurons in turn pass signals to'), Document(metadata={}, page_content='the neurons to which they are connected, some of which also fire, and so on.\\nIn this way, a complex pattern of firings is arranged throughout the network,\\nwith the final result being that some neurons in the final output layer fire.\\nThe connections between neurons are weighted, and by modifying these\\nweights, the neural network can be arranged to perform extremely complex\\nclassification tasks such as handwriting analysis and face recognition.'), Document(metadata={}, page_content='As we see in Chapter 11 where we discuss them in more detail, neural net-\\nworks have a number of advantages over other learning methods. Many of'), Document(metadata={}, page_content='10.14 Unsupervised Learning 285\\nthese advantages derive from features of the human brain. For example,\\nneural networks are extremely robust, both to errors in any training data\\nand to damage that may be caused to the network itself.\\n10.13 Supervised Learning\\nSupervised learning networks learn by being presented with preclassified\\ntraining data. The techniques we have discussed so far in this chapter use\\nforms of supervised learning. Neural networks that use supervised learning'), Document(metadata={}, page_content='learn by modifying the weights of the connections within their networks to\\nmore accurately classify the training data. In this way, neural networks are\\nable to generalize extremely accurately in many situations from a set of\\ntraining data to the full set of possible inputs.\\nOne of the most commonly used methods for supervised learning is back-\\npropagation, which will be discussed in Chapter 11.\\n10.14 Unsupervised Learning\\nUnsupervised learning methods learn without any human intervention. A'), Document(metadata={}, page_content='good example of an unsupervised learning network is a Kohonen map.A\\nKohonen map is a neural network that is able to learn to classify a set of\\ninput data without being told what the classifications are and without\\nbeing given any training data. This method is particularly useful in situa-\\ntions where data need to be classified, or clustered, into a set of classifica-\\ntions but where the classifications are not known in advance.'), Document(metadata={}, page_content='For example, given a set of documents retrieved from the Internet (perhaps\\nby an intelligent information agent), a Kohonen map could cluster similar\\ndocuments together and automatically provide an indication of the distinct\\nsubjects that are covered by the documents.\\nAnother method for unsupervised learning in neural networks was pro-\\nposed by Donald Hebb in 1949 and is known as Hebbian learning. Hebbian\\nlearning is based on the idea that if two neurons in a neural network are'), Document(metadata={}, page_content='connected together, and they fire at the same time when a particular input\\nis given to the network, then the connection between those two neurons\\nshould be strengthened. It seems likely that something not dissimilar from\\nHebbian learning takes place in the human brain when learning occurs\\n(Edelman 1987).'), Document(metadata={}, page_content='286 CHAPTER 10 Introduction to Machine Learning\\n10.15 Reinforcement Learning\\nClassifier systems, which are discussed in Chapter 13, use a form of rein-\\nforcement learning. A system that uses reinforcement learning is given a\\npositive reinforcement when it performs correctly and a negative reinforce-\\nment when it performs incorrectly. For example, a robotic agent might\\nlearn by reinforcement learning how to pick up an object. When it success-'), Document(metadata={}, page_content='fully picks up the object, it will receive a positive reinforcement.\\nThe information that is provided to the learning system when it performs\\nits task correctly does not tell it why or how it performed it correctly, simply\\nthat it did.\\nSome neural networks learn by reinforcement. The main difficulty with\\nsuch methods is the problem of credit assignment. The classifier systems\\n(which are discussed in Chapter 13) use a bucket brigade algorithm for'), Document(metadata={}, page_content='deciding how to assign credit (or blame) to the individual components of\\nthe system. Similar methods are used with neural networks to determine to\\nwhich neurons to give credit when the network performs correctly and\\nwhich to blame when it does not.\\n10.16 Chapter Summary\\n■ Many learning methods use some form of training to learn to gen-\\neralize from a set of preclassified training data to be able to cor-\\nrectly classify unseen data.'), Document(metadata={}, page_content='rectly classify unseen data.\\n■ Rote learning involves simply memorizing the classifications of\\ntraining data. A rote learning system is not able to generalize and\\nso is only able to classify data it has seen before.\\n■ A general-to-specific ordering of hypotheses can be used to learn\\nto generalize from a set of training data to a hypothesis that\\nmatches all input data. This is known as concept learning.\\n■ A version space, which consists of all possible hypotheses that'), Document(metadata={}, page_content='match a given set of training data, can be used to generalize from\\nthose training data to learn to classify unseen data.\\n■ Candidate elimination is a method that uses the general-to-specific\\nordering to produce a set of hypotheses that represent the entire\\nversion space for a problem.'), Document(metadata={}, page_content='10.17 Review Questions 287\\n■ The inductive bias of a learning method is the assumptions it\\nmakes about the possible hypotheses that can be used. A learning\\nsystem with no inductive bias is not capable of generalizing beyond\\nthe training data it is given.\\n■ Decision-tree induction can be used to learn a decision tree that\\nwill correctly classify a set of input data. The inductive bias of deci-\\nsion-tree induction is to prefer shorter trees.'), Document(metadata={}, page_content='sion-tree induction is to prefer shorter trees.\\n■ The problem of overfitting occurs when there is noise in the train-\\ning data that causes a learning method to develop a hypothesis that\\ncorrectly matches the training data but does not perform well on\\nother input data.\\n■ The nearest neighbor algorithm simply memorizes the classifica-\\ntions of the training data, and when presented with a new piece of\\ndata gives the majority answer given by the closest neighbors to'), Document(metadata={}, page_content='this piece of data in n-dimensional space.\\n■ Neural networks are based on biological networks of neurons con-\\ntained within the human brain.\\n■ Supervised learning methods learn from manually classified\\ntraining data.\\n■ Unsupervised learning methods such as Kohonen maps learn\\nwithout any manual intervention.\\n■ A system that uses reinforcement learning is given a positive rein-\\nforcement when it performs correctly. Credit and blame assign-\\nment are important features of such methods.'), Document(metadata={}, page_content='ment are important features of such methods.\\n10.17 Review Questions\\n10.1 Explain the idea behind learning by generalization.\\n10.2 What is meant by inductive bias? Is it a good thing? What is the\\ninductive bias of the ID3 algorithm?\\n10.3 Explain how candidate elimination uses version spaces to learn.\\n10.4 Explain how a system can learn by building decision trees, using\\nthe ID3 algorithm.\\n10.5 How does the nearest neighbor algorithm work?'), Document(metadata={}, page_content='10.6 Explain the problem of overfitting and how it can be avoided.'), Document(metadata={}, page_content='288 CHAPTER 10 Introduction to Machine Learning\\n10.7 Explain the differences and similarities between the following\\nthree types of learning methods:\\nsupervised\\nunsupervised\\nreinforcement\\n10.18 Exercises\\n10.1 Use the ID3 algorithm to build the full decision tree for the data set\\ngiven in Section 10.9.2.\\n10.2 Implement the nearest neighbor algorithm in the programming\\nlanguage of your choice. The algorithm should work with vectors'), Document(metadata={}, page_content='of up to 10 integer values and allow up to 10 integer classifications.\\nBy mapping each value to a number, use your program to learn\\nfrom the training data given in Section 10.9.2. Have your program\\nnow classify the following films:\\nFilm Country of origin Big star Genre\\nFilm 11 United States no Science fiction\\nFilm 12 United States yes Romance\\nFilm 13 United States no Romance\\nFilm 14 Europe no Science fiction\\nFilm 15 Rest of world no Romance\\nComment on the results.\\n10.19 Further Reading'), Document(metadata={}, page_content='Comment on the results.\\n10.19 Further Reading\\nMitchell (1997) provides an excellent coverage of many aspects of machine\\nlearning. An excellent background from a biological perspective is pro-\\nvided by Pfeifer and Scheier (1999). Winston (1992) developed many of the\\nconcepts that are used today in machine learning.\\nFurther references on neural networks are given in the Further Reading\\nsection of Chapter 11 of this book.'), Document(metadata={}, page_content='10.19 Further Reading 289\\nLearning from Data: Concepts, Theory, and Methods by Vladimir Cher-\\nkassky and Filip Mulier (1998 – Wiley Interscience)\\nNeural Darwinism: The Theory of Neuronal Group Selection by Gerald M.\\nEdelman (1990 – Oxford University Press)\\nLearning and Soft Computing: Support Vector Machines, Neural Networks,\\nand Fuzzy Logic Models (Complex Adaptive Systems) by Vojislav Kecman\\n(2001 – MIT Press)\\nMachine Learning by T om M. Mitchell (1997 – McGraw Hill)'), Document(metadata={}, page_content='Machine Learning: A Theoretical Approach by Balas K. Natarajan (1991 –\\nMorgan Kaufmann)\\nUnderstanding Intelligence by Rolf Pfeifer and Christian Scheier (1999 –\\nMIT Press)\\nInduction of Decision Treesby J. R. Quinlan (1986 – from Machine Learning,\\nVol. 1, pp. 81–106)\\nA Two Dimensional Interpolation Function for Irregularly Spaced Data by\\nD. Shepard (1968 - Proceedings of the 23rd National Conference of the ACM,\\npp. 517–523)'), Document(metadata={}, page_content='pp. 517–523)\\nReinforcement Learning: An Introduction (Adaptive Computation and Machine\\nLearning) by Richard S. Sutton and Andrew G. Barto (1998 – MIT Press)\\nStatistical Learning Theoryby Vladimir N.Vapnik (1998 – Wiley Interscience)\\nAn Introduction to Computational Learning Theory by Michael J. Kearns\\nand Umesh V . Vazirani (1994 – MIT Press)\\nLearning and Generalization: With Applications to Neural Networks by\\nMathukumalli Vidyasagar (2002 – Springer V erlag)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='11CHAPTER\\nNeural Networks\\nMan, unlike any other thing organic or inorganic in the universe, grows\\nbeyond his work, walks up the stairs of his concepts, emerges ahead of his\\naccomplishments.\\n—John Steinbeck, The Grapes of Wrath\\nBehind a pot of ferns the wagging clock\\nTells me the hour’s word, the neural meaning\\nFlies on the shafted disc, declaims the morning\\nAnd tells the windy weather in the cock.\\n—Dylan Thomas, Especially When the October Wind'), Document(metadata={}, page_content='—Dylan Thomas, Especially When the October Wind\\nHis high pitched voice already stood out above the general murmur of well-\\nbehaved junior executives grooming themselves for promotion within the Bell\\ncorporation. Then he was suddenly heard to say: ‘No, I’m not interested in\\ndeveloping a powerful brain. All I’m after is just a mediocre brain, something\\nlike the President of the American Telephone and Telegraph Company. ’\\n—Alan Turing, quoted in Alan Turing the \\nEnigma of Intelligence by A. Hodge'), Document(metadata={}, page_content='Enigma of Intelligence by A. Hodge\\n11.1 Introduction\\nThis chapter introduces the relationship between biological neurons, which\\nmake up human brains, and artificial neurons, which are used in artificial\\nneural networks. McCulloch and Pitts neurons are explained, and the capa-\\nbilities and limitations of perceptrons are examined. Multilayer neural'), Document(metadata={}, page_content='292 CHAPTER 11 Neural Networks\\nOUTPUT\\nAXONSOMA\\nSYNAPSE\\nDENDRITES\\nFigure 11.1\\nA neuron in the human\\nbrain\\nnetworks are explored, and the backpropagation algorithm for supervised\\nlearning in multilayer networks is explained. Recurrent networks, such as\\nHopfield networks and other bidirectional associative memories, are also\\nexplained. Unsupervised learning is explained through the use of Kohonen\\nmaps and Hebb’s law.\\nAlthough the neural networks presented in this chapter are very simplistic,'), Document(metadata={}, page_content='real-world networks can be extremely complex, consisting of hundreds or\\neven thousands of neurons. Networks of this size can often appear like a\\n“black box, ” in the sense that it is not clear why they behave in the way they\\ndo. In fact, the behavior of complex neural networks is often emergent.\\n11.2 Neurons\\n11.2.1 Biological Neurons\\nThe human brain contains over ten billion neurons, each of which is con-\\nnected, on average, to several thousand other neurons. These connections'), Document(metadata={}, page_content='are known as synapses, and the human brain contains about 60 trillion\\nsuch connections.\\nNeurons are in fact very simple processing elements. Each neuron contains\\na soma, which is the body of the neuron, an axon, and a number of den-\\ndrites. A simplified diagram of a biological neuron is shown in Figure 11.1.\\nThe neuron receives inputs from other neurons along its dendrites, and\\nwhen this input signal exceeds a certain threshold, the neuron “fires”—in'), Document(metadata={}, page_content='11.2 Neurons 293\\nfact, a chemical reaction occurs, which causes an electrical pulse, known as\\nan action potential, to be sent down the axon (the output of the neuron),\\ntoward synapses that connect the neuron to the dendrites of other neurons.\\nAlthough each neuron individually is extremely simple, this enormously\\ncomplex network of neurons is able to process information at a great rate\\nand of extraordinary complexity. The human brain far exceeds in terms of'), Document(metadata={}, page_content='complexity any device created by man, or indeed, any naturally occurring\\nobject or structure in the universe, as far as we are aware today.\\nThe human brain has a property known asplasticity, which means that neu-\\nrons can change the nature and number of their connections to other neu-\\nrons in response to events that occur. In this way, the brain is able to learn. As\\nis explained in Chapter 10, the brain uses a form of credit assignment to'), Document(metadata={}, page_content='strengthen the connections between neurons that lead to correct solutions to\\nproblems and weakens connections that lead to incorrect solutions. The\\nstrength of a connection, or synapse, determines how much influence it will\\nhave on the neurons to which it is connected, and so if a connection is weak-\\nened, it will play less of a role in subsequent computations.\\n11.2.2 Artificial Neurons\\nArtificial neural networks are modeled on the human brain and consist of'), Document(metadata={}, page_content='a number of artificial neurons. Neurons in artificial neural networks tend\\nto have fewer connections than biological neurons, and neural networks\\nare all (currently) significantly smaller in terms of number of neurons than\\nthe human brain.\\nThe neurons that we examine in this chapter were invented by McCulloch\\nand Pitts (1943) and so are often referred to as McCulloch and Pitts neurons.\\nEach neuron (or node) in a neural network receives a number of inputs. A'), Document(metadata={}, page_content='function called the activation function is applied to these input values,\\nwhich results in the activation level of the neuron, which is the output\\nvalue of the neuron. There are a number of possible functions that can be\\nused in neurons. Some of the most commonly used activation functions\\nare illustrated in Figure 11.2.\\nIn Figure 11.2, thex-axis of each graph represents the input value to the neu-\\nron, and they-axis represents the output, or the activation level, of the neuron.'), Document(metadata={}, page_content='294 CHAPTER 11 Neural Networks\\nYY\\n+1 +1\\nt XX\\nY\\nX\\n–1\\n(a) Step function (b) Sigmoid function (c) Linear function\\nFigure 11.2\\nThree activation functions\\nOne of the most commonly used functions is the step function, or linear\\nthreshold function. In using this function, the inputs to the neuron are\\nsummed (having each been multiplied by a weight), and this sum is com-\\npared with a threshold, t. If the sum is greater than the threshold, then the'), Document(metadata={}, page_content='neuron fires and has an activation level of +1. Otherwise, it is inactive and has\\nan activation level of zero. (In some networks, when the sum does not exceed\\nthe threshold, the activation level is considered to be /H110021 instead of 0).\\nHence, the behavior of the neuron can be expressed as follows:\\nX is the weighted sum of the n inputs to the neuron, x\\n1 to xn, where each\\ninput, xn is multiplied by its corresponding weight wn. For example, let us'), Document(metadata={}, page_content='consider a simple neuron that has just two inputs. Each of these inputs has\\na weight associated with it, as follows:\\nw\\n1 = 0.8\\nw2 = 0.4\\nThe inputs to the neuron are x1 and x2:\\nx1 = 0.7\\nx2 = 0.9\\nSo, the summed weight of these inputs is\\n(0.8 /H110030.7) + (0.4 /H110030.9) = 0.92\\nThe activation level Y, is defined for this neuron as\\nY\\nfor X t\\nfor X t=\\n+>\\n≤\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\n0\\nXw x ii\\ni\\nn\\n=\\n=\\n∑\\n1'), Document(metadata={}, page_content='11.3 Perceptrons 295\\nHence, if t is less than or equal to 0.92, then this neuron will fire with this\\nparticular set of inputs. Otherwise, it will have an activation level of zero.\\nA neuron that uses the linear activation function simply uses the weighted\\nsum of its inputs as its activation level. The sigmoid function converts inputs\\nfrom a range of/H11002/H11009to +/H11009into an activation level in the range of 0 to +1.\\nA neural network consists of a set of neurons that are connected together.'), Document(metadata={}, page_content='Later in this chapter we explore the ways in which neurons are usually con-\\nnected together. The connections between neurons have weights associated\\nwith them, and each neuron passes its output on to the inputs of the neu-\\nrons to which it is connected. This output depends on the application of\\nthe activation function to the inputs it receives. In this way, an input signal\\nto the network is processed by the entire network and an output (or multi-'), Document(metadata={}, page_content='ple outputs) produced. There is no central processing or control mecha-\\nnism—the entire network is involved in every piece of computation that\\ntakes place.\\nThe way in which neurons behave over time is particularly interesting.\\nWhen an input is given to a neural network, the output does not appear\\nimmediately because it takes some finite period of time for signals to pass\\nfrom one neuron to another. In artificial neural networks this time is usu-'), Document(metadata={}, page_content='ally very short, but in the human brain, neural connections are surprisingly\\nslow. It is only the enormously parallel nature of the brain that enables it to\\ncalculate so quickly.\\nFor neural networks to learn, the weight associated with each connection\\n(equivalent to a synapse in the biological brain) can be changed in response\\nto particular sets of inputs and events. As is mentioned in Chapter 10, Heb-\\nbian learning involves increasing the weight of a connection between two'), Document(metadata={}, page_content='neurons if both neurons fire at the same time. We learn more about this\\nlater in the chapter.\\n11.3 Perceptrons\\nThe perceptron, which was first proposed by Rosenblatt (1958), is a simple\\nneuron that is used to classify its inputs into one of two categories.\\nThe perceptron can have any number of inputs, which are sometimes\\narranged into a grid. This grid can be used to represent an image, or a field\\nof vision, and so perceptrons can be used to carry out simple image classi-'), Document(metadata={}, page_content='fication or recognition tasks.'), Document(metadata={}, page_content='296 CHAPTER 11 Neural Networks\\nA perceptron uses a step function that returns +1 if the weighted sum of the\\ninputs, X, is greater than a threshold,t, and /H110021i f X is less than or equal tot:\\nThis function is often written as Step (X):\\nin which case, the activation function for a perceptron can be written as\\nNote that here we have allowed i to run from 0 instead of from 1. This\\nmeans that we have introduced two new variables: w0 and x0. We define x0\\nas 1, and w0 as /H11002t.'), Document(metadata={}, page_content='as 1, and w0 as /H11002t.\\nA single perceptron can be used to learn a classification task, where it\\nreceives an input and classifies it into one of two categories: 1 or 0. We can\\nconsider these to represent true and false, in which case the perceptron can\\nlearn to represent a Boolean operator, such as AND or OR.\\nThe learning process for a perceptron is as follows:\\nFirst, random weights are assigned to the inputs. Typically, these weights\\nwill be chosen between /H110020.5 and +0.5.'), Document(metadata={}, page_content='will be chosen between /H110020.5 and +0.5.\\nNext, an item of training data is presented to the perceptron, and its output\\nclassification observed. If the output is incorrect, the weights are adjusted\\nto try to more closely classify this input. In other words, if the perceptron\\nincorrectly classifies a positive piece of training data as negative, then the\\nweights need to be modified to increase the output for that set of inputs.'), Document(metadata={}, page_content='This can be done by adding a positive value to the weight of an input that\\nhad a negative input value, and vice versa.\\nThe formula for this modification, as proposed by Rosenblatt (Rosenblatt\\n1960) is as follows:\\nY Step w x ii\\ni\\nn\\n= \\uf8eb\\n\\uf8ed\\uf8ec\\n\\uf8f6\\n\\uf8f8\\uf8f7\\n=\\n∑\\n0\\nStep X\\nfor X t\\nfor X t( ) =\\n+>\\n≤\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\n0\\nXw x\\nY\\nfor X t\\nfor X t\\nii\\ni\\nn\\n=\\n=\\n+>\\n≤\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n=\\n∑\\n1\\n1\\n0'), Document(metadata={}, page_content='11.3 Perceptrons 297\\nwi ← wi + (a /H11003xi /H11003e)\\nwhere e is the error that was produced, anda is the learning rate,w h e r e0<\\na <1 ; e is defined as 0 if the output is correct, and otherwise it is positive if\\nthe output is too low and negative if the output is too high. In this way, if the\\noutput is too high, a decrease in weight is caused for an input that received\\na positive value. This rule is known as theperceptron training rule.'), Document(metadata={}, page_content='Once this modification to the weights has taken place, the next piece of\\ntraining data is used in the same way. Once all the training data have been\\napplied, the process starts again, until all the weights are correct and all\\nerrors are zero. Each iteration of this process is known as an epoch.\\nLet us examine a simple example: we will see how a perceptron can learn to\\nrepresent the logical-OR function for two inputs. We will use a threshold of\\nzero (t = 0) and a learning rate of 0.2.'), Document(metadata={}, page_content='zero (t = 0) and a learning rate of 0.2.\\nFirst, the weight associated with each of the two inputs is initialized to a\\nrandom value between /H110021 and +1:\\nw\\n1 = /H110020.2\\nw2 = 0.4\\nNow, the first epoch is run through. The training data will consist of the\\nfour combinations of 1’s and 0’s possible with two inputs.\\nHence, our first piece of training data is\\nx1 = 0\\nx2 = 0\\nand our expected output is x1 ∨ x2 = 0.\\nWe apply our formula for Y:'), Document(metadata={}, page_content='We apply our formula for Y:\\nHence, the output Y is as expected, and the error, e, is therefore 0. So the\\nweights do not change.\\nNow, for x1 = 0 and x2 = 1:\\nY Step w x\\nStep\\nii\\ni\\nn\\n= \\uf8eb\\n\\uf8ed\\uf8ec\\n\\uf8f6\\n\\uf8f8\\uf8f7\\n=× − ( ) +×( )( )\\n=\\n=\\n∑\\n0\\n00 20 0 4\\n0\\n..'), Document(metadata={}, page_content='298 CHAPTER 11 Neural Networks\\nY = Step ((0 /H11003/H110020.2) + (1 /H110030.4))\\n= Step (0.4)\\n= 1\\nAgain, this is correct, and so the weights do not need to change.\\nFor x1 = 1 and x2 = 0:\\nY = Step ((1 /H11003/H110020.2) + (0 /H110030.4))\\n= Step (/H110020.2)\\n= 0\\nThis is incorrect because 1 \\n∨ 0 = 1, so we should expect Y to be 1 for this set\\nof inputs. Hence, the weights are adjusted.\\nWe will use the perceptron training rule to assign new values to the weights:\\nwi ← wi + (a /H11003xi /H11003e)'), Document(metadata={}, page_content='wi ← wi + (a /H11003xi /H11003e)\\nOur learning rate is 0.2, and in this case, the e is 1, so we will assign the fol-\\nlowing value to w1:\\nw1 = /H110020.2 + (0.2 /H110031 /H110031)\\n= /H110020.2 + (0.2)\\n= 0\\nWe now use the same formula to assign a new value to w2:\\nw2 = 0.4 + (0.2 /H110030 /H110031)\\n= 0.4\\nBecause w2 did not contribute to this error, it is not adjusted.\\nThe final piece of training data is now used (x1 = 1 and x2= 1):\\nY = Step ((0 /H110031) + (0.4 /H110031))\\n= Step (0 + 0.4)'), Document(metadata={}, page_content='= Step (0 + 0.4)\\n= Step (0.4)\\n= 1\\nThis is correct, and so the weights are not adjusted.\\nThis is the end of the first epoch, and at this point the method runs again\\nand continues to repeat until all four pieces of training data are classified\\ncorrectly.'), Document(metadata={}, page_content='11.3 Perceptrons 299\\nTable 11.1 A sample run showing how the weights change for a simple perceptron \\nwhen it learns to represent the logical OR function\\nEpoch X1 X2 Expected Y Actual Y Error w1 w2\\n1 000 0 0 /H110020.2 0.4\\n1 011 1 0 /H110020.2 0.4\\n1 101 0 1 0 0 . 4\\n1 111 1 0 0 0 . 4\\n2 000 0 0 0 0 . 4\\n2 011 1 0 0 0 . 4\\n2 1 0 1 0 1 0.2 0.4\\n2 1 1 1 1 0 0.2 0.4\\n3 0 0 0 0 0 0.2 0.4\\n3 0 1 1 1 0 0.2 0.4\\n3 1 0 1 1 0 0.2 0.4\\n3 1 1 1 1 0 0.2 0.4'), Document(metadata={}, page_content='3 1 0 1 1 0 0.2 0.4\\n3 1 1 1 1 0 0.2 0.4\\nTable 11.1 shows the complete sequence—it takes just three epochs for the\\nperceptron to correctly learn to classify input values. Lines in which an\\nerror was made are marked in bold.\\nAfter just three epochs, the perceptron learns to correctly model the logi-\\ncal-OR function.\\nIn the same way, a perceptron can be trained to model other logical func-\\ntions such as AND, but there are some functions that cannot be modeled'), Document(metadata={}, page_content='using a perceptron, such as exclusive OR.\\nThe reason for this is that perceptrons can only learn to model functions\\nthat are linearly separable. A linearly separable function is one that can be\\ndrawn in a two-dimensional graph, and a single straight line can be drawn\\nbetween the values so that inputs that are classified into one classification\\nare on one side of the line, and inputs that are classified into the other are\\non the other side of the line. Figure 11.3 shows how such a line can be'), Document(metadata={}, page_content='drawn for the OR function, but not for the exclusive-OR function. Four'), Document(metadata={}, page_content='300 CHAPTER 11 Neural Networks\\nx2\\n1\\nx1\\n10\\nx2\\n1\\nx1\\n10\\nFigure 11.3\\nIllustrating the difference\\nbetween a linearly separa-\\nble function and one\\nwhich is not\\npoints are plotted on each graph, and a solid dot represents true, and a hol-\\nlow dot represents a value of false. It should be clear that no dashed line\\ncould be drawn in the second case, for the exclusive OR function, that\\nwould separate solid dots from hollow ones.\\nThe reason that a single perceptron can only model functions that are lin-'), Document(metadata={}, page_content='early separable can be seen by examining the following function:\\nUsing these functions, we are effectively dividing the search space using a\\nline for which X = t. Hence, in a perceptron with two inputs, the line that\\ndivides one class from the other is defined as follows:\\nw\\n1x1 + w2x2 = t\\nThe perceptron functions by identifying a set of values for wi, which gener-\\nates a suitable function. In cases where no such linear function exists, the\\nperceptron cannot succeed.'), Document(metadata={}, page_content='perceptron cannot succeed.\\n11.4 Multilayer Neural Networks\\nMost real-world problems are not linearly separable, and so although per-\\nceptrons are an interesting model for studying the way in which artificial\\nneurons can work, something more powerful is needed.\\nAs has already been indicated, neural networks consist of a number of neu-\\nrons that are connected together, usually arranged in layers.\\nXw x\\nY\\nfor X t\\nfor X t\\nii\\ni\\nn\\n=\\n=\\n+>\\n−≤\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n=\\n∑\\n1\\n1\\n1'), Document(metadata={}, page_content='11.4 Multilayer Neural Networks 301\\nFigure 11.4\\nA simple three-layer feed-\\nforward neural network\\nA single perceptron can be thought of as a single-layer perceptron. Multi-\\nlayer perceptrons are capable of modeling more complex functions, includ-\\ning ones that are not linearly separable, such as the exclusive-OR function.\\nT o see that a multilayer network is capable of modeling a function that is\\nnot linearly separable, such as exclusive-OR, note that the functions NOR'), Document(metadata={}, page_content='and NAND are both linearly separable and so can be represented by a sin-\\ngle perceptron. By combining these functions together, all other Boolean\\nfunctions can be generated. Hence, by combining single perceptrons in just\\ntwo layers, any binary function of two inputs can be generated.\\nA typical architecture for a multilayer neural network is shown in Figure 11.4.\\nThe network shown in Figure 11.4 is a feed-forward network, consisting of\\nthree layers.'), Document(metadata={}, page_content='three layers.\\nThe first layer is the input layer . Each node (or neuron) in this layer\\nreceives a single input signal. In fact, it is usually the case that the nodes in\\nthis layer are not neurons, but simply act to pass input signals on to the\\nnodes in the next layer, which is in this case a hidden layer.\\nA network can have one or more hidden layers, which contain the neurons\\nthat do the real work. Note that each input signal is passed to each of the'), Document(metadata={}, page_content='nodes in this layer and that the output of each node in this layer is passed to\\neach node in the final layer, which is the output layer. The output layer car-\\nries out the final stage of processing and sends out output signals.\\nThe network is called feed-forward because data are fed forward from the\\ninput nodes through to the output nodes. This is in contrast with recur-\\nrent networks, which we examine in Section 11.5, where some data are\\npassed back from the output nodes to the input nodes.'), Document(metadata={}, page_content='302 CHAPTER 11 Neural Networks\\nA typical feed-forward neural network consists of an input layer, one or\\ntwo hidden layers, and an output layer, and may have anywhere between 10\\nand 1000 neurons in each layer.\\n11.4.1 Backpropagation\\nMultilayer neural networks learn in much the same way as single percep-\\ntrons. The main difference is that in a multilayer network, each neuron has\\nweights associated with its inputs, and so there are a far greater number of'), Document(metadata={}, page_content='weights to be adjusted when an error is made with a piece of training data.\\nClearly, an important question is how to assign blame (or credit) to the var-\\nious weights. One method that is commonly used is backpropagation.\\nRather than using the simple Step function that single perceptrons use,\\nmultilayer backpropagation networks usually use the sigmoid function,\\nwhich is illustrated in Figure 11.2(b).\\nThe sigmoid function is defined as follows:\\nThis function is easy to differentiate because'), Document(metadata={}, page_content='This function is easy to differentiate because\\nThis is in contrast with the Step function used by perceptrons, which has\\nno simple derivative.\\nAs with the single perceptron, the backpropagation algorithm starts by ini-\\ntializing the weights in the network to random values, which are usually set\\nto small values, say in the range of /H110020.5 to 0.5. Alternatively, the weights\\ncan be normally distributed over the range from /H110022.4/n to 2.4/n, where n is'), Document(metadata={}, page_content='the number of inputs to the input layer.\\nEach iteration of the algorithm involves first feeding data through the net-\\nwork from the inputs to the outputs. The next phase, which gives the algo-\\nrithm its name, involves feeding errors back from the outputs to the inputs.\\nThese error values feed back through the network, making changes to the\\nweights of nodes along the way. The algorithm repeats in this way until the\\noutputs produced for the training data are sufficiently close to the desired'), Document(metadata={}, page_content='values—in other words, until the error values are sufficiently small.\\ndx\\ndx xxσ σσ( ) = ( ) ⋅− ( )( )1\\nσ x\\ne x( ) =\\n+ −\\n1\\n1'), Document(metadata={}, page_content='11.4 Multilayer Neural Networks 303\\nBecause the sigmoid function cannot actually reach 0 or 1, it is usual to\\naccept a value such as 0.9 as representing 1 and 0.1 as representing 0.\\nNow we shall see the formulae that are used to adjust the weights in the\\nbackpropagation algorithm. We will consider a network of three layers and\\nwill use i to represent nodes in the input layer, j to represent nodes in the\\nhidden layer, and k to represent nodes in the output layer. Hence, for exam-\\nple, w'), Document(metadata={}, page_content='ple, w\\nij refers to the weight of a connection between a node in the input\\nlayer and a node in the hidden layer.\\nThe function that is used to derive the output value for a node j in the net-\\nwork is as follows:\\nwhere n is the number of inputs to node j;w ij is the weight of the connec-\\ntion between each node i and node j; /H9258j is the threshold value being used for\\nnode j, which is set to a random value between 0 and 1;xi is the input value'), Document(metadata={}, page_content='for input node I; and yj is the output value produced by node j.\\nOnce the inputs have been fed through the network to produce outputs, an\\nerror gradient is calculated for each node k in the output layer.\\nThe error signal for k is defined as the difference between the desired value\\nand the actual value for that node:\\nek = dk /H11002yk\\ndk is the desired value for node k, and yk is the actual value, in this iteration.\\nThe error gradient for output node k is defined as the error value for this'), Document(metadata={}, page_content='node multiplied by the derivative of the activation function:\\nxk is the weighted sum of the input values to the node k.\\nBecause y is defined as a sigmoid function of x, we can use the formula that\\nwas given above for the derivative of the sigmoid function to obtain the fol-\\nlowing formula for the error gradient:\\nδk\\nk\\nk\\nk\\ny\\nx e= ∂\\n∂ ⋅\\nXx w\\nY\\ne\\nj i ij j\\ni\\nn\\nj X j\\n=⋅ −\\n=\\n+\\n=\\n−\\n∑ θ\\n1\\n1\\n1'), Document(metadata={}, page_content='304 CHAPTER 11 Neural Networks\\nSimilarly, we calculate an error gradient for each node j in the hidden layer,\\nas follows:\\nwhere n is the number of nodes in the output layer, and thus the number of\\noutputs from each node in the hidden layer.\\nNow each weight in the network,wij or wjk, is updated according to the fol-\\nlowing formula:\\nwhere xi is the input value to input node i, and /H9251is the learning rate, which\\nis a positive number below 1, and which should not be too high.'), Document(metadata={}, page_content='This method is known as gradient descent because it involves following\\nthe steepest path down the surface that represents the error function to\\nattempt to find the minimum in the error space, which represents the set of\\nweights that provides the best performance of the network.\\nIn fact, the iteration of the backpropagation algorithm is usually termi-\\nnated when the sum of the squares of the errors of the output values for all\\ntraining data in an epoch is less than some threshold, such as 0.001.'), Document(metadata={}, page_content='Note that this method assigns blame to individual nodes within the net-\\nwork by comparing the weights attached to each node with the error asso-\\nciated with that node. In the case of hidden nodes, there is no error value\\nbecause there is no specific desired output value for these nodes. In this\\ncase, the weight of each connection between a hidden layer node and an\\noutput node is multiplied by the error of that output node to attempt to'), Document(metadata={}, page_content='distribute the blame between the nodes in the hidden layer according to\\nhow much each one contributes to the error.\\nUnlike Hebbian learning, which is discussed in more detail in Section\\n11.6.3, backpropagation does not appear to occur in the human brain.\\nAdditionally, it is rather inefficient and tends to be too slow for use in solv-\\nww x\\nww y\\nij ij i j\\njk jk j k\\n←+ ⋅ ⋅\\n←+ ⋅ ⋅\\nαδ\\nαδ\\nδδjj j j k k\\nk\\nn\\nyy w=⋅ − ( )\\n=\\n∑1\\n1\\nδkk k kyy e=⋅ − ( ) ⋅1'), Document(metadata={}, page_content='11.4 Multilayer Neural Networks 305\\ning real-world problems. With some simple problems it can take hundreds\\nor even thousands of epochs to reach a satisfactorily low level of error.\\n11.4.2 Improving the Performance of Backpropagation\\nA common method used to improve the performance of backpropagation\\nis to include momentum in the formula that is used to modify the weights.\\nThe momentum takes into account the extent to which a particular weight'), Document(metadata={}, page_content='was changed on the previous iteration. We shall use t to represent the cur-\\nrent iteration, and t /H110021 to represent the previous iteration. Hence, we can\\nwrite our learning rules as follows:\\n/H9004w\\nij(t) is the amount that is added to the weight of the connection between\\nnodes i and j, wij at iteration t; /H9252is the momentum value, which is a positive\\nnumber between 0 and 1. Typically, a fairly high value such as 0.95 is used.'), Document(metadata={}, page_content='If /H9252is zero, this is the same as the backpropagation algorithm without\\nusing momentum.\\nThis rule, including the momentum value, is known as the generalized\\ndelta rule.\\nThe inclusion of the momentum value has the benefit of enabling the\\nbackpropagation method to avoid local minima and also to move more\\nquickly through areas where the error space is not changing.\\nAn alternative method of speeding up backpropagation is to use the hyper-'), Document(metadata={}, page_content='bolic tangent function, tanh, instead of the sigmoid function, which tends\\nto enable the network to converge on a solution in fewer iterations. The\\ntanh function is defined as:\\nwhere a and b are constants, such as a = 1.7 and b = 0.7.\\nA final way to improve the performance of backpropagation is to vary the\\nvalue of the learning rate, /H9251during the course of training the network. Two\\nheuristics proposed by R. A. Jacobs (1988) use the direction of change\\ntanh x a\\ne\\nabx( ) =\\n+\\n−−\\n2\\n1\\n∆∆\\n∆∆'), Document(metadata={}, page_content='tanh x a\\ne\\nabx( ) =\\n+\\n−−\\n2\\n1\\n∆∆\\n∆∆\\nwt x wt\\nwt y wt\\nij i j ij\\njk j k jk\\n( ) =⋅⋅ + − ( )\\n( ) =⋅ ⋅ + − ( )\\nαδ β\\nαδ β\\n1\\n1'), Document(metadata={}, page_content='306 CHAPTER 11 Neural Networks\\n(increase or decrease) of the sum of the square of the errors from one\\nepoch to the next to determine the change in learning rate:\\nIf for several epochs the sum of the square of the errors changes in the same\\ndirection, increase the learning rate.\\n1. If for several epochs the sum of the square of the errors changes in\\nthe same direction, increase the learning rate.\\n2. If the sum of the square of the errors alternates its change in'), Document(metadata={}, page_content='direction over several epochs, decrease the learning rate.\\nBy using these heuristics in combination with the generalized delta rule,\\nthe performance of the backpropagation algorithm can be significantly\\nimproved.\\n11.5 Recurrent Networks\\nThe neural networks we have been studying so far are feed-forward net-\\nworks. A feed-forward network is acyclic, in the sense that there are no\\ncycles in the network, because data passes from the inputs to the outputs,'), Document(metadata={}, page_content='and not vice versa,. Once a feed-forward network has been trained, its state\\nis fixed and does not alter as new input data is presented to it. In other\\nwords, it does not have memory.\\nA recurrent network can have connections that go backward from output\\nnodes to input nodes and, in fact, can have arbitrary connections between\\nany nodes. In this way, a recurrent network’s internal state can alter as sets\\nof input data are presented to it, and it can be said to have a memory.'), Document(metadata={}, page_content='This is particularly useful in solving problems where the solution depends\\nnot just on the current inputs, but on all previous inputs. For example,\\nrecurrent networks could be used to predict the stock market price of a par-\\nticular stock, based on all previous values, or it could be used to predict what\\nthe weather will be like tomorrow, based on what the weather has been.\\nClearly, due to the lack of memory, feed-forward networks are not able to\\nsolve such tasks.'), Document(metadata={}, page_content='solve such tasks.\\nWhen learning, the recurrent network feeds its inputs through the net-\\nwork, including feeding data back from outputs to inputs, and repeats this\\nprocess until the values of the outputs do not change. At this point, the net-\\nwork is said to be in a state of equilibrium or stability. For this reason,'), Document(metadata={}, page_content='11.5 Recurrent Networks 307\\nrecurrent networks are also known as attractor networks because they are\\nattracted to certain output values. The stable values of the network, which\\nare also known as fundamental memories, are the output values used as\\nthe response to the inputs the network received.\\nHence, a recurrent network can be considered to be a memory, which is\\nable to learn a set of states—those that act as attractors for it. Once such a'), Document(metadata={}, page_content='network has been trained, for any given input it will output the attractor\\nthat is closest to that input.\\nFor example, a recurrent network can be used as an error-correcting net-\\nwork. If only a few possible inputs are considered “valid, ” the network can\\ncorrect all other inputs to the closest valid input.\\nIt is not always the case that a recurrent network will reach a stable state:\\nsome networks are unstable, which means they oscillate between different\\noutput values.\\n11.5.1 Hopfield Networks'), Document(metadata={}, page_content='output values.\\n11.5.1 Hopfield Networks\\nIn the 1980s, John Hopfield invented a form of recurrent network that has\\ncome to be known as a Hopfield network.\\nThe activation function used by most Hopfield networks is the sign activa-\\ntion function, which is defined as:\\nNote that this definition does not provide a value for Sign(0). This is\\nbecause when a neuron that uses the sign activation function receives an\\ninput of 0, it stays in the same state—in other words, it continues to output'), Document(metadata={}, page_content='1 if it was outputting 1 in the previous iteration, and continues to output\\n/H110021 if it was outputting /H110021.\\nWhen considering the operation of a Hopfield network, it is usual to use\\nmatrix arithmetic. The weights of the network are represented by a matrix,\\nW, which is calculated as follows:\\nWX X N ii\\nt\\ni\\nN\\n=−\\n=\\n∑ I\\n1\\nSign X\\nfor X\\nfor X( ) =\\n+>\\n−<\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n10\\n10'), Document(metadata={}, page_content='308 CHAPTER 11 Neural Networks\\nwhere each Xi is an input vector, representing the m input values to the\\nnetwork; Xit is the matrix transposition of Xi; I is the m /H11003m identity\\nmatrix; N is the number of states ( Xi) that are to be learned. The trans-\\nposition of a matrix is simply one where the rows and columns are\\nswapped. If\\nthen the transposition of X\\n1 is\\nThe identity matrix, I, is a matrix with zeros in every row and column, but\\nwith 1s along the leading diagonal. For example,'), Document(metadata={}, page_content='with 1s along the leading diagonal. For example,\\nNow let us examine an example. We will imagine a single-layer Hopfield\\nnetwork with five nodes and three training inputs that are to be learned by\\nthe network. We will have our network learn the following three states:\\nWe thus have three states (vectors) that are to be learned, each of which\\nconsists of five input values. The inputs can be either 1 or /H110021; similarly, the'), Document(metadata={}, page_content='output values can be either 1 or /H110021, and so the output can be represented\\nas a similar vector of five values, each of which is either 1 or /H110021.\\nThe weight matrix is calculated as follows:\\nXX X12 3\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nI =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n100\\n010\\n001\\nXi\\nt =−[]11 1\\nX1\\n1\\n1\\n1\\n=−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'), Document(metadata={}, page_content='11.5 Recurrent Networks 309\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n01331\\n10113\\n31031\\n31301\\n13110\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n+\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n+\\n−−\\n−− −\\n−−\\n−−\\n−\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11 1 11\\n11 1 11\\n11 1 11\\n11 1 11\\n11 −−−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n11 1\\n30000\\n03000\\n00300\\n00030\\n00003\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n[] +\\n−\\n−\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−−−−−\\n[] +\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'), Document(metadata={}, page_content='\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−−−−−\\n[] +\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−−\\n[] −\\n1\\n1\\n1\\n1\\n1\\n11111\\n1\\n1\\n1\\n1\\n1\\n11111\\n1\\n1\\n1\\n1\\n1\\n1 111 1 3\\n10000\\n01000\\n00100 0\\n00010\\n00001\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nWX X\\nXX X X XX\\nii\\nt\\ni\\ni\\nt\\ni\\nt\\ni\\nt\\n=−\\n=++−\\n=\\n∑ 3\\n3\\n1\\n3\\n123\\nI\\nI'), Document(metadata={}, page_content='310 CHAPTER 11 Neural Networks\\nNote that the weight matrix has zeros along its leading diagonal. This\\nmeans that each node in the network is not connected to itself (i.e., wii = 0\\nfor all i). A further property of a Hopfield network is that the two connec-\\ntions between a pair of nodes have the same weight. In other words, wij =\\nwji for any nodes i and j.\\nThe three training states used to produce the weight matrix will be stable'), Document(metadata={}, page_content='states for the network. We can test this by determining the output vectors\\nfor each of them.\\nThe output vector is defined by\\nwhere \\nθ is the threshold matrix, which contains the thresholds for each of\\nthe five inputs. We will assume that the thresholds are all set at zero.\\nHence, the first input state is a stable state for the network. Similarly, we can\\nshow that Y2 = X2 and that Y3 = X3.\\nNow let us see how the network treats an input that is different from the\\ntraining data. We will use\\nY Sign'), Document(metadata={}, page_content='training data. We will use\\nY Sign\\nSign\\n1\\n01331\\n10113\\n31031\\n31301\\n13110\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n8\\n6\\n8\\n8\\n6\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n1\\n1\\n1\\n1\\n1\\n1X\\nY Sign Xii =− ( )W θ'), Document(metadata={}, page_content='11.5 Recurrent Networks 311\\nNote that this vector differs from X1 in just one value, so we would expect\\nthe network to converge on X1 when presented with this input.\\nNow we will try an input that is very different from the training data:\\nX5\\n1\\n1\\n1\\n1\\n1\\n=\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nY Sign\\nSign\\n4\\n01331\\n10113\\n31031\\n31301\\n13110\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n2\\n4\\n8\\n2\\n4\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0'), Document(metadata={}, page_content='\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n1\\n1\\n1\\n1\\n1\\n1X\\nX4\\n1\\n1\\n1\\n1\\n1\\n= −\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'), Document(metadata={}, page_content='312 CHAPTER 11 Neural Networks\\nLet us apply the network to this input data:\\nBecause this is different from X5 and is not one of the attractors, we need to\\napply the rule again:\\nThe use of the Hopfield network involves three stages. In the first stage, the\\nnetwork is trained to learn the set of attractor states. This can be thought of\\nas a storage or memorization stage. This is done by setting the weights of\\nthe network according to the values given by the weights matrix, W, which'), Document(metadata={}, page_content='is calculated as described above.\\nThe second phase involves testing the network, by providing the attractor\\nstates as inputs, and checking that the outputs are identical. The final stage\\nY Sign\\nSign\\n5\\n01331\\n10113\\n31031\\n31301\\n13110\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n2\\n4\\n2\\n8\\n4\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n1\\n1\\n1\\n1\\n1\\n1X\\nY Sign\\nSign\\n5\\n01331'), Document(metadata={}, page_content='\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n1\\n1\\n1\\n1\\n1\\n1X\\nY Sign\\nSign\\n5\\n01331\\n10113\\n31031\\n31301\\n13110\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n2\\n2\\n2\\n4\\n2\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n1\\n1\\n1\\n1\\n1'), Document(metadata={}, page_content='11.5 Recurrent Networks 313\\ninvolves using the network, in which the network, in acting as a memory, is\\nrequired to retrieve data from its memory.\\nIn each case, the network will retrieve the attractor closest to the input that\\nit is given. In this case, the nearest attractor is X1, which differs in just two\\ninputs. The measure of distance that is usually used for such vectors is the\\nHamming distance. The Hamming distance measures the number of ele-'), Document(metadata={}, page_content='ments of the vectors that differ. The Hamming distance between two vec-\\ntors, X and Y, is written \\n||X, Y ||.\\nFor the vectors we have used\\nHence, the Hopfield network is a memory that usually maps an input vec-\\ntor to the memorized vector whose Hamming distance from the input vec-\\ntor is least.\\nIn fact, although a Hopfield network always converges on a stable state, it\\ndoes not always converge on the state closest to the original input. No'), Document(metadata={}, page_content='method has yet been found for ensuring that a Hopfield network will\\nalways converge on the closest state.\\nA Hopfield network is considered to be an autoassociative memory, which\\nmeans that it is able to remember an item itself, or a similar item that might\\nhave been modified slightly, but it cannot use one piece of data to remem-\\nber another. The human brain is fully associative, or heteroassociative,\\nwhich means one item is able to cause the brain to recall an entirely differ-'), Document(metadata={}, page_content='ent item. A piece of music or a smell will often cause us to remember an old\\nmemory: this is using the associative nature of memory. A Hopfield net-\\nwork is not capable of making such associations.\\n11.5.2 Bidirectional Associative Memories (BAMs)\\nA Bidirectional Associative Memory , or BAM, is a neural network first\\ndiscussed by Bart Kosko (1988) that is similar in structure to the Hopfield\\nnetwork and which can be used to associate items from one set to items in\\nanother set.\\nXX\\nXX\\n14\\n15\\n1\\n2\\n,'), Document(metadata={}, page_content='another set.\\nXX\\nXX\\n14\\n15\\n1\\n2\\n,\\n,\\n=\\n='), Document(metadata={}, page_content='314 CHAPTER 11 Neural Networks\\nThe network consists of two layers of nodes, where each node in one layer\\nis connected to every other node in the other layer—this means that the\\nlayers are fully connected . This is in contrast to the Hopfield network,\\nwhich consists of just a single layer of neurons: in the Hopfield network,\\neach neuron is connected to every other neuron within the same layer,\\nwhereas in the BAM, each neuron is connected just to neurons in the other'), Document(metadata={}, page_content='layer, not to neurons in its own layer.\\nAs with Hopfield networks, the weight matrix is calculated from the items\\nthat are to be learned. In this case, two sets of data are to be learned, so that\\nwhen an item from set X is presented to the network, it will recall a corre-\\nsponding item from set Y.\\nThe weights matrix W is defined as:\\nThe BAM uses a neuron with a sign activation function, which is also used\\nby a Hopfield network.\\nWhen the network is given a vectorX'), Document(metadata={}, page_content='When the network is given a vectorX\\ni as an input, it will recall the correspon-\\nding vectorYi, and similarly, when presented withYi, the network will recallXi.\\nLet us examine a simple example:\\nWe are using our network to learn two sets of vectors. The network has two\\nlayers: the input layer has two neurons, and the output layer has three neurons.\\nThe weights matrix is calculated as follows:\\nW = \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa[] + −\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa −−−[]\\n= \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n1\\n1 111 1\\n1 111\\n222\\n222\\nX\\nY\\n12\\n12\\n1\\n1\\n1\\n1\\n1\\n1\\n1'), Document(metadata={}, page_content='\\uf8fa\\n1\\n1 111 1\\n1 111\\n222\\n222\\nX\\nY\\n12\\n12\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n= \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa =\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nX\\nY\\nWX Y= ∑ ii\\nt\\ni\\nn'), Document(metadata={}, page_content='11.5 Recurrent Networks 315\\nNow we will test the network. When presented with input X1, the network\\nwill output the following vector:\\nIf the network is functioning correctly, this should be equal to Y1:\\nSo the network has correctly recalled Y1 when presented with X1.\\nSimilarly, the association should work in reverse: when presented with Y1,\\nthe network should recall X1:\\nNote that in this case, we are using the output layer as if it were an input'), Document(metadata={}, page_content='layer, and vice versa—hence, the network is bidirectional.\\nLike a Hopfield network, the BAM is guaranteed to produce a stable output for\\nany given inputs and for any training data. In fact, a Hopfield network is a type\\nof BAM, with the additional requirement that the weight matrix be square and\\nSign\\nSign\\nSign\\nWY\\nX\\n1\\n1\\n222\\n222\\n1\\n1\\n1\\n6\\n6\\n1\\n1\\n( )\\n= \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n= \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa = \\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa =\\nSign Sign\\nSign\\ntWX 1\\n1\\n22\\n22\\n22\\n1\\n1\\n4\\n4\\n4\\n1\\n1\\n1\\n( ) =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa'), Document(metadata={}, page_content='22\\n22\\n22\\n1\\n1\\n4\\n4\\n4\\n1\\n1\\n1\\n( ) =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\n\\uf8ec\\uf8ec\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\n\\uf8f7\\uf8f7\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n= Y\\nSign tWX 1( )'), Document(metadata={}, page_content='316 CHAPTER 11 Neural Networks\\nthat each neuron not have a connection to itself (or to its corresponding neu-\\nron in the other layer). BAMs are extremely useful neural networks, although\\ntheir capabilities (and limitations) are not yet fully understood.\\n11.6 Unsupervised Learning Networks\\nThe networks we have studied so far in this chapter use supervised learn-\\ning: they are presented with preclassified training data before being asked'), Document(metadata={}, page_content='to classify unseen data. We will now look at a number of methods that are\\nused to enable neural networks to learn in an unsupervised manner.\\n11.6.1 Kohonen Maps\\nA Kohonen map,o r self-organizing feature map, is a form of neural net-\\nwork invented by Kohonen in the 1980s. The Kohonen map uses the win-\\nner-take-all algorithm , which leads to a form of unsupervised learning\\nknown as competitive learning . The winner-take-all algorithm uses the'), Document(metadata={}, page_content='principle that only one neuron provides the output of the network in\\nresponse to a given input: the neuron that has the highest activation level.\\nDuring learning, only connections to this neuron have their weights altered.\\nThe purpose of a Kohonen map is toclusterinput data into a number of clus-\\nters. For example, a Kohonen map could be used to cluster news stories into\\nsubject categories. A Kohonen map is not told what the categories are: it deter-'), Document(metadata={}, page_content='mines the most useful segmentation itself. Hence, a Kohonen map is particu-\\nlarly useful for clustering data where the clusters are not known in advance.\\nA Kohonen map has two layers: an input layer and a cluster layer, which\\nserves as the output layer. Each input node is connected to every node in\\nthe cluster layer, and typically the nodes in the cluster layer are arranged in\\na grid formation, although this is not essential.'), Document(metadata={}, page_content='a grid formation, although this is not essential.\\nThe method used to train a Kohonen map is as follows: Initially, all weights\\nare set to small random values. The learning rate, /H9251, is also set, usually to a\\nsmall positive value.\\nAn input vector is presented to the input layer of the map. This layer feeds\\nthe input data to the cluster layer. The neuron in the cluster layer that most\\nclosely matches the input data is declared the winner. This neuron provides'), Document(metadata={}, page_content='the output classification of the map and also has its weights updated.'), Document(metadata={}, page_content='11.6 Unsupervised Learning Networks 317\\nT o determine which neuron wins, its weights are treated as a vector, and\\nthis vector is compared with the input vector. The neuron whose weight\\nvector is closest to the input vector is the winner.\\nThe Euclidean distance d\\ni from the input vector x of a neuron with weight\\nvector wi is calculated as follows:\\nwhere n is the number of neurons in the input layer and hence the number\\nof elements in the input vector.'), Document(metadata={}, page_content='of elements in the input vector.\\nFor example, let us calculate the distance between the following two vectors:\\nSo the Euclidean distance between these two vectors is 4.\\nThe neuron for which di is the smallest is the winner, and this neuron has\\nits weight vector updated as follows:\\nThis adjustment moves the weight vector of the winning neuron closer to\\nthe input vector that caused it to win.\\nIn fact, rather than just the winning neuron having its weights updated, a'), Document(metadata={}, page_content='neighborhood of neurons around the winner are usually updated. The\\nneighborhood is usually defined as a radius within the two-dimensional\\ngrid of neurons around the winning neuron.\\nww x wij ij j ij←+ − ( )α\\nwx\\nd\\ni\\ni\\n=\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n∴= −\\n( ) ++( ) +−−( )\\n=+ +\\n=\\n=\\n1\\n2\\n1\\n3\\n1\\n2\\n13 21 12\\n439\\n16\\n4\\n22 2\\ndw xii j j\\nj\\nn\\n=− ( )\\n=\\n∑\\n2\\n1'), Document(metadata={}, page_content='318 CHAPTER 11 Neural Networks\\n123\\n456\\n789\\nFigure 11.5\\nThe cluster layer of a\\nsimple Kohonen map\\nTypically, the radius decreases over time as the training data are examined,\\nending up fixed at a small value. Similarly, the learning rate is often reduced\\nduring the training phase.\\nThis training phase usually terminates when the modification of weights\\nbecomes very small for all the cluster neurons. At this point, the network has'), Document(metadata={}, page_content='extracted from the training data a set of clusters, where similar items are con-\\ntained within the same cluster, and similar clusters are near to each other.\\n11.6.2 Kohonen Map Example\\nLet us examine a simplified example of a Kohonen map.\\nOur Kohonen map has just two inputs and nine cluster neurons, which are\\narranged into a 3/H110033 grid, as shown in Figure 11.5.\\nFigure 11.5 shows how the neurons are arranged in a grid. Each node in the'), Document(metadata={}, page_content='cluster layer is connected to each of the two input nodes. The cluster layer\\nnodes are not connected to each other. The grid shown in Figure 11.5 does\\nnot represent physical connection, but rather spatial proximity—node 1 is\\nclose to nodes 2 and 4. This spatial proximity of neurons is used to calculate\\nthe neighborhood set that is used to determine which weights to update\\nduring the training phase.\\nNote that this square arrangement is by no means necessary. The nodes are'), Document(metadata={}, page_content='often arranged in a rectangular grid, but other shapes can be used equally\\nsuccessfully.\\nBecause there are two input nodes in the network, we can represent each\\ninput as a position in two-dimensional space. Figure 11.6 shows the nine\\ninput values that are to be used to train this network.\\nIn Figure 11.6, x\\n1 and x2 are the two input values that are to be presented to\\nthe input layer, which contains two neurons.'), Document(metadata={}, page_content='11.6 Unsupervised Learning Networks 319\\nX2\\nX1\\nFigure 11.6\\nTraining data for the\\nKohonen map shown in\\nFigure 11.5\\n1\\n2 3\\n456\\n7\\n8 9\\nw2\\nw1\\nFigure 11.7\\nInitial weight vectors for\\nthe Kohonen map\\nNote that the training data have been selected randomly from the available\\nspace, such that they fill as much of the space as possible. In this way the\\ndata will be as representative as possible of all available input data, and so\\nthe Kohonen map will be able to cluster the input space optimally.'), Document(metadata={}, page_content='Because each neuron in the cluster layer has connections to the two input\\nlayer neurons, their weight vectors can be plotted in two-dimensional\\nspace. These weight vectors are initially set to random values, which are\\nshown in Figure 11.7. The connections between nodes in Figure 11.7 repre-\\nsent spatial proximity again, as in Figure 11.5.\\nBecause there are nine cluster nodes and nine pieces of training data, we\\nexpect the network to assign each neuron to one piece of training data.'), Document(metadata={}, page_content='Most real Kohonen maps consist of far more neurons, and many more\\ntraining data are usually used.\\nIn our simple example, by running a number of iterations of the Kohonen\\nmap, the weight vectors are modified to those shown in Figure 11.8.\\nIn this case, it is easy to see what the map has done: by modifying the\\nweight vector of each neuron so that it closely resembles one training vec-\\ntor, the nodes have been modified so that each node will respond extremely'), Document(metadata={}, page_content='well to one of the input data. When a new piece of input data is presented,'), Document(metadata={}, page_content='320 CHAPTER 11 Neural Networks\\n12 3\\n54\\n6\\n7 8 9\\nw2\\nw1\\nFigure 11.8\\nWeight vectors after train-\\ning the Kohonen map\\nit will be classified by the node whose weight vector is closest to it. Addi-\\ntionally, that node’s weight vector will be moved slightly toward the new\\npiece of input data. In this way, the network continues to learn as new data\\nare presented to it. By decreasing the learning rate over time, the network\\ncan be forced to reach a stable state where the weights no longer change, or'), Document(metadata={}, page_content='change only very slightly, when presented with new input data.\\nThis example illustrates the self-organizing nature of Kohonen maps.\\nThe space-filling shape shown in Figure 11.8 is typical of the behavior of\\nthese networks.\\n11.6.3 Hebbian Learning\\nHebbian learning is based on Hebb’s law, which was stated by D. O. Hebb\\nin 1949. Hebb’s law is stated as follows:\\nWhen an axon of cell A is near enough to excite a cell B and repeatedly or'), Document(metadata={}, page_content='persistently takes part in firing it, some growth process or metabolic change\\ntakes place in one or both cells such that A ’s efficiency, as one of the cells\\nfiring B, is increased.\\nIn terms of artificial neural networks, this rule can be restated as follows:\\nIf two neurons that are connected to each other fire at the same time, the\\nweight of the connection between those neurons is increased.\\nConversely, if the neurons fire at different times, the weight of the connec-'), Document(metadata={}, page_content='tion between them is decreased.\\nNeural networks use Hebbian learning to learn without needing to be given\\npreclassified training data.'), Document(metadata={}, page_content='11.7 Evolving Neural Networks 321\\nUsing Hebbian learning, the weight of a connection between neurons i and\\nj is increased according to the following rule:\\nwhere /H9251is the learning rate; xi is the input to node i, and yi is the output of\\nnode i (and thus the input contributed to node j by node i). This rule is\\nknown as the activity product rule.\\nBy treating the weights of neuron i as a vector, Wi, this rule can also be\\nwritten as'), Document(metadata={}, page_content='written as\\nwhere Xi is the input vector to node i, and yi is the output of node i.\\nThe activity product rule does not allow for decreasing weights, which is\\nrequired by Hebb’s law. The rule can be modified to allow weights to be\\ndecreased by using a forgetting factor, /H9278, as follows:\\nWhen /H9278is zero, the network cannot “forget, ” and the weights are always\\nincreased during learning. If /H9278were set to 1, the network would not be able'), Document(metadata={}, page_content='to learn at all because it would forget everything. Usually a small value,\\nsuch as between 0.01 and 0.1, is used as the forgetting factor.\\nUsing Hebb’s law, a neural network is able to learn to associate one input\\nwith another input. This can be thought of as analogous to the experiment\\nconducted by Pavlov in which he rang a bell whenever he fed his dogs,\\nwhich led the dogs to salivate whenever they heard a bell ring.\\n11.7 Evolving Neural Networks'), Document(metadata={}, page_content='11.7 Evolving Neural Networks\\nThe ideas that we cover in Chapter 14 on genetic algorithms can be applied\\nto neural networks. Genetic algorithms can be used to evolve suitable start-\\ning weight vectors for a network. This is useful because the initial weight\\nvector that is chosen for a network can significantly affect the ability of the\\nnetwork to solve a particular problem. Neural networks suffer from many\\nof the problems faced by search methods presented in Part 2 of this book,'), Document(metadata={}, page_content='ww y x y wij ij i i i ij←+ ⋅ ⋅ − ⋅ ⋅αφ\\nWW Xii i i y←+ ⋅ ⋅ α\\nww y xij ij i i←+ ⋅ ⋅ α'), Document(metadata={}, page_content='322 CHAPTER 11 Neural Networks\\nsuch as falling into local minima. By repeatedly running a full training ses-\\nsion on a neural network with different random starting weights, this prob-\\nlem can be avoided. Clearly, this problem can also be avoided by using\\nevolutionary methods to select starting weight vectors.\\nSimilarly, a genetic algorithm can be used to determine the connectivity of\\nthe network. In this way, the number of neurons and the connections'), Document(metadata={}, page_content='between those neurons can be evolved to produce an optimal architecture.\\n11.8 Chapter Summary\\n■ Biological neurons are the building blocks of the human brain.\\nEach neuron has a number of inputs, and one output, which fires\\ndepending on the inputs.\\n■ Artificial neurons are modeled on biological neurons and are used\\nto build artificial neural networks. Artificial neurons often use a\\nfunction such as a Step function to calculate their output based on\\nthe weighted sum of their inputs.'), Document(metadata={}, page_content='the weighted sum of their inputs.\\n■ A perceptron is a very simple neuron that can model problems that\\nare linearly separable.\\n■ Multilayer neural networks, using backpropagation, can solve\\nproblems that are not linearly separable.\\n■ Recurrent networks, such as Hopfield networks, allow arbitrary con-\\nnections between neurons within the network, which is particularly\\nuseful for modeling functions such as the value of the stock market,'), Document(metadata={}, page_content='where the value at one point in time is dependent on previous values.\\n■ Unsupervised neural networks, such as Kohonen maps, learn to\\nclassify without being presented any preclassified training data.\\n■ Hebbian learning is an unsupervised learning technique based on\\nthe idea that if two neurons fire at the same time, then the connec-\\ntion between them should be strengthened.\\n11.9 Review Questions\\n11.1 Explain how the human brain uses neurons to learn. What are the'), Document(metadata={}, page_content='similarities and differences between artificial neurons and biologi-\\ncal neurons?'), Document(metadata={}, page_content='11.10 Exercises 323\\n11.2 How likely do you think it is that a neural network of the complex-\\nity of the human brain will ever be built in software? In hardware?\\n11.3 Explain how the backpropagation algorithm is used. Why is\\nmomentum used with backpropagation?\\n11.4 Explain the limitations of a perceptron. What kind of problems\\ncan they solve? Give a real-world example.\\n11.5 Explain how Hopfield networks operate.\\n11.6 Explain the difference between supervised and unsupervised learn-'), Document(metadata={}, page_content='ing. When might each be most useful?\\n11.7 Explain what is meant by Hebbian learning. Why is forgetting\\nimportant to Hebbian learning?\\n11.8 Explain in detail how a Kohonen map might be used to cluster a set\\nof web documents in response to a user’s keyword query.\\n11.9 What are the advantages and disadvantages of applying evolution-\\nary techniques to neural networks? What could be the ultimate\\ngoal of such a combination?\\n11.10 Exercises'), Document(metadata={}, page_content='goal of such a combination?\\n11.10 Exercises\\n11.10 Run through the training process for a perceptron to calculate the\\nbinary AND function on three inputs.\\n11.11 Design a multilayer neural network with two inputs and one hid-\\nden layer that uses the backpropagation algorithm to learn to rep-\\nresent the logical exclusive-OR function for two inputs. Y our\\nnetwork should have two nodes in the input layer, two in the hid-\\nden layer, and one in the output layer. Initialize the weights to ran-'), Document(metadata={}, page_content='dom values, and run the algorithm (on paper) for three epochs.\\nComment on your results. Implement this network in the pro-\\ngramming language of your choice. Run it until the sum of the\\nsquares of the errors is less than 0.001. How many epochs does the\\nnetwork take to learn the exclusive-OR function? Try to modify\\nyour program so that it learns the function in fewer epochs.'), Document(metadata={}, page_content='324 CHAPTER 11 Neural Networks\\nFigure 11.9\\nThe 10 digits possible with\\na seven-segment display\\n11.12 On paper, calculate the weight matrix for a Hopfield network that\\nis to learn the following two input vectors:\\nNow calculate the behavior of the network when it is presented\\nwith X1 as an input. How does it behave when it is presented with\\nthe following input?\\n11.14 Design and implement a neural network system for recognizing\\nnumbers. Y ou could start by building a network to recognize the 10'), Document(metadata={}, page_content='possible digits represented in a seven-segment LED, as shown in\\nFigure 11.9. If you are feeling ambitious, extend your algorithm to\\nwork with numbers displayed in a dot-matrix display of 8-by-8\\ndots. What problems do you encounter?\\n11.11 Further Reading\\nThere are a number of excellent introductory texts on neural networks, as\\nwell as many more advanced ones. Introductions by Gurney (1997) and\\nX3\\n1\\n1\\n1\\n1\\n=\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nXX12\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0'), Document(metadata={}, page_content='1\\n1\\n1\\n1\\n1\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n=\\n−\\n−\\n−\\n−\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'), Document(metadata={}, page_content='11.11 Further Reading 325\\nCallan (1999) cover most of the material introduced in this chapter. For\\nmore advanced readings, consult the papers and books referenced below.\\nT o learn more about evolutionary neural networks, consult Negnevitsky\\n(2002) or Bäck et al. (1997).\\nThe Handbook of Brain Theory and Neural Networks: Second Edition edited\\nby Michael A. Arbib (2002 – MIT Press)\\nHandbook of Evolutionary Computation edited by T. Bäck, D. B. Fogel, and'), Document(metadata={}, page_content='Z. Michalewicz (1997 – Institute of Physics Publishing)\\nNeural Networks for Pattern Recognition by Christopher M. Bishop (1996 –\\nOxford University Press)\\nUnderstanding 99% of Artificial Neural Networks: Introduction & Tricks by\\nMarcelo Bosque (2002 – Writers Club Press)\\nThe Essence of Neural Networks by Robert Callan (1999 – Prentice Hall)\\nFundamentals of Neural Networksby Laurene V . Fausett (1994 – Prentice Hall)\\nAn Introduction to Neural Networks by Kevin Gurney (1997 – UCL Press)'), Document(metadata={}, page_content='Neural Networks: A Comprehensive Foundationby Simon S. Haykin (1998 –\\nPrentice Hall)\\nThe Organisation of Behavior: A Neuropsychological Theory by D. O. Hebb\\n(1949 – republished in 2002 by Lawrence Erlbaum Assoc.)\\nIncreased Rates of Convergence Through Learning Rate Adaptation by R. A.\\nJacobs (1987 – in Neural Networks, Vol. 1, pp. 295–307).\\nSelf-Organizing Maps by T euvo Kohonen (2000 – Springer V erlag)\\nBidirectional Associative Memories by Bart Kosko (1988 – in IEEE Transac-'), Document(metadata={}, page_content='tions Systems, Man & Cybernetics, Vol. 18, pp. 49–60).\\nA Logical Calculus of the Ideas Immanent in Nervous Activity by W. S.\\nMcCulloch and W. Pitts (1943 – inBulletin of Mathematical Biophysics, Vol.\\n5, pp. 115–137).\\nPerceptrons by Marvin Minsky and Seymour A. Papert (1969 – now avail-\\nable in an extended edition: Perceptrons - Expanded Edition: An Introduc-\\ntion to Computational Geometry. 1987 – MIT Press)\\nMachine Learning by T om M. Mitchell (1997 – McGraw Hill)'), Document(metadata={}, page_content='Artificial Intelligence: A Guide to Intelligent Systems by Michael Negnevitsky\\n(2002 – Addison Wesley)'), Document(metadata={}, page_content='326 CHAPTER 11 Neural Networks\\nComputational Explorations in Cognitive Neuroscience: Understanding the\\nMind by Simulating the Brain by Randall C. O’Reilly (Author) and Yuko\\nMunakata (2000 – MIT Press)\\nUnderstanding Intelligence by Rolf Pfeifer and Christian Scheier (2000 –\\nMIT Press)\\nThe Perceptron: A Probabilistic Model for Information Storage and Organiza-\\ntion in the Brain by F. Rosenblatt (1958 – in Psychological Review, Vol. 65,\\npp. 386–408)'), Document(metadata={}, page_content='12CHAPTER\\nProbabilistic Reasoning and\\nBayesian Belief Networks\\nBut to us, probability is the very guide of life.\\n—Joseph Butler,The Analogy of Religion\\nProbable Impossibilities are to be preferred to improbable possibilities.\\n—Aristotle, Poetics\\nDo not expect to arrive at certainty in every subject which you pursue. There\\nare a hundred things wherein we mortals must be content with probability,\\nwhere our best light and reasoning will reach no farther.\\n—Isaac Watts\\n12.1 Introduction'), Document(metadata={}, page_content='—Isaac Watts\\n12.1 Introduction\\nThis chapter introduces the ideas behind probabilistic reasoning, and in\\nparticular Bayes’ Theorem. Thomas Bayes was an English mathematician\\nand theologian who lived from 1702 to 1761. His theorem is used exten-\\nsively today in dealing with situations that lack certainty.\\nThis chapter explains the relationship between probability theory and the\\nlogic that we saw in Part 3. It explains joint probability distributions and'), Document(metadata={}, page_content='goes on to explain Bayes’ theorem, using two examples.\\nThis chapter explains how Bayesian belief networks can be built and used\\nto learn from data about which certainty is lacking. Bayesian classifiers are\\nalso explained. The chapter also includes an introduction to the ideas'), Document(metadata={}, page_content='328 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nbehind collaborative filtering and explains how this increasingly popular\\ntechnique relates to Bayesian reasoning.\\n12.2 Probabilistic Reasoning\\nIn this section, we will present a brief introduction to probability theory and\\nthe notation that is used to express it. Probability theory is used to discuss\\nevents, categories, and hypotheses about which there is not 100% certainty.'), Document(metadata={}, page_content='The notation that we saw in Chapter 7 for making and analyzing logical\\nstatements does not function in situations that are lacking certainty.\\nFor example, we might write\\nA → B\\nwhich means that if A is true, then B is true. If we are unsure whether A is\\ntrue, then we cannot make use of this expression. In many real-world situ-\\nations, it is very useful to be able to talk about things that lack certainty. For\\nexample, what will the weather be like tomorrow? We might formulate a'), Document(metadata={}, page_content='very simple hypothesis based on general observation, such as “it is sunny\\nonly 10% of the time, and rainy 70% of the time. ” We can use a notation\\nsimilar to that used for predicate calculus to express such statements:\\nP(S) = 0.1\\nP(R) = 0.7\\nThe first of these statements says that the probability of S (“it is sunny”) is\\n0.1. The second says that the probability of R is 0.7. Probabilities are always\\nexpressed as real numbers between 0 and 1. A probability of 0 means “defi-'), Document(metadata={}, page_content='nitely not” and a probability of 1 means “definitely so. ” Hence, P(S) = 1\\nmeans that it is always sunny.\\nMany of the operators and notations that are used in prepositional logic\\ncan also be used in probabilistic notation. For example, P(\\n¬S) means “the\\nprobability that it is not sunny”; P(S ∧ R) means “the probability that it is\\nboth sunny and rainy. ”\\nP(A ∨ B), which means “the probability that either A is true or B is true, ” is\\ndefined by the following rule:'), Document(metadata={}, page_content='defined by the following rule:\\nP(A ∨ B) = P(A) + P(B) /H11002P(A ∧ B)'), Document(metadata={}, page_content='12.2 Probabilistic Reasoning 329\\nA B\\nA ∧ B\\nFigure 12.1\\nIllustrating the relation-\\nship between A ∧B and\\nA ∨B\\nThis rule can be seen to be true by examining the V enn diagram shown in\\nFigure 12.1.\\nThe notation P(B|A) can be read as “the probability of B, given A. ” This is\\nknown as conditional probability—it is conditional onA. In other words, it\\nstates the probability thatB is true, given that we already know thatA is true.\\nP(B|A) is defined by the following rule:'), Document(metadata={}, page_content='P(B|A) is defined by the following rule:\\nOf course, this rule cannot be used in cases where P(A) = 0.\\nFor example, let us suppose that the likelihood that it is both sunny and\\nrainy at the same time is 0.01. Then we can calculate the probability that it\\nis rainy, given that it is sunny as follows:\\nPR S PR S\\nPS( ) = ∧( )\\n( )\\n=\\n=\\n00 1\\n01\\n01\\n.\\n.\\n.\\nPB A PB A\\nPA( ) = ∧( )\\n( )'), Document(metadata={}, page_content='330 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nNote that the probability that it is sunny given that it is overcast—P(S|R)—\\nis different from this: 0.01/0.7 = 0.14; hence, P(A|B) ≠ P(B|A).\\n12.3 Joint Probability Distributions\\nA joint probability distribution (also known as a joint) can be used to\\nrepresent the probabilities of combined statements, such as A ∧ B.F o r\\nexample, the following table shows a joint probability distribution of two\\nvariables, A and B:\\nA ¬A'), Document(metadata={}, page_content='variables, A and B:\\nA ¬A\\nB 0.11 0.09\\n¬B 0.63 0.17\\nThis shows, for example, that P(A ∧ B) = 0.11, and that P(A ∧¬ B) = 0.63.\\nBy summing these two values, we can find P(A) = 0.11 + 0.63 = 0.74. Simi-\\nlarly, P(B) = 0.11 + 0.09 = 0.2.\\nWe can use this table to determine the probability of any logical combina-\\ntion of A and B. For example, P(A ∨ B) = 0.11 + 0.09 + 0.63 = 0.83. We\\ncould have obtained this result by noting that P(¬A ∧¬ B) = 0.17 and that'), Document(metadata={}, page_content='P(¬A ∧¬ B) = 1 /H11002P(A ∨ B) = 1 /H110020.17 = 0.83.\\nSimilarly, we can determine conditional probabilities, such as P(B|A) using\\nthe following rule:\\nIn this case, P(B ∧ A) = 0.11 and P(A) = 0.11 + 0.63 = 0.74, so P(B|A) =\\n0.11 / 0.74 = 0.15.\\nCalculations like this are easy when we use a joint probability of just two\\nvariables. Real-world problems will often involve much greater numbers of\\nvariables, and in these cases, drawing up probability distribution tables is'), Document(metadata={}, page_content='clearly much less straightforward.\\n12.4 Bayes’ Theorem\\nBayes’ theorem can be used to calculate the probability that a certain event\\nwill occur or that a certain proposition is true, given that we already know\\na related piece of information.\\nPB A PB A\\nPA( ) = ∧( )\\n( )'), Document(metadata={}, page_content='12.4 Bayes’ Theorem 331\\nThe theorem is stated as follows:\\nP(B) is called the prior probability of B. P(B|A), as well as being called the\\nconditional probability, is also known as the posterior probability of B.\\nLet us briefly examine how Bayes’ theorem is derived:\\nWe can deduce a further equation from the rule given in Section 12.2\\nabove. This rule is known as the product rule:\\nP(A\\n∧ B) = P(A|B)P(B)\\nNote that due to the commutativity of ∧, we can also write\\nP(A ∧ B) = P(B|A)P(A)'), Document(metadata={}, page_content='P(A ∧ B) = P(B|A)P(A)\\nHence, we can deduce:\\nP(B|A)P(A) = P(A|B)P(B)\\nThis can then be rearranged to give Bayes’ theorem:\\n12.4.1 Example: Medical Diagnosis\\nLet us examine a simple example to illustrate the use of Bayes’ theorem for\\nthe purposes of medical diagnosis.\\nWhen one has a cold, one usually has a high temperature (let us say, 80% of\\nthe time). We can use A to denote “I have a high temperature” and B to\\ndenote “I have a cold.” Therefore, we can write this statement of posterior'), Document(metadata={}, page_content='probability as\\nP(A\\n|B) = 0.8\\nNote that in this case, we are using A and B to represent pieces of data that\\ncould each either be a hypothesis or a piece of evidence. It is more likely\\nthat we would use A as a piece of evidence to help us prove or disprove the\\nhypothesis, B, but it could work equally well the other way around (at least,\\nmathematically speaking).\\nNow, let us suppose that we also know that at any one time around 1 in'), Document(metadata={}, page_content='every 10,000 people has a cold, and that 1 in every 1000 people has a high\\ntemperature. We can write these prior probabilities as\\nPB A PA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )\\nPB A\\nPA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )'), Document(metadata={}, page_content='332 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nP(A) = 0.001\\nP(B) = 0.0001\\nNow suppose that you have a high temperature. What is the likelihood that\\nyou have a cold? This can be calculated very simply by using Bayes’ theorem:\\nHence, we have shown that just because you have a high temperature does\\nnot necessarily make it very likely that you have a cold—in fact, the chances\\nthat you have a cold are just 8 in 1000.'), Document(metadata={}, page_content='that you have a cold are just 8 in 1000.\\nBayes’ theorem can be extended to express a conditional probability involv-\\ning more than two variables as follows:\\nProvided the n pieces of evidence E\\n1 ... En are independent of each other,\\ngiven the hypothesis H,1 then this can be rewritten as follows:\\n12.4.2 Example: Witness Reliability\\nLet us examine a further example. In the city of Cambridge, there are two\\ntaxi companies. One taxi company uses yellow taxis, and the other uses'), Document(metadata={}, page_content='white taxis. The yellow taxi company has 90 cars, and the white taxi com-\\npany has just 10 cars.\\nA hit-and-run incident has been reported, and an eye witness has stated\\nthat she is certain that the car was a white taxi.\\nPH E E PEH PE H PH\\nPE En\\nn\\nn\\n1\\n1\\n1\\n∧∧( ) = ( ) ⋅⋅ ( ) ⋅ ( )\\n∧∧( )\\nK K\\nK\\nPH E E PE E H PH\\nPE En\\nn\\nn\\n1\\n1\\n1\\n∧∧( ) = ∧∧( ) ⋅ ( )\\n∧∧( )\\nK K\\nK\\nPB A PA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )\\n= ⋅\\n=\\n0 8 0 0001\\n0 001\\n0 008\\n..\\n.\\n.'), Document(metadata={}, page_content='( )\\n= ⋅\\n=\\n0 8 0 0001\\n0 001\\n0 008\\n..\\n.\\n.\\n1In other words, if H is true, then the truth or otherwise of Ei should have no effect on the\\ntruth of Ej for any i and j.\\n∧ ∧\\n∧ ∧\\n∧ ∧\\n∧ ∧\\n∧ ∧'), Document(metadata={}, page_content='12.4 Bayes’ Theorem 333\\nSo far, we have the following information:\\nP(Y) = 0.9 (the probability of any particular taxi being yellow)\\nP(W) = 0.1 (the probability of any particular taxi being white)\\nLet us further suppose that experts have asserted that given the foggy\\nweather at the time of the incident, the witness had a 75% chance of cor-\\nrectly identifying the taxi.\\nGiven that the lady has said that the taxi was white, what is the likelihood\\nthat she is right?\\nLet us denote by P(C'), Document(metadata={}, page_content='that she is right?\\nLet us denote by P(C\\nW) the probability that the culprit was driving a white\\ntaxi and by P(CY) the probability that it was a yellow car.\\nWe will use P(WW) to denote the probability that the witness says she saw a\\nwhite car and P(WY) to denote that she says she saw a yellow car. (We\\nassume the witness tells the truth!)\\nNow, if the witness really saw a yellow car, she would say that it was yellow\\n75% of the time, and if she says she saw a white car, she would say it was'), Document(metadata={}, page_content='white 75% of the time. Hence, we now know the following:\\nP(C\\nY) = 0.9\\nP(CW) = 0.1\\nP(WW | CW) =0.75\\nP(WY | CY) = 0.75\\nHence, we can apply Bayes’ theorem to find the probability, given that she is\\nsaying that the car was white, that she is correct:\\nWe now need to calculate P(WW)—the prior probability that the lady\\nwould say she saw a white car.\\nLet us imagine that the lady is later shown a random sequence of 1000 cars.\\nWe expect 900 of these cars to be yellow and 100 of them to be white. The'), Document(metadata={}, page_content='witness will misidentify 250 of the cars: Of the 900 yellow cars, she will\\nincorrectly say that 225 are white. Of the 100 white cars, she will incorrectly\\nsay that 25 are yellow. Hence, in total, she will believe she sees 300 white\\nPC W PWWW\\nW\\n( ) = ⋅\\n( )\\n07 5 01..'), Document(metadata={}, page_content='334 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\ncars—even though only 100 of them are really white. So, P(WW) is\\n300/1000 = 0.3.\\nWe can now complete our equation to find P(CW|WW):\\nIn other words, if the lady says that the car was white, the probability that it\\nwas in fact white is only 0.25—it is three times more likely that it was actu-\\nally yellow!\\nIn this example, Bayes’ theorem takes into account the actual number of'), Document(metadata={}, page_content='each color of taxi in the city. If the witness had said she saw a yellow taxi, it\\nwould be very likely that she was right—but this is likely anyway because\\nthere are so many more yellow taxis than white taxis. If the witness were a\\nperfect observer who made no errors, then the probability P(C\\nW|WW)\\nwould, of course, be 1.\\nThis example also helps to illustrate the fact that in many real-world situa-\\ntions we do have enough information to be able to use Bayes’ theorem. It'), Document(metadata={}, page_content='can look as though Bayes’ theorem will apply only in contrived situations,\\nbut in fact it is usually the case that obtaining the data needed to use Bayes’\\ntheorem is easier than obtaining the posterior probability by other means.\\nThis is particularly true in cases where there are a large number of individ-\\nuals being discussed.\\n12.4.3 Comparing Conditional Probabilities\\nIn many situations, it can be useful to compare two probabilities. In partic-'), Document(metadata={}, page_content='ular, in making a diagnosis from a set of evidence, one will often have to\\nchoose from a number of possible hypotheses.\\nFor example, let us extend the medical example given in Section 12.4.1.\\nThere we used A to represent the piece of evidence “I have a high tempera-\\nture” and B to represent the hypothesis “I have a cold, ” where\\nP(A) = 0.001\\nP(B) = 0.0001\\nPC WWW( ) = ⋅\\n=\\n07 5 01\\n03\\n02 5\\n..\\n.\\n.'), Document(metadata={}, page_content='12.4 Bayes’ Theorem 335\\nP(A|B) = 0.8\\nLet us further use C to represent the hypothesis “I have plague, ” where\\nP(C) = 0.000000001\\nP(A|C) = 0.99\\nIn other words, it is highly unlikely for anyone to have plague, but if they\\ndo, they will almost certainly have a high temperature.\\nIn this case, when carrying out a diagnosis of a patient that has a high tem-\\nperature, it will be useful to determine which is the more likely hypothe-\\nsis—B or C.\\nBayes’ theorem gives us the following:'), Document(metadata={}, page_content='Bayes’ theorem gives us the following:\\nClearly, to find the more likely of B and C,g i v e n  A, we can eliminate P(A)\\nfrom these equations and can determine the relative likelihood of B and C\\nas follows:\\nHence, it is hundreds of thousands of times more likely given that a patient\\nhas a high temperature that he has a cold than that he has plague.\\n12.4.4 Normalization\\nNormalization is the process whereby the posterior probabilities of a pair'), Document(metadata={}, page_content='of variables are divided by a fixed value to ensure that they sum to 1.\\nPB A\\nPCA\\nPA B PB\\nPA C P C\\n( )\\n( )\\n= ( ) ⋅ ( )\\n( ) ⋅ ( )\\n= ⋅\\n⋅\\n=\\n08 00 0 1\\n0 95 0 000000001\\n842 105\\n..\\n..\\n,\\nPB A PA B PB\\nPA\\nPCA PA C P C\\nPA\\n( ) = ( ) ⋅ ( )\\n( )\\n( ) = ( ) ⋅ ( )\\n( )'), Document(metadata={}, page_content='336 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nThis can be done by considering the following two equations:\\nGiven that A is true, B must either be true or false, which means that P(B|A)\\n+ P(¬B|A) = 1.\\nHence, we can add the two equations above to give\\nNow we can replace P(A) in the equation for Bayes’ theorem, to give\\nHence, it is possible to use Bayes’ theorem to obtain the conditional proba-\\nbility P(B|A) without needing to know or calculate P(A), providing we can'), Document(metadata={}, page_content='obtain P(A|¬B). [P(¬B) is simply 1–P(B)].\\nThis equation is often written as follows:\\nP(B|A) = /H9251/H11080P(A|B) /H11080P(B)\\nwhere /H9251represents the normalizing constant:\\nLet us examine our diagnosis example again. The facts we have are as follows:\\nP(A) = 0.001\\nP(B) = 0.0001\\nP(A\\n|B) = 0.8\\nα =\\n( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\n1\\nPA B PB PA B P B\\nPB A PA B PB\\nPA B PB PA B P B( ) = ( ) ⋅ ( )\\n( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\n1 = ( ) ⋅ ( )\\n( )\\n+ ¬( ) ⋅ ( )\\n( )\\n∴ ( ) = ( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\nPA B PB\\nPA\\nPA B PB\\nPA'), Document(metadata={}, page_content='PA B PB\\nPA\\nPA B PB\\nPA\\nPA PA B PB PA B P B\\nPB A PA B PB\\nPA\\nPB A PA B P B\\nPA\\n( ) = ( ) ⋅ ( )\\n( )\\n¬( ) = ¬( ) ⋅¬( )\\n( )'), Document(metadata={}, page_content='12.5 Simple Bayesian Concept Learning 337\\nLet us now suppose that P(A|¬B) = 0.00099. This conditional probability\\nstates the likelihood that a person will have a high temperature if she does\\nnot have a cold (\\n¬B). We can now thus use the following equation to calcu-\\nlate P(B|A):\\nSimilarly, we can calculate P(¬B|A):\\nThe net result of this normalization process has been to ensure that P(B|A)\\n+ P(¬B|A) = 1. We could now carry out a similar process to calculateP(C|A)'), Document(metadata={}, page_content='and P(¬C|A), which would enable us to ensure that they also sum to 1.\\n12.5 Simple Bayesian Concept Learning\\nA very simple model for learning can be developed using Bayes’ rule.\\nThroughout the above discussion we have been talking about probabilities\\nof hypotheses or of specific pieces of evidence. T o use probability theory in\\nlearning, it is useful to talk about the probability that some hypothesis is\\ntrue, given a particular set of evidence. We can use the same notation for\\nthis, and write'), Document(metadata={}, page_content='this, and write\\nP(H\\n|E)\\nPB A PA B P B\\nPA B P B PA B PB¬( ) = ¬( ) ⋅¬( )\\n¬( ) ⋅¬( ) + ( ) ⋅ ( )\\n= ⋅\\n⋅+ ⋅\\n=\\n=\\n0 00099 0 9999\\n0 00099 0 9999 0 8 0 0001\\n0 000989901\\n0 001069901\\n0 925\\n..\\n.. . .\\n.\\n.\\n.\\nPB A PA B PB\\nPA B PB PA B P B( ) = ( ) ⋅ ( )\\n( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\n= ⋅\\n⋅+ ⋅\\n=\\n=\\n0 8 0 0001\\n0 8 0 001 0 00099 0 9999\\n0 00008\\n0 001069901\\n0 075\\n..\\n.. . .\\n.\\n.\\n.'), Document(metadata={}, page_content='338 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nHence, given a set of evidence, the learner can determine which hypothesis\\nto believe in by identifying the posterior probability of each. Let us suppose\\nthat there are n possible hypotheses, H\\n1 ... Hn.H e n c e ,f o r  e a c h  Hi\\nSo the algorithm could calculate P(Hi|E) for each possible hypothesis and\\nselect the one that has the highest probability. Similarly, the system could'), Document(metadata={}, page_content='use this method to determine an action to take, where H\\ni is the hypothesis\\nthat the best action to take in the current situation is action Ai.\\nIn fact, the formula above can be simplified in this situation: because P(E)\\nis independent of Hi, it will have the same value for each hypothesis. So\\nbecause we are simply looking for the hypothesis with the maximum poste-\\nrior probability, we can eliminate P(E) from the calculation and simply aim\\nto maximize the following value:\\nP(E\\n|Hi) /H11080P(Hi)'), Document(metadata={}, page_content='P(E\\n|Hi) /H11080P(Hi)\\nIn fact, if we assume that all hypotheses are equally likely, given no addi-\\ntional information (i.e., P(Hi) = P(Hj) for any i and j), we can in fact reduce\\nthis further and simply choose the hypothesis for whom the value P(E|Hi)\\nis the highest. This value is known as the likelihood of the evidence E,g i v e n\\nhypothesis Hi. Of course, by learning from observations what the prior\\nprobabilities are of each of the hypotheses, more accurate results can be'), Document(metadata={}, page_content='obtained, but the simpler formula is more efficient in calculation time.\\nRecall the discussion from Chapter 7 of abduction and inductive reason-\\ning. These are really a form of learning: by observing the events that occur,\\nwe are able to make reasonable guesses about future events, and these\\nguesses can often guide our actions. For example, if a robot observed that\\nevery time it heard a particular noise, an enemy robot appeared, it might'), Document(metadata={}, page_content='learn to hide when it heard that noise. In doing so, it is learning from expe-\\nrience and using Bayesian reasoning to decide upon the correct course of\\naction. The robot is not using rules of logical deduction, such as modus\\nponens, which was explained in Chapter 7, but a rather more probabilistic\\nform of reasoning, along the lines of “I have noticed in the past that when\\nthis noise occurs, an enemy appears. I have also noticed in the past that if I\\nPH E\\nPE H PH\\nPEi\\nii\\n( ) = ( ) ⋅ ( )\\n( )'), Document(metadata={}, page_content='12.6 Bayesian Belief Networks 339\\nA\\nC D\\nB\\nE\\nFigure 12.2\\nA simple belief network\\ndo not hide when an enemy appears, I get hurt by the enemy. Hence, I\\nshould probably hide when I hear the noise. ”\\nHumans use learning of this kind all the time, and it is essential for learning\\nin situations in which there is very little certainty, such as the real world.\\n12.6 Bayesian Belief Networks\\nThe concept of dependence is very important in probability theory. Two'), Document(metadata={}, page_content='events, A and B,a r e  independent if the likelihood of occurrence of A is\\nentirely unrelated to whether or not B occurs.\\nFor example, in tossing two coins, the likelihood that the first coin will come\\nup heads and the likelihood that the second coin will come up heads are two\\nindependent probabilities because neither one depends on the other.\\nIf A and B are independent, then the probability that A and B will both\\noccur can be calculated very simply:\\nP(A\\n∧ B) = P(A).P(B)'), Document(metadata={}, page_content='P(A\\n∧ B) = P(A).P(B)\\nWe know that this equation does not hold if A depends on B because we\\nhave already seen the following equation:\\nBy comparing these two equations, we can see that A and B are independ-\\nent if P(B|A) = P(B). In other words, the likelihood of B is unaffected by\\nwhether or not A occurs. B is independent of A.I f B is dependent on A,\\nthen P(B|A) will be different from P(B).\\nThese relationships can be expressed extremely succinctly in a belief net-'), Document(metadata={}, page_content='work, such as the one shown in Figure 12.2.\\nPB A PB A\\nPA( ) = ∧( )\\n( )'), Document(metadata={}, page_content='340 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nA Bayesian belief network is an acyclic directed graph, where the nodes in\\nthe graph represent evidence or hypotheses, and where an arc that connects\\ntwo nodes represents a dependence between those two nodes.\\nThe belief network in Figure 12.2 contains five nodes that represent pieces\\nof evidence (A and B) and three hypotheses (C, D, and E). The arcs between'), Document(metadata={}, page_content='these nodes represent the interdependence of the hypotheses. According to\\nthis diagram, C and D are both dependent on A, and D and E are both\\ndependent on B. Two nodes that do not have an arc between them are inde-\\npendent of each other. For example,B is independent of A.\\nEach node in the network has a set of probabilities associated with it, based on\\nthe values of the nodes on which it is dependent. Hence,A and B both have'), Document(metadata={}, page_content='just prior probabilities,P(A) andP(B), because they are not dependent on any\\nother nodes.C and E are each dependent on just one other node. Hence, for\\nexample,P(C) must be represented in the two cases—A is true andA is false.\\nP(D) must be represented in four cases, depending on the values ofA and B.\\nFor example, the following conditional probabilities might be used in the\\nnetwork shown in Figure 12.2:\\nP(A) = 0.1\\nP(B) = 0.7\\nP(C\\n|A) = 0.2\\nP(C|¬A) = 0.4\\nP(D|A ∧ B) = 0.5\\nP(D|A ∧¬ B) = 0.4'), Document(metadata={}, page_content='P(C|¬A) = 0.4\\nP(D|A ∧ B) = 0.5\\nP(D|A ∧¬ B) = 0.4\\nP(D|¬A ∧ B) = 0.2\\nP(D|¬A ∧¬ B) = 0.0001\\nP(E|B) = 0.2\\nP(E|¬B) = 0.1\\nThe above list of probabilities, combined with the diagram shown in Figure\\n12.2, represent a complete (rather simple) Bayesian belief network. The\\nnetwork states beliefs about a set of hypotheses or pieces of evidence and\\nthe ways that they interact.\\nThese probabilities can also be expressed in the form of conditional prob-\\nability tables, as follows:'), Document(metadata={}, page_content='12.6 Bayesian Belief Networks 341\\nCompare these tables with the logical truth tables described in Chapter 7.\\nIn those tables, a logical value ( true or false) was given for a variable that\\ndepended on the values of one or more other variables. Hence, a condi-\\ntional probability table is very similar to a truth table, except that it\\nexpresses the probability of one variable, given the truth values of one or\\nmore other variables.'), Document(metadata={}, page_content='more other variables.\\nA joint probability can be calculated from the Bayesian belief network\\nusing the definition of conditional probability:\\nHence,\\nP(A,B,C,D,E) = P(E\\n|A,B,C,D) /H11080P(A,B,C,D)\\nWe can apply this rule recursively to obtain\\nP(A,B,C,D,E)= P(E|A,B,C,D,) /H11080P(D|A,B,C,) /H11080P(C|A,B) /H11080P(B|A) /H11080P(A)\\nIn fact, the nature of our belief network allows us to simplify this expres-\\nsion, and because we know that, for example,E is not dependent on A, C,o r'), Document(metadata={}, page_content='D, we can reduce P(E|A,B,C,D) to P(E|B).\\nP(A,B,C,D,E) = P(E|B) /H11080P(D|A,B) /H11080P(C|A) /H11080P(B) /H11080P(A)\\nPB A PB A\\nPA( ) = ∧( )\\n( )\\nP(A) P(B)\\n0.1 0.7\\nA P(C) B P(E)\\ntrue 0.2 true 0.2\\nfalse 0.4 false 0.1\\nA B P(D)\\ntrue true 0.5\\ntrue false 0.4\\nfalse true 0.2\\nfalse false 0.0001'), Document(metadata={}, page_content='342 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nWe have now greatly reduced the complexity of the calculation needed to\\ncompute the joint probability. This has only been possible due to the way in\\nwhich the nodes were ordered in the original expression. For example, if we\\nused the same method blindly on the expression\\nP(E,D,C,B,A)\\nwe would be left with the following expression:\\nP(E,D,C,B,A)= P(A\\n|E,D,C,B) /H11080P(B|E,D,C) /H11080P(C|E,D) /H11080P(D|E) /H11080P(E)'), Document(metadata={}, page_content='This is not correct because E is dependent on B, and so we need to include\\nP(E|B). Similarly, D is dependent on A and B, which is not reflected in this\\nexpression.\\nIn other words, to calculate the joint probability, the nodes must be ordered\\nin the expression in such a way that if a node X is dependent on another\\nnode Y, then X appears before Y in the joint. Hence, we could have used any\\nordering in which A and B appear before C, D, and E; B,A,E,D,C would\\nhave worked equally well, for example.'), Document(metadata={}, page_content='have worked equally well, for example.\\nAs a result of this, when constructing a Bayesian belief network, it is essential\\nthat the graph be constructed in the correct order—in other words, in an\\norder such that the connections between nodes makes logical sense. This usu-\\nally means starting with causes and then adding the events they cause, and\\nthen treating those events as causes, and adding any further events they cause.\\nThe nature of Bayesian belief networks means that in general they are an'), Document(metadata={}, page_content='efficient way of storing a joint probability distribution. The network does\\nnot store the conditional probability P(X\\n|Y) if X and Y are independent of\\neach other, given the parents ofX. In the network shown in Figure 12.2, for\\nexample, this means that P(E|A) does not need to be stored.\\n12.6.1 Example: Life at College\\nLet us examine the simple Bayesian belief network shown in Figure 12.3.\\nIn Figure 12.3, the five nodes represent the following statements:\\nC = that you will go to college'), Document(metadata={}, page_content='C = that you will go to college\\nS = that you will study\\nP = that you will party\\nE = that you will be successful in your exams\\nF = that you will have fun'), Document(metadata={}, page_content='12.6 Bayesian Belief Networks 343\\nC\\nS\\nE F\\nP\\nFigure 12.3\\nA Bayesian network to rep-\\nresent activities at college\\nThis network shows us at a glance that if you go to college, this will affect\\nthe likelihood that you will study and the likelihood that you will party.\\nStudying and partying affect your chances of exam success, and partying\\naffects your chances of having fun.\\nT o complete the Bayesian belief network, we need to include the condi-'), Document(metadata={}, page_content='tional probability tables. Let us define these as follows:\\nP(C)\\n0.2\\nC P(S)\\ntrue 0.8\\nfalse 0.2\\nC P(P)\\ntrue 0.6\\nfalse 0.5\\nS P P(E)\\ntrue true 0.6\\ntrue false 0.9\\nfalse true 0.1\\nfalse false 0.2'), Document(metadata={}, page_content='344 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nNote that according to this belief network there is a dependence between F\\nand C, but because it is not a direct dependence, no information needs to\\nbe stored about it.\\nThese conditional probability tables give us all the information we need\\nto carry out any reasoning about this particular domain. For example, we\\ncan clearly obtain values such as P(\\n¬C) by using the fact that\\nP(¬C)=1  /H11002P(C)=1 /H110020.2 = 0.8.'), Document(metadata={}, page_content='P(¬C)=1  /H11002P(C)=1 /H110020.2 = 0.8.\\nWe can use the network to determine conditional probabilities, such as\\nP(F|P) by observing that in the final table, if P is true, then P(F ) = 0.9.\\nHence, P(F|P) = 0.9.\\nThe joint probability distribution for this domain represents the entire\\nstate of the domain. We can represent such a state using the notation as\\nused in the following example:\\nP(C = true, S = true, P = false, E = true, F = false)\\nWe can simplify this notation as follows:\\nP(C, S,'), Document(metadata={}, page_content='We can simplify this notation as follows:\\nP(C, S,\\n¬P, E ,¬F)\\nThis represents the probability that you will go to college and that you will\\nstudy and be successful in your exams, but will not party or have fun. This\\nprobability can be calculated using the following rule:\\nwhere E is the evidence on which each x\\ni is dependent—in other words, in\\nthe Bayesian belief network, E consists of the nodes that are parents of xi.\\nFor example, using the network shown in Figure 12.3, we can calculate the'), Document(metadata={}, page_content='following probability:\\nPx x Px Eni\\ni\\nn\\n1\\n1\\n,,K( ) = ( )\\n=\\n∏\\nP P(F)\\ntrue 0.9\\nfalse 0.7'), Document(metadata={}, page_content='12.6 Bayesian Belief Networks 345\\nP(C,S,¬P,E,¬F) = P(C) /H11080P(S|C) /H11080P(¬P|C) /H11080P(E|S ∧¬ P) /H11080P(¬F|¬P)\\n= 0.2 /H110800.8 /H110800.4 /H110800.9 /H110800.3\\n= 0.01728\\nHence, for S we need to include in the product P(S|C) because S is only\\ndependent on C, and C is true in the situation we are examining. Similarly,\\nfor E we need to include P(E|S ∧¬ P) because E is dependent on S and on P,\\nand S is true and P is not true in the scenario.'), Document(metadata={}, page_content='and S is true and P is not true in the scenario.\\nWe can also calculate more complex conditional probabilities. In fact, this\\nis an extremely simple process, due to the way in which the belief network\\nhas been created. For example, let us look at the following conditional\\nprobability:\\nP(E\\n|F ∧¬ P ∧ S ∧ C)\\nThis is the probability that you will have success in your exams if you have\\nfun and study at college, but don’t party.'), Document(metadata={}, page_content='fun and study at college, but don’t party.\\nThe assumption behind the Bayesian belief network is that because there is\\nno direct connection between E and C, E is independent of C,g i v e n  S and\\nP. In other words, if we wish to calculate the following:\\nP(E|C ∧ S ∧ P)\\nwe can in fact drop C from this altogether, and simply obtain\\nP(E|S ∧ P) = 0.6\\nSimilarly, the more complex conditional probability above can be simpli-\\nfied by dropping F and C to give\\nP(E|S ∧¬ P) = 0.9'), Document(metadata={}, page_content='P(E|S ∧¬ P) = 0.9\\nHence, any calculation that we might need to make about this domain\\ncan be made simply using the conditional probability tables of the\\nbelief network.\\nSimilarly, we can make diagnoses about your college life by determining\\nposterior probabilities. For example, let us say that we know that you had\\nfun and studied hard while at college and we know that you succeeded in\\nyour exams, but we want to know whether you partied or not.'), Document(metadata={}, page_content='346 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nClearly, we know C, S, E, and F, but we do not know P. We need to deter-\\nmine the most likely value for P. Hence, we can compare the values of the\\nfollowing two expressions:\\nP(C ∧ S ∧ P ∧ E ∧ F ) = P(C) /H11080P(S|C) /H11080P(P|C) /H11080P(E|S ∧ P) /H11080P(F |P)\\n= 0.2 /H110800.8 /H110800.6 /H110800.6 /H110800.9\\n= 0.05184\\nP(C ∧ S ∧¬ P ∧ E ∧ F )= P(C) /H11080P(S|C) /H11080P(¬P|C) /H11080P(E|S ∧¬ P) /H11080P(F |¬P)'), Document(metadata={}, page_content='= 0.2 /H110800.8 /H110800.4 /H110800.9 /H110800.7\\n= 0.04032\\nHence, it is slightly more likely that you did party while at college than that\\nyou did not.\\n12.6.2 Example: Chapter Dependencies\\nWe will now examine a simple example of a slightly unusual Bayesian net-\\nwork. Rather than each node representing a hypothesis or a piece of diag-\\nnostic information, each node in the Bayesian network shown in Figure\\n12.4 represents a chapter of this book. The arcs between nodes represent'), Document(metadata={}, page_content='the dependencies between chapters. For example, the network shows that if\\nyou plan to read Chapter 8, which covers logical proof by resolution, it is a\\ngood idea to have read Chapter 7 on propositional and predicate logic first.\\nT o see this as a more standard belief network, we can consider each node to\\nrepresent the likelihood that you have read a given chapter and that a\\ndependency from Chapter 8 to Chapter 7, for example, represents the fact'), Document(metadata={}, page_content='that, if you have read Chapter 8, it is likely that you have also read Chapter\\n7. For this network to be useful to you in deciding which order to read the\\nchapters, you can think of the dependencies as being advice about whether\\nyou should read a particular chapter before reading another.\\n12.7 The Noisy-V Function\\nThus far, we have assumed that the probabilities contained with a joint\\nprobability distribution are unrelated to each other, in the sense that they'), Document(metadata={}, page_content='have been determined by observing the way in which events occur. In\\nsome situations, it can be possible to use the fact that events in a Bayesian'), Document(metadata={}, page_content='12.7 The Noisy-V Function 347\\n7\\nPropositional and\\nPredicate Logic\\n8\\nInference and Resolution\\nfor Problem Solving\\n9\\nRules and\\nExpert Systems\\n3\\nKnowledge\\nRepresentation\\n4\\nSearch\\nMethodologies\\n5\\nAdvanced\\nSearch\\n6\\nGame Playing\\n13\\nArtificial Life\\n14\\nGenetic\\nAlgorithms\\n19\\nIntelligent\\nAgents\\n10\\nIntroduction to\\nMachine Learning\\n11\\nNeural\\nNetworks\\n12\\nProbablilistic Reasoning and\\nBayesian Belief Networks\\n16\\nPlanning Methods\\n15\\nIntroduction to\\nPlanning\\n17\\nAdvanced Knowledge\\nRepresentation\\n18\\nFuzzy'), Document(metadata={}, page_content='17\\nAdvanced Knowledge\\nRepresentation\\n18\\nFuzzy\\nReasoning\\n20\\nUnderstanding\\nLanguage\\n21\\nMachine\\nVision\\nFigure 12.4\\nA Bayesian belief network that shows dependencies between chapters in this book\\nbelief network are related to each other by some kind of mathematical or\\nlogical relation.\\nClearly, logical relations such as ∧ and ∨, as defined in propositional logic,\\nwill not do because they do not provide a way to handle probabilities.'), Document(metadata={}, page_content='Fuzzy logic (which is described in Chapter 18) could provide suitable rela-\\ntions. Another useful class of relations is noisy logical relationships.'), Document(metadata={}, page_content='348 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nLet us return to our diagnosis example. We use P(A|B) to represent the\\nprobability that if one has a cold, then one will also have a high tempera-\\nture. Similarly, we used P(A\\n|C) to represent the probability that if one has\\nthe plague, then one will also have a high temperature.\\nWe have the following:\\nP(A|B) = 0.8\\nP(A|C) = 0.99\\nThe noisy- ∨ function is based on the assumption that the only possible'), Document(metadata={}, page_content='causes of a high temperature are a cold and the plague (i.e., that P(A|B ∨ C)\\n= 1. Clearly this is not true for our example, but we can fix this by including\\na leak node in the network, which represents all other possible causes.\\nHence, we will further include P(A\\n|D) = 0.9, where D is the leak node,\\nwhich represents other causes of a high temperature.\\nLet us now define the noise parameters for these relationships. The noise\\nparameters are simply defined as the conditional probabilities for ¬A,'), Document(metadata={}, page_content='rather than for A, and can be obtained as follows:\\nP(¬A|B) = 1 /H11002P(A|B) = 0.2\\nP(¬A|C) = 1 /H11002P(A|C) = 0.01\\nP(¬A|D) = 1 /H11002P(A|D) = 0.1\\nA further assumption in using the noisy- ∨ function is that the causes of a\\nhigh temperature are independent of each other and, similarly, that the\\nnoise parameters (whatever it is that stops each illness from causing a high\\ntemperature) are independent of each other.\\nThe noisy-\\n∨ function for B, C, and D is defined as follows:'), Document(metadata={}, page_content='∨ function for B, C, and D is defined as follows:\\nIf B, C, and D are all false, then P(A) = 0. Otherwise, P(¬A) is equal to the\\nproduct of the noise parameters for all the variables that are true. For\\nexample, if B is true and C and D are false, then P(\\n¬A) is equal to the noise\\nparameter for B, and so\\nP(A) = 1 /H110020.2\\n= 0.8\\nIf C and D are both true, and B is false, then P(¬A) is equal to the product\\nof the noise parameters for C and D, and so\\nP(A) = 1 /H11002(0.01 /H110030.1)\\n= 0.999'), Document(metadata={}, page_content='12.8 Bayes’ Optimal Classifier 349\\nNow we can define the noisy-∨ function for our diagnosis example:\\nB C D P(A) P( ¬A)\\nfalse false false 0 1\\nfalse false true 0.9 0.1\\nfalse true false 0.99 0.01\\nfalse true true 0.999 0.01 /H110030.1 = 0.001\\ntrue false false 0.8 0.2\\ntrue false true 0.98 0.2 /H110030.1 = 0.02\\ntrue true false 0.998 0.2 /H110030.01 = 0.002\\ntrue true true 0.9998 0.2 /H110030.01 /H110030.1 = 0.0002\\nNote that this noisy logical function is defined by just three conditional'), Document(metadata={}, page_content='probabilities, as opposed to needing to store eight values. For Bayesian\\nbelief networks used in the real world with hundreds or even thousands of\\nnodes, this can make a significant difference.\\n12.8 Bayes’ Optimal Classifier\\nIt is possible to use Bayesian reasoning to build a system that learns to clas-\\nsify data.\\nFor example, let us suppose that for a given piece of data, y, there are five\\npossible hypotheses, H\\n1 ... H5, each of which assigns a classification to y.'), Document(metadata={}, page_content='The classification, c, can be any value from a set C. For this example, let us\\nassume that C consists of the values true and false.\\nOur classifier knows the posterior probabilities of each of the five hypothe-\\nses to be the following:\\nP(H1|x1,... ,x n) = 0.2\\nP(H2|x1,... ,x n) = 0.3\\nP(H3|x1,... ,x n) = 0.1\\nP(H4|x1,... ,x n) = 0.25\\nP(H5|x1,... ,x n) = 0.15\\nwhere x1 to xn are the training data.'), Document(metadata={}, page_content='350 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nThe probability that the new item of data, y, should be classified with clas-\\nsification cj is defined by the following:\\nwhere m is the number of available hypotheses, which in this case is 5. The\\noptimal classification for y is the classification cj for which P(cj|x1 ... xn) is\\nthe highest.\\nIn our case, there are two classifications:\\nc1 = true\\nc2 = false\\nLet us suppose that hypotheses H3 and H5 each define y as true, while H1,'), Document(metadata={}, page_content='H2, and H4 define y as false.\\nHence, we have the following posterior probabilities:\\nP(false|H1) = 0 P(true |H1) = 1\\nP(false|H2) = 0 P(true |H2) = 1\\nP(false|H3) = 1 P(true |H3) = 0\\nP(false|H4) = 0 P(true |H4) = 1\\nP(false|H5) = 1 P(true |H5) = 0\\nThus we can calculate the posterior probabilities for each of the two possi-\\nble classifications for y as follows:\\nHence, the optimal classification for y is true.\\nP false x x P false H P H x xni i n\\ni\\n11\\n1\\n5\\n01 01 5\\n02 5\\nKK( ) = ( ) ⋅ ( )\\n=+\\n=\\n=\\n∑\\n..\\n.'), Document(metadata={}, page_content='1\\n5\\n01 01 5\\n02 5\\nKK( ) = ( ) ⋅ ( )\\n=+\\n=\\n=\\n∑\\n..\\n.\\nP true x x P true H P H x xni i n\\ni\\n11\\n1\\n5\\n02 03 02 5\\n07 5\\nKK( ) = ( ) ⋅ ( )\\n=++\\n=\\n=\\n∑\\n...\\n.\\nPc x x Pc h Ph x xjn j i in\\ni\\nm\\n11\\n1\\nKK( ) = ( ) ⋅ ( )\\n=\\n∑'), Document(metadata={}, page_content='12.9 The Naïve Bayes Classifier 351\\nThis method is known as an optimal classifier because it provides the best\\npossible classification system. Another classification system, given the same\\ndata, can only hope to classify unseen data as well as this method—it can-\\nnot do better than the optimal classifier, on average.\\n12.9 The Naïve Bayes Classifier\\nThe naïve Bayes classifier is a simple but effective learning system. Each'), Document(metadata={}, page_content='piece of data that is to be classified consists of a set of attributes, each of\\nwhich can take on a number of possible values. The data are then classified\\ninto a single classification.\\nT o identify the best classification for a particular instance of data ( d\\n1,... ,\\ndn), the posterior probability of each possible classification is calculated:\\nP(ci| d1,... ,d n)\\nwhere ci is the ith classification, from a set of |c| classifications.'), Document(metadata={}, page_content='The classification whose posterior probability is highest is chosen as the\\ncorrect classification for this set of data. The hypothesis that has the highest\\nposterior probability is often known as the maximum a posteriori ,o r\\nMAP hypothesis. In this case, we are looking for the MAP classification.\\nT o calculate the posterior probability, we can use Bayes’ theorem and\\nrewrite it as\\nBecause we are simply trying to find the highest probability, and because\\nP(d'), Document(metadata={}, page_content='P(d\\n1,... , dn) is a constant independent of ci, we can eliminate it and simply\\naim to find the classification ci, for which the following is maximized:\\nP(d1,... , dn|ci) /H11080P(ci)\\nThe naïve Bayes classifier now assumes that each of the attributes in the\\ndata item is independent of the others, in which case P(d1,... , dn|ci) can be\\nrewritten and the following value obtained:\\nPc Pd cij i\\nj\\nn\\n( ) ⋅ ( )\\n=\\n∏\\n1\\nPd d c Pc\\nPd d\\nni i\\nn\\n1\\n1\\n,,\\n,,\\nK\\nK\\n( ) ⋅ ( )\\n( )'), Document(metadata={}, page_content='352 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nThe naïve Bayes classifier selects a classification for a data set by finding the\\nclassification ci for which the above calculation is a maximum.\\nFor example, let us suppose that each data item consists of the attributes x,\\ny, and z, where x, y, and z are each integers in the range 1 to 4.\\nThe available classifications are A, B, and C.\\nThe example training data are as follows:\\nx y z Classification\\n232A\\n414B\\n132A\\n243A\\n424B\\n213C'), Document(metadata={}, page_content='232A\\n414B\\n132A\\n243A\\n424B\\n213C\\n124A\\n233B\\n224A\\n333C\\n321A\\n121B\\n214A\\n434C\\n224A\\nHence, we have 15 pieces of training data, each of which has been classified.\\nEight of the training data are classified as A, four as B, and three as C.\\nNow let us suppose that we are presented with a new piece of data, which is\\n(x = 2, y = 3, z = 4)\\nWe need to obtain the posterior probability of each of the three classifica-\\ntions, given this piece of training data. Note that if we were to attempt to cal-\\nculate P(c'), Document(metadata={}, page_content='culate P(c\\ni|x =2 , y =3 , z = 4) without having made the simplifying step that'), Document(metadata={}, page_content='12.9 The Naïve Bayes Classifier 353\\nwe took above, in assuming that the attribute values are independent of each\\nother, then we would need to have had many more items of training data to\\nproceed. The naïve Bayes classifier requires far fewer items of training data.\\nWe must now calculate each of the following:\\nP(A) /H11080P(x = 2\\n|A) /H11080P(y = 3|A) /H11080P(z = 4|A)\\nP(B) /H11080P(x = 2|B) /H11080P(y = 3|B) /H11080P(z = 4|B)\\nP(C) /H11080P(x = 2|C) /H11080P(y = 3|C) /H11080P(z = 4|C)'), Document(metadata={}, page_content='Hence, for classification A, we obtain the following:\\nThis was calculated by observing that of the 15 items of training data, 8 were\\nclassified as A, and so P(A) = 8/15. Similarly, of the eight items of training\\ndata that were classified asA,f i v eh a dx =2 ,t w oh a dy = 3, and four hadz =\\n4, and so P(x =2 |A) = 5/8, P(y =3 |A) = 2/8, and P(z =4 |A) = 4/8.\\nSimilarly, we obtain the posterior probability for category B:\\nand for category C:'), Document(metadata={}, page_content='and for category C:\\nHence, category A is chosen as the best category for this new piece of data,\\nwith category C as the second best choice.\\nLet us now suppose that we are to classify the following piece of unseen data:\\n(x = 1, y = 2, z = 2)\\nAs before, we would calculate the posterior probability for A.H o w e v e r ,i n\\ncalculating the probabilities for B and C, we would have problems. In the\\ncase of category B, we would have\\nP(x = 1|B) = 1/5\\nP(y = 2|B) = 1/5\\nP(z = 2|B) = 0'), Document(metadata={}, page_content='P(x = 1|B) = 1/5\\nP(y = 2|B) = 1/5\\nP(z = 2|B) = 0\\nBecause there are no training examples withz = 2 that were classified asB,w e\\nhave a posterior probability of 0. Similarly, for categoryC, we end up with\\n3\\n15\\n1\\n3\\n2\\n3\\n1\\n3 0 015⋅⋅⋅ = .\\n4\\n15\\n1\\n4\\n1\\n4\\n2\\n4 0 0083⋅⋅⋅ = .\\n8\\n15\\n5\\n8\\n2\\n8\\n4\\n8 0 0417⋅⋅⋅ = .'), Document(metadata={}, page_content='354 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nP(x = 1|C) = 0\\nP(y = 2|C) = 0\\nP(z = 2|C) = 0\\nIn this case, we clearly must select category A as the best choice for the data,\\nbut it appears to be based on a fairly inadequate comparison because insuf-\\nficient training data were available to properly compute posterior probabil-\\nities for the other categories.\\nThis problem can be avoided by using the m-estimate, as follows:'), Document(metadata={}, page_content='We wish to determine the probability of a particular attribute value, given a\\nparticular classification, such as P(x = 1\\n|C). We will estimate this probabil-\\nity according to the following formula:\\nwhere a = the number of training examples that exactly match our require-\\nments (e.g., for P(x = 1|C ), a is the number of training examples where x =\\n1 and that have been categorized as C. In this example,a is 0); b = the num-'), Document(metadata={}, page_content='ber of training examples that were classified in the current classification\\n(i.e., for P(x = 1\\n|C), b is the number of items of training data that were\\ngiven classification C ); p = an estimate of the probability that we are trying\\nto obtain (usually this is obtained by simply assuming that each possible\\nvalue is equally likely—hence, in our example, for P(x = 1\\n|C ), p = 1/4 =\\n0.25, as it would be for each of the other three possible values for x); m is a'), Document(metadata={}, page_content='constant value, known as the equivalent sample size.\\nFor example, let us use an equivalent sample size of 5 and determine the\\nbest classification for (x = 1, y = 2, z = 2):\\nFor category A, we first need to calculate the probability for each of the\\nthree attributes.\\nHence, for x = 1:\\nFor y = 2:\\n2 54\\n85 02 5\\n+\\n+ = .\\nam p\\nbm\\n+\\n+'), Document(metadata={}, page_content='12.9 The Naïve Bayes Classifier 355\\nFor z = 2:\\nHence, the posterior probability estimate for A is\\nSimilarly, we can now obtain posterior probability estimates for cate-\\ngories B and C:\\nFor category B, we obtain the following three probabilities:\\nThis gives us a posterior probability for category B as follows:\\nFinally, the posterior probability for category C can be obtained. We note\\nfirst that each of the three probabilities is the same because none of the'), Document(metadata={}, page_content='attribute values occur in the training data with category C.H e n c e ,t h e\\nprobability we use will be\\nHence, the posterior probability for category C is as follows:\\nHence, using this estimate for probability, we find that category B is the\\nbest match for the new data, and not category A as would have been\\nobtained using the simpler probability estimates.\\n3\\n15 0 156 0 156 0 156 0 0008⋅⋅⋅ =... .\\n0 54\\n35 0 156\\n+\\n+ = .\\n5\\n15 0 225 0 325 0 125 0 0091⋅⋅⋅ =... .\\n1 54\\n55 0 225\\n2 54\\n55 0 325\\n0 54'), Document(metadata={}, page_content='1 54\\n55 0 225\\n2 54\\n55 0 325\\n0 54\\n55 0 125\\n+\\n+ =\\n+\\n+ =\\n+\\n+ =., ., .\\n8\\n15 0 25 0 33 0 17 0 0076⋅⋅⋅ =... .\\n1 54\\n85 01 7\\n+\\n+ = .\\n3 54\\n85 03 3\\n+\\n+ = .'), Document(metadata={}, page_content='356 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nIt is possible to further simplify the naïve Bayes classifier by considering the val-\\nues to be positionless within each item of data. In other words, when consider-\\ning a new item of data, rather than assigning values to three attributes, we can\\nsimply think of the data as consisting of three values, whose order is arbitrary.\\nFor example, consider the piece of new data (2, 3, 4).'), Document(metadata={}, page_content='In this case, we use the same method as before, but rather than considering\\nthe probability that, for example, x = 2 when an item is classified as A,w e\\nsimply consider the probability that any attribute has value 2.\\nThis simplified version of the naïve Bayes classifier is often used in text clas-\\nsification applications. Here, the categories are often simply “relevant” and\\n“irrelevant, ” and the data to be classified consist of the words contained'), Document(metadata={}, page_content='within textual documents. For example, an item of data might be (“the, ”\\n“cat, ” “sat, ” “on, ” “the, ” “mat”). Training data would be presented in the\\nform of a set of documents that has been preclassified as relevant and a set\\nthat has been preclassified as irrelevant. This form of textual analysis is dis-\\ncussed in more detail in Chapter 20, which is concerned with information\\nretrieval and natural language processing.\\n12.10 Collaborative Filtering'), Document(metadata={}, page_content='12.10 Collaborative Filtering\\nA further practical use for Bayesian reasoning is in collaborative filtering.\\nCollaborative filtering is a technique that is increasingly used by online\\nstores (such as Amazon.com) to provide plausible suggestions to customers\\nbased on their previous purchases. The idea behind collaborative filtering\\ncan be stated very simply: if we know that Anne and Bob both like items A,\\nB, and C, and that Anne likes D, then it is reasonable to suppose that Bob'), Document(metadata={}, page_content='would also like D.\\nCollaborative filtering can be implemented in a number of ways, and the\\nBayesian inference has proved to be a successful method. This involves\\nworking with posterior probabilities such as the following:\\nP(Bob Likes Z \\n| B o b  l i k e s  A ,B o b  l i k e s  B ,... ,B o b  L i k e s  Y )\\nClearly, for this mechanism to work accurately, large amounts of data must\\nbe collected. Information about thousands of individuals is needed, and'), Document(metadata={}, page_content='information is required about dozens or hundreds of items for each indi-\\nvidual. In the case of commerce sites, this information can be collected on'), Document(metadata={}, page_content='12.11 Chapter Summary 357\\nP(Will Enjoy C) = 0.5\\nP(Will Enjoy C) = 0.7P(Will Enjoy C) = 0.9\\nYe s\\nEnjoys\\nBook A?\\nNo\\nYe s No\\nEnjoys\\nBook B?\\nFigure 12.5\\nA decision tree for collabo-\\nrative filtering\\nthe basis of assuming that if a user buys a book or a CD, then he probably\\nlikes it. More accurate data can be collected by asking users to rate products.\\nT o see how collaborative filtering works, consider the simple decision tree\\nshown in Figure 12.5.'), Document(metadata={}, page_content='shown in Figure 12.5.\\nThe decision tree in Figure 12.5 relates enjoyment of book C to informa-\\ntion about enjoyment of books A and B. It states that if you did not enjoy\\nbook A, then you will only have a 0.5 probability of enjoying book C.O n\\nthe other hand, if you did enjoy book A and also enjoyed book B, then you\\nwill have a 0.9 chance of enjoying book C.\\nA full collaborative filtering system would have one decision tree for each'), Document(metadata={}, page_content='item. A full Bayesian belief network would then be built from these deci-\\nsion trees, which can be used to make inferences about a new person on the\\nbasis of their likes or dislikes.\\n12.11 Chapter Summary\\n■ Probabilistic reasoning uses a notation similar to first-order predi-\\ncate calculus, but with the addition of terms such as P(A) = 0.5,\\nwhich states that the probability that A is true (or that A will\\noccur) is 0.5.\\n■ Conditional probability is defined as\\nPB A PB A\\nPA( ) = ∧( )\\n( )'), Document(metadata={}, page_content='358 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\n■ This means the probability that B will occur, given that we\\nalready know A.\\n■ The joint probability distribution (or joint) is used to represent\\nprobabilities concerning more than one variable.\\n■ Bayes’ theorem can be used to determine the posterior (condi-\\ntional) probability:\\n■ Bayesian concept learning involves selecting the most likely\\nhypothesis to explain a piece of data, using Bayes’ theorem to cal-'), Document(metadata={}, page_content='culate posterior probabilities.\\n■ A Bayesian belief network is an acyclic directed graph, where the\\nnodes in the graph represent evidence or hypotheses, and where an\\narc that connects two nodes represents a dependence between\\nthose two nodes.\\n■ Bayes’ optimal classifier uses Bayes’ theorem to learn to classify\\nitems of data. No other classifier can perform better than the opti-\\nmal classifier, on average.\\n■ The naïve Bayes classifier uses the simplifying assumption that all'), Document(metadata={}, page_content='the variables used to represent data for classification are independ-\\nent of each other.\\n■ Collaborative filtering is used to guess an individual’s likes or dis-\\nlikes based on prior information about other interests. One very\\nsuccessful method for collaborative filtering is to build a Bayesian\\nbelief network, based on a set of decision trees.\\n12.12 Review Questions\\n12.1 Explain what is meant by the conditional probability of an event.'), Document(metadata={}, page_content='12.2 “Bayes’ theorem uses a conditional probability and two prior prob-\\nabilities to calculate just one conditional probability. That doesn’t\\nsound like it’s very helpful. ” Discuss this comment.\\n12.3 Explain the purpose of the noisy-\\n∨ function.\\n12.4 Explain how Bayes’ theorem can be used to develop learning systems.\\nPB A PA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )'), Document(metadata={}, page_content='12.14 Further Reading 359\\n12.5 Explain how Bayes’ optimal classifier and the naïve Bayes classifier\\nwork.\\n12.6 Explain why collaborative filtering is such a useful technique. How\\nsuccessful do you believe it can be? What might limit its efficacy?\\n12.13 Exercises\\n12.1 Implement a Bayesian belief network in the programming lan-\\nguage of your choice to represent a subject in which you are inter-\\nested (for example, you might use it to diagnose medical'), Document(metadata={}, page_content='conditions from symptoms, or to deduce a band from a descrip-\\ntion of the band’s music).\\n12.2 Implement the naïve Bayes classifier in the programming language\\nof your choice, and use it to classify pages of text, based on which\\nwords appear on the page. T o do this, you will first need to train the\\nclassifier with preclassified examples of pages. Y ou should choose\\ntwo classifications: interesting and not interesting, and try to make'), Document(metadata={}, page_content='the interesting category fairly narrow: for example, it might be\\n“pages about Bayesian reasoning. ” What happens if you make the\\ncategory very broad (such as “pages I find interesting”)?\\n12.3 Use the following facts to calculate normalized values for P(B\\n|A)\\nand P(¬B|A):\\nP(A) = 0.0025\\nP(B) = 0.015\\nP(A\\n|B) = 0.6\\nP(A|¬B) = 0.25\\n12.4 Examine the collaborative filtering mechanism used by an online\\nshopping system. How effective do you think it is? Might there be'), Document(metadata={}, page_content='more effective methods to achieve the same goal? What kinds of\\nmistakes does the mechanism make? In what situations does it per-\\nform well?\\n12.14 Further Reading\\nIf you are interested in seeing the original proposal of Bayes’ theorem by\\nThomas Bayes from 1763, you can find it in Swinburne (2002). Y ou can read'), Document(metadata={}, page_content='360 CHAPTER 12 Probabilistic Reasoning and Bayesian Belief Networks\\nmore about collaborative filtering by exploring the writings of Patti Maes. A\\nless technical explanation can be found in Riedl and Konstan (2002).\\nModeling the Internet and the Web: Probabilistic Methods and Algorithms by\\nPierre Baldi, Paolo Frasconi, and Padhraic Smyth (2003 – John Wiley & Sons)\\nBayesian Theory by José M. Bernardo and Adrian F. M. Smith (2001 – John\\nWiley & Sons)'), Document(metadata={}, page_content='Wiley & Sons)\\nEmpirical Analysis of Predictive Algorithms for Collaborative Filtering by\\nJohn S. Breese, David Heckerman, and Carl Kadie (1998 – in Proceedings of\\nthe Fourteenth Conference on Uncertainty in Artificial Intelligence)\\nExpert Systems and Probabilistic Network Models by Enrique Castillo, Jose\\nManuel Gutierrez, and Ali S. Hadi (1997 – Springer V erlag)\\nProbabilistic Networks and Expert Systems edited by Robert G. Cowell (1999\\n– Springer V erlag)'), Document(metadata={}, page_content='– Springer V erlag)\\nBayesian Methods for Nonlinear Classification and Regression by David G. T.\\nDenison, Christopher C. Holmes, Bani K. Mallick, and Adrian F. M. Smith\\n(2002 – John Wiley & Sons)\\nBayesian Data Analysis by Andrew Gelman, Donald B. Rubin, and Hal S.\\nStern (2003 – CRC Press)\\nProbabilistic Theory of Pattern Recognition by Luc Devroye, Laszlo Gyorfi,\\nand Gabor Lugosi (1998 – Springer V erlag)\\nMaking Decisions by D. V . Lindley (1991 – John Wiley & Sons)'), Document(metadata={}, page_content='Learning Bayesian Networksby Richard E. Neapolitan (2003 – Prentice Hall)\\nProbabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference\\nby Judea Pearl (1997 – Morgan Kaufmann)\\nWord of Mouse: The Marketing Power of Collaborative Filtering by John\\nRiedl and Joseph Konstan (2002–Warner Books)\\nThe Bayesian Choice: From Decision-Theoretic Foundations to Computa-\\ntional Implementation by Christian P . Robert (2001 – Springer V erlag)'), Document(metadata={}, page_content='Monte Carlo Statistical Methods by Christian P . Robert and George Casella\\n(1999 – Springer V erlag)\\nThe Evidential Foundations of Probabilistic Reasoning by David A. Schum\\n(2001 – Northwestern University Press)'), Document(metadata={}, page_content='12.14 Further Reading 361\\nSocial Information Filtering: Algorithms for Automating “Word of Mouth by\\nU. Shardanand and P . Maes (1995 – in Proceedings of CHI’95—Human Fac-\\ntors in Computing Systems, pp. 210–217)\\nData Analysis: A Bayesian Tutorial by D. S. Sivia (1996 – Oxford Univer-\\nsity Press)\\nBayes’s Theorem (Proceedings of the British Academy, Vol. 113) edited by\\nRichard Swinburne (2002 – British Academy)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='13CHAPTER\\nArtificial Life: Learning\\nthrough Emergent Behavior\\nNatural Selection is the blind watchmaker, blind because it does not see\\nahead, does not plan consequences, has no purpose in view. Yet the living\\nresults of natural selection overwhelmingly impress us with the appearance of\\ndesign as if by a master watchmaker, impress us with the illusion of design\\nand planning.\\n—Richard Dawkins, The Blind Watchmaker\\nAgents can become more complex in two ways. First, a designer can identify a'), Document(metadata={}, page_content='functionality that the agent needs to achieve, then investigate possible behav-\\niors that could realize the functionality, and then introduce various mecha-\\nnisms that give rise to the behavior. Second, existing behavior systems in\\ninteraction with each other and the environment can show side effects, in other\\nwords, emergent behavior.\\n—Luc Steels, The Artificial Life Roots of Artificial Intelligence\\nAll things are artificial, for nature is the art of God.\\n—Sir Thomas Browne, Religio Medici'), Document(metadata={}, page_content='—Sir Thomas Browne, Religio Medici\\n13.1 Introduction\\nThis chapter provides a broad introduction to the subject of Artificial Life.\\nArtificial Life techniques use methods modeled on the behavior of living\\nsystems in much the same way that Artificial Intelligence techniques use\\nmethods modeled on the way the human brain works. Many Artificial Life'), Document(metadata={}, page_content='364 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\ntechniques (in particular, genetic algorithms) are an established part of the\\nfield of Artificial Intelligence.\\nThis chapter starts by attempting to define “life”—a difficult problem, but\\none that needs to be discussed in order to consider Artificial Life. Emergent\\nbehavior is one of the most important concepts of Artificial Life—the idea\\nthat systems that are defined in a simple way can produce their own behav-'), Document(metadata={}, page_content='ior, which can be remarkably complex.\\nThe chapter introduces a number of Artificial Life techniques, many of which\\nillustrate emergent behavior. T echniques such as cellular automata, genetic\\nprogramming, evolutionary programming, and classifier systems are dis-\\ncussed. Discussion of classifier systems provides an introduction to the subject\\nof genetic algorithms, which is covered in much more detail in Chapter 14.\\nThe chapter also looks at the ways in which systems might be built that are'), Document(metadata={}, page_content='self-reproducing, and a number of systems are explored that model evolution.\\nAs you will see in this chapter and the next, Artificial Life (or A-Life) tech-\\nniques build on a number of Artificial Intelligence techniques and provide\\nways in which systems can adapt (or evolve) to changing conditions. Classi-\\nfier systems, which are explored in Section 13.12, show how the addition of\\nevolutionary techniques to production systems (which are discussed in'), Document(metadata={}, page_content='Chapter 9) can enable them to respond to changes in their environment\\nand to learn to deal with unexpected situations.\\n13.2 What Is Life?\\nWhat does it mean to be alive? What differentiates living creatures from\\nnonliving things? This is a question to which there is still no satisfactory\\nanswer. Aristotle, the Greek philosopher, said that a thing was alive if it\\ncould “nourish itself and decay. ” The following is a list of properties that are\\nalso often considered to be indicative of life:'), Document(metadata={}, page_content='also often considered to be indicative of life:\\n■ self-reproduction\\n■ ability to evolve by Darwinian natural selection\\n■ response to stimuli\\n■ ability to die\\n■ growth or expansion'), Document(metadata={}, page_content='13.3 Emergent Behavior 365\\nEven this short list has problems. Mules are certainly alive, but they cannot\\nreproduce. The question of whether viruses are alive is not universally\\nagreed. Most lists of properties of life exclude some living creatures or\\ninclude some things that may not be alive.\\nIn other words, it is very difficult to define what life is. Hence, it is not nec-\\nessarily easy to exclude artificial entities—even patterns of data or com-'), Document(metadata={}, page_content='puter programs such as computer viruses. In this chapter, we examine\\nsystems that exhibit many properties of life, but we are not necessarily\\nclaiming that these systems are actually alive. The important thing is that\\nwe are building processes and systems modeled on the ways in which living\\norganisms behave and evolve. In much the same way that Artificial Intelli-\\ngence uses techniques modeled on the way in which the human brain'), Document(metadata={}, page_content='works, so Artificial Life, a somewhat wider subject in some ways, uses tech-\\nniques modeled on the way in which life works.\\n13.3 Emergent Behavior\\nThe idea of emergent behavior is fundamental to the field of Artificial Life.\\nBy observing the ways in which patterns of sensible behavior emerge in real\\nlife, researchers have been able to develop systems that can produce their\\nown behavior. We have seen this idea already: CYC, the system that has'), Document(metadata={}, page_content='thousands of pieces of information, has been able to form its own analogies\\nabout the world by observing the patterns in the data that it sees.\\nMuch of Artificial Life is based around a simple idea: Evolution works. The\\nprocess or mechanism of evolution may not be fully understood, but the\\nfact remains that complex creatures have evolved in such a way that they\\nare able to survive, despite changing environments, lack of food or warmth,\\nand other complications that are presented by nature.'), Document(metadata={}, page_content='The reason that evolution works is that creatures that are “successful” in\\nsome way survive and reproduce. If a creature survives and reproduces,\\nthen it will pass on its genetic structure to its offspring. Although this\\nprocess takes place over hundreds of thousands of years, it is possible to use\\nmethods that are based on the same principle that can take place on a com-\\nputer within hours, minutes, or even seconds.\\nOne of the early principles of Artificial Life is that complex behavior can be'), Document(metadata={}, page_content='generated (i.e., it emerges) from simple rules. An excellent example of this'), Document(metadata={}, page_content='366 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nprinciple is the Boids system, developed by Craig Reynolds in 1987. The\\nidea of this system was that it would model the flocking behavior of birds.\\nRather than having an overall mechanism for controlling the flock, his sys-\\ntem had just a few simple rules that each individual bird obeyed.\\nOne rule ensured that each boid would stay near to other boids by having'), Document(metadata={}, page_content='each boid tend to move toward the center of gravity of the whole flock.\\nAnother rule ensured that boids did not collide with each other.\\nIn running his simulation, Reynolds found that the boids moved in a way\\nextremely similar to the way in which flocks of birds and shoals of fish\\nmove. This technique is now widely used in animation software and in pro-\\nducing computer graphics for movies.\\nOne of the most interesting aspects of the boids was the way in which their'), Document(metadata={}, page_content='behavior emerged from the rules. For example, no one told the system how\\nto behave when the flock encountered obstacles. Reynolds found that when\\npresented with a series of pillar-shaped obstacles, his computer-simulated\\nboids split into two separate flocks to go around the pillars and then\\nrejoined on the other side of the pillars.\\nClearly, the boids knew that they could not fly through obstacles and that\\nthey should not collide with the obstacles, but the behavior that enabled'), Document(metadata={}, page_content='the flock to navigate the obstacles was entirely emergent.\\nThis shows how complex behavior can emerge from simple rules. As we\\nwill see later in this chapter, the introduction of evolutionary methods can\\nproduce even more startling results.\\n13.4 Finite State Automata\\nA finite state automaton (FSA) is a simple device that has a finite set of\\nstates and an input string (often thought of as being on a tape, running\\nthrough a device that can read one symbol at a time). Each symbol that the'), Document(metadata={}, page_content='FSA reads in is compared with a rule that dictates which state to move to\\nfrom that state, with that input. After reading the entire input, the finite\\nstate machine is either in an accepting state, which means its answer is\\n“yes” to some question, or it is in some other state, in which case the answer\\nis “no. ” A finite state machine can be represented by a diagram such as the\\none in Figure 13.1.'), Document(metadata={}, page_content='13.4 Finite State Automata 367\\nbb\\na\\na\\n1 2\\nFigure 13.1\\nA finite state automaton\\nThe FSA in Figure 13.1 determines whether an input string has an even\\nnumber of a’s or not. The two circles represent the two states, 1 and 2,\\nwhich the FSA can be in. The possible input symbols are a and b. The arrow\\nat the top left of the diagram shows where the FSA starts: in state 1. When\\nthe FSA is in state 1, it will stay there until it receives an a, which sends it to'), Document(metadata={}, page_content='state 2. Similarly, when it is in state 2, it will stay there until it receives an a,\\nwhich will send it back to state 1.\\nHence, if the FSA receives an input with an even number of a’s, it will finish\\nin state 1, otherwise it will finish in state 2.\\nState 1 is an accepting state, which is shown by its having a thicker outline\\nthan state 2.\\nFSAs provide an extremely useful tool for Artificial Intelligence, and com-\\nputer science in general. They also provide an interesting model for Artifi-'), Document(metadata={}, page_content='cial Life, as we see elsewhere in this chapter (Sections 13.5 and 13.10).\\nThe FSA in Figure 13.1 has just two states, but in theory an FSA could\\nhave an extremely large number of states and a much larger vocabulary of\\ninput symbols.\\nA rather simplistic view of living entities might be to consider that each one\\nis simply an FSA. In other words, place an entity in a particular situation\\nand provide it with certain inputs from its environment, and its response'), Document(metadata={}, page_content='will be deterministically decided by a set of rules. Of course, this is not how\\nliving creatures work at all, but it is possible to mimic certain behaviors of\\nliving creatures using FSAs. For example, boids can be thought of as FSAs.\\nEach boid has a set of inputs (its own location and speeds, and information\\nabout where the other birds and obstacles are) and a state (which direction'), Document(metadata={}, page_content='368 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nit is flying and how fast) and a set of rules that determine which state to\\nmove to from each state, according to the input data.\\nIn the next section, we see how much simpler automata can be built, which\\nwhen combined together can produce extremely interesting behavior.\\n13.5 Cellular Automata\\n13.5.1 Conway’ s Life\\nConway’s Life, also known as the Game of Life, is a system that uses a grid'), Document(metadata={}, page_content='of squares and a set of simple rules. Conway’s Life is an excellent illustra-\\ntion of the power of emergent behavior.\\nConway’s Life consists of a two-dimensional grid of squares (or cells), each\\nof which can be either alive or dead. This could be considered to model a\\nreal-world terrain, where each square represented a piece of land, and a\\nsquare would be considered alive if it had a living creature in it and dead\\n(or empty) if it did not.'), Document(metadata={}, page_content='(or empty) if it did not.\\nAny given configuration is changed into a successive configuration, or gen-\\neration, by the application of a set of four rules. These rules determine\\nwhat will happen to each cell on the basis of its eight neighbors (assuming\\nan infinite grid). The rules can be defined as follows:\\n1. If a dead cell has exactly three living neighbors in one generation,\\nthen those neighbors will reproduce in the next generation, and\\nthe empty cell will “come to life. ”'), Document(metadata={}, page_content='the empty cell will “come to life. ”\\n2. If a living cell has two or three living neighbors, then that cell is\\n“happy, ” and remains alive in the following generation.\\n3. If a living cell has less than two living neighbors, then it dies of\\nloneliness in the next generation.\\n4. If a living cell has more than three living neighbors, then it dies of\\novercrowding in the next generation.\\nFigure 13.2 shows a set of configurations of Conway’s Life, where each cell'), Document(metadata={}, page_content='is either empty (dead) or contains an O (in which case it is alive). The first\\nconfiguration shown in Figure 13.2 is transformed by the rules to the sec-\\nond configuration in the next generation. The second is transformed into\\nthe third, and so on. Hence, the five illustrations in Figure 13.2 show five\\nsuccessive generations of Conway’s Life.'), Document(metadata={}, page_content='13.5 Cellular Automata 369\\nFigure 13.2\\nFive successive genera-\\ntions of Conway’ s Life\\nThe most interesting aspect of this particular sequence is that the final con-\\nfiguration is almost exactly the same as the first configuration, except that it\\nhas been shifted across and down by one cell. Clearly, by applying the same\\nrules again, the shape will continue to move in this way. This particular\\nconfiguration is known as a glider.'), Document(metadata={}, page_content='configuration is known as a glider.\\nConway’s Life becomes more interesting when played over a larger grid (for\\nexample, using a computer monitor with each pixel representing a cell) and\\nwith the starting configuration selected randomly. In some cases, after a\\nnumber of generations, all the cells in the grid have died. In other cases, the\\nsystem reaches a stable state where each generation is the same as the previ-\\nous generation, or where the system oscillates between a few patterns.'), Document(metadata={}, page_content='One very interesting pattern is known as a glider gun. This configuration\\nconstantly spews out gliders, which then glide away from it. In this way, we\\ncan see a system that in a very simple way can be said to reproduce. Rather\\nthan just changing, or stagnating, the system is able to constantly produce\\nnew “entities, ” if we can consider a glider to be an entity. We will see how this\\nconcept can be more reasonably applied in other areas of Artificial Life.'), Document(metadata={}, page_content='Conway’s Life is an example of a cellular automaton. A cellular automaton\\nconsists of a set of cells, each of which contains data (in this case, “alive” or\\n“dead” or “empty” or “full” or 1 or 0). The system is an automaton, or com-\\nputer, in the sense that it acts on a set of input data to produce an output.\\nCellular automata can use more complex sets of rules, and cells can be\\nallowed many more possible values than the two used in Conway’s Life.'), Document(metadata={}, page_content='John Von Neumann and Stanislaw Ulam invented the concept of Cellular\\nautomata in the 1950s. Ulam and Von Neumann considered each cell in the\\ngrid of the cellular automata to be a finite state automaton where each cell’s\\nstate could be determined by applying a set of rules to its previous state. In\\ntheir system, each cell could be in one of 29 possible states.\\nBy applying the rules of the system to an initial configuration, cells would\\ntransform their neighbors into different kinds of cells.'), Document(metadata={}, page_content='370 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nFigure 13.3\\nFive generations of a one-\\ndimensional cellular\\nautomaton\\nVon Neumann’s idea was that in this way, a machine could be created that\\ncould reproduce itself. This is a profound idea and is something that is still\\nresearched today, as we discuss in Section 13.6.\\n13.5.2 One-Dimensional Cellular Automata\\nThe cellular automata that Von Neumann and Conway invented were two'), Document(metadata={}, page_content='dimensional, so that each cell has eight neighbors. Much interesting\\nresearch has been carried out on one-dimensional cellular automata, where\\ncells are arranged in a line, rather than a grid, and each cell has two direct\\nneighbors. It is usual in such systems for the rules to be based not just on\\nthe immediate neighbors, but on the cells one square away from those as\\nwell. So a cell is affected by a total of five values: its four neighbors (two on\\neach side), as well as its own value.'), Document(metadata={}, page_content='each side), as well as its own value.\\nFor example, we could create a rule that says that if a living cell has at least\\ntwo living neighbors on either side of it, then it will live, but if it has less\\nthan two neighbors, then it will die. We will further say that if a dead cell\\nhas at least three living neighbors, then it will come to life.\\nThis kind of rule is known as a legal rule, in that if a cell is not alive, and has'), Document(metadata={}, page_content='no living neighbors, then it will stay dead. It is also known as a totalistic\\nrule, which means that the next state of a cell is determined solely by the\\ntotal number of living cells there are in its vicinity. A totalistic rule does not\\ntake into account which side the living cells are on, for example.\\nLegal and totalistic rules for cellular automata can be expressed as a single\\nfive-bit number. Our rule above would be expressed as follows:\\n12345\\n00111'), Document(metadata={}, page_content='12345\\n00111\\nFigure 13.3 shows a cellular automaton in which this rule has been applied to\\nproduce five successive generations. The first line of the diagram shows the\\nfirst generation. The second line shows the second generation, and so on.'), Document(metadata={}, page_content='13.6 Self-Reproducing Systems 371\\nClearly, this particular cellular automaton is not going to produce very inter-\\nesting behavior because it will eventually fill up the entire system with life\\nand will reach a stable (orstagnant) configuration that will never change.\\nBecause the rules consist of five bits, there are 32 possible rules for such cel-\\nlular automata, some of which will produce much more interesting pat-\\nterns than the one shown in Figure 13.3. Some sets of rules have been used'), Document(metadata={}, page_content='to produce patterns that quite closely resemble the patterns that grow nat-\\nurally on some sea shells.\\nAgain, we are seeing how complexity can emerge from a simple set of rules.\\nCellular automata have been applied in a number of fields, including\\npathology, where they are used to analyze blood smears. They have also\\nbeen applied in the field of image processing, and it has been suggested that\\ncellular automata rules resemble the manner in which the visual cortex is\\nstructured.'), Document(metadata={}, page_content='structured.\\n13.6 Self-Reproducing Systems\\nAs we have already seen, Von Neumann postulated the idea of a self-repro-\\nducing system based on cellular automata in the 1950s. Another form of\\nself-reproducing system was invented by Christopher Langton at the end of\\nthe 1970s. Langton’s aim was to develop the simplest system that could\\nreproduce itself.\\nHis creations were called loops. Each loop consisted of just 94 cells,'), Document(metadata={}, page_content='arranged in a shape rather like a lower-case letter q. Each cell could take one\\nof eight possible values. Each loop contained all the information that was\\nneeded to produce another identical loop, which in turn could produce a\\nfurther loop, and so on.\\nThe loops’ reproduction was carried out through the tail of the q shape,\\nwhich contained cells that grew to produce a new loop, which then broke\\noff once it had fully formed.'), Document(metadata={}, page_content='off once it had fully formed.\\nOf course, the loops were not real “living” creatures in any way, but they did\\nexhibit one important property of life: reproduction. The loops only “existed”\\nas data in a computer, or as images on a screen, but they represented a step\\nforward—the first artificial system that was capable of self-reproducing.'), Document(metadata={}, page_content='372 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nVon Neumann’s work predicted that it would be feasible to have a self-\\nreproducing system in the real world: he imagined robots that could\\nvisit other planets, mine minerals from the planet, refine those miner-\\nals, and create new versions of themselves from the materials they\\nfound.\\n13.7 Evolution\\nEach change that occurs from one generation to the next in cellular'), Document(metadata={}, page_content='automata such as Conway’s Life is simple. By running a large number of\\ngenerations of such a system, reasonably complex patterns can be observed.\\nIn his book,The Blind Watchmaker, Richard Dawkins (1991) describes such\\nchanges as single-step selection . By contrast, the process of evolution\\ninvolves cumulative selection.\\nCumulative selection means that at each step, existing entities or items of\\ndata “reproduce” to form new entities. Rather than each step being based'), Document(metadata={}, page_content='on simple rules that define how one state will change to the next, the next\\nstate is based on the best features of the previous state and, in general,\\nimproves on that previous state.\\nIn nature, natural selection is the process that chooses which entities will\\nreproduce. Darwin’s idea of “survival of the fittest” means that the creatures\\nthat manage to reproduce are probably the strongest, in some way, and so sub-\\nsequent generations will tend to inherit stronger features from their parents.'), Document(metadata={}, page_content='In many Artificial Life systems, natural selection is replaced by artificial\\nselection—for example, in some cases, a person chooses which entity\\nshould reproduce from a population of entities. In The Blind Watchmaker,\\nDawkins described biomorphs, a system of artificial selection that he orig-\\ninally designed to evolve tree-like shapes. The shape of any biomorph was\\ndetermined by just nine variables, or genes. Each gene represented a feature'), Document(metadata={}, page_content='of the biomorphs, such as the branching angle between branches or the\\nlength of branches.\\nThe system produced a set of slightly different biomorphs, and the user\\ncould select one to reproduce. The next generation would consist of a set of\\nbiomorphs that were very similar to the one chosen by the user, but each\\none would differ slightly in one gene. This process of modification is\\nknown as mutation, and we see how it is applied in genetic algorithms in\\nChapter 14.'), Document(metadata={}, page_content='13.8 Evolution Strategies 373\\nAlthough Dawkins had intended his biomorphs to resemble trees, after\\nrunning just a few generations of his system, he found that the biomorphs\\nwere evolving into shapes that looked like insects. His system had produced\\ncreatures that he had never imagined it would be capable of generating.\\nThis is another example of emergent behavior: complex changes emerging\\nfrom simple rules.\\nDawkins’ biomorphs exhibited artificial selection, where a human chose'), Document(metadata={}, page_content='which entities could reproduce in each generation. Systems that more closely\\nresemble natural selection are also possible. As we see in Chapter 14, genetic\\nalgorithms are evolved to solve particular problems by using a measure of\\nfitness based on how close each algorithm comes to solving the problem.\\n13.8 Evolution Strategies\\nEvolution strategies were first developed in the 1960s by Ingo Rechenberg\\nand Hans-Paul Schwefel as a way of solving engineering problems. The idea'), Document(metadata={}, page_content='is similar to hill climbing, which we see in Chapter 4. A possible solution to\\nthe problem is represented as a set of parameters. The initial generation is a\\nrandomly selected set of parameters, and each subsequent generation is pro-\\nduced by adding anormally distributed value to each of the parameters.\\n(The normally distributed mutation values have a mean of zero, and smaller\\nvalues are more likely than larger values. This is based on the fact that in'), Document(metadata={}, page_content='nature, mutations tend to be small changes, rather than large changes).\\nIf the new set of parameters (the offspring) gives a better solution than the\\nprevious set (theparent), then the process continues with the offspring. Oth-\\nerwise, the offspring is rejected, and a new offspring is generated for the parent.\\nNote that in evolving evolution strategies, each offspring is produced from\\njust one parent. In other words, the system uses asexual reproduction.W e'), Document(metadata={}, page_content='will see how sexual reproduction can be used to develop artificial evolu-\\ntion systems where offspring are produced from more than one parent,\\noften combining good features of each parent to produce offspring that are\\n“better, ” by some metric.\\nThe idea of metrics is an important one, when applying artificial natural\\nselection. T o have a system that evolves entities without human interven-\\ntion, a metric is needed that can be used to determine fitness. A fitter entity'), Document(metadata={}, page_content='is one that is “better” by some criteria: better able to solve a particular prob-\\nlem, stronger, or more beautiful, for example.'), Document(metadata={}, page_content='374 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nlog\\n+\\n*\\n2xy\\nFigure 13.4\\nTree representation of \\n2x +  log y\\nSelecting a suitable metric is usually the first hurdle when developing an evo-\\nlutionary solution to a problem. For example, to evolve a solution to the prob-\\nlem of sorting a set of numbers, a metric could count how many numbers\\nwere in the correct locations. A more sophisticated metric might count how'), Document(metadata={}, page_content='many numbers were in the correct order, even if not in the right locations.\\n13.9 Genetic Programming\\nGenetic programming was developed by John Koza in the early 1990s.\\nKoza used genetic programming to evolve solutions to problems in the\\nform of LISP programs, or S-expressions (symbolic expressions). LISP\\nprograms and the data manipulated by LISP programs are both S-expres-\\nsions, and so LISP programs can manipulate each other, or even themselves.'), Document(metadata={}, page_content='Genetic programming can be thought of as a way to search through the\\nspace of possible S-expressions for the one that best solves a given problem.\\nEach S-expression can be represented as a tree, with the operators and val-\\nues in the expression at nodes in the tree. For example, Figure 13.4 shows\\nthe tree for the expression 2x + log y, which in LISP would be expressed as\\n+(*(2x) (log (y))).\\nT o apply genetic programming, the following five steps must first be taken:'), Document(metadata={}, page_content='1. Select a set of terminals.\\nThe terminals are the variables to be used in expressions. In the\\nexample above, the terminals are x and y.\\n2. Select a set of primitive functions.\\nThe primitive functions are the functions that are allowed in our\\nexpressions. In the expression above, we have used the primitive\\nfunctions *, +, and log. We could allow other primitive functions,\\ndepending on the nature of the problem that is to be solved.'), Document(metadata={}, page_content='13.10 Evolutionary Programming 375\\n3. Select a fitness function.\\nThe fitness function is a way of determining how successful or fit\\nany given expression is. Typically, this will involve applying the S-\\nexpression as a program to a set of sample data and seeing how\\nclose to the correct solutions the results are.\\n4. Select parameters for the system.\\nThe parameters to be chosen include the population size (that is,\\nhow many entities will exist in each generation) and the number of'), Document(metadata={}, page_content='generations to run the system for.\\n5. Select a method for determining the result of a run.\\nEach run of the system will produce a new generation. A method\\nneeds to be chosen that will determine which program that has\\nbeen generated so far is the best. Similarly, a termination condition\\nis often chosen that enables the system to stop when it has found a\\nperfect solution.\\nT o produce a new generation, mutation and crossover are applied to the'), Document(metadata={}, page_content='current generation. Mutation simply involves making small changes to an\\nS-expression (such as replacing the “+” operator with the “–” operator, or\\nincreasing the value of a constant from 2 to 2.1).\\nCrossover involves taking two entities from the population and combining\\nfeatures of each to produce a new offspring. In Chapter 14, we see how\\ncrossover is an important aspect of genetic algorithms.\\n13.10 Evolutionary Programming\\nEvolutionary programming (EP) was invented by Lawrence Fogel in 1966.'), Document(metadata={}, page_content='EP was used to evolve solutions to the problem of working out what the\\nnext symbol would be in a finite sequence of symbols: a\\n1, a2, a3, a4, a5,...,\\nan. The method works by evolving FSAs. In the first generation, a set of ran-\\ndom FSAs is generated. The next generation is evolved by producing one\\noffspring from each FSA in the previous generation. Reproduction involves\\napplying one of five mutation operators:\\n1. changing an output symbol\\n2. changing a state transition'), Document(metadata={}, page_content='376 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\n3. adding a state\\n4. deleting a state\\n5. changing the initial state\\nT o determine the fitness of an FSA, it is run against each initial subset of the\\nlist of symbols that exists so far and its prediction compared with the actual\\nnext values.\\nHence, if the existing sequence is 1,2,3,4,5,6,7,8,9, then the FSA would first\\nbe run just with the number 1, and its output compared with 2. Next, it'), Document(metadata={}, page_content='would be run with the sequence 1,2 and the output compared with 3.\\nFinally, it would be run with 1,2,3,4,5,6,7,8 and its output compared with 9.\\nA successful FSA would probably generate 10 as the next number in the\\ncomplete sequence.\\nEach generation contains the parents from the previous generation and\\neach parent’s offspring. Half of these FSAs are allowed to survive—the ones\\nthat make the most correct guesses on the subsequences. These FSAs are'), Document(metadata={}, page_content='then allowed to reproduce to generate the next generation, and so on.\\n13.11 L-Systems\\nIn the late 1960s, a biologist, Aristid Lindenmayer, developed a set of rules\\nto describe the growth patterns of plants. His “plants” consisted of cells,\\neach of which could take on one of two values— a or b. These represented\\nthe two types of cells seen in the early growth stages of a particular type of\\nalgae. The rules Lindenmayer applied to the cells were as follows:\\nRule 1: a -> ab\\nRule 2: b -> a'), Document(metadata={}, page_content='Rule 1: a -> ab\\nRule 2: b -> a\\nHence, if we start out with a in the first step, then on the next step this will\\nbecome ab. On the next step, this will become aba, followed by abaab and\\nthen abaababa. This pattern of growth fairly closely matched the growth\\npatterns of the plants that Lindenmayer was studying.\\nThese sets of rules were called L-systems, and it turned out that L-systems\\ncould be used to produce images of remarkably lifelike artificial plants. By'), Document(metadata={}, page_content='applying the L-system rules, strings of thousands of cells could be gener-\\nated, and by interpreting the symbols in those strings as branching pat-\\nterns, images of plant-like structures could be created. By using graphic'), Document(metadata={}, page_content='13.12 Classifier Systems 377\\nrendering techniques, images can be generated from L-systems that are\\nindistinguishable from real plants. These images are often used in com-\\nputer games and films.\\nPerhaps more usefully, L-systems can also be used to model biological sys-\\ntems, such as the development processes involved in the growth of plants,\\nthus making it possible to study the workings of life itself, by simulating it\\nin a virtual laboratory.\\n13.12 Classifier Systems'), Document(metadata={}, page_content='in a virtual laboratory.\\n13.12 Classifier Systems\\nClassifier systems, based on the expert systems we saw in Chapter 9, were\\ninvented by John Holland in 1986. As with expert systems, a classifier sys-\\ntem consists of a set of rules that tell the system how to behave in particular\\ncircumstances—how to respond to features in its environment.\\nA classifier system, though, also has the ability to generate better responses\\nand to learn to respond to unfamiliar situations by treating its rules as a'), Document(metadata={}, page_content='population to be evolved.\\nThe classifier system consists of the following components:\\n■ detectors that receive inputs from the environment\\n■ effectors that send outputs to the environment, and carry out actions\\n■ a rule system, which consists of a population of classifiers; a vari-\\nable measure of fitness is associated with each rule\\n■ detectors that receive feedback from the environment concerning\\nhow well the system is performing\\n■ a bucket-brigade algorithm for assigning credit and blame to'), Document(metadata={}, page_content='classifiers\\n■ a procedure for reproducing classifiers by application of a set of\\ngenetic operators\\n■ a set of message lists—for input, output, and internal messages\\nThe operation of the classifier system is as follows:\\nFirst, the environment sends the system an input message, which is received\\nby the input detector.\\nThis message tells the system about some feature of the environment, or about\\nsome event that has occurred (such as a move that has been made in a game).'), Document(metadata={}, page_content='378 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nThis message is placed on the input message list and translated into a set of\\ninternal messages that the system can interpret, using its classifier rules.\\nThese internal messages cause some of the classifiers to fire—the choice of\\nwhich classifier rules fire is based on the relative fitness of the rules, and\\nalso on how well their antecedents match the internal messages. This is'), Document(metadata={}, page_content='known as a bidding system, where the classifiers that generate the highest\\nbid (based on fitness and closeness of match) get to fire.\\nThe effect of the classifiers firing is either to generate further internal mes-\\nsages, which may cause further classifiers to fire, or to generate output mes-\\nsages, which are passed back to the environment.\\nThe environment then evaluates the system’s actions and provides feedback\\non how successful they were.'), Document(metadata={}, page_content='on how successful they were.\\nAt this point, the system uses a bucket-brigade algorithm to assign credit\\nor blame to the various classifiers in the system. This involves increasing\\nthe fitness of the classifiers that contributed most to a successful outcome\\nand decreasing the fitness of those that contributed most to an unsuccess-\\nful outcome.\\nFinally, successful rules are allowed to reproduce using crossover and\\nmutation operators to produce new rules, whereas unsuccessful rules are'), Document(metadata={}, page_content='dropped from the system altogether.\\nEach classifier consists of three parts:\\n■ a condition (the antecedent of the rule)\\n■ a message (the action of the rule)\\n■ a fitness measure\\nWe can represent classifier rules in the form (c1, c2, c3, c4, c5) -> M, f.H e r e  c1\\nto c5 are the variables that make up the input to the system, and M is the\\noutput message that results from firing this classifier rule, which represents\\nan action or a classification. f is the fitness of the classifier rule.'), Document(metadata={}, page_content='For example, we can assume that the inputs to the system are numeric vari-\\nables that can take on values from 1 to 10 and that the classification or\\naction that results from each classification is one of five possible actions:\\nA\\n1, A2, A3, A4,o r  A5. Classifier rules do not need to specify a value for each\\nvariable and can specify * to indicate that any value can match that vari-\\nable. Hence, possible classifier rules might be:'), Document(metadata={}, page_content='13.12 Classifier Systems 379\\n(1, 2, 3, 4, 5) -> A1, 0.7\\n(1, *, *, *, *) -> A3, 2.4\\n(4, 2, *, 1, *) -> A2, 9.1\\n(*, 9, *, 6, 2) -> A3, 7.2\\n(3, 4, 5, *, *) -> A4, 4.5\\n(1, 2, *, *, *) -> A5, 6.2\\nRule 1, for example, specifies that the string (1, 2, 3, 4, 5) is classified as clas-\\nsification A1, with a fitness of 0.7.\\nNow let us imagine that an input message arrives from the environment,\\nwhich is (1, 2, 3, 4, 5). This will match classifiers 1, 2, and 6. These three'), Document(metadata={}, page_content='classifiers now bid. The value of a classifier’s bid is a function of that classi-\\nfier’s fitness and how closely its antecedent matches the input message. This\\nmeasure of closeness is determined by adding 1 for each exact match and\\n0.5 for each * (which matches any input symbol). These values are summed\\nand divided by the length of the message. This number is then multiplied\\nby the classifier’s fitness to produce its total bid.'), Document(metadata={}, page_content='Hence, the bids for the three matching classifiers in our example are as follows:\\nFor classifier 1:\\nbid = ((1 + 1 + 1 + 1 + 1) / 5) * 0.7 = 0.7\\nFor classifier 2:\\nbid = ((1 + 0.5 + 0.5 + 0.5 + 0.5) / 5) * 2.4 = 0.96\\nFor classifier 6:\\nbid = ((1 + 1 + 0.5 + 0.5 + 0.5) / 5) * 6.2 = 4.34\\nThe classifier with the highest bid is successful, and fires, providing a classi-\\nfication of A5. This is fed back to the environment as an output message,'), Document(metadata={}, page_content='and the environment evaluates it to determine if this is correct or not.\\nIf the classifier has made a correct assessment, its fitness is increased, which\\nis determined by subtracting the bid value from a positive reward score. If\\nit made an incorrect assessment, the reward will be negative (or lower than\\nthe bid value) and so its fitness level will decrease.\\nIn fact, in most classifier systems, the bidding process is far more complex,'), Document(metadata={}, page_content='and more than one classifier can be successful by forming joint bids. This is\\nwhere the bucket-brigade algorithm becomes important for determining'), Document(metadata={}, page_content='380 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nwhich classifiers to reward and to what extent, based on how much they\\ncontributed to the success (or failure) of the system as a whole. Holland\\nbased this bidding system on economic processes, with individual classi-\\nfiers acting like businesses bidding for contracts.\\nFinally, reproduction occurs. Let us examine how this happens, by assuming\\nthat the system has decided to reproduce from the two fittest classifiers, 3 and 4.'), Document(metadata={}, page_content='These classifiers are defined as follows:\\n3. (4, 2, *, 1, *) -> A2, 9.1\\n4. (*, 9, *, 6, 2) -> A3, 7.2\\nFirst, a position is chosen randomly from within the antecedent. This point\\nis called the crossover position. For our example, we will assume that the\\nsystem has chosen the position between the third and fourth variables as its\\ncrossover position, as follows:\\n3. (4, 2, *,\\n| 1, *) -> A2, 9.1\\n4. (*, 9, *, | 6, 2) -> A3, 7.2'), Document(metadata={}, page_content='4. (*, 9, *, | 6, 2) -> A3, 7.2\\nNow crossover is applied as follows: the first half of classifier 3, before the\\ncrossover position, is joined to the second half of classifier 4, after the\\ncrossover position. This produces an offspring classifier, which we will call\\nclassifier 7:\\n7. (4, 2, *, 6, 2) -> A2, 8.4\\nNote that the output message for this new classifier is chosen to be A2,\\nbecause this is the output classifier of the parent classifier that contributed'), Document(metadata={}, page_content='the larger part to the offspring (3 variables).\\nThe fitness of the offspring is determined by taking proportionally from\\nthe parents—three-fifths of the fitness of classifier 3 (because it con-\\ntributed three of the five variables) and two-fifths of the fitness of classifier\\n4. Hence the fitness of classifier 7 is defined as\\n(3 / 5) * 9.1 + (2 / 5) * 7.2 = 8.4\\nSimilarly, crossover is applied in the other proportions by attaching the first'), Document(metadata={}, page_content='part of classifier 4 to the second part of classifier 3, to produce\\n8. (*, 9, *, 1, *) -> A3, 7.96\\nThe final part of the reproduction process involves the optional application\\nof a mutation operator. This simply involves changing one of the parts of'), Document(metadata={}, page_content='13.13 Artificial Immune Systems 381\\nthe offspring after it has been produced. For example, a variable value\\nmight change to another value, or to *. Typically, as we see in Chapter 14,\\nmutation is applied sparingly, so that not too much of the parents’ genetic\\ninformation is lost. Hence, we might apply mutation to one of the symbols\\nin offspring 7 to produce\\n7. (4, 2, *, 6, *) -> A2, 8.4\\nThe description of classifier systems so far has been rather abstract. We can'), Document(metadata={}, page_content='imagine classifier systems being used, for example, to play a game such as\\nchess, where a static evaluation function is used to determine whether a\\nmove was good or bad, and where the inputs are the positions of the pieces\\non the board.\\nIn the 1980s, Stewart Wilson, a researcher at Polaroid, used classifier sys-\\ntems to build an artificial creature he called “*” . * was placed in a world\\nconsisting of rocks and food. Over a period of time, * learned to deal with'), Document(metadata={}, page_content='its world more and more efficiently. For example, it learned that food was\\noften near a rock, but that banging into a rock was painful. Hence, when it\\nencountered a rock, it would stop and then walk around the rock to see if\\nany food was present. This artificial creature had learned to survive in its\\nown environment without anyone needing to teach it how. Its survival\\nemerged from a combination of its environment and its reasonably simple\\nclassifier system “brain.”'), Document(metadata={}, page_content='classifier system “brain.”\\nOf course, this did not take place in the real world but, like most Artificial\\nLife, took place inside a computer in the form of binary data.\\n13.13 Artificial Immune Systems\\nArtificial immune systems (AIS) are a relatively recent innovation. The idea\\nbehind AIS is to build systems based on the immune system in human beings\\nand other animals. The biological immune system is a massively parallel sys-'), Document(metadata={}, page_content='tem that is able to deal with changes in individual bodies, changes in envi-\\nronment, and even to adapt to rapidly evolving viruses and other attackers.\\nOne of the first uses for AIS was to build a system that could defend com-\\nputers against viruses. Early antivirus systems based on this technique\\nrelied on people reporting a new virus to the “immune system, ” which\\nwould then make attempts to analyze the virus to determine ways to iden-\\ntify and block it.'), Document(metadata={}, page_content='382 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nMore advanced methods are now applied using artificial immune systems\\nto solve combinatorial search problems and are also applied in computer\\nsecurity, machine learning, and fault diagnosis.\\n13.14 Chapter Summary\\n■ A definition of life is not easy to produce. Counterexamples can be\\nfound for most definitions.\\n■ Artificial Life is modeled on life in the same way that Artificial\\nIntelligence is modeled on the human brain.'), Document(metadata={}, page_content='Intelligence is modeled on the human brain.\\n■ Complex behavior tends to emerge from simple systems when\\nusing techniques modeled on life. Such behaviors are emergent.\\n■ Cellular automata such as Conway’s life show how life can be simu-\\nlated in an extremely simple system based on finite state automata.\\n■ Langton’s loops were an example of a simple self-reproducing sys-\\ntem. Von Neumann postulated an entity that could physically\\nreproduce itself.'), Document(metadata={}, page_content='reproduce itself.\\n■ Evolution strategies use asexual reproduction to search for solu-\\ntions to engineering problems.\\n■ Genetic programming methods evolve S-expressions or LISP pro-\\ngrams to solve problems.\\n■ Evolutionary programming involves evolving finite state automata\\nto predict the next item in a sequence of symbols.\\n■ L-systems use simple rules to build complex plant-like structures.\\n■ Classifier systems combine evolutionary methods (genetic algo-'), Document(metadata={}, page_content='rithm) with a production system to build a system that is able to\\nadapt to changes in its environment.\\n13.15 Review Questions\\n13.1 What is life?\\n13.2 What is Artificial Life? How does it relate to Artificial Intelligence?\\nIs one an alternative to the other, or are they complementary?\\n13.3 Explain what is meant by emergent behavior.'), Document(metadata={}, page_content='13.16 Further Reading 383\\n13.4 Explain how Conway’s Life is modeled on life. What interesting\\nproperties does it exhibit? Why do you think it has fascinated peo-\\nple for so long?\\n13.5 Explain how a system might be built that could reproduce itself.\\nWould such a system be alive?\\n13.6 Explain how genetic programming could be used to solve problems.\\n13.7 What is evolutionary programming? How does it differ from\\ngenetic programming?'), Document(metadata={}, page_content='genetic programming?\\n13.8 Explain why L-systems are of interest to Artificial Life researchers.\\n13.9 Explain the relationship between classifier systems and production\\nsystems. How are classifier systems built? What advantages do they\\nhave over production systems?\\n13.10 Explain how systems modeled on the human immune system\\nmight provide a solution to the problem of computer viruses or of\\nunsolicited bulk e-mails (“spam”).\\n13.16 Further Reading'), Document(metadata={}, page_content='13.16 Further Reading\\nThere is a great deal of literature on the subject of Artificial Life. A good\\nintroduction to the subject from a relatively nontechnical point of view can\\nbe found in Levy (1993) and Kelly (1994). Adami (1997) is a more\\nadvanced text on the subject.\\nLangton (1995) provides a number of interesting articles on Artificial Life,\\nincluding philosophic and sociologic perspectives.\\nClassifier systems and genetic algorithms are covered by many of the main'), Document(metadata={}, page_content='Artificial Intelligence texts, but few of them cover any other aspects of Arti-\\nficial Life.\\nInformation on artificial immune systems can be found in de Castro and\\nTimmis (2002) and Dasgupta (1999).\\nDawkins (1991) gives an excellent view of the subject from the biologic\\nevolutionary perspective.\\nA fictional account of the potentials of Artificial Life can be found in Prey\\nby Michael Crichton.\\nIntroduction to Artificial Life, by Christoph Adami (1997 – T elos)'), Document(metadata={}, page_content='384 CHAPTER 13 Artificial Life: Learning through Emergent Behavior\\nGenetic Programming: An Introduction: On the Automatic Evolution of Com-\\nputer Programs and Its Applications , by Wolfgang Banzhaf, Peter Nordin,\\nRobert E. Keller, and Frank D. Francone (1997 – Morgan Kaufmann)\\nDigital Biology, by Peter Bentley (2002 – Simon & Schuster)\\nThe Philosophy of Artificial Life, by Margaret A. Boden (1996 – Oxford Uni-\\nversity Press)'), Document(metadata={}, page_content='versity Press)\\nSwarm Intelligence: From Natural to Artificial Systems , by Eric Bonabeau,\\nMarco Dorigo, and Guy Theraulaz (1999 – Oxford University Press)\\nArtificial Immune Systems and Their Applications, edited by Dipankar Das-\\ngupta (1999 – Springer V erlag)\\nThe Blind Watchmaker, by Richard Dawkins (1996 – W. W. Norton & Com-\\npany)\\nArtificial Immune Systems: A New Computational Intelligence Paradigm ,b y\\nLeandro N. de Castro and Jonathan Timmis (2002 – Springer V erlag)'), Document(metadata={}, page_content='Evolutionary Computation in Bioinformatics , edited by Gary B. Fogel and\\nDavid W. Corne (2002 – Morgan Kaufmann)\\nCreation: Life and How to Make It, by Steve Grand (2001 – Harvard Univer-\\nsity Press)\\nFrom Animals to Animats 7: Proceedings of the Seventh International Confer-\\nence on Simulation of Adaptive Behavior , edited by Bridget Hallam, Dario\\nFloreano, John Hallam, Gillian Hayes, and Jean-Arcady Meyer (2002 – MIT'), Document(metadata={}, page_content='Press ; also available are the proceedings from the first to sixth conferences)\\nSilicon Second Nature: Culturing Artificial Life in a Digital World , by Stefan\\nHelmreich (2000 – University of California Press)\\nEmergence: The Connected Lives of Ants, Brains, Cities, and Software ,b y\\nSteven Johnson (2001 – Scribner)\\nOut of Control: The New Biology of Machines , by Kevin Kelly (1994 –\\nFourth Estate)\\nSwarm Intelligence by James Kennedy, Russell C. Eberhart, and Yuhui Shi\\n(2001 – Morgan Kaufmann)'), Document(metadata={}, page_content='(2001 – Morgan Kaufmann)\\nGenetic Programming: On the Programming of Computers by Means of Nat-\\nural Selection, by John R. Koza (1992 – MIT Press)'), Document(metadata={}, page_content='13.16 Further Reading 385\\nGenetic Programming II: Automatic Discovery of Reusable Programs, by John\\nR. Koza (1994 – MIT Press)\\nArtificial Life: An Overview, edited by Christopher Langton (1995 – MIT Press)\\nArtificial Life: A Report from the Frontier Where Computers Meet Biology,b y\\nSteven Levy (1993 – Vintage Books)\\nEvolutionary Algorithms for Single and Multicriteria Design Optimization ,\\nby Andrzej Osyczka (2001 – Physica V erlag)'), Document(metadata={}, page_content='by Andrzej Osyczka (2001 – Physica V erlag)\\nArtificial Life VIII: Proceedings of the Eighth International Conference on\\nArtificial Life, edited by Russell Standish, Mark A. Bedau, and Hussein A.\\nAbbass (2003 – MIT Press; also available are the proceedings from the first\\nthrough the seventh conferences)\\nEvolutionary Art and Computers , by Stephen T odd and William Latham\\n(1992 – Academic Press)\\nVirtual Organisms: The Startling World of Artificial Life ,b y  M a r k  W a r d'), Document(metadata={}, page_content='(2000 – St Martin’s Press)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='14CHAPTER\\nGenetic Algorithms\\nSome call it evolution,\\nAnd others call it God.\\n—William Herbert Carruth, Each in His Own Tongue\\nThe first technical descriptions and definitions of adaptation come from biol-\\nogy. In that context adaptation designates any process whereby a structure is\\nprogressively modified to give better performance in its environment. The\\nstructures may range from a protein molecule to a horse’s foot or a human'), Document(metadata={}, page_content='brain or, even, to an interacting group of organisms such as the wildlife of the\\nAfrican veldt.\\n—John H. Holland, Adaptation in Natural and Artificial Systems\\n14.1 Introduction\\nThe idea of local search is introduced in Chapter 5. Local search methods\\ninvolve making small changes to potential solutions to a problem until an\\noptimal solution is identified. Genetic algorithms are a form of local search\\nthat use methods based on evolution to make small changes to a popula-'), Document(metadata={}, page_content='tion of chromosomes in an attempt to identify an optimal solution.\\nIn this chapter, the representations used for genetic algorithms are dis-\\ncussed, including the idea of schemata. The genetic operators, crossover\\nand mutation, are explained, as is the idea of fitness.\\nThe procedures used to run genetic algorithms are also discussed, and an\\nattempt is made to explain why genetic algorithms work.'), Document(metadata={}, page_content='388 CHAPTER 14 Genetic Algorithms\\nAn example is given of how genetic algorithms might be used to evolve a\\nstrategy for playing a simple game (Prisoner’s Dilemma), and the idea of\\nallowing humans to input into the process in order to evolve images of\\n“creatures” is also explored.\\n14.2 Representations\\nWe have seen a number of different representations that can be used in\\nevolutionary techniques like genetic algorithms. Genetic programming is'), Document(metadata={}, page_content='used to evolve S-expressions, which can be used as LISP programs to solve\\nproblems. Classifier systems use a string of numbers that represent proper-\\nties of the environment and symbols that represent responses to those\\nproperties.\\nThe simplest representation for genetic algorithms is the one that was used\\nby John Holland: a string of bits. A string of bits is known as a chromo-\\nsome, and each bit is known as a gene. Both of these terms are directly bor-'), Document(metadata={}, page_content='rowed from genetics and illustrate the close manner in which genetic\\nalgorithms mirror biological processes.\\nFor most of this chapter, we discuss genetic algorithms using this represen-\\ntation, but it is worth remembering that many other representations are\\npossible, and different representations will be more appropriate for partic-\\nular problems.\\nThe population consists of a set of chromosomes, each of which, as we\\nhave seen, is made up of genes. A chromosome is usually taken to represent'), Document(metadata={}, page_content='a complete “individual” within the population—in other words, a complete\\nrepresentation of a solution, or a classification. It is also possible to com-\\nbine chromosomes together to form creatures, which more closely mirrors\\nreal genetics because each individual in the real world has a number of\\nchromosomes. For now, we will continue with Holland’s approach, where a\\nchromosome represents an entire individual.\\nEach gene in the chromosome represents some facet of that individual’s'), Document(metadata={}, page_content='genetic makeup. For example, the genes could be entirely independent and\\nrepresent the presence or otherwise of certain body parts in an animal.\\nMore usually, the genes are combined together in a less transparent way.\\nFor example, we will see how genetic algorithms can be used to solve math-'), Document(metadata={}, page_content='14.3 The Algorithm 389\\nematical problems, where the bits of a chromosome are usually treated as\\nthe bits of a binary number that represents a solution to the problem.\\n14.3 The Algorithm\\nThe process for running a genetic algorithm is as follows. Note that this\\nprocess is largely independent of the representation that is being used.\\n1. Generate a random population of chromosomes (this is the first\\ngeneration).\\n2. If the termination criteria are satisfied, stop. Otherwise, continue\\nwith step 3.'), Document(metadata={}, page_content='with step 3.\\n3. Determine the fitness of each chromosome.\\n4. Apply crossover and mutation to selected chromosomes from the\\ncurrent generation to generate a new population of chromo-\\nsomes—the next generation.\\n5. Return to step 2.\\nNote that the evolutionary part of the classifier system process that we saw\\nin Chapter 13 is in fact a genetic algorithm.\\nThe size of the population should be determined in advance. Usually, the\\npopulation size remains constant from one generation to the next. In some'), Document(metadata={}, page_content='situations, it can be useful to have a population that changes size.\\nThe size of each chromosome must remain the same for crossover to be\\napplied. It is possible to run genetic algorithms with variable chromosome\\nsizes, but this is unusual.\\nTypically, the fittest chromosomes are selected in each generation to mate\\nwith each other, and each pair of chromosomes is allowed to produce two\\noffspring. The resultant set of offspring chromosomes then replaces the\\nprevious generation.'), Document(metadata={}, page_content='previous generation.\\nIt is also possible to allow particularly fit parents to produce relatively more\\noffspring and to allow certain members of a generation to survive to the\\nnext generation. For most of this chapter, we will assume that each pair of\\nparents produces two offspring and that those offspring replace the parents.'), Document(metadata={}, page_content='390 CHAPTER 14 Genetic Algorithms\\n14.4 Fitness\\nRichard Dawkins’ biomorph world, which is discussed in Chapter 13, is a\\nform of genetic algorithm. Rather than applying an objective fitness level,\\nfitness was determined subjectively by a human operator. Additionally,\\neach generation was the offspring of just one parent, to which mutation\\nwas applied.\\nWith more traditional genetic algorithms, a metric is needed whereby the\\nfitness of a chromosome can be objectively determined. For example, in'), Document(metadata={}, page_content='using genetic algorithms to sort numbers into numeric order, a suitable fit-\\nness measure might be determined by running the algorithm and counting\\nhow many numbers it places in the correct position. A more sophisticated\\nmeasure of fitness could be obtained by measuring how far from its correct\\nplace each incorrectly placed number is.\\nKarl Sims evolved “creatures” that were bred according to their abilities to\\nperform simple tasks, such as walking, jumping, and swimming (Sims'), Document(metadata={}, page_content='1994). Sims used a representation and a set of rules that determined how\\nthe various body parts of his creatures interacted with each other and with\\ntheir environment. In this case, then, the fitness measure was based on the\\nextent to which the physical form ( phenotype) represented by the genetic\\ninformation (genotype) met certain criteria.\\n14.5 Crossover\\nIn Chapter 13, we see how crossover is used in classifier systems. The crossover'), Document(metadata={}, page_content='operator is applied to two chromosomes of the same length as follows:\\n1. Select a random crossover point.\\n2. Break each chromosome into two parts, splitting at the crossover\\npoint.\\n3. Recombine the broken chromosomes by combining the front of\\none with the back of the other, and vice versa, to produce two new\\nchromosomes.\\nFor example, consider the following two chromosomes:\\n110100110001001\\n010101000111101'), Document(metadata={}, page_content='14.5 Crossover 391\\n100110001\\n011100110\\n101100001\\n010110110\\nFigure 14.1\\nIllustrating two-point\\ncrossover\\nA crossover point might be chosen between the sixth and seventh genes:\\n110100 | 110001001\\n010101 | 000111101\\nNow the chromosome parts are recombined as follows:\\n110100 | 000111101 => 110100000111101\\n010101 | 110001001 => 010101110001001\\nThis process is based on the way in which DNA strands recombine with each\\nother in human reproduction to combine features of each parent in a child.'), Document(metadata={}, page_content='Single-point crossover is the most commonly used form, but it is also pos-\\nsible to apply crossover with two or more crossover positions.\\nIn two-point crossover, two points are chosen that divide the chromosomes\\ninto two sections, with the outer sections considered to be joined together\\nto turn the chromosome into a ring. The two sections are swapped with\\neach other, as shown in Figure 14.1.\\nIn Figure 14.1, the genes from parent 1 are shaded in grey, while the genes'), Document(metadata={}, page_content='from parent 2 are not shaded.\\nAnother form of crossover is uniform crossover. Here, a probability, p,i s\\nused to determine whether a given bit from parent 1 will be used, or from\\nparent 2. In other words, a child can receive any random bits from each of\\nits parents. For example, let us assume we have the following two parents:\\nParent 1: 10001101\\nParent 2: 00110110\\nThe offspring of these two chromosomes might be determined as shown in\\nFigure 14.2.\\n100110001\\n011100110\\n110100101\\n001110010'), Document(metadata={}, page_content='100110001\\n011100110\\n110100101\\n001110010\\nFigure 14.2\\nIllustrating uniform\\ncrossover of two-parent\\nchromosomes to produce\\ntwo offspring'), Document(metadata={}, page_content='392 CHAPTER 14 Genetic Algorithms\\nThe first bit of the first child is chosen to be from parent 1 with probability\\np and from parent 2 with probability 1 – p. If a bit from parent 1 is chosen\\nfor child 1, then the corresponding bit from parent 2 is chosen for child 2,\\nand vice versa. Uniform crossover is also often used to produce just one off-\\nspring from each pair of parents, unlike traditional one- or two-point\\ncrossover, which usually produces two offspring from each pair of parents.'), Document(metadata={}, page_content='Uniform crossover does mix up the genes of the gene pool substantially,\\nand in some cases it can be sensible to use a very high (or very low) value of\\np to ensure that most of the genes come from one parent or the other.\\nIn some cases, cloning can be applied, whereby crossover is not applied at\\nall, and a new offspring is produced that is identical to its single parent.\\nDawkins’ biomorph system can be thought of as a genetic algorithm with'), Document(metadata={}, page_content='cloning and mutation, where fitness is determined subjectively.\\n14.6 Mutation\\nY ou may recognize genetic algorithms as being rather similar to the hill-\\nclimbing methods we see in Chapter 4. Hill-climbing involves generating a\\npossible solution to the problem and moving toward a better solution than\\nthe current one until a solution is found from which no better solution can\\nbe found. Hill climbing does not perform well with problems where there'), Document(metadata={}, page_content='are local maxima. T o enable genetic algorithms to avoid this problem, the\\nmutation operator was introduced.\\nMutation is a unary operator (i.e., it is applied to just one argument—a\\nsingle gene) that is usually applied with a low probability, such as 0.01 or\\n0.001. Mutation simply involves reversing the value of a bit in a chromo-\\nsome. For example, with a mutation rate of 0.01, it might be expected that\\none gene in a chromosome of 100 genes might be reversed. Here we see'), Document(metadata={}, page_content='mutation applied to one of the offspring from our example above:\\n010101110001001\\n⇓\\n010101110101001\\n14.7 Termination Criteria\\nThere are typically two ways in which a run of a genetic algorithm is termi-\\nnated. Usually, a limit is put on the number of generations, after which the\\nrun is considered to have finished.'), Document(metadata={}, page_content='14.8 Optimization of a Mathematic Function 393\\nWith some problems, the run can stop when a particular solution has been\\nreached, or when the highest fitness level in the population has reached a\\nparticular value. For example, we see in the following section how a genetic\\nalgorithm can be used to solve a mathematical function. In this case, it is\\nclear that the run can stop when the correct solution has been reached,\\nwhich can be easily tested for.'), Document(metadata={}, page_content='which can be easily tested for.\\nIn the case of Dawkins’ biomorph world, no such termination conditions\\nexist. It does not make sense to impose an artificial limit on the number of\\ngenerations in the run, and because no objective measure of fitness is\\ninvolved, the system cannot determine when to stop on that basis.\\nThis is an important distinction. In many cases, genetic algorithms are used\\nto solve problems that have an objective solution, in which case the algo-'), Document(metadata={}, page_content='rithm can stop when it reaches that solution. In other cases, they are used\\nfor more abstract purposes, such as to generate interesting pictures. In these\\ncases, human judgment must be used to determine when to terminate.\\n14.8 Optimization of a Mathematic Function\\nWe will see how a genetic algorithm can be used to find a maximum value\\nof a mathematic function.\\nWe will attempt to maximize the following function:\\nf(x) = sin(x)\\nover the range of x from 1 to 15, where x is in radians.'), Document(metadata={}, page_content='Each chromosome represents a possible value of x using four bits.\\nFigure 14.3 shows the discrete graph for this function.\\n1.50\\n1.00\\n0.50\\n0.00\\n–0.50\\n–1.00\\n–1.50\\nx\\nf(x)\\nFigure 14.3\\nDiscrete graph for the\\nfunction f(x) = sin(x),\\nwhere x ranges from \\n0 to 15.'), Document(metadata={}, page_content='394 CHAPTER 14 Genetic Algorithms\\nTable 14.1 Generation 1\\nChromosome Genes Integer value f(x) Fitness f/H11032(x) Fitness ratio\\nc1 1001 9 0.41 70.61 46.3%\\nc2 0011 3 0.14 57.06 37.4%\\nc3 1010 10 /H110020.54 22.80 14.9%\\nc4 0101 5 /H110020.96 2.05 1.34%\\nWe will use a population size of four chromosomes. The first step is to gen-\\nerate a random population, which is our first generation:\\nc1 = 1001\\nc2 = 0011\\nc3 = 1010\\nc4 = 0101\\nT o calculate the fitness of a chromosome, we need to first convert it to a'), Document(metadata={}, page_content='decimal integer and then calculate f (x) for this integer.\\nWe will assign fitness as a numeric value from 0 to 100, where 0 is the least\\nfit and 100 is the most fit.\\nf(x) generates real numbers between /H110021 and 1. We will assign a fitness of\\n100 to f(x) = 1 and fitness of 0 to f (x) = /H110021. Fitness of 50 will be assigned\\nto f(x) = 0. Hence, fitness of x, f /H11032(x) is defined as follows:\\nf/H11032(x) = 50(f(x) + 1)\\n= 50(sin(x) + 1)'), Document(metadata={}, page_content='f/H11032(x) = 50(f(x) + 1)\\n= 50(sin(x) + 1)\\nThe fitness ratio of a chromosome is that chromosome’s fitness as a per-\\ncentage of the total fitness of the population. We will see later why this is a\\nuseful calculation.\\nTable 14.1 shows the calculations that are used to calculate the fitness val-\\nues for our first generation.\\nNow we need to run a single step of our genetic algorithm to produce the\\nnext generation. The first step is to select which chromosomes will repro-'), Document(metadata={}, page_content='duce. Roulette-wheel selection involves using the fitness ratio to randomly\\nselect chromosomes to reproduce. This is done as follows:'), Document(metadata={}, page_content='14.8 Optimization of a Mathematic Function 395\\nThe range of real numbers from 0 to 100 is divided up between the chro-\\nmosomes proportionally to each chromosome’s fitness. Hence, in our first\\ngeneration, c1 will have 46.3% of the range (i.e., from 0 to 46.3), c2 will\\nhave 37.4% of the range (i.e., from 46.3 to 83.7), and so on.\\nA random number is now generated between 0 and 100. This number will\\nfall in the range of one of the chromosomes, and this chromosome has'), Document(metadata={}, page_content='been selected for reproduction. The next random number is used to select\\nthis chromosome’s mate. Hence, fitter chromosomes will tend to produce\\nmore offspring than less fit chromosomes.\\nIt is important that this method does not stop less fit chromosomes from\\nreproducing at all, though, because this helps to ensure that populations do\\nnot stagnate, by constantly breeding from the same parents.\\nIn our example, though, chromosome c4 will be very unlikely to reproduce'), Document(metadata={}, page_content='because this would only occur if the random number fell in the narrow\\nrange between 98.6 and 100.\\nWe will need to generate four random numbers to find the four parents\\nthat will produce the next generation. Our first random number is 56.7,\\nwhich means that c2 has been chosen as the first parent. Next, 38.2 is cho-\\nsen, so its mate is c1.\\nWe now need to combine c1 and c2 to produce two new offspring. First, we\\nneed to randomly select a crossover point. We will choose the point'), Document(metadata={}, page_content='between the second and third bits (genes):\\n10 | 01\\n00 | 11\\nCrossover is now applied to produce two offspring, c5 and c6:\\nc5 = 1011\\nc6 = 0001\\nIn a similar way, c1 and c3 are chosen to produce offspring c7 and c8, using\\na crossover point between the third and fourth bits:\\nc7 = 1000\\nc8 = 1011\\nThe population c1 to c4 is now replaced by the second generation, c5 to c8.\\nc4 did not have a chance to reproduce, and so its genes will be lost. c1,'), Document(metadata={}, page_content='396 CHAPTER 14 Genetic Algorithms\\nTable 14.2 Generation 2\\nChromosome Genes Integer value f(x) Fitness f/H11032(x) Fitness ratio\\nc5 1011 11 /H1100210 0 %\\nc6 0001 1 0.84 92.07 48.1%\\nc7 1000 8 0.99 99.47 51.9%\\nc8 1011 11 /H1100210 0 %\\nwhich was the fittest chromosome in the first generation, was able to repro-\\nduce twice, thus passing on its highly fit genes to all members of the next\\ngeneration.\\nThe fitness values for the second generation are shown in Table 14.2.'), Document(metadata={}, page_content='This generation has produced two extremely fit chromosomes and two\\nvery unfit chromosomes. In fact, one of the chromosomes, c7, is the opti-\\nmal solution. At this point, the termination criteria would probably deter-\\nmine that the run could stop. Otherwise, the algorithm will continue to run\\nbut will not find any better solutions. It has taken just one step to get from\\na random configuration to the optimal solution.\\nClearly, this was a very simplistic example. Real problems are likely to be'), Document(metadata={}, page_content='much harder to solve. They are also likely to involve much larger popula-\\ntion sizes (typically population sizes of between 100 and 500 are used), and\\nchromosomes are likely to contain far greater numbers of bits.\\nIn many cases, genetic algorithms quickly produce optimal or near-optimal\\nsolutions to combinatorial problems that would otherwise be impractical\\nto solve. This raises an interesting question: Why do genetic algorithms\\nwork? We will now address this question.'), Document(metadata={}, page_content='work? We will now address this question.\\n14.9 Why Genetic Algorithms Work\\nGenetic algorithms are a local search method (see Chapter 5), in some ways\\nsimilar to simulated annealing and hill-climbing methods.\\nIt is possible to explain genetic algorithms by comparison with natural evo-\\nlution: small changes that occur on a selective basis combined with repro-\\nduction will tend to improve the fitness of the population over time. This'), Document(metadata={}, page_content='14.9 Why Genetic Algorithms Work 397\\nargument is not very convincing, and John Holland (1975) invented\\nschemata (the plural of schema) to provide an explanation for genetic\\nalgorithms that is more rigorous.\\n14.9.1 Schemata\\nIn Chapter 13, we see how strings of numbers are used to represent input\\npatterns in classifier systems. In these patterns, * is used to represent “any\\nvalue” or “don’t care, ” so that the following string:\\n1011*001*0\\nmatches the following four strings:\\n1011000100\\n1011000110'), Document(metadata={}, page_content='1011000100\\n1011000110\\n1011100100\\n1011100110\\n(The bits which have matched * are shown in bold).\\nA schema is a string of bits that represents a possible chromosome, using *\\nto represent “any value. ” A schema is said to match a chromosome if the bit\\nstring that represents the chromosome matches the schema in the way\\nshown above. For example, the following schema:\\n*11*\\nmatches the following four chromosomes:\\n0110\\n0111\\n1110\\n1111\\nNote that a schema with n *’s will match a total of 2n chromosomes.'), Document(metadata={}, page_content='Each chromosome of r bits will match 2r different schemata. For example,\\nthe following chromosome:\\n101\\nmatches the following eight schemata:\\n101\\n10*\\n1*1\\n1**\\n*01\\n*0*'), Document(metadata={}, page_content='398 CHAPTER 14 Genetic Algorithms\\n**1\\n***\\nBecause schemata are made up of three different values (0, 1, and *), there\\nare 3m different schemata of length m. For example, there are nine possible\\nschemata of just two bits:\\n00\\n01\\n0*\\n10\\n*0\\n11\\n*1\\n1*\\n**\\nThe defining length of a schema is defined as the distance between the first\\nand last defined bits (bits that are not *) in the schema. For example, the\\ndefining length of each of the following schemata is 4:\\n**10111*\\n1*0*1**\\n11111\\n1***1'), Document(metadata={}, page_content='**10111*\\n1*0*1**\\n11111\\n1***1\\n***********10**1***************\\nNote that a schema’s defining length is not dependent on the number of\\nbits it has, except that its defining length must be less than or equal to its\\nlength. We write this as\\nd\\nL(S) ≤ L(S)\\nwhere dL(S) is the defining length of schema S, and L(S) is the length of\\nschema S.\\nThe order of a schema is defined as the number of defined bits (i.e., the\\nnumber of bits that are not *) in the schema. Hence, the following'), Document(metadata={}, page_content='schemata all have order 4:\\n**10*11*\\n1*0*1**1\\n1111\\n1***1***1***1\\n1***********10**1***************'), Document(metadata={}, page_content='14.9 Why Genetic Algorithms Work 399\\nWe will write the order of a schema S as O(S). The order of a schema tells us\\nhow specific it is. A schema with a high order is more specific than one\\nwith a lower order.\\n14.9.2 How Reproduction Affects Schemata\\nWe can think of genetic algorithms as a way of manipulating schemata.\\nThis will help us to reason about why genetic algorithms work.\\nFirst of all, we consider what it means for a schema to be present in a pop-'), Document(metadata={}, page_content='ulation. Let us consider the following population of 10 chromosomes, each\\nof length 32:\\nC1 = 01000100101010010001010100101010\\nC2 = 10100010100100001001010111010101\\nC3 = 01010101011110101010100101010101\\nC4 = 11010101010101001101111010100101\\nC5 = 11010010101010010010100100001010\\nC6 = 00101001010100101010010101111010\\nC7 = 00101010100101010010101001010011\\nC8 = 11111010010101010100101001010101\\nC9 = 01010101010111101010001010101011\\nC10 = 11010100100101010011110010100001'), Document(metadata={}, page_content='C10 = 11010100100101010011110010100001\\nLet us consider the following schema:\\nS0 = 11010***************************\\nThis schema is matched by three chromosomes in our population: c 4,c 5,\\nand c10. We say that schema S0 matches three chromosomes in generation i\\nand write this as follows:\\nm(S0, i) = 3\\nIt is useful now to consider the concept of fitness as it applies to schemata.\\nThe fitness of a schema, S, in generation i is written as follows:\\nf(S, i)'), Document(metadata={}, page_content='f(S, i)\\nThe fitness of a schema is defined as the average fitness of the chromo-\\nsomes in the population that match the schema. Hence if we define the fit-\\nness of c\\n4,c 5, and c10 as follows:\\nf(C4, i) = 10\\nf(C5, i) = 22\\nf(C10, i) = 40'), Document(metadata={}, page_content='400 CHAPTER 14 Genetic Algorithms\\nhence, the fitness of the schemaS0 is defined as the average of these three values:\\nf(S0, i) = (10 + 22 + 40) / 3\\n= 24\\nWe will now investigate which factors affect the likelihood of a particular\\nschema surviving from one generation to the next. In other words, what\\nprobability is there that a given schema that is present in the parent gener-\\nation will be in the subsequent generation?\\nFirst, let us consider the process whereby chromosomes reproduce, without'), Document(metadata={}, page_content='introducing crossover or mutation.\\nFirst, let us assume that there is a chromosome that matches a schema,S,i n\\nthe population at time i.\\nThe number of occurrences of S in the population at time i is\\nm(S, i)\\nand the number of occurrences of S in the population in the subsequent\\ngeneration is:\\nm(S, i+1)\\nThe fitness of S in generation i is\\nf(S, i)\\nNow we will calculate the probability that a given chromosome, c, which\\nmatches the schema S at time i, will reproduce and thus its genes will be'), Document(metadata={}, page_content='present in the population at time i + 1. The probability that a chromosome\\nwill reproduce is proportional to its fitness, so the expected number of off-\\nspring of chromosome c is\\nwhere a(i) is the average fitness of the chromosomes in the population\\nat time i.\\nBecause chromosome c is an instance of schema S, we can thus deduce\\n(1)\\nwhere c\\n1 to cn are the chromosomes in the population at time i that match\\nthe schema S.\\nmS ,  i + 1 fc  i fc  i\\nai\\n1n( ) = ( ) ++ ( )\\n( )\\n,, K\\nm(c,  i +1) fc ,  i'), Document(metadata={}, page_content='1n( ) = ( ) ++ ( )\\n( )\\n,, K\\nm(c,  i +1) fc ,  i\\nai= ( )\\n( )'), Document(metadata={}, page_content='14.9 Why Genetic Algorithms Work 401\\nLet us compare this with the definition of the fitness of schema S, f (S, i),\\nwhich is defined as follows:\\n(2)\\nBy combining formula 1 with formula 2 above, we obtain:\\nThis tells us that the more fit a schema is compared with the average fitness\\nof the current population, the more likely it is that that schema will appear\\nin a subsequent population of chromosomes. A schema whose fitness is the'), Document(metadata={}, page_content='same as the average fitness of the population will likely maintain the same\\nnumber of occurrences from one generation to the next. In contrast, there\\nwill be fewer occurrences of a given schema whose fitness is lower than the\\naverage fitness of the population and more occurrences of a given schema\\nwhose fitness is higher than the average.\\n14.9.3 How Mutation and Crossover Affect Schemata\\nThe above calculations have not taken into account mutation or crossover.'), Document(metadata={}, page_content='Both mutation and crossover can destroy the presence of a schema. In\\nother words, mutation and crossover are both capable of reducing the\\nnumber of occurrences of a particular schema in a population of chromo-\\nsomes. They are also capable of increasing the number of occurrences of a\\ngiven schema.\\nA given schema can be said to have survived crossover, if the crossover\\noperation produces a new chromosome that matches the schema from a\\nparent that also matches the schema.'), Document(metadata={}, page_content='parent that also matches the schema.\\nIf the crossover point is chosen so that it is within the defining length of a\\nschema, S, then that schema will be destroyed by the crossover operation.\\nFor a schema to survive crossover, the crossover point must be outside the\\ndefining length. Hence, the probability that a schema S of defining length\\nd\\nL(S) and of length L(S) will survive crossover is\\npS dS\\nLSs\\nL( ) =− ( )\\n( ) −1 1\\nm(S,  i +1) f S,  i m S,  i\\nai= ( ) ⋅ ( )\\n( )\\nfS ,  i fc  i fc  i\\nmS ,  i'), Document(metadata={}, page_content='ai= ( ) ⋅ ( )\\n( )\\nfS ,  i fc  i fc  i\\nmS ,  i\\n1n( ) = ( ) ++ ( )\\n( )\\n,, K'), Document(metadata={}, page_content='402 CHAPTER 14 Genetic Algorithms\\nHence, a shorter schema is more likely to survive from one generation to\\nthe next than a longer schema. In practical terms, this means that a feature\\nof the chromosomes that is expressed by relatively few bits is more likely to\\nbe passed on from a parent to its offspring than a feature that is expressed\\nby a large number of bits.\\nThe above formula assumes that crossover is applied to each pair of parents'), Document(metadata={}, page_content='that reproduce. In fact, it is usually the case that some chromosomes are able\\nto reproduce asexually (by cloning). Hence, if the crossover operator is applied\\nwith probabilityp\\nc , then the above formula can be modified as follows:\\nHence, the less likely crossover is, the more likely any given schema is to\\nsurvive from one generation to the next.\\nIn fact, even if the crossover point is chosen within the defining length, it is'), Document(metadata={}, page_content='still possible for a schemata to survive crossover, as in the following example.\\nLet us apply crossover to the following two chromosomes:\\n10111101\\n01001110\\nThe schema **0011** is matched by the second of these chromosomes. If a\\ncrossover point is chosen between the fourth and fifth bits, then the off-\\nspring will be\\n10111110\\n01001101\\nIn this generation, the second chromosome also matches the schema\\n**0011**, despite the fact that the crossover point was chosen within the'), Document(metadata={}, page_content='defining length of the schema. Hence, we can modify our formula to allow\\nfor this (fairly unlikely) occurrence:\\nNow let us consider the effect of mutation on schemata. The probability\\nthat mutation will be applied to any given bit in a chromosome is p\\nm.\\nHence, a schema will survive mutation if mutation is not applied to any of\\npS p dS\\nLSsc\\nL( ) ≥− ⋅ ( )\\n( ) −1 1\\npS p dS\\nLSsc\\nL( ) =− ⋅ ( )\\n( ) −1 1'), Document(metadata={}, page_content='14.9 Why Genetic Algorithms Work 403\\nthe defined bits of the schema. Because a schema S has O(S) defined bits,\\nthe probability of survival can be defined as\\nps(S) = (1 /H11002pm)O(S)\\nHence, a schema is more likely to survive mutation if it has a lower order.\\nWe can combine the three equations we have to give one equation that\\ndefines the likelihood of a schema surviving reproduction using crossover\\nand mutation. This formula defines the expected number of chromosomes'), Document(metadata={}, page_content='that match a schema, S, in a generation at time i + 1:\\nThis rather daunting formula represents the schema theorem, developed\\nby Holland, which can be stated as “Short, low order schemata which are\\nfitter than the average fitness of the population will appear with exponen-\\ntially increasing regularity in subsequent generations” .\\nThe above analysis provides a way to understand the behavior of genetic\\nalgorithms and goes some way toward explaining why they work, but it'), Document(metadata={}, page_content='does not really provide a full answer to that question.\\nAlthough genetic algorithms have been widely studied, and there is\\ngood empirical evidence that they work, there is yet no theoretical proof\\nthat use of crossover provides better solutions than other local search\\ntechniques.\\n14.9.4 The Building-Block Hypothesis\\nThe building-block hypothesis is a consequence of the schema theorem,\\nwhich can be stated as “Genetic algorithms manipulate short, low-order,'), Document(metadata={}, page_content='high fitness schemata in order to find optimal solutions to problems.” These\\nshort, low-order, high-fitness schemata are known as building blocks.\\nIn other words, genetic algorithms work well when a small group of genes\\nthat are close together represent a feature that contributes to the fitness of a\\nchromosome. Hence, the representation that is chosen for genetic algo-\\nrithms is very important. Randomly selecting bits to represent particular'), Document(metadata={}, page_content='features of a solution is not good enough. Bits should be chosen in such a\\nmS ,  i + 1 f S,  i m S,  i\\nai p dS\\nLS pc\\nL\\nm\\nOS\\n( ) ≥ ( ) ⋅ ( )\\n( )\\n⋅− ⋅ ( )\\n( ) − ⋅−( )\\uf8eb\\uf8ed \\uf8f6\\uf8f8\\n\\uf8eb\\n\\uf8ed\\uf8ec\\n\\uf8f6\\n\\uf8f8\\uf8f7\\n()1 1 1'), Document(metadata={}, page_content='404 CHAPTER 14 Genetic Algorithms\\nway that they group naturally together into building blocks, which genetic\\nalgorithms are designed to manipulate.\\n14.9.5 Deception\\nOne problem faced by genetic algorithms is known as deception.L e t  u s\\nassume a population of chromosomes of 8 bits. We will consider four\\nschemata and their fitness levels:\\nS\\n1 = 11****** f(S 1) = 50\\nS2 = ******11 f(S 2) = 40\\nS3 = 11****11 f(S 3) = 5\\nS4 = 00****00 f(S 4) = 65'), Document(metadata={}, page_content='S4 = 00****00 f(S 4) = 65\\nNote that S1 and S2 are two building blocks, which combine together to give\\nS3, but that S3 is much less fit than S1 or S2.\\nLet us now assume that the optimal solution in this problem is\\nS5 = 11111111 f(S 5) = 100\\nThe genetic algorithm will find it hard to reach this optimal solution\\nbecause it will prefer to match the most fit building blocks with chromo-\\nsomes such as\\n00111100\\nHence, genetic algorithms can be misled or deceived by some building'), Document(metadata={}, page_content='blocks into heading toward suboptimal solutions.\\nOne way to minimize the effects of deception is to use inversion, which is a\\nunary operator that reverses the order of a subset of the bits within a chro-\\nmosome. For example, inversion applied to the following chromosome:\\n1010011100\\nbetween the fourth and eighth bits would produce the following chromosome:\\n1011110000\\nLike mutation, inversion is applied with a low probability (such as one in a'), Document(metadata={}, page_content='thousand) and can help to avoid converging on incorrect solutions.\\nAnother way to avoid deception is to use messy genetic algorithms, which\\nare described in the next section.'), Document(metadata={}, page_content='14.10 Messy Genetic Algorithms 405\\n14.10 Messy Genetic Algorithms\\nMessy genetic algorithms were developed as an alternative to standard\\ngenetic algorithms.\\nWith messy genetic algorithms (mGAs), each bit is labeled with its position.\\nA chromosome does not have to contain a value for each position, and, in\\nfact, a given position in a chromosome can have more than one value.\\nEach bit in a chromosome is represented by a pair of numbers: the first'), Document(metadata={}, page_content='number represents the position within the chromosome, and the second\\nnumber is the bit value (0 or 1).\\nHence, the following chromosome:\\n((1,0), (2,1), (4,0))\\ncould be a chromosome with four bit positions, where the third bit posi-\\ntion is not specified. The following chromosome, in contrast, has two val-\\nues specified for the third position:\\n((1,0), (2,1), (3,1), (3,0), (4,0))\\nGoldberg (1989) specifies methods for dealing with chromosomes that are'), Document(metadata={}, page_content='underspecified (i.e., where a bit position is not defined) or that are over-\\nspecified (where a bit position is defined twice).\\nUnderspecified bits are filled in by copying bits from a template chromo-\\nsome that is usually chosen as the best-performing chromosome from the\\nprevious generation.\\nA method is needed to deal with overspecified chromosomes: the most\\nusual method is simply to work on a left-to-right basis and use the first'), Document(metadata={}, page_content='value that is assigned to a given bit position. Hence, for example, the fol-\\nlowing chromosome:\\n((1, 0), (3, 0), (2, 1), (1,1))\\nwould be modified to\\n((1, 0), (3, 0), (2, 1))\\nBecause bit 1 is overspecified. The first occurrence, working from left to\\nright, is used, and any other occurrences are discarded.\\nmGAs use the mutation operator as with standard genetic algorithms.\\nInstead of crossover, mGAs use the splice and cut operators.'), Document(metadata={}, page_content='406 CHAPTER 14 Genetic Algorithms\\nTwo chromosomes can be spliced together by simply joining one to the end\\nof the other. Hence the following two chromosomes:\\n((1,0), (3,0), (4,1), (6,1))\\n((2,1), (3,1), (5,0), (7,0), (8,0))\\ncan be spliced to produce the following chromosome:\\n((1,0), (3,0), (4,1), (6,1), (2,1), (3,1), (5,0), (7,0), (8,0))\\nNote that the genes do not need to be in any particular order because each\\none has its position specified as part of its representation.'), Document(metadata={}, page_content='The cut operator splits one chromosome into two smaller chromosomes.\\nHence, the result of the above splice operation could be cut to produce the\\nfollowing two chromosomes:\\n((1,0), (3,0), (4,1))\\n((6,1), (2,1), (3,1), (5,0), (7,0), (8,0))\\nMGAs are more immune to deception than standard genetic algorithms\\nand have been shown to converge on optimal solutions with extremely\\ndeceptive functions (Goldberg 1989).\\n14.11 Prisoner’ s Dilemma'), Document(metadata={}, page_content='14.11 Prisoner’ s Dilemma\\nWe will now see how genetic algorithms can be used to evolve strategies for\\nplaying a simple game: the Prisoner’s Dilemma.\\nThe background of the game is as follows:\\nTwo prisoners have been arrested on suspicion of committing a crime.\\nThey are kept in separate cells, and each is told that if he betrays his friend\\nhe will receive a reward. If his friend does not betray him, then he will go\\nfree, and receive a reward, while his friend is tortured. If both betray each'), Document(metadata={}, page_content='other, they will both be tortured, and if neither betrays the other, they will\\nbe set free.\\nThe dilemma for each prisoner is whether to defect and betray his friend,\\nor whether to cooperate with his friend and keep silent. Defection always\\nbrings a greater reward than cooperation, but the best overall result is\\nobtained if both prisoners cooperate.\\nThe game of Prisoner’s Dilemma is played over a number of turns, where\\non each turn each player can choose whether to defect or to cooperate, and'), Document(metadata={}, page_content='points are awarded to each player according to the outcome, as defined in\\nTable 14.3.'), Document(metadata={}, page_content='14.11 Prisoner’ s Dilemma 407\\nTable 14.3 Point allocation in Prisoner’ s Dilemma\\nPlayer 1 Player 2 S1 S2 Notes\\ndefects defects 1 1 Penalty for mutual defection\\ndefects cooperates 5 0 Player 1 has been tempted to defect.\\nPlayer 2 is the “sucker.”\\ncooperates defects 0 5 Player 1 is the “sucker.” Player 2 has been\\ntempted to defect.\\ncooperates cooperates 3 3 Reward for mutual cooperation\\nIn Table 14.3, S1 and S2 are the number of points received by player 1 and'), Document(metadata={}, page_content='player 2, respectively, in each given situation.\\n14.11.1 Strategy Representation\\nWe will choose a representation that represents the strategy of a given\\n“player” or chromosome in the population. For our system, we will allow\\neach player to determine its move on a given turn in the game based on the\\nresults of the previous three turns.\\nEach turn in the game can have one of four outcomes, as shown in Table\\n14.3. We shall represent each of these outcomes by a number from 0 to 3:'), Document(metadata={}, page_content='0: reward (both players have cooperated)\\n1: sucker (the player cooperates, and the opponent defects)\\n2: penalty (both players defected)\\n3: temptation (the player defects and the opponent cooperates)\\nNow, by using 0 to represent defection and 1 to represent cooperation, a\\nthree-dimensional array of binary values can be used to represent a strat-\\negy. Because there are four possible choices for each turn, and our strategies'), Document(metadata={}, page_content='will be based on the previous three turns, our array will need to be 4 /H110034 /H11003\\n4 = 64 bits. We will also include three bits that represent the player’s behav-\\nior on the first three turns, so each chromosome will be represented by 67\\nbits. (We will place the three bits that represent the first three turns at the\\nend of the chromosome.)'), Document(metadata={}, page_content='408 CHAPTER 14 Genetic Algorithms\\nHence, a chromosome that consists of 67 1s would cooperate on every go,\\nregardless of the behavior of its opponent. Similarly, 67 0s would mean that\\nthe chromosome defected on every turn.\\nHence, the following chromosome:\\n10000000000000000000000000000000000000000000000000000000000000000111\\nrepresents the following rather simple strategy:\\nCooperate on the first three turns, and thereafter, only cooperate in the'), Document(metadata={}, page_content='event that both players have cooperated on the previous three turns. (The\\nfirst bit represents the [0,0,0] position in the array, which corresponds to\\nthree consecutive occurrences of “reward. ”)\\nThe following chromosome represents a slightly more sophisticated strategy:\\n1001000000001001000000000000000000000000000000001001000000001001111\\nThe last three chromosomes represent the first three decisions: this chromo-\\nsome will cooperate on the first three turns. Thereafter, this chromosome'), Document(metadata={}, page_content='will cooperate only if the opponent has cooperated on the previous three\\nturns. If the opponent cooperates, then the possible results are either reward\\nor temptation. Hence, there are eight possible combinations of three moves\\nin which the opponent has cooperated. These are represented by the eight\\n1’s in the chromosome above at positions 0, 3, 12, 15, 48, 51, 60, and 63.\\nBit position 0 represents the decision when the previous three outcomes'), Document(metadata={}, page_content='have all been value 0 - reward (i.e., both players have cooperated on the\\nprevious three turns). Position three represents (0, 0, 3) or (reward, reward,\\ntemptation)—in other words, the opponent has cooperated on each of the\\nthree turns, and the player cooperated on the first two turns and defected\\non the third turn.\\nEach chromosome represents a complete strategy for how to play Prisoner’s\\nDilemma over an arbitrary number of turns, basing each decision on the'), Document(metadata={}, page_content='outcome of the previous three turns, and with the first three decisions\\nhard-coded. Clearly, there are an astronomical number of possible chro-\\nmosomes (2\\n67 /H3336115 /H110031019) and therefore a correspondingly large number\\nof possible strategies.\\n14.11.2 Possible Strategies\\nThe simplest strategies for playing Prisoner’s Dilemma are “always defect”\\nand “always cooperate, ” whose chromosomes consist of 67 0s and 67 1s,\\nrespectively.'), Document(metadata={}, page_content='14.11 Prisoner’ s Dilemma 409\\nEach of these strategies is a reasonable way to play Prisoner’s Dilemma.\\n“Always defect” ensures that the opponent’s score is minimized, whereas\\n“always cooperate” ensures a maximum possible combined score, in the\\nevent that the opponent also always cooperates. Of course, “always cooper-\\nate” does not do well against “always defect. ” In fact, no strategy does well\\nagainst “always defect, ” being able to achieve a maximum score of only 1'), Document(metadata={}, page_content='out of a possible 5 for each turn. Conversely, the player using the “always\\ndefect” strategy can achieve the full 5 points if its opponent cooperates but\\ngets just 1 point if the opponent also defects.\\nOne very successful strategy is called tit-for-tat. Tit-for-tat involves coop-\\nerating to begin with and thereafter doing whatever the opponent did on\\nthe previous turn.\\nThis strategy only uses information about the previous turn, so much of'), Document(metadata={}, page_content='the data of the chromosome that plays tit-for-tat is redundant. Our\\nhope is that chromosomes whose strategies are based on the previous\\nthree turns, rather than just one turn, can perform better overall than\\ntit-for-tat.\\nWe will assume that a game consists of 100 turns of Prisoner’s Dilemma,\\nand that the total score for a chromosome for a game is the sum of the\\npoints awarded to it in those 100 turns.\\nHence, playing tit-for-tat against tit-for-tat or against “always cooperate”'), Document(metadata={}, page_content='would achieve a total of 300 points. Playing “always defect” can achieve a\\ntotal of 500 points when playing against “always cooperate. ” Table 14.4\\nshows the six possible total scores for these three strategies played against\\neach other over 100 turns.\\nTable 14.4 Total scores for three strategies played over 100 turns\\nPlayer1 Player2 S1 S2\\nAlways cooperate Always cooperate 300 300\\nAlways cooperate Always defect 0 500\\nAlways cooperate Tit-for-tat 300 300\\nAlways defect Always defect 100 100'), Document(metadata={}, page_content='Always defect Always defect 100 100\\nAlways defect Tit-for-tat 104 99\\nTit-for-tat Tit-for-tat 300 300'), Document(metadata={}, page_content='410 CHAPTER 14 Genetic Algorithms\\nClearly, the scores for a game can vary between 0 and 500, depending on\\nthe strategy chosen and the strategy of the opponent.\\n14.11.3 Evolution of Strategies\\nThe process for running this genetic algorithm is as follows:\\n1. Produce a random population of chromosomes. We will start with\\na population of just 100 chromosomes.\\n2. Determine a score for each chromosome by playing its strategy\\nagainst a number of opponents.'), Document(metadata={}, page_content='against a number of opponents.\\n3. Select a number of chromosomes from the population to repro-\\nduce, applying crossover and mutation according to appropriate\\nprobabilities.\\n4. Replace the previous generation with the new population pro-\\nduced by reproduction.\\n5. Return to step 2.\\nA method must be applied to determine for how many generations to run\\nthe genetic algorithm. We will use 100 runs of the genetic algorithm. A ter-\\nmination condition could be applied that determined when an optimal'), Document(metadata={}, page_content='solution has been reached, but as we will see, it is not necessarily clear how\\nto identify such a strategy.\\n14.11.4 Choice of Opponents\\nAs a simple example, we will start by considering playing the chromosomes\\nagainst a fixed strategy.\\nFirst, we will determine each chromosome’s fitness by playing its strategy\\nagainst “always defect” over 100 turns.\\nClearly, the best strategy against “always defect” is also “always defect. ” In'), Document(metadata={}, page_content='our experiments, a population of 100 chromosomes evolved such that the\\naverage score of the 100 chromosomes reached the maximum of 100 after\\njust two generations, as shown in Figure 14.4.\\nSimilar results were found when playing the chromosomes against “always\\ncooperate”: the best strategy here is to play “always defect. ” After just a few\\ngenerations, the average score of the population converged on the maxi-\\nmum of 500 points.'), Document(metadata={}, page_content='14.12 Diversity 411\\n95\\n85\\n75\\n65\\n55\\n45\\n1 5 9 1 31 72 12 52 93 33 74 14 54 9\\nGeneration\\nAverage Score\\naverage score over 50 generations\\nagainst \"always defect\"\\nFigure 14.4\\nAverage scores of a \\npopulation of 100 \\nchromosomes playing\\nagainst “always defect”\\nover 50 generations\\nWhen playing against tit-for-tat, the genetic algorithms converged after just\\na few generations to play “always cooperate, ” which is the best strategy to'), Document(metadata={}, page_content='play against tit-for-tat. In fact, the best strategy to play against tit-for-tat is\\nto cooperate on every turn except the last turn, but our representation does\\nnot allow this strategy.\\nMore interesting results were obtained when the chromosomes were able to\\nplay against each other, rather than against fixed strategies. This genetic\\nalgorithm was found to evolve strategies that were as successful as the best\\nheuristic methods developed by humans (Axelrod 1987).'), Document(metadata={}, page_content='Playing the chromosomes against each other is similar to the idea of intro-\\nducing predators into a population, which is discussed in Section 14.14.\\n14.12 Diversity\\nOne problem with the genetic algorithm we have described above for play-\\ning Prisoner’s Dilemma is that the populations tend to stagnate. That is,\\nonce a chromosome evolves that achieves a very high score, chromosomes\\nthat are different and score less well than this one will tend to die out, and'), Document(metadata={}, page_content='the population will end up with all chromosomes playing the same strat-\\negy. In other words, the population lacks diversity.\\nDiversity is a useful measure that is often used in genetic algorithms to\\navoid stagnation. Like mutation, it also helps to avoid local maxima.\\nHence, it is often a good idea to incorporate a measure of diversity into a\\ngenetic algorithm’s fitness metric.'), Document(metadata={}, page_content='412 CHAPTER 14 Genetic Algorithms\\nFor example, a diversity score of a chromosome could be calculated by\\ncomparing that chromosome with the highest scoring chromosome and\\ncounting the number of bits that differ. This count could then be added to\\nthe fitness score (Winston 1992).\\n14.13 Evolving Pictures\\nIn many genetic algorithm systems, the fitness of an individual chromo-\\nsome is determined by treating the data that is contained in its genes as a'), Document(metadata={}, page_content='representation of a possible solution to the problem or, in some cases, a\\nstrategy for dealing with particular situations.\\nIn the case of Dawkins’ biomorphs, the fitness was determined by user\\nchoice. In this case, the genes contained in each chromosome were not\\ninterpreted as a strategy or a solution to a problem, but as a visual repre-\\nsentation of an image that resembled a living creature—a biomorph.\\nSimilar work was done by T odd and Latham (1992), who used complex'), Document(metadata={}, page_content='chromosomes and human determination of fitness to evolve extremely\\ncomplex, rendered creatures. Their book includes pictures of surprisingly\\nbeautiful creatures that appear to have been designed by an extremely\\nimaginative artist.\\nIn biological terms, the chromosome is the genotype of the creature, and\\nthe physical (or visual) manifestation of the genotype is the phenotype.\\nUsing this model, in most genetic algorithm systems, the phenotype is a'), Document(metadata={}, page_content='kind of behavior, or a solution to a problem. It is possible to have the phe-\\nnotype of a chromosome be interpreted in more than one way. For exam-\\nple, a genetic algorithm could be used to evolve strategies for playing a\\ngame such as Prisoner’s Dilemma and where each chromosome is also\\ninterpreted as a visual representation of a creature. In this way, creatures\\nwould be evolved whose appearance was dependent in some nontranspar-'), Document(metadata={}, page_content='ent way on their behavior. By adding human intervention in determining\\nfitness, a system can be developed that automatically evolves creatures, but\\nwhere humans can override a creature’s fitness in order to bias evolution\\ntoward particular kinds of images.\\nUsing evolutionary techniques for drawing pictures has other applications.\\nIn principle, a person who has an idea of what he or she wants to draw can\\nproduce a picture using crossover and mutation without having any artistic'), Document(metadata={}, page_content='ability. For example, police are able to use this technique to allow a witness'), Document(metadata={}, page_content='14.14 Predators and Co-evolution 413\\nto produce a picture of a suspect in a crime, by repeatedly selecting the face\\nthat looks most like the person they are thinking of.\\nEvolutionary techniques can also be used to evolve behaviors. Karl Sims\\nevolved creatures whose genes were used to represent not only a physical\\nbody but a set of rules to determine how those body parts would behave.\\nHis aim was to evolve creatures that could perform simple tasks such as\\nwalking, swimming, and jumping.'), Document(metadata={}, page_content='walking, swimming, and jumping.\\n14.14 Predators and Co-evolution\\nIn Section 14.11 we see that when genetic algorithms were evolved to play\\nPrisoner’s Dilemma against each other, they developed much more com-\\nplex strategies than they did when they were evolved to play against fixed\\nopponents.\\nThere is strong empirical evidence to suggest that one of the key driving\\nfactors behind the evolution of most complex behaviors and abilities in'), Document(metadata={}, page_content='real-world organisms is the existence of predators. In a world without\\npredators, there is less pressure to perform well and less need to develop\\nsophisticated behaviors in order to survive. This principle also applies in\\nartificial evolution.\\nIn the 1980s, Danny Hillis used evolutionary methods to evolve algorithms\\nfor sorting a sequence of numbers. He called the entities he was evolving\\n“ramps. ” He found that when he introduced “parasites, ” which generated'), Document(metadata={}, page_content='increasingly complex sets of numbers to sort, his ramps were able to per-\\nform far better than they had without the parasites.\\nWhen the evolution of two or more species are intertwined in this way, the\\nprocess is known as co-evolution. This phenomenon was first observed by\\nCharles Darwin (1859). Coevolution can be thought of as an arms race:\\nwhen one country develops the bow and arrow, its enemies need to develop\\na similarly powerful weapon or an appropriate defense. In doing so, this'), Document(metadata={}, page_content='country might stumble upon explosives and thus force their enemies to\\ndevelop suitable defenses to this. In much the same way that the develop-\\nment of one country’s military might causes other countries to develop\\nsimilar capabilities, coevolution in animals means that if one predator\\ndevelops the ability to run faster, its prey must match its speed, or develop\\nanother defense, or it will be wiped out.'), Document(metadata={}, page_content='414 CHAPTER 14 Genetic Algorithms\\nOf course, this process does not happen quickly: at first, the prey will do\\nbadly, and a great many of them will be wiped out. This fact will ensure that\\nthe stronger, faster, better-defended individuals will be more likely to sur-\\nvive and produce offspring. In this way, over a period of many generations,\\nthe species will gradually become better able to survive.\\nAs a result, of course, the predator will need to become faster or develop'), Document(metadata={}, page_content='new abilities that enable it to catch enough prey. In some cases the cycle is\\nbroken as one species dies out or a new species supersedes it.\\nCo-evolution is a vital element of biological evolutionary processes and can\\nalso be taken advantage of in genetic algorithms and other processes that\\ninvolve artificial evolution.\\n14.15 Other Problems\\nGenetic algorithms have been successfully applied to a number of problems\\nin computer science. Most combinatorial search problems (described in'), Document(metadata={}, page_content='more detail in Chapter 5) can be successfully solved using genetic algo-\\nrithms. A great deal of work has been done on applying genetic algorithms\\nto the traveling salesman problem, for example, but also to a number of\\nother problems, including the following:\\n■ The Knight’s T our (moving a knight over a chess board using valid\\nmoves, such that it lands on each square exactly once)\\n■ The CNF-satisfiability problem\\n■ Robot navigation\\n■ The knapsack problem'), Document(metadata={}, page_content='■ Robot navigation\\n■ The knapsack problem\\n■ The timetable problem (assigning teachers to pupils and classrooms)\\n14.16 Chapter Summary\\n■ A genetic algorithm consists of a population of chromosomes,\\neach of which contains a number of genes. The genetic algorithm\\nmanipulates these chromosomes using crossover and mutation to\\nproduce a new, superior generation of chromosomes. This opera-\\ntion is repeated, until an optimal solution is obtained.'), Document(metadata={}, page_content='■ A fitness metric is essential if a problem is to be solved using\\ngenetic algorithms.'), Document(metadata={}, page_content='14.17 Review Questions 415\\n■ Mathematical functions can be readily solved using genetic algorithms.\\n■ Genetic algorithms manipulate schemata. A schema consists of a\\nset of 1s, 0s, and *, where a * represents “don’t care” .\\n■ The schema theory states: Short, low order schemata which are fitter\\nthan the average fitness of the population will appear with exponen-\\ntially increasing regularity in subsequent generations.\\n■ The building-block hypothesis states that genetic algorithms solve'), Document(metadata={}, page_content='problems using discrete building blocks.\\n■ Genetic algorithms are susceptible to deception—a problem\\nwhereby inadequate building blocks appear in highly fit entities.\\n■ Genetic algorithms can be used to evolve strategies for playing the\\nPrisoner’s Dilemma.\\n■ When playing against fixed strategies, the genetic algorithms\\nquickly converge on the optimal strategy. When playing against\\neach other, more complex strategies emerge.\\n■ Diversity can be as important as fitness in evaluating chromo-'), Document(metadata={}, page_content='somes for genetic algorithms.\\n■ Pictures can be evolved using strategies similar to genetic algo-\\nrithms, but with a degree of human intervention.\\n■ Coevolution is the process whereby the development of one\\nspecies affects the evolutionary path taken by another species.\\nCoevolution has been used successfully to improve the perform-\\nance of systems developed using artificial evolution.\\n14.17 Review Questions\\n13.1 Explain the meaning of the following terms in the context of\\ngenetic algorithms:'), Document(metadata={}, page_content='genetic algorithms:\\n■ fitness\\n■ chromosome\\n■ gene\\n■ population\\n■ generation\\n■ crossover'), Document(metadata={}, page_content='416 CHAPTER 14 Genetic Algorithms\\n■ mutation\\n■ deception\\n13.2 Explain how crossover, mutation, and reproduction affect schemata.\\n13.3 Explain how schemata help us to understand why genetic algo-\\nrithms work. What does the schema theorem tell us?\\n13.4 Explain why genetic algorithms are susceptible to deception. Why\\ndo messy genetic algorithms help avoid this problem?\\n13.5 Explain why diversity is important when using genetic algorithms\\nto solve problems.'), Document(metadata={}, page_content='to solve problems.\\n13.6 Explain why introducing predators can help in systems that use\\nartificial evolutionary techniques.\\n13.7 Describe three problems that might be solved using genetic algo-\\nrithms that were not described in this chapter.\\n13.8 Could genetic algorithms be used to play complex games such as\\nchess and checkers? Explain your answer.\\n13.9 “Once you’ve chosen a good representation for your problem you\\nmight as well solve the problem using traditional means—genetic'), Document(metadata={}, page_content='algorithms are a waste of effort” . Discuss.\\n14.18 Exercises\\n13.1 Using pen and paper and a die as a random number generator,\\nwork through five generations of evolution starting from the fol-\\nlowing five chromosomes. Y ou will need to select a mutation rate,\\nand determine a strategy for crossover.\\n1100110011\\n0001111010\\n1010100001\\n0000101000\\n0111000101\\n13.2 Write a program in the programming language of your choice that\\nuses a genetic algorithm to evolve strategies for playing Prisoner’s'), Document(metadata={}, page_content='Dilemma. Start out by having your chromosomes play against a'), Document(metadata={}, page_content='14.19 Further Reading 417\\nfixed strategy, and observe the behavior of the system. Now have\\nthe chromosomes play against each other. How does this affect\\ntheir performance?\\n14.19 Further Reading\\nFor a detailed description of messy genetic algorithms, see Goldberg\\n(1989). The original text on genetic algorithms is Holland (1992). More\\nintroductory coverage is available in the standard Artificial Intelligence\\ntexts. For a discussion of the connection between computer viruses and'), Document(metadata={}, page_content='Artificial Life, see Spafford (1989). The best modern discussions of coevo-\\nlution can be found in Kelly (1994) and Dawkins (1996).\\nWork carried out by Forrest and Mitchell (1992) on Royal Road functions\\nhas shown that in fact the schema theory does not model the real behavior\\nof genetic algorithms as accurately as one would hope.\\nGenetic Algorithm for the Prisoner Dilemma Problem , by R. Axelrod (1987 -\\nin Genetic Algorithms and Simulated Annealing, edited by L. Davis – Hyper-\\nion Books)'), Document(metadata={}, page_content='ion Books)\\nEfficient and Accurate Parallel Genetic Algorithms, by Erick Cantu-Paz (2002\\n– Kluwer Academic Publishers)\\nPractical Handbook of Genetic Algorithms , by Lance Chambers (1995 –\\nCRC Press)\\nGenetic Algorithms and Genetic Programming in Computational Finance ,\\nedited by Shu-Heng Chen (2002 – Kluwer Academic Publishers)\\nAn Introduction to Genetic Algorithms for Scientists and Engineers, by David\\nA. Coley (1999 – World Scientific Publishing Company)'), Document(metadata={}, page_content='The Origin of Species, by Charles Darwin (1859 – reprinted by Penguin)\\nAdaptive Learning by Genetic Algorithms: Analytical Results and Applications\\nto Economic Models, by Herbert Dawid (1999 – Springer V erlag)\\nThe Blind Watchmaker, by Richard Dawkins (1996 – W. W. Norton & Com-\\npany)\\nGenetic Algorithms and Engineering Optimization, by Mitsuo Gen and Run-\\nwei Cheng (1991 – Wiley Interscience)\\nGenetic Algorithms in Search, Optimization and Machine Learning, by David'), Document(metadata={}, page_content='E. Goldberg (1989 – Addison Wesley)'), Document(metadata={}, page_content='418 CHAPTER 14 Genetic Algorithms\\nMessy Genetic Algorithms: Motivation, Analysis and First Results by D.E.\\nGoldberg (1989 - in Complex Systems, Vol. 3, pp. 493–530)\\nRapid, Accurate Optimization of Difficult Problems Using Fast Messy Genetic\\nAlgorithms, by David E. Goldberg, Kalyanmoy Deb, Hillol Kargupta, and\\nGeorges Harik (1993 – in Proceedings of the Fifth International Conference\\non Genetic Algorithms, pp. 56–64)\\nPractical Genetic Algorithms, by Randy L. Haupt and Sue Ellen Haupt (1998'), Document(metadata={}, page_content='– Wiley Interscience)\\nAdaptation in Natural and Artificial Systems: An Introductory Analysis with\\nApplications to Biology, Control, and Artificial Intelligence , by John H. Hol-\\nland (1992 – MIT Press)\\nGenetic Algorithms + Data Structures = Evolution Programs , by Zbigniew\\nMichalewicz (1999 – Springer)\\nAn Introduction to Genetic Algorithms, by Melanie Mitchell (1998 – MIT Press)\\nThe Royal Road for Genetic Algorithms: Fitness Landscapes and GA Perfor-'), Document(metadata={}, page_content='mance, by Melanie Mitchell, Stephanie Forrest, and John H. Holland (1992\\n- In Towards a Practice of Autonomous Systems: Proceedings of the First Euro-\\npean Conference on Artificial Life , edited by Francisco J. Varela and Paul\\nBourgine, pp. 245–254, MIT Press)\\nPrisoner’s Dilemma: John Von Neumann, Game Theory and the Puzzle of the\\nBomb, by William Poundstone (1994 – MIT Press)\\nRepresentations for Genetic and Evolutionary Algorithms, by Franz Rothlauf'), Document(metadata={}, page_content='and David E. Goldberg (2002 – Springer V erlag)\\nArtificial Evolution for Computer Graphicsby Karl Sims (1991 –Siggraph ’91\\n- Annual Conference Proceedings, 1991, pp. 319–328).\\nEvolving Virtual Creatures, by Karl Sims (1994 - Siggraph ’94 - Annual Con-\\nference Proceedings, 1994, pp. 43–50)\\nComputer Viruses as Artificial Life, by Eugene Spafford (1989 – inArtificial Life,\\nAn Overview, edited by Christopher G. Langton, 1995, MIT Press, pp. 249–265)'), Document(metadata={}, page_content='The Simple Genetic Algorithm: Foundations and Theory, by Michael D. Vose\\n(1999 – MIT Press)'), Document(metadata={}, page_content='Planning\\n5\\nIntroduction To Part 5\\nPart 5 is divided into two chapters.\\nIntroduction to Planning\\nThis chapter introduces the ideas behind planning. It starts\\nby explaining the relationship between search and plan-\\nning, and explains why in many real-world problems, plan-\\nning is preferable to search. It also explains situation\\ncalculus, which is an extension to first-order predicate cal-\\nculus used in planning systems, to represent the way the\\nworld changes over time.'), Document(metadata={}, page_content='world changes over time.\\nThis chapter explains the frame problem and introduces\\nways to overcome the problem. It also explains means–ends\\nanalysis, which is covered in more detail in Chapter 16.\\nPlanning Methods\\nChapter 16 expands on the ideas introduced in Chapter 15.\\nIt presents a number of representations and methods that\\nare used in planning, starting with STRIPS. It presents a\\nnumber of examples to illustrate how STRIPS is able to\\nsolve planning problems and also discusses the kinds of'), Document(metadata={}, page_content='problems that might be difficult to solve using STRIPS. A\\nnumber of other representations such as planning graphs\\nand ADL are also explored, as well as some more advanced\\nplanning methods such as probabilistic planning and\\ndynamic world planning.\\nPART\\n15\\nCHAPTER\\n16\\nCHAPTER'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='15CHAPTER\\nIntroduction to Planning\\nThe best laid schemes o’ mice an’ men gang aft a-gley.\\n—Robert Burns, To a Mouse\\nYou can never plan the future by the past.\\n—Edmunde Burke, Letter to a Member of the National Assembly\\nGrow old along with me!\\nThe best is yet to be,\\nThe last of life, for which the first was made:\\nOur times are in His hand\\nWho saith ‘A whole I planned,\\nYouth shows but half; trust God: see all nor be afraid!’\\n—Robert Browning, Rabbi Ben Ezra'), Document(metadata={}, page_content='—Robert Browning, Rabbi Ben Ezra\\nIn preparing for battle I have always found that plans are useless, but that\\nplanning is indispensable.\\n—Dwight D. Eisenhower\\n15.1 Introduction\\nPlanning has recently become a very exciting area of research in Artificial\\nIntelligence. Much of the work of Artificial Intelligence is concerned with\\nproblem solving. As we will see, planning is no exception. Planning is a very\\nsimple concept: a planner starts in an initial state and has a particular goal'), Document(metadata={}, page_content='422 CHAPTER 15 Introduction to Planning\\nit needs to achieve. T o reach the goal state, the planner develops a plan and\\nthen executes that plan.\\nThe planner might be a robot arm, or a mobile robot, but in many cases it\\nexists purely in software and is designed to plan solutions to virtual problems.\\nA planner has a set of possible actions it can take, and as we will see, these\\nactions are usually limited, depending on the current state of the planner.'), Document(metadata={}, page_content='For example, a robot cannot open a door if the door is already open, or if it\\nis nowhere near the door, or if its hands are full.\\nThe actions that a planner can take are its atomic actions—these actions\\nare usually described in terms of the effect they have on the planner and on\\nthe world. For example, a planning system might use actions such as “pick\\nup object X” or “close door” or “go to supermarket. ”\\nNote that thus far we have assumed that the planner is the same entity as'), Document(metadata={}, page_content='the entity that will execute the plan. This is not necessarily the case because\\na planner may be intended to develop plans for another entity. In this book,\\nthough, we tend to continue with the assumption that the system that\\ndevelops the plans is the same system that executes the plans.\\nA plan can usually be considered as a set of subgoals, much like the goal\\ntrees we see in Chapter 3. In this chapter and in Chapter 16, we see some of'), Document(metadata={}, page_content='the representations that are used to describe the state of a planner and its\\nactions. We also examine the methods used by planners to efficiently design\\nand execute plans.\\nIn this chapter, the ideas behind planning are introduced, starting with\\nusing search as a way of identifying a plan of actions.\\nThis chapter also introduces situation calculus, which is a form of first-\\norder predicate calculus, which enables us to reason about the way in which'), Document(metadata={}, page_content='predicates change over time as the result of actions. Effect axioms and\\nframe axioms are introduced, and we discuss the frame problem and how\\nit can be solved by using successor state axioms.\\nThis chapter also introduces the idea of means–ends analysis, an idea that\\nis extensively illustrated in Chapter 16 in a discussion of STRIPS. T o illus-\\ntrate means–ends analysis, this chapter introduces the General Problem\\nSolver, or GPS.'), Document(metadata={}, page_content='15.2 Planning as Search 423\\n15.2 Planning as Search\\nOne approach to planning is to use search techniques, as described in Part\\n2 of this book. For example, a robotic planning agent might have a state\\nthat is described by the following variables:\\nRoom the robot is in\\nRoom the cheese is in\\nIs the robot holding the cheese?\\nLet us further suppose that there are just three rooms—room 1, room 2,\\nand room 3—and that these rooms are arranged such that there is a door'), Document(metadata={}, page_content='from each room to each other room. The robot starts out in room 1, and\\nthe cheese starts in room 3. The robot’s goal is to find the cheese.\\nThe actions the robot can take are as follows:\\nMove from room 1 to room 2\\nMove from room 1 to room 3\\nMove from room 2 to room 1\\nMove from room 2 to room 3\\nMove from room 3 to room 1\\nMove from room 3 to room 2\\nPick up cheese\\nNote that each of these rules has a set of dependencies: to move from room'), Document(metadata={}, page_content='1 to room 2, the robot must currently be in room 1, in order to pick up the\\ncheese, and the robot and the cheese must be in the same room, and so on.\\nIn this chapter, and in Chapter 16, we examine in more detail how these\\nrules are expressed, but for this example we will assume that the actions can\\nonly be carried out in a way that makes sense in our world.\\nWe will use a three-value vector to represent the current state:\\n(room robot is in, room cheese is in, is robot holding cheese?)'), Document(metadata={}, page_content='So the initial state can be described as (1, 3, no), and the goal state can be\\ndescribed as ( x, x, yes) where x is a variable that indicates the room in\\nwhich both the cheese and the robot are located.'), Document(metadata={}, page_content='424 CHAPTER 15 Introduction to Planning\\n2,3,no 3,3,no\\n3,3,yes\\nGo to room 3Go to room 2\\nPick up cheese\\n1,3,no\\nFigure 15.1\\nA highly simplistic search\\ntree used to develop a plan\\nNote that there are 18 possible states that can be described by this vector,\\nbut that the following 6 states are not possible because they involve the\\nrobot holding the cheese, but with the cheese in a different room from the\\nrobot: (1, 2, yes), (1, 3, yes), (2, 1, yes), (2, 3, yes), (3, 1, yes), (3, 2, yes).'), Document(metadata={}, page_content='Hence, there are actually only 12 valid states.\\nIn each state where the robot and the cheese are in different rooms, there\\nare two possible actions that can be taken. For example, in the state (1, 2,\\nno), the robot can either move from room 1 to room 2, or can move from\\nroom 1 to room 3.\\nT o develop a plan, the robot can simply produce a search tree, such as the\\none shown in Figure 15.1, which starts with the initial state and shows'), Document(metadata={}, page_content='every state that can be achieved by applying each action from that state. A\\nsuitable plan is found when the goal state is reached. As with the search\\ntrees we built when examining the cannibals and missionaries problem in\\nChapter 3, we have excluded repeated states.\\nNote that in this simple problem, the search tree is very small, and finding a\\nsuitable plan is thus very easy.\\nIn fact, in most real-world problems, and, indeed, in most Artificial Intelli-'), Document(metadata={}, page_content='gence problems, there are many more possible states, many more actions\\nthat can be taken, and many more variables to consider.\\nFor a robotic agent to be of any use in the real world, it would need to have\\nhundreds or even thousands of possible actions it could take to be able to\\ndeal with the enormous complexities it would surely face. Similarly, for the'), Document(metadata={}, page_content='15.2 Planning as Search 425\\nrobot to understand the world in sufficient detail, it would need to have a\\nstate representation that consisted of a very large number of variables.\\nGiven these factors, the search tree that would be produced for even a very\\nsimple problem would become prohibitively large.\\nThe main problem with using search for planning is that it does not take\\nany account of the effects of actions when considering which action to take.'), Document(metadata={}, page_content='If the agent’s goal is to find the cheese that is in the next room, then consid-\\nering paths in the search tree that start with actions such as “phone the doc-\\ntor, ” “look out of the window, ” “switch on the television set, ” and so on\\nwould be wasteful.\\nAdditionally, as we will see, it does not necessarily make sense for a plan to\\nbe built starting from the initial state. As we have seen in Chapters 3 and 4,\\nit often makes sense to approach a problem by starting from the goal state'), Document(metadata={}, page_content='and working back toward the initial state. Search does not necessarily func-\\ntion well in such situations.\\nAnother reason that search is not usually the best way to approach plan-\\nning is that it does not take into account the independence of multiple\\ngoals. When a planner is presented with a goal that consists of several parts,\\nsuch as “buy some eggs, feed the cat, and get little Johnny a haircut, ” it can\\nconsider each of these goals separately and solve each of them consecu-'), Document(metadata={}, page_content='tively, rather than trying to solve them as a whole. Because a search-based\\napproach would consider the three goals as a single problem, it would end\\nup enormously overcomplicating the problem.\\nOf course, in some situations, the order in which subgoals are achieved can\\nbe very important. For example, the planner might have as a goal “feed the\\ncat, and wash the cat’s bowl. ” In this case, the planner might need to con-'), Document(metadata={}, page_content='sider carefully the order in which it carries out its actions if it is to avoid\\nmaking a simple error, such as putting food into the cat’s bowl and then\\nimmediately washing it up, without waiting for the cat to eat.\\nSimilarly, when solving problems such as the 8-puzzle, described in Chap-\\nter 4, each of the subgoals very much depends on the others. When taking\\nactions to slide tile 5 into its square, it is very likely that all the other tiles'), Document(metadata={}, page_content='will be moved about, and so independently placing each tile in its appro-\\npriate square will not do. Hence, search is a very good way to plan a solu-\\ntion to problems such as the 8-puzzle or the eight-queens problem.'), Document(metadata={}, page_content='426 CHAPTER 15 Introduction to Planning\\n15.3 Situation Calculus\\nIn the example used above, we have used a very simple notation to indicate\\nthe current state of the planner. For most real problems this notation\\nwould not make sense because the number of variables to consider in each\\nstate would be prohibitively large.\\nOne notation that is often used in discussing planning is situation calcu-\\nlus. Situation calculus is a form of first-order predicate calculus.'), Document(metadata={}, page_content='As we see in Chapter 7, first-order predicate calculus allows us to make\\nassertions about objects but does not provide a very good way of expressing\\nchange, or temporal relationships. Situation calculus allows us to describe\\nan object in one state, or situation, and then describe how that object will\\nchange when a given action is taken.\\nFor example, the following situation calculus expression represents the\\nassertion that in situation S\\n1, the robot is in the same room as the cheese:'), Document(metadata={}, page_content='1, the robot is in the same room as the cheese:\\nHere the predicateIn is used to indicate which room a given entity is in. The\\npredicate has been augmented with a third variable, which is the situation that\\nis being described. This third variable,S\\n1, is known as asituation variable.\\nOf course, not all objects change over time, and so we can continue to use\\nstandard predicates without a situation variable. For example, we could use'), Document(metadata={}, page_content='the following expression to assert that the robot’s name is Robbie and that\\nthis will not change over time:\\nName (Robot, Robbie)\\nT o describe the effects that actions have on the world, we use the Result\\nfunction. The Result function takes as arguments an action and a situation\\nand returns the situation that occurs as a result of that action. For example,\\nResult (Move\\ni,j ,S 1) = S2\\nHere we are using the notation Movei,j to indicate the action of moving'), Document(metadata={}, page_content='from room i to room j. Hence, if the current situation is S1, and the robot\\ncarries out the action of moving from room 1 to room 2, this will result in\\nsituation S\\n2.\\n∃ ( ) ∧ ( )( )x In Robot x S In cheese x S,, ,, 11'), Document(metadata={}, page_content='15.4 The Frame Problem 427\\nA number of rules are needed to describe the effects of actions. These rules\\nare known as effect axioms . For example, we might have the following\\neffect axiom:\\nThis effect axiom states the following rule:\\nIf the robot is in a room, y, and an object x is also in that room, then if the\\nrobot carries out a Ta keaction, this will result in a new situation in which\\nthe robot has object x. We might want to further refine this axiom, by'), Document(metadata={}, page_content='ensuring that x is the kind of object that the robot can carry—for example,\\nby stating that it must be light enough and small enough to carry and that\\nit must not be fixed down.\\n15.4 The Frame Problem\\nAs we have seen, when we carry out an action, the world changes. In fact,\\nsome aspects of the world change, but others stay the same. Determining\\nwhich stay the same is known as the frame problem.\\nAn effect axiom states what changes when the robot carries out a particular'), Document(metadata={}, page_content='action in a particular situation. It does not make any statements about what\\ndoes not change. For example, when the robot carries out a Ta keaction, it\\ndoes not find itself in a different room. This kind of rule can be expressed\\nin a frame axiom, such as the following:\\nOf course, most actions that we take do not have any effect on the vast\\nmajority of objects in the real world. This is likely to be true in the world of\\na robot or software planner. As a result, many of frame axioms are needed'), Document(metadata={}, page_content='if we are to describe all of the effects that do not result from carrying out a\\nparticular action. The problem of having enormous numbers of frame\\naxioms is known as the representational frame problem.\\nThe representational frame problem can be solved by using successor state\\naxioms, which effectively combine the effect axioms with the frame\\n∀ ( ) ⇒ ( )( )y s In Robot y s In Robot y Result Take s,, , , , ,\\n∀ ( ) ∧ ( ) ⇒ ( )( )x y s In Robot y s In x y s Has Robot x Result Take s,, ,, ,, ,, ,'), Document(metadata={}, page_content='428 CHAPTER 15 Introduction to Planning\\naxioms. Successor state axioms describe the way in which a predicate\\nchanges over time. For example,\\nThe axiom states the following:\\nThere are only two ways in which an action,a, can result in the robot hold-\\ning object x. The first of these is if the action is Ta ke, and the robot is in the\\nsame room ( y) as the object. The second possibility (after the ∨ in the\\nexpression) is if the robot already has the object, and the action is not Drop.'), Document(metadata={}, page_content='Note that this axiom uses iff (⇔) rather than implies. A ⇔ B means that A\\nimplies B, but that if B is not true, then A can also not be true. In other\\nwords, it is stating that if either the robot takes the object or it already has it,\\nthen it will have it, but that if neither of those is true, then it will not have it.\\nIn this way, one successor state axiom is needed for each predicate whose\\nvalue can change. Although these axioms may become very complex, they'), Document(metadata={}, page_content='avoid the enormous number of unhelpful rules that a system based on\\neffect axioms and frame axioms would have (such as “If I pick up the\\ncheese, then the room’s walls will not change color. ”)\\n15.5 Means–Ends Analysis\\nTypically, a planner needs to find a correct set of actions (a plan) that will\\ntake it from one state to another state—the goal state. One approach to\\nplanning is to consider the differences between the goal state and the cur-'), Document(metadata={}, page_content='rent state, and select actions that aim to lessen those differences: this is\\ncalled means–ends analysis.\\nUnlike search techniques, means–ends analysis can select an action even if it\\nis not possible in the current state. If a planner selects an action that results in\\nthe goal state, but is not currently possible, then it will set as a new goal the\\nconditions necessary for carrying out that action. For example, let us con-'), Document(metadata={}, page_content='sider the blocks world, which is often used to illustrate planning problems.\\nThe blocks world contains a number of blocks and a surface or table top.\\nBlocks can be placed on top of each other or can be placed onto the table.\\n∀ ( )( ) ⇔\\n=∧ ( ) ∧ ( )( )\\n( ) ∧≠( )\\na x y s Has Robot x Result a s\\na Take In Robot y s In x y s\\nHas Robot x s a Drop\\n,,, ,, ,\\n,, ,,\\n,,'), Document(metadata={}, page_content='15.5 Means–Ends Analysis 429\\na\\nb\\nFigure 15.2\\nA start state in the blocks\\nworld\\nThe planner is a robot arm that is able to pick blocks up and to move them\\naround. Let us suppose that the world consists of two blocks, a and b,a s\\nshown in Figure 15.2.\\nLet us suppose that our robot’s goal is to place block b on top of block a,\\nwith block a resting on the table. Using means–ends analysis, our planner\\nstarts by considering how the goal state differs from the current state. In'), Document(metadata={}, page_content='this case, the differences are:\\nBlock b is not on top of block a.\\nBlock a is on top of block b.\\nOur planner could now consider the following two possible actions:\\n1. Place block b on top of block a.\\n2. Remove block a from on top of block b.\\nEach of these actions is interesting because it reduces the differences\\nbetween the current state and the goal state. Our planner might start by\\nselecting action 2 and removing block a from on top of block b. The differ-'), Document(metadata={}, page_content='ences between this new state and the goal state are now as follows:\\nBlock b is not on top of block a.\\nBlock a is not on the table.\\nThe planner’s next action might therefore be to place block a on the table.\\nNow the difference between the current state and the goal state is as follows:\\nBlock b is not on top of block a.\\nThe next action to consider is thus\\n3. Place block b on top of block a.\\nUnfortunately, action 3 cannot be carried out because the robot arm is not'), Document(metadata={}, page_content='currently holding block b. So we have a new goal state, which is\\nRobot arm is holding block b.\\nHence, before carrying out action 3, the planner must achieve this goal,\\nwhich it does by carrying out the following action:\\n4. Pick up block b.'), Document(metadata={}, page_content='430 CHAPTER 15 Introduction to Planning\\nNote that all of this planning is carried out before the robot starts to move.\\nHence, when it has completed building the plan, it is able to carry out the\\nfollowing actions:\\nRemove block a from on top of block b.\\nPlace block a on the table.\\nPick up block b.\\nPlace block b on top of block a.\\nHence, the goal has been achieved. As we will see, this approach can be used in\\nmuch more complex planning systems to solve far more interesting problems.'), Document(metadata={}, page_content='The General Problem Solver,o r  GPS, was developed by Newell, Shaw, and\\nSimon in the late 1950s (Newell et al. 1959, Newell and Simon 1963) as an\\nattempt to simulate human thought, with the intention of using this\\napproach to solve problems of a general nature.\\nGPS uses means–ends analysis to solve logic problems, such as showing the\\nequivalence of the following two logical expressions:\\n(R →\\n¬P) ∧ (R → Q)\\n¬(¬Q ∧ P)\\nIn their 1963 paper, Newell and Simon explain how GPS examines the dif-'), Document(metadata={}, page_content='ferences between these two expressions and use a set of three simple meth-\\nods that can be used to transform expressions, based on the logical\\nequivalence rules shown in Chapter 7 of this book.\\nFor example, GPS might start by removing the → operators in the first\\nexpression, using the following rule:\\nA → B\\n⇔¬ A V B\\nAs we see in Chapter 16, STRIPS is a planning system that uses\\nmeans–ends analysis in a manner similar to that used by GPS to control the'), Document(metadata={}, page_content='actions of a robot through a simple environment.\\n15.6 Chapter Summary\\n■ One way to find a plan to solve a problem is to apply a search\\nmethod and search through the search space of all possible plans.\\nThis works well for very simple problems, but is not efficient for\\ncomplex problems or those involving many possible actions and\\nvariables.'), Document(metadata={}, page_content='15.8 Exercises 431\\n■ In solving problems where each action can undo the effects of\\nother actions, it is necessary to use search.\\n■ Situation calculus is an extension of first-order predicate calcu-\\nlus, which uses situation variables to express how objects change\\nover time.\\n■ Effect axioms for an action state what changes after that action\\ntakes place.\\n■ Frame axioms state what variables do not change after carrying out\\nan action.\\n■ The frame problem is the problem of determining what does not'), Document(metadata={}, page_content='change when an action is carried out. Using successor state axioms,\\nwhich combine features of effect axioms and frame axioms, solves\\nthis problem.\\n■ Means–ends analysis, as used by GPS, involves determining the dif-\\nferences between the current state and the goal state, and choosing\\nactions that minimize those differences.\\n15.7 Review Questions\\n15.1 What is planning?\\n15.2 Explain why the search methods described in Chapter 4 can be\\nused for planning.'), Document(metadata={}, page_content='used for planning.\\n15.3 Why can first-order predicate calculus not be used for planning\\nwithout the addition of situation variables?\\n15.4 Explain what is meant by the frame problem. What is the represen-\\ntational frame problem? Why are these problems so important to\\nthe study of planning?\\n15.5 Explain the idea behind means–ends analysis. Compare it with\\nsearch as a planning method. Which more closely matches the\\nmethods people use when formulating plans in everyday life?\\n15.8 Exercises'), Document(metadata={}, page_content='15.8 Exercises\\n15.1 Write a program in the language of your choice that uses search to\\nformulate plans for moving from one arrangement of three blocks\\nto another. Assume that the available actions that can be taken are to'), Document(metadata={}, page_content='432 CHAPTER 15 Introduction to Planning\\nmove a block from one of three locations to another, and that if one\\nblock is placed into a location in which another block is already\\npresent, then the second block is placed on top of the first block.\\n15.2 Consider extending your program to work with larger numbers of\\nblocks. How well do you think it will work?\\n15.9 Further Reading\\nMost of the standard texts provide good coverage of planning. Russell and'), Document(metadata={}, page_content='Norvig provide a particularly thorough treatment, in the context of intelli-\\ngent agents. Newell, Shaw, and Simon’s papers on GPS and Fikes and Nilsson’s\\npaper on STRIPS provide good introductions to those systems. The Further\\nReading section of Chapter 16 contains more references on planning.\\nSTRIPS: A New Approach to the Application of Theorem Proving to Problem\\nSolving, by Richard E. Fikes and Nils J. Nilsson (1971 – in Computation &\\nIntelligence, edited by George F. Luger, 1995 – MIT Press)'), Document(metadata={}, page_content='The Robots Dilemma Revisited: The Frame Problem in Artificial Intelligence ,\\nby Kenneth M. Ford and Zenon W. Pylyshyn (1996 – Ablex Publishing)\\nGPS, A Program That Simulates Human Thought, by Alan Newell and Her-\\nbert A. Simon (1963 – in Computation & Intelligence , edited by George F.\\nLuger, 1995 – MIT Press)\\nReport on a General Problem Solving Program , by Alan Newell, J. C. Shaw,\\nand Herbert A. Simon (1959 – in Proceedings of the International Conference'), Document(metadata={}, page_content='on Information Processing, pp. 256–264)\\nThe Robots Dilemma: The Frame Problem in Artificial Intelligence, by Zenon\\nW. Pylyshyn (1987 – Ablex Publishing)\\nChoices, Values, and Frames, edited by Daniel Kahneman and Amos Tversky\\n(2000 – Cambridge University Press)\\nRecent Advances in AI Planning , by Daniel S. Weld (1998 – appeared in AI\\nMagazine, 1999)'), Document(metadata={}, page_content='16CHAPTER\\nPlanning Methods\\nI have a cunning plan. . .\\n—Baldrick from Blackadder\\nThis very remarkable man\\nCommends a most practical plan:\\nYou can do what you want\\nIf you don’t think you can’t,\\nSo don’t think you can’t – think you can.\\n—Charles Inge, ‘On Monsieur Coué’\\nAwake, my St John! Leave all meaner things\\nTo low ambition, and the pride of kings.\\nLet us (since Life can little more supply\\nThan just to look about us and to die)\\nExpatiate free o’er all this scene of man;'), Document(metadata={}, page_content='Expatiate free o’er all this scene of man;\\nA mighty maze! but not without a plan.\\n—Alexander Pope, An Essay on Man\\n16.1 Introduction\\nPlanning methods are used to solve problems where a sequence of actions\\nmust be carried out to reach a goal. In this chapter, we often consider the\\nblocks world, in which a robot arm must reorganize a set of blocks from\\none arrangement to another. Planning is also used a great deal in industry,'), Document(metadata={}, page_content='434 CHAPTER 16 Planning Methods\\nfor routing transportation, organizing allocation of machines in factories,\\nand controlling robots and intelligent agents.\\nIn Chapter 19, we see how intelligent agents can base their actions on beliefs,\\ndesires, and intentions. Agents have beliefs about the world and desires that\\nmust be fulfilled. T o achieve these desires, an agent forms intentions, or\\nplans, which specify in advance what it will do. An agent that does not plan'), Document(metadata={}, page_content='is able only to respond to its environment as it encounters it and will often\\nfind itself falling into traps that a planning agent would have foreseen.\\nPlanning is an extremely important part of Artificial Intelligence research.\\nThis chapter explores a number of algorithms and representations that are\\nused in planning and introduces some of the ideas that have been\\nresearched in the past 10 years.\\nWe start by examining STRIPS, which was an early planning system based'), Document(metadata={}, page_content='on the means–ends strategy discussed in Chapter 15. Although STRIPS has\\nbeen superseded by a number of more sophisticated methods, the language\\nit uses to represent planning problems is still widely used.\\nWe then briefly explore partial order planning, in which plans are specified\\nsuch that the order in which some actions are carried out is unimportant.\\nThis chapter explores the ways in which propositional calculus can be used'), Document(metadata={}, page_content='to represent and solve planning problems, including producing plans by\\nexamining the satisfiability of propositional sentences.\\nWe then explore some other representations for planning problems includ-\\ning planning graphs (used by the GraphPlan algorithm) and ADL, which is\\nan extension of the STRIPS language.\\nWe also examine ways in which planning can be carried out in an uncertain\\nworld and ways in which planners can learn from their past actions and\\nmistakes.'), Document(metadata={}, page_content='mistakes.\\nFinally, we briefly explore the relationship between planning and scheduling.\\n16.2 STRIPS\\nSTRIPS (Stanford Research Institute Problem Solver) is an operator-based\\nplanning approach that was developed by Fikes and Nilsson in the 1970s\\n(Fikes and Nilsson 1971). This is in contrast with the use of situation vari-'), Document(metadata={}, page_content='16.2 STRIPS 435\\nables and frame axioms that we see in Chapter 15, when using the logic of\\nsituation calculus.\\nSTRIPS uses a means–ends analysis strategy, which was described in Sec-\\ntion 15.5. Means–ends analysis simply involves identifying the differences\\nbetween the current state and the goal state, and selecting actions that\\nreduce those differences.\\nSTRIPS uses well-formed formulae (wffs) in first-order predicate calculus\\nto describe the world, in much the same way that we see in Chapter 15.'), Document(metadata={}, page_content='STRIPS was designed to provide planning for robotic agents to enable\\nthem to navigate through a world of blocks, but the approach can also be\\nused in other planning problems.\\nFor example, the following wff can be used to state the rule that if an object\\nis in one location, then it cannot be in another:\\nThis wff states that if an object, o, is in location x, where x is not the same\\nlocation as y, then object o cannot be in location y.'), Document(metadata={}, page_content='Note that unlike the examples in Chapter 15, locations in STRIPS are\\nexpressed as vectors, rather than as entire rooms. In other words, in the\\nabove expression, x and y represent the physical coordinates of the robot,\\nmeasured in some units of distance from a point that is considered to be\\nthe origin: (0,0).\\n16.2.1 Planning and Executing\\nSTRIPS uses a set of operators, which represent the actions that can be\\ntaken, or the steps that can be included in a plan. For example, operator'), Document(metadata={}, page_content='Push (o, x, y) enables the robot to push object o from location x to location\\ny. Note that there is a distinct difference between considering the operator\\nPush and actually carrying out the act of pushing. This is the difference\\nbetween planning and executing. Most of this chapter is concerned with\\nplanning, which means selecting a suitable sequence of operators. Once the\\nsequence has been chosen, the plan can be executed, which means carrying'), Document(metadata={}, page_content='out the actions described. This has some important implications: if carry-\\ning out an action has an unexpected effect that was not planned for, the\\nplan may not succeed. Should the robot continue with the plan regardless,\\n∀∀∀( ) ( ) ∧≠( )( ) ⇒¬ ( )( )o x y A Tox x y A Toy,,'), Document(metadata={}, page_content='436 CHAPTER 16 Planning Methods\\nor should it stop and develop a new plan based on the unexpected state it\\nhas found the world in? We consider these issues in Section 16.11.\\n16.2.2 Operators\\nEach operator that STRIPS uses is defined by two components. The first is\\nthe effect that the operator will have on the world, and the second is the\\npreconditions that must be met for the action to be carried out.\\nThe preconditions are specified as a set of wffs that must be proven to hold'), Document(metadata={}, page_content='for the current state, or world model. The world model contains a list of\\nwffs that are true of the world in the current state, such as AT(r, x), which\\nmeans that the robot is at position x,o r  AT(o, y), which means that object o\\nis at position y.\\nSTRIPS includes information on two different types of effect that an oper-\\nator can have: the statements (or wffs) that become true after carrying out\\nthe action and the statements that are no longer true. Each operator can'), Document(metadata={}, page_content='thus be defined by a list of wffs that must be added to the world model and\\na list of wffs that must be deleted. These lists are often called the add list\\nand the delete list.\\nHence, the Push (o, x, y) operator could be fully defined as in the following\\nexample:\\nPrecondition: AT(r, x)\\n∧ AT(o, x)\\nDelete: AT(r, x)\\nAT(o, x)\\nAdd: AT(r, y)\\nAT(o, y)\\nIn other words, to push object o from position x to position y, the robot\\nand the object must both start out in position x. As a result of this action,'), Document(metadata={}, page_content='neither the robot nor the object will still be in position x: both will be in\\nposition y.\\nThis definition defines an operator schema, which means that it does not\\ndefine an actual action, but rather a type of action. A real action is an\\ninstance of the schema, in which the variables are instantiated with actual\\nobjects. Hence, for example, we could describe pushing object o\\n1 from'), Document(metadata={}, page_content='16.2 STRIPS 437\\nposition with coordinates (2,3) to position (1,4) by the following operator\\ninstance:\\nPush (o1, (2,3), (1,4))\\nWhen the world model includes statements that can be used to instantiate\\nthe preconditions of a particular operator, then we say that this operator is\\napplicable.\\nThe final element of STRIPS is the goal state, which is described by a wff, or\\na set of wffs, that define the state that the robot wants to reach. Once the'), Document(metadata={}, page_content='planner finds a way to reach this goal state, it has successfully solved its\\nproblem and is ready to execute the plan.\\n16.2.3 Implementation of STRIPS\\nThe algorithm used by the original STRIPS program to develop plans was\\nas follows:\\nFirst, the current world model is compared with the wffs that define the\\ngoal state. If the goal can be satisfied by the current world model, then the\\nproblem is solved, and the planning can terminate.'), Document(metadata={}, page_content='In fact, STRIPS used the method explained in Chapter 8 for proving theo-\\nrems using resolution. This method involves assuming the negation of the\\ngoal, and then showing that this is inconsistent with the current world\\nstate, by using the method of unification to instantiate variables in the\\nschemata with real-world objects. If this method successfully shows an\\ninconsistency, then the goal is consistent with the world state.\\nIf this is not the case, then a plan must be developed.'), Document(metadata={}, page_content='T o select a suitable operator (or action) to apply, STRIPS used the same\\nmethod as GPS (which is described in Chapter 15), which means determin-\\ning the differences between the current state and the goal state and select-\\ning an operator that lessens those differences.\\nHaving applied unification and resolution, the original STRIPS program\\nused the resulting partial proof as a representation of these differences.\\nHence, the running of STRIPS involved alternately applying resolution'), Document(metadata={}, page_content='(theorem proving) and means–ends analysis.\\nSTRIPS solves the frame problem by making what is known as the STRIPS\\nassumption: that any statement that is true before applying an operator is'), Document(metadata={}, page_content='438 CHAPTER 16 Planning Methods\\nb\\nca\\nFigure 16.1\\nStart state for the blocks\\nworld problem\\nalso true after applying the operator, unless it is included in the operator’s\\ndelete list.\\n16.2.4 Example: STRIPS\\nWe will now examine a simple example of STRIPS in action, in the blocks\\nworld, which consists of a table, three blocks (a, b, and c) and a robot arm\\nthat can move blocks around.\\nThe initial state of the world is shown in Figure 16.1. Block a is on the table,'), Document(metadata={}, page_content='and block b is on top of block c, which is in turn placed directly on the\\ntable.\\nWe will use two predicates to describe the world:\\nOn (x, y) means that block x is on top of block y.\\nClear (x) means that block x has no block on top of it.\\nWe will also use t to represent the table. Hence,On (a, t) means that block a\\nis on the table.Clear (t) will always be true because we assume that the table\\nis large enough to hold at least three blocks at once.'), Document(metadata={}, page_content='Our goal is to place block c on top of block a, which can be stated as\\nOn (c, a)\\nOur start state can be described as\\nOn (a, t)\\nOn (b, c)\\nOn (c, t)\\nClear (b)\\nClear (a)\\nClear (t)\\nWe have one available operator schema: MoveOnto (x,y), which means\\n“move object x from wherever it is, and place it on top of object y.”'), Document(metadata={}, page_content='16.2 STRIPS 439\\nMoveOnto (x,y) is defined as\\nPreconditions: On (x, z) ∧ Clear (x) ∧ Clear (y)\\nDelete: On (x, z)\\nClear (y)\\nAdd: On (x, y)\\nClear (z)\\nIn fact, this is not quite correct because if we move object b from on top of\\nobject c and place it on the table,Clear (t) is still true. We could address this\\nby including an additional operator schema MoveOntoT able(x), which is\\ndefined as follows:\\nPreconditions: On (x, y) ∧ Clear (x)\\nDelete: On (x, y)\\nAdd: On (x, t)\\nClear (y)'), Document(metadata={}, page_content='Delete: On (x, y)\\nAdd: On (x, t)\\nClear (y)\\nA number of approaches can be used to build the plan. The first approach\\nwe will consider is to use forward chaining. In other words, we will simply\\nsearch through the space of possible plans until we find a suitable one. This\\nwill involve constructing a tree where the root node represents the start\\nstate, and other nodes represent other possible states that can be obtained\\nby applying operators.'), Document(metadata={}, page_content='by applying operators.\\nFor example, from the initial state, there are three operators we could\\napply:\\nMoveOnto (a, b)\\nMoveOnto (b, a)\\nMoveOntoT able(b)\\nOther operators, such as MoveOntoT able(c) are not possible because their\\npreconditions are not met by the current world state.\\nLet us suppose that we choose to apply MoveOntoT able(b). This has pre-\\ncondition\\nOn (b, y) \\n∧ Clear (b)\\nwhich is matched by instantiating y with c. Hence, after using the operator,'), Document(metadata={}, page_content='we will need to apply the following add and delete lists to our current state:'), Document(metadata={}, page_content='440 CHAPTER 16 Planning Methods\\nDelete: On (b, c)\\nAdd: On (b, t)\\nClear (c)\\nHence, our state description becomes\\nOn (a, t)\\nOn (b, t)\\nOn (c, t)\\nClear (b)\\nClear (a)\\nClear (c)\\nClear (t)\\nFrom this position we could apply any of the following operators:\\nMoveOnto (a, b)\\nMoveOnto (a, c)\\nMoveOnto (b, a)\\nMoveOnto (b, c)\\nMoveOnto (c, a)\\nMoveOnto (c, b)\\nUsing the blind search method, we would simply try each of these and add\\na new node to the tree for each resulting state. In fact, by applying'), Document(metadata={}, page_content='MoveOnto (c, a), we produce a state that matches the goal state, and so a\\nsuitable plan has been found.\\nThis method did not use means–ends analysis, and although it would be\\nfeasible for a problem of this scale, it would not work at all for real-world\\nproblems involving hundreds of operators and objects.\\nThe means–ends analysis approach would start by noticing the differences\\nbetween the start state and the goal state: block c is not on block a and is in'), Document(metadata={}, page_content='fact under block b. The fact that block c is not clear does not matter because\\nthis is not mentioned explicitly in the goal.\\nT o reduce this difference, we could apply the operator MoveOnto (c, a).\\nHowever, this operator’s preconditions are not currently met because it'), Document(metadata={}, page_content='16.2 STRIPS 441\\nrequires that c be clear. We note that operator MoveOntoT able(b) has the\\ndesired effect of clearing c. Hence, we have arrived at a suitable plan by\\nusing a form of backward chaining—starting at the goal and identifying\\nsteps that could lead to the goal.\\nOf course, we were lucky in our second choice. We might equally have cho-\\nsen MoveOnto (b, a) because this also has the effect of clearing c. In this\\ncase, we would then find ourselves in a position where we had further diffi-'), Document(metadata={}, page_content='culties. At this point, the planner would likely backtrack and try a different\\nchoice because it clearly made the problem harder, rather than moving\\ncloser to a solution.\\n16.2.5 Example: STRIPS and Resolution\\nLet us now consider in more detail the mechanics of STRIPS and, in partic-\\nular, how the original program used resolution and unification. We will\\nconsider a slightly different version of the blocks world, as designed by\\nFikes and Nilsson in their original description of STRIPS. The robot'), Document(metadata={}, page_content='(named Shakey, due to his unstable gait) starts out in position x, and his\\ntask is to bring two objects, a and b, together. The two objects start out in\\npositions y and z. We will assume that two objects are together when they\\nare both at the same position, and we will further assume that the robot can\\npush two objects together, ignoring the difficulties that this would pose in\\nthe real world.\\nHence, the initial world state is described by\\nAT(r, x)\\nAT(a, y)\\nAT(b, z)'), Document(metadata={}, page_content='AT(r, x)\\nAT(a, y)\\nAT(b, z)\\nThe goal can be described by the following wff:\\nThe operators available to the planner are the following:\\nP\\nush (o, x, y)\\nPrecondition: AT(r, x) ∧ AT(o, x)\\nDelete: AT(r, x)\\nAT(o, x)\\n∃( ) ( ) ∧ ( )( )p A Tap A Tbp,,'), Document(metadata={}, page_content='442 CHAPTER 16 Planning Methods\\nAdd: AT(r, y)\\nAT(o, y)\\nGo (x, y)\\nPrecondition: AT(r, x)\\nDelete: AT(r, x)\\nAdd: AT(r, y)\\nThe first stage is to negate the wff that describes the goal:\\n(Note that we have used the equivalence between \\n¬(∃x)e and (∀x)¬e and\\nhave then applied DeMorgan’s law to obtain this expression.)\\nFor the purposes of resolution, we can consider this to be the set of clauses:\\n{(¬AT(a,p),¬AT(b, p))}\\nWe now attempt to prove by using resolution that this expression is incon-'), Document(metadata={}, page_content='sistent with the current world state. In fact, this will not be possible, and we\\nwill obtain only a partial proof, which can then be used to describe the dif-\\nference between the initial state and the goal state.\\nThe first stage of this resolution would be to unify the following sets of\\nclauses:\\n{(\\n¬AT(a,p),¬AT(b,p))}\\n{(AT(r,x)), (AT(a, y)), (AT(b,z))}\\nWe will apply the unifier {y/p} and obtain the following set of clauses:\\n{(¬AT(a,y),¬AT(b,y)), (AT(r,x)), (AT(a,y)), (AT(b,z))}'), Document(metadata={}, page_content='This resolves to give the following set of clauses:\\n{(¬AT(b,y)), (AT(r,x)), (AT(b,z))}\\nClearly, a difference that needs to be rectified is that object b is not at loca-\\ntion y, but is at location z. Hence, STRIPS will see if it can apply operator\\nPush (b, z, y).\\nT o determine whether this operator’s preconditions are met, the precondi-\\ntions are negated and added to the set of clauses. The preconditions for the\\npush operator are\\n∀( ) ¬ ( ) ∨¬ ( )( )p A Tap A Tbp,,'), Document(metadata={}, page_content='16.3 The Sussman Anomaly 443\\nb\\nca\\na\\nb\\nc Figure 16.2\\nThe start and goal states\\nfor the Sussman anomaly\\nproblem\\nAT(r, z) ∧ AT(b, z)\\nWe negate this and apply DeMorgan’s law to give\\n¬AT (r,z) ∨¬ AT (b,z)\\nWe add these to the clauses and obtain\\n{(¬AT(b,y)), (AT(r,x)), (AT(b,z)), (¬AT(r,z),¬AT(b,z))}\\nThis resolves to give the following partial proof:\\n{(¬AT(b,y)), (AT(r,x)), (¬AT(r,z))}\\nAgain, a complete proof was not possible, and we are left with a partial res-'), Document(metadata={}, page_content='olution proof. This shows that a further difference that needs to be rectified\\nis that the robot is at position x, whereas it should be in position z, in order\\nto carry out the Push (b,z,y) operator. Hence, the Go (x,z) operator is\\nattempted.\\nIn this case, the precondition when negated is\\n¬AT(r, x)\\nThis is now added to the set of clauses that are to be resolved, and the\\nprocess continues.\\nEventually, a set of operators is found that enables the clauses to resolve to'), Document(metadata={}, page_content='falsum, meaning that the goal state has been reached.\\n16.3 The Sussman Anomaly\\nWe return now to the blocks world of the example in Section 16.2.4.\\nConsider the start state shown in Figure 16.2. Our goal now is to place\\nblock c on top of block b, and block a on top of block b. This is the second\\nstate shown in Figure 16.2.\\nThe STRIPS approach to this problem would start by either moving b onto\\nthe table, and then placing c on a, or by moving a on top of b without first'), Document(metadata={}, page_content='removing b from c. In either of these cases, the solution cannot be reached\\nwithout undoing this first move. Many early planning systems could not'), Document(metadata={}, page_content='444 CHAPTER 16 Planning Methods\\nd\\nc\\nb\\na\\nc\\nd\\na\\nb\\nFigure 16.3\\nStart and goal states for a\\npartial order planning\\nproblem\\nsolve problems of this nature, in which the two aspects of the goal have\\ndependencies on each other. The correct solution, of course, is to first move\\nb onto the table, then place a on top of b, and finally move c on top of a,b u t\\nthis involves interleaving the solutions to the two components of the goal,'), Document(metadata={}, page_content='which is not easily achieved using STRIPS in its original form. Later in this\\nchapter, we see methods that are able to deal with Sussman’s anomaly more\\nelegantly.\\n16.4 Partial Order Planning\\nThe plans that we have considered so far are known as total order plans\\nbecause they dictate the order in which each action must be carried out. In\\nsome cases, a partial order plan can be used, in which actions that are\\ndependent on each other are ordered in relation to each other but not nec-'), Document(metadata={}, page_content='essarily in relation to other independent actions.\\nFor example, let us consider the blocks world problem with the start and\\ngoal states as shown in Figure 16.3.\\nA total order plan for this problem might be described as follows:\\nMoveOntoT able(b)\\nMoveOntoT able(d)\\nMoveOnto (a, b)\\nMoveOnto (c, d)\\nT o place a on top of b, it is important that b is moved onto the table first. It\\ndoes not matter whether c or d have moved yet.\\nHence, a partial order plan can be described as shown in Figure 16.4.'), Document(metadata={}, page_content='This partial order plan shows that before MoveOnto (a,b) can be carried\\nout, MoveOntoT able(b) must be applied; similarly, it shows that MoveOn-\\ntoT able(c) must be applied before MoveOnto (a,b) can be carried out.\\nT o finish, or reach the goal state, the robot must have carried out all four\\nactions.'), Document(metadata={}, page_content='16.4 Partial Order Planning 445\\nMoveOnto Table (b)\\nMoveOnto Table (a, b)\\nMoveOnto Table (c)\\nMoveOnto Table (a, b)\\nStart\\nFinish Figure 16.4\\nA partial order plan\\nStart\\nGoal\\nFigure 16.5\\nThe initial stage in build-\\ning a partial order plan\\nThis representation for a plan enables the planner to consider a number of\\ndifferent plans, without worrying about ordering actions that are not\\ndependent on each other. At the time of execution, the robot can select any'), Document(metadata={}, page_content='ordering that matches this partial order. Note that the partial plans can be\\ninterleaved. Hence, a suitable total plan might be\\nMoveOntoT able(b)\\nMoveOnto (a, b)\\nMoveOntoT able(d)\\nMoveOnto (c, d)\\nThe idea behind partial order planning is to develop a partial order plan\\nthat ends with the goal state and where each transition in the plan is legal,\\naccording to the definitions of the available operators.\\nT o see how this works, let us consider the blocks world problem shown in'), Document(metadata={}, page_content='Figure 16.3. We will examine how the planning system might build up a\\npartial order plan.\\nAt first, the system has a start state and a goal state, as shown in Figure 16.5.'), Document(metadata={}, page_content='446 CHAPTER 16 Planning Methods\\nMoveOnto (a, b) MoveOnto (c, d)\\nStart\\nGoal\\nFigure 16.6\\nThe next stage in building\\nthe partial order plan\\nOur task is now to build a plan that gets us from the start state to the goal\\nstate.\\nThe first step is to add in operators that achieve the conditions set in the\\ndefinition of the goal state. As we saw above, these operators are MoveOnto\\n(a,b) and MoveOnto (c,d). In Figure 16.6, we have added these operators to'), Document(metadata={}, page_content='the partial plan (it is a partial order plan, but it is also a partial plan, in the\\nsense that it is not yet complete).\\nIn Figure 16.6, the solid arrows represent causal links or establishes links,\\nwhich show how an action causes or establishes a set of conditions that\\nmatch the criteria for the goal state. The dashed arrows are not yet causal\\nlinks because they do not explain how one gets from the start state to the\\nnext states. These dashed arrows simply show the order in which actions'), Document(metadata={}, page_content='must occur, so in this case, the two operators shown must occur after the\\nstart state.\\nNext we must find operators that will enable us to satisfy the preconditions\\nof the operators we have just added. These preconditions are met by\\nMoveOntoT able(b) and MoveOntoT able(d).\\nAdding these operators to the partial plan shown in Figure 16.6 leads to the\\npartial order plan (which is no longer a partial plan because it gives all the'), Document(metadata={}, page_content='steps necessary to get from the start to the goal) shown in Figure 16.4.\\nIn building a partial order plan, there is a potential problem to consider,\\nwhich is that one action might undo the effects of another action, in which\\ncase the order in which those actions are carried out can be important. A\\ncausal link is said to be protected when it is needed to establish the precon-\\nditions of an operator below it in the plan. If another operator has the'), Document(metadata={}, page_content='16.5 The Principle of Least Commitment 447\\nFinish\\nOp 1 Op 2\\nStart\\nOp 2\\nStart\\nOp 3\\nFinish\\nOp 1\\nOp 3\\nStart\\nFinish\\nOp 1\\nOp 3 Op 2\\nFigure 16.7\\nAn example of a causal link\\nbeing threatened by\\nanother operator and two\\nways in which this problem\\ncan be rectified\\neffect of deleting some necessary part of that precondition, then the pro-\\ntected link is said to be threatened by this second operator. This is shown\\nin the first part of Figure 16.7.'), Document(metadata={}, page_content='in the first part of Figure 16.7.\\nLet us suppose that one of the preconditions of Op 3 is x, and that Op 1 has\\nx in its add list. In other words,x is one effect of carrying out the action Op\\n1. Unfortunately, Op 2 has x in its delete list, meaning that one effect of car-\\nrying out Op 2 is to undo x or to set ¬x to be true. Hence, the partial order\\nplan shown in the first part of Figure 16.7 does not work because the nature\\nof the partial order is such that Op 2 might be carried out between Op 1'), Document(metadata={}, page_content='and Op 3, thus ruining the plan. Hence, the second and third parts of Fig-\\nure 16.7 show ways in which the plan can be rearranged to ensure that this\\nproblem is avoided.\\nIn the second part of Figure 16.7, Op 2 has been demoted, which means\\nthat the plan ensures that it must occur before Op 1. In the final part of\\nFigure 16.7, Op 2 has been promoted, meaning it must be carried out after\\nOp 3. The partial order already dictated that Op 3 must take place after Op'), Document(metadata={}, page_content='1, so this also ensures that Op 2 must take place after Op 1.\\n16.5 The Principle of Least Commitment\\nIn many planning problems, a number of additional variables exist that are\\nnot relevant to the problem nor need to be modified to solve it. For exam-\\nple, suppose that in solving the problem shown in Figure 16.3 there was\\nactually another block, block e, on the table. Because the goal does not state\\nanything about this block, it does not matter and can be left alone, unless it'), Document(metadata={}, page_content='448 CHAPTER 16 Planning Methods\\nin some way gets in the way of solving the problem (e.g., if it is on top of\\nblock b).\\nHowever, the planning system may still want to use block e because in for-\\nward or backward chaining it will make use of the variables that are pres-\\nent.\\nIn such cases, planners use the principle of least commitment , which\\nmeans that as few variables as possible are instantiated in producing a plan.\\nHence, an operator schema such as MoveOnto (a, y) should be used in pref-'), Document(metadata={}, page_content='erence to one where y has been instantiated [e.g., MoveOnto (a, b)], wher-\\never that is possible. In that way, the planner works with an accurate and\\nworking plan but does so in as efficient a manner as possible.\\nFor example, if the solution involved moving block a onto block b via the\\ntable or another block, the planner could consider this without deciding\\nwhether to move it via a block or the table.\\n16.6 Propositional Planning'), Document(metadata={}, page_content='16.6 Propositional Planning\\nMany planning problems can be expressed purely in propositional logic\\nnotation. In fact, plans expressed in STRIPS notation can always be con-\\nverted to propositional notation, although this will often involve increasing\\nthe number of variables required enormously.\\nFor example, if we consider a blocks world problem in which there are two\\nblocks, A and B, we might represent the various possible states in STRIPS\\nnotation using the following predicates:\\nClear (x)'), Document(metadata={}, page_content='Clear (x)\\nOn (x, y)\\nIn propositional notation, we will use one propositional variable for each\\npossible state variable; hence:\\nX\\n1 is equivalent to Clear (A)\\nX2 is equivalent to Clear (B)\\nX3 is equivalent to On (A, B)\\nX4 is equivalent to On (B, A)\\nIn this case, we can represent any state by the use of just four propositional\\nvariables. Of course, if there were four blocks instead of two, then we would'), Document(metadata={}, page_content='16.6 Propositional Planning 449\\nrequire 4 variables to represent Clear and 12 variables to represent On, and,\\nin general, for n blocks, we will require n2 propositional variables. Of\\ncourse, in most planning problems we will have more than just blocks to\\nconsider, and so the number of propositional variables required will\\nincrease accordingly.\\nA state can be represented by an assignment of truth values to each of the\\navailable variables; hence, in our simple blocks world we could have a state'), Document(metadata={}, page_content='represented by the following sentence:\\nX\\n1 ∧¬ X2 ∧ X3 ∧¬ X4\\nThis state can be represented in STRIPS notation as\\nClear (A) ∧¬ Clear (B) ∧ On (A, B) ∧¬ On (B, A)\\nOf course, a propositional logic sentence can also represent a number of\\nstates. For example, the following sentence represents all states in which A\\nis clear and B is not clear:\\nX1 ∧¬ X2\\nIn fact, due to the simplicity of this example, there is only one such state,'), Document(metadata={}, page_content='but if we allow additional blocks, then ¬X2 (B is not clear) could be caused\\nby a block other than A, and so X 1 ∧¬ X2 represents a set of states, rather\\nthan a single state.\\nActions can also be represented using propositional sentences. T o do this,\\nwe use a new notation to represent the state that results from an action. If\\nwe use X\\n1 to represent the fact that A is clear before an action is taken, then\\nwe use ¬X1/H11032to represent that A is no longer clear after the action. Hence,'), Document(metadata={}, page_content='the action MoveOnto (A, B) can be represented by the following proposi-\\ntional sentence:\\nX1 ∧ X2 ∧¬ X3 ∧¬ X4 ∧ X1/H11032∧¬ X2/H11032∧ X3/H11032∧¬ X4/H11032\\nThis sentence states that the preconditions for the action are X1 ∧ X2 ∧¬ X3\\n∧¬ X4, and that after the action is taken the state can be described as X1/H11032∧\\n¬X2/H11032∧ X3/H11032∧¬ X4/H11032.\\nIn this case, there is only one state in which the MoveOnto (A, B) action can'), Document(metadata={}, page_content='be carried out. In most real problems, there will be several states from\\nwhich a given action can be applied. If we assume that it is possible to move\\nblock A onto block B even if A is not clear (i.e., if some other object is on'), Document(metadata={}, page_content='450 CHAPTER 16 Planning Methods\\ntop of A), then we would express the action MoveOnto (A, B) by the follow-\\ning sentence, in disjunctive normal form:\\n(X1 ∧ X2 ∧¬ X3 ∧¬ X4 ∧ X1/H11032∧¬ X2/H11032∧ X3/H11032∧¬ X4/H11032) ∨\\n(¬X1 ∧ X2 ∧¬ X3 ∧¬ X4 ∧¬ X1/H11032∧¬ X2/H11032∧ X3/H11032∧¬ X4/H11032)\\nThe reason that this propositional notation is useful for planning is that a\\nnumber of techniques exist for manipulating propositional expressions,'), Document(metadata={}, page_content='and these techniques can, in theory, be applied to planning, as we will see in\\nthe next section.\\n16.7 SAT Planning\\nOne way in which propositional notation can be used in planning is to\\ndetermine the satisfiability of a set of sentences that express the problem.\\nAs explained in Chapter 7, a sentence is satisfiable if some assignment of\\ntruth values to the variables in the sentence makes the sentence true. The\\nsatisfiability problem (also known as SAT) in general is NP-complete,'), Document(metadata={}, page_content='which means that in the worst case, solving a SAT problem for n variables\\nwill involve testing m\\nn possible assignments of variables, where n is the\\nnumber of variables in the expression, and m is the number of values each\\nvariable can take.\\nDuring the 1990s, a number of techniques were developed that improved\\nthe performance of systems designed to solve the satisfiability problem.\\nGSAT is an example of such a system, which is explained in detail in Sel-\\nman et al. (1992).'), Document(metadata={}, page_content='man et al. (1992).\\nThere are two main approaches to SAT. One class of solutions uses a sys-\\ntematic approach, meaning that each possible assignment of truth values is\\ntested until a solution is found. These methods are guaranteed to find a\\nsolution if one exists, but in the worst case can be very inefficient.Stochas-\\ntic methods involve randomly testing assignments. One such method is\\nW alksat, which operates in a similar way to the exchanging heuristics seen'), Document(metadata={}, page_content='in Chapter 5. Walksat involves repeatedly changing the value of variables in\\nunsatisfied clauses until a solution is found (Selman et al. 1994).\\nSAT planning involves encoding the start state, goal state, and operators\\n(frame axioms and effect axioms) in conjunctive or disjunctive normal\\nform and then using a method such as GSAT to show whether the sen-'), Document(metadata={}, page_content='16.8 Planning Graphs 451\\npreconditions preconditionseffects effects\\nState 0 State 1 State 2\\nFigure 16.8\\nA stylized illustration of a\\nplanning graph\\ntences are satisfiable or not. If they are, then a suitable plan can be formu-\\nlated.\\n16.8 Planning Graphs\\nA planning graph can be used to develop plans for problems that can be\\nrepresented using propositional logic. GraphPlan is an example of an algo-\\nrithm that uses planning graphs to develop plans for problems that are'), Document(metadata={}, page_content='expressed in STRIPS notation (and can, therefore, as we saw above, be con-\\nverted to propositional form).\\nA planning graph consists of a number of levels. This is illustrated in Figure\\n16.8. The first level (usually called the zeroth level) contains the proposi-\\ntions that are true in the start state for the problem. The next level of the\\ngraph contains the actions that can be carried out in this state. The level\\nafter that contains the states that can be led to by carrying out these actions.'), Document(metadata={}, page_content='Hence, each even-numbered level in the plan represents a state, and each\\nodd-numbered level represents actions. The final state in the graph repre-\\nsents the goal state.\\nThe links between level 0 and level 1 show how the preconditions of the\\nactions in level 1 are met by the propositions in level 0. Similarly, the links\\nfrom level 1 to level 2 show how the actions in level 1 produce the state con-\\ntained in level 3 (state 1).'), Document(metadata={}, page_content='tained in level 3 (state 1).\\nIt is useful to be able to show which propositions do not change as a result\\nof a given action. These are shown by persistence actions, which are equiv-\\nalent to the frame axioms discussed in Section 15.4. A persistence action is'), Document(metadata={}, page_content='452 CHAPTER 16 Planning Methods\\nba\\na\\nb\\nFigure 16.9\\nA blocks world problem,\\nshowing start state and\\ngoal state\\nusually shown on a planning graph as an arrow with a clear box on it (see\\nFigure 16.10).\\nPlanning graphs form a very compact representation: because the graph\\nonly shows actions that are possible in each state, it reduces significantly\\nthe number of actions that must be considered in constructing a plan.\\nA final feature of the planning graphs is the inclusion of mutual exclusion'), Document(metadata={}, page_content='information, or mutexes. A mutex exists between two effects or actions\\nthat are mutually exclusive. For example, in our blocks work, Clear (B) is\\nmutually exclusive with On (A, B) because the two statements cannot be\\ntrue at the same time. At each level of the planning graph, lines are drawn\\nbetween actions or propositions that are mutually exclusive with each\\nother.\\nLet us examine the planning graph for the problem illustrated in Figure\\n16.9.'), Document(metadata={}, page_content='16.9.\\nThe available actions are MoveOnto and MoveOntoT able, as defined above\\nin Section 16.2.4. We will continue to use the STRIPS notation predicates\\nOn (x, y) and Clear (x).\\nAn incomplete planning graph for this problem is shown in Figure 16.10.\\nThe planning graph in Figure 16.10 shows the actions that are possible\\nfrom the initial state (State 0). The actions in level 1 are connected by links\\nto their preconditions in Level 0. Similarly, the results of the actions in level'), Document(metadata={}, page_content='1 are shown by links to propositions in level 2 (which represents state 1).\\nMutexes between actions and between propositions are shown as heavy\\nblack lines. For example, Clear (A) is mutex with both \\n¬Clear (A) and On\\n(B, A).\\nNote that not all mutexes have been shown because to do so would involve\\nrendering the diagram hard to follow. Similarly, not all propositions are\\nincluded. For example, propositions such as \\n¬On (A, Table) and ¬On (B,'), Document(metadata={}, page_content='¬On (A, Table) and ¬On (B,\\nTable) have been excluded, again for the sake of clarity.'), Document(metadata={}, page_content='16.8 Planning Graphs 453\\nOn (B, Table)\\nOn (A, Table)\\nClear (A)\\nClear (B)\\nOn (A, B)\\nOn (B, A)\\n¬Clear (B)\\n¬Clear (A)\\nMoveOnto (B, A)\\nMoveOnto (A, B)\\nState 0 Action 0 State 1\\nOn (B, Table)\\nOn (A, Table)\\nClear (B)\\nClear (B)\\nFigure 16.10\\nPartial planning graph for\\nthe blocks world problem\\nshown in Figure 16.9\\nThe graph shows persistence actions as lines with squares on them. These\\nrepresent the possibility that a proposition might not change from one\\nstate to the next.'), Document(metadata={}, page_content='state to the next.\\nNote that for even an extremely simple problem, the planning graph can\\nappear very complex. In fact, planning graphs produce a much more com-\\npact representation than many other methods.\\nThe planning graph shows at each state every proposition that could possi-\\nbly be true in that state, as a result of the actions that are in the previous\\nlevel.\\nHaving produced the planning graph, it is possible to determine immedi-'), Document(metadata={}, page_content='ately whether it is possible to formulate a plan that will solve the problem.\\nIf any of the literal propositions that are included in the goal state defini-\\ntion are not included in the final level of the planning graph, then it is not\\npossible to formulate a plan to reach the goal state. On the other hand, if all\\nthe goal propositions are included in the final level, then it may be possible\\nto formulate a suitable plan. This will depend on the mutexes in the final'), Document(metadata={}, page_content='level, which restrict which states can be achieved.'), Document(metadata={}, page_content='454 CHAPTER 16 Planning Methods\\nNote that each state level in the graph contains information about a num-\\nber of different possible states, which can be determined by examining the\\nmutex information at that level.\\nThe next stage in using planning graphs is to extract a plan from the plan-\\nning graph. This can be done using an algorithm such as GraphPlan, which\\nis explained in the next section.\\n16.8.1 GraphPlan\\nGraphPlan is a planning algorithm that was invented by Avrim Blum and'), Document(metadata={}, page_content='Merrick Furst (1997). It uses planning graphs to formulate plans to prob-\\nlems that are expressed in STRIPS notation.\\nThe GraphPlan algorithm runs by iteratively building a planning graph,\\nstarting from the initial state and working toward the goal state.\\nFirst, the propositions that describe the goal state are compared with the\\ncurrent state. If all of these propositions are present, and no two of them\\nare joined by a mutex link, then it is possible that a solution has already'), Document(metadata={}, page_content='been reached. At this stage, a second phase of the algorithm is run to try to\\nextract a plan from the current graph plan.\\nIf the current state does not contain all the necessary propositions, then the\\nnext level of the planning graph is produced by applying all applicable\\noperators, and determining all possible propositions that can be made true\\nby these operators.\\nThis algorithm repeats until a suitable plan is found, or until it can be\\nshown that no plan exists.'), Document(metadata={}, page_content='shown that no plan exists.\\nGraphPlan has the desirable property that if a plan exists, it is guaranteed\\nto find it, and it is guaranteed to find the shortest possible plan due to the\\niterative way in which it builds the planning graph. It is also guaranteed to\\nterminate in the case where no plan exists. In such cases, the planning\\ngraph will reach a state where each new level that is added is the same. At\\nthis stage, the graph is said to have leveled off. If the graph levels off, and'), Document(metadata={}, page_content='the final level does not have all of the desired propositions or some of them\\nare connected to each other by mutex links, then no suitable plan exists.\\nWhen a state is found in which all goal propositions are present and are not\\nmutex, the method for finding a plan is applied, which works as follows:\\nStarting from the final level in the planning graph and working backward,'), Document(metadata={}, page_content='16.9 ADL and PDDL 455\\noperators are selected at each level that are not mutex and that provide all\\nof the conditions required at each level, either to meet the goal conditions\\nor to meet the preconditions of the actions in the next level.\\nThe plan that GraphPlan produces is a partial order plan, in which no\\nordering constraint is placed on actions that are at the same level.\\n16.8.2 Mutex Conditions\\nThere are a number of reasons that a pair of actions or propositions are'), Document(metadata={}, page_content='mutually exclusive, or mutex, to each other:\\n1. Two actions that have effects inconsistent with each other are\\nmutex. For example, MoveOnto (A, B) and MoveOntoT able(A) are\\nmutex because one has the effect of adding On (A, B) and the other\\nadds On (A, Table).\\n2. If the effect of one action interferes with the precondition of\\nanother, then the two actions are mutex. For example, MoveOnto\\n(A, B) has the effect of deleting Clear (B) and so is mutex with'), Document(metadata={}, page_content='MoveOnto (B, A), which has the precondition Clear (B).\\n3. If one action has proposition P as its precondition, and another\\naction has precondition \\n¬P, then the two actions are mutex.\\n4. If one proposition is inconsistent with, or the negation of, another\\nproposition, then the two are mutex. For example, in our simple\\nblocks world, On (A, B) is mutex with On (A, Table) and is also\\nmutex with \\n¬On (A, B).\\n16.9 ADL and PDDL\\nA number of alternative representations exist for expressing planning'), Document(metadata={}, page_content='problems, in addition to STRIPS. ADL (Action Description Language) is a\\nmore expressive language than STRIPS, which can be used to represent a\\nnumber of problems that cannot be adequately represented in STRIPS.\\nUnlike STRIPS, which can only represent unquantified expressions such as\\nA\\n∧ B, goals in ADL can be quantified, allowing expressions such as ∃x.\\nP(x) ∧¬ Q(x).\\nPreconditions in STRIPS must be expressed as conjunctions (such as A ∧ B'), Document(metadata={}, page_content='∧¬ C), but preconditions in ADL can be expressed as disjunctions (such as\\nA ∨ B). Additionally, ADL allows for conditional effects, which state effects'), Document(metadata={}, page_content='456 CHAPTER 16 Planning Methods\\nthat will occur as a result of carrying out a particular action depending on\\ncertain conditions.\\nFor example, in a more complex blocks world, it might be that block A is\\ntwice as big as blocks B and C, and so the action MoveOnto (B, A) might\\nonly have the effect of negating Clear (A) if On (C, A) is already true. This\\ntype of conditional effect would be hard to express in STRIPS notation.\\nAnother feature of ADL is that it enables types to be attached to variables.'), Document(metadata={}, page_content='This means that in many situations, fewer rules need to be expressed than\\nwith STRIPS because rules can be set up that ensure that objects involved\\nin actions have the correct type.\\nPDDL (Planning Domain Definition Language) is a standardized syntax\\nfor expressing planning problems, which was developed for the AIPS (Arti-\\nficial Intelligence Planning Systems) planning competition. PDDL can be\\nused to represent STRIPS and ADL, and was introduced to provide a com-'), Document(metadata={}, page_content='mon notation that could be used by all planning systems.\\n16.10 Probabilistic Planning\\nIn all of our discussion of planning so far, we have assumed that actions are\\ndeterministic. That is, we have assumed that if you apply action A in state S,\\nthen we can state with certainty what the resulting state will be. Of course,\\nthis is unrealistic for many real-world planning situations, and probabilis-\\ntic planners have been developed that aim to deal with this uncertainty.'), Document(metadata={}, page_content='In some systems, it is possible to consider nondeterministic actions, where\\nan action applied in a particular state will nondeterministically lead to one\\nof several possible states.\\nSituation calculus can be extended to express probabilistic relationships\\n(Mateus et al. 2001). This enables the language to express the various effects\\nthat can occur as a result of a particular action and how probable each of\\nthose effects are. Deterministic actions are a special case of probabilistic'), Document(metadata={}, page_content='actions in which the probability of the effect is 1.\\n16.11 Dynamic World Planning\\nIn addition to assuming that the actions our planner takes are determinis-\\ntic, we have also assumed in this discussion that the world itself is static. Of\\ncourse, the real world is dynamic, and in many situations there are other\\nagents that can also affect the world.'), Document(metadata={}, page_content='16.12 Case-Based Planning Systems 457\\nIt has been said that planning in a dynamic world is pointless because the\\nworld may change in such a way that the plan becomes useless. In spite of\\nthis difficulty, there are methods that can be applied to planning in\\ndynamic environments.\\nOne principle that is often applied is execution monitoring. Once a plan-\\nner has produced a plan, let us say for a robot, that robot is usually expected'), Document(metadata={}, page_content='to simply carry out, or execute, the plan. A planner that uses execution\\nmonitoring checks the preconditions of each action as it executes it. If the\\npreconditions are no longer met, because something has changed, then the\\nplanner may need to start again and devise a new plan.\\nThis process of devising a new plan when something has gone wrong is\\nknown as replanning.\\nSimilarly, the planner checks the goal conditions at each step, in case it has'), Document(metadata={}, page_content='accidentally solved the problem. For example, while executing a plan for a\\nblocks world problem, another robot may have arrived and solved the rest\\nof the problem, in which case our robot can stop executing its plan.\\nAn alternative method for dealing with dynamic environments, or uncer-\\ntainty, is to use conditional planning. Conditional planning assumes that\\nat each step of the plan, one of several different possible situations could'), Document(metadata={}, page_content='result. In other words, the planner does not have complete information\\nabout the problem domain before it starts planning.\\nThe conditional planning approach involves developing a plan that covers\\nevery possible eventuality. This is a good way to guarantee that a plan will\\nnot fail, but in the real world, there may be far too many possibilities to\\nplan for.\\n16.12 Case-Based Planning Systems\\nA traditional planning system must reformulate its plan every time it is'), Document(metadata={}, page_content='presented with a new problem. Of course, in some situations it will be pre-\\nsented with a problem it has seen before, or a problem that shares elements\\nwith previous problems.\\nA case-based planning system stores each plan it formulates in memory\\nand is able to reuse these plans to help it solve new problems.\\nCHEF is an example of a case-based planning system, which was designed\\nto produce recipes for Chinese food based on a given set of ingredients.'), Document(metadata={}, page_content='458 CHAPTER 16 Planning Methods\\nWhen CHEF is presented with a set of ingredients that it has not encoun-\\ntered before, such as chicken and carrots, it is able to formulate a recipe\\nbased on an existing recipe, such as stir-fried beef and onions.\\nIn situations where CHEF’s plan has not been successful, it is able to learn\\nfrom its errors, in order to avoid making such errors again. If for example it\\novercooks the chicken, it will learn that in future plans it should not cook'), Document(metadata={}, page_content='chicken for as long as it might cook beef.\\nOne important aspect of case-based planning systems is the memory that is\\nused to store plans. Clearly it must be possible to look up a variety of items\\nin this memory to find the plan or plans that best suit the current situation.\\n16.13 Planning and Scheduling\\nThe planning techniques we have discussed in this chapter are extremely\\nuseful for solving a range of problems. We have mainly considered prob-'), Document(metadata={}, page_content='lems in the toy blocks world, which involve selecting the right sequence of\\nactions to rearrange a collection of blocks from one configuration to\\nanother. Planning can also be helpful in solving problems such as the trav-\\neling salesman problem, which was discussed in Chapter 3, along with a\\nrange of other similar problems that involve selecting a suitable course of\\ntravel that meets a set of constraints, and enable the traveler to move from\\none location to another in a desired time frame.'), Document(metadata={}, page_content='one location to another in a desired time frame.\\nA rather different kind of problem is job shop scheduling, which is used to\\nplan a sensible allocation of machinery to a set of jobs. Each job consists of\\na set of tasks that must be carried out, usually specified as a partial order\\n(hence, some tasks must be done sequentially, but other tasks can be car-\\nried out in parallel). Each machine can perform a subset of the available\\ntasks, and the problem of job shop scheduling is to allocate tasks to'), Document(metadata={}, page_content='machines such that no machine is being used for two tasks at the same time\\nand so that all the tasks get carried out. In some cases, it is desirable to find\\nthe most efficient such arrangement, so that the jobs are completed as\\nquickly as possible.\\nThe problem of scheduling a number of tasks among a set of machines is\\nvery similar in many ways to the planning problems we have examined\\nalready. The main difference is that a schedule must specify when each task'), Document(metadata={}, page_content='is carried out and how long it will take, whereas the plans we have exam-'), Document(metadata={}, page_content='16.14 Chapter Summary 459\\nined simply specify a sequence of actions, with no concern about how long\\neach action takes or when it should be done.\\nOne approach to scheduling is to treat it as a straightforward planning\\nproblem. This results in a plan that describes the order in which actions\\nshould be carried out. A human operator can then augment this plan with\\ninformation about when to perform each task.\\nAlternatively, scheduling can be seen as a constraint satisfaction problem'), Document(metadata={}, page_content='(see Chapter 5), where the constraints specify how long each task will take\\nand that one machine cannot be used for two tasks at a time.\\nIn practice, a combination of approaches is usually used. Planning tech-\\nniques such as the ones we have discussed in this chapter are applied in\\nconjunction with search methods suitable for solving constraint satisfac-\\ntion problems.\\n16.14 Chapter Summary\\n■ STRIPS is an operator-based planning approach based on\\nmeans–ends analysis.'), Document(metadata={}, page_content='means–ends analysis.\\n■ An operator can be defined by an operator schema that describes a\\nnumber of possible operators, using variables that are instantiated\\nto provide an operator.\\n■ The Sussman anomaly occurs in problems in which a planner\\nneeds to be able to consider two aspects of the problem independ-\\nently. Such problems cannot be readily solved using the traditional\\nSTRIPS approach.\\n■ A total order plan specifies the order in which all actions must be'), Document(metadata={}, page_content='carried out. A partial order plan allows some operators to be spec-\\nified in parallel, such that the order is determined at execution\\ntime.\\n■ The principle of least commitment states that it is a good idea at\\neach stage of planning to commit to as few decisions as possible.\\n■ Most plans (and in particular, all plans that can be represented\\nusing the STRIPS language) can be represented in propositional\\nlogic notation, meaning that plans can be developed using meth-'), Document(metadata={}, page_content='ods that solve the satisfiability problem for a set of propositions.'), Document(metadata={}, page_content='460 CHAPTER 16 Planning Methods\\n■ A planning graph represents states and actions at alternate levels,\\nand shows all possible states and all possible actions at each point\\nby using mutex relationships to show which combinations are not\\nallowed.\\n■ GraphPlan is an algorithm that uses planning graphs to extract\\nplans.\\n■ ADL is an alternative planning representation that is more expres-\\nsive than the STRIPS language.\\n■ Probabilistic planning involves working with operators where the'), Document(metadata={}, page_content='outcome of a given operator is not certain. Similarly, planning in\\nmany situations needs to function in a dynamic environment in\\nwhich the world can change from one time-step to the next.\\nDynamic world planners often use replanning to cope when such\\nchanges interfere with their plans.\\n■ Case-based planning involves storing plans in a searchable mem-\\nory and reusing them to solve new problems.\\n■ Planning means selecting which operators to apply; scheduling is'), Document(metadata={}, page_content='used to determine at what time to carry out the actions in order to\\nmeet a set of constraints.\\n16.15 Review Questions\\n16.1 Explain the difference between the STRIPS language and the ADL\\nlanguage. Why is ADL described as being more expressive than\\nSTRIPS? What kinds of problems might ADL be used to solve for\\nwhich STRIPS might not be adequate?\\n16.2 Explain what is meant by the principle of least commitment. How\\ndo you think it might relate to the generation of partial order\\nplans?'), Document(metadata={}, page_content='plans?\\n16.3 Explain how the satisfiability problem relates to planning. How\\nefficient do you think this method might be compared with\\nSTRIPS planning or using GraphPlan?\\n16.4 Explain what is meant by dynamic world planning. What is meant\\nby probabilistic planning? What is the difference between proba-\\nbilistic planning and nondeterministic planning?\\n16.5 What is meant by replanning?'), Document(metadata={}, page_content='16.17 Further Reading 461\\nc\\nb\\nda ca\\nb\\nd\\nFigure 16.11\\nStart and goal state for\\nExercise 16.1\\n16.6 Explain why case-based planning can be used to produce a plan-\\nning system that is able to learn.\\n16.7 Compare and contrast planning and scheduling.\\n16.16 Exercises\\n16.1 Use the operators described in Section 16.2.4 and the STRIPS\\nmethod to solve the blocks world planning problem shown in Fig-\\nure 16.11. The first state shown is the start state, and the second\\nstate is the goal state.'), Document(metadata={}, page_content='state is the goal state.\\n16.2 Produce a planning graph for the blocks world problem shown in\\nFigure 16.11.\\n16.3 Use resolution and unification to solve the blocks world problem\\nshown in Figure 16.11. How does this plan compare with the one\\nyou generated in exercises 16.1 and 16.2?\\n16.17 Further Reading\\nPlanning has increased in prominence in the Artificial Intelligence world in\\nthe past decade, and as a result, better coverage can be found in the more'), Document(metadata={}, page_content='recent textbooks. Russell and Norvig (1995) provide the fullest coverage of\\nthe standard texts.\\nReasoning About Plans, by James F. Allen, Henry A. Kautz, Josh T enenberg,\\nand Richard Pelavin (1991 – Morgan Kaufmann)\\nRecent Advances in AI Planning, by Susanne Biundo and Maria Fox (2000 –\\nSpringer V erlag)\\nFast Planning Through Planning Graph Analysis , by A. Blum and M. Furst\\n(1997 – in Artificial Intelligence, Vol. 90, pp. 281–300).'), Document(metadata={}, page_content='Robot Motion: Planning and Control, edited by Michael Brady, John Holler-\\nbach, Timothy Johnson, T omás Lozano-Pérez, and Matthew T. Mason\\n(1983 – MIT Press)'), Document(metadata={}, page_content='462 CHAPTER 16 Planning Methods\\nSTRIPS: A New Approach to the Application of Theorem Proving to Problem\\nSolving, by Richard E. Fikes and Nils J. Nilsson (1971 – in Computation &\\nIntelligence, edited by George F. Luger, 1995, MIT Press)\\nArtificial Intelligence & Manufacturing Research Planning Workshop , edited\\nby George F. Luger (1998 – AAAI)\\nProbabilistic Situation Calculus , by Paulo Mateus, António Pacheco, Javier\\nPinto, Amílear Sernadas, and Cristina Sernadas (2001 – in Annals of Math-'), Document(metadata={}, page_content='ematics and Artificial Intelligence)\\nA New Method for Solving Hard Satisfiability Problems , by B. Selman, H.\\nLevesque, and D. Mitchell (1992 – in AAAI, Vol. 92, pp. 440–446)\\nNoise Strategies for Improving Local Search , by B. Selman, H. A. Kautz, and\\nB. Cohen (1994 – in AAAI, Vol. 94, pp. 337–343)\\nPlanning and Learning by Analogical Reasoning , by Manuela M. V eloso\\n(1994 – Springer V erlag T elos)\\nRecent Advances in AI Planning , by Daniel S. Weld (in AI Magazine,S u m -\\nmer 1999)'), Document(metadata={}, page_content='mer 1999)\\nPractical Planning: Extending the Classical AI Planning Paradigm , by David\\nE. Wilkins (1989 – Morgan Kaufman)\\nIntelligent Scheduling, edited by Monte Zweben and Mark S. Fox (1998 –\\nMorgan Kaufmann)\\nIntelligent Planning: A Decomposition and Abstraction Based Approach ,b y\\nQiang Y ang (1998 – Springer V erlag)'), Document(metadata={}, page_content='Advanced Topics\\n6\\nIntroduction to Part 6\\nPart 6 is divided into five chapters.\\nAdvanced Knowledge Representation\\nThis chapter builds on the ideas presented in several of the\\nearlier chapters in this book, in particular Chapters 7, 9, 15,\\nand 16. It presents a number of more sophisticated knowl-\\nedge representation methods, including the blackboard\\narchitecture, scripts, and the Copycat architecture.\\nIt also presents more material on nonmonotonic reasoning'), Document(metadata={}, page_content='and reasoning about change. Finally, this chapter expands\\non topics introduced elsewhere in this book by discussing\\ncase-based reasoning and knowledge engineering.\\nFuzzy Reasoning\\nThis chapter introduces the subject of fuzzy logic. It dis-\\ncusses fuzzy sets and explains how they are used in fuzzy\\nsystems. It also explains how fuzzy logic provides an alter-\\nnative to the traditional logic presented in Chapters 7 and 8\\nof this book. It also discusses the ideas of fuzzy expert sys-'), Document(metadata={}, page_content='tems and neuro-fuzzy systems.\\nIntelligent Agents\\nChapter 19 introduces the concept of software agents and,\\nin particular, intelligent agents, which are able to independ-\\nently carry out tasks on behalf of a user. The chapter dis-\\ncusses a number of properties that agents can have such as\\nPART\\n17\\nCHAPTER\\n18\\nCHAPTER\\n19\\nCHAPTER'), Document(metadata={}, page_content='intelligence, autonomy, benevolence, the ability to learn, and the ability to\\nmove about through a network, such as the Internet. The chapter intro-\\nduces a number of types of agents, such as interface agents, reactive agents,\\ncollaborative agents, and mobile agents. It also discusses architectures and\\nmethods that can be used to build agents. The chapter also discusses\\nrobotic agents, such as the Braitenberg vehicles.\\nUnderstanding Language'), Document(metadata={}, page_content='Understanding Language\\nThis chapter discusses a number of techniques that are used by computer sys-\\ntems to understand written or spoken human language. In particular, it\\nfocuses on natural language processing (NLP) and information retrieval (IR).\\nIt presents the methods used to parse sentences and explains how semantic\\nand pragmatic analysis are used to derive meaning from sentences while\\navoiding being confused by the ambiguity inherent in human language.\\nMachine Vision'), Document(metadata={}, page_content='Machine Vision\\nThis chapter presents a range of methods that are used to enable computers\\nto analyze visual data. It discusses edge detection and explains how convo-\\nlution can be used to detect edges in images. It also explains how images are\\nsegmented, and how the edges of three-dimensional line drawings can be\\nlabeled. It discusses texture and explains how important it is for computer\\nvision systems to use information derived from texture.'), Document(metadata={}, page_content='This chapter also briefly discusses one method that is used for face recognition.\\n464 Part 6 Advanced Topics\\n20\\nCHAPTER\\n21\\nCHAPTER'), Document(metadata={}, page_content='17CHAPTER\\nAdvanced Knowledge\\nRepresentation\\nLet knowledge grow from more to more,\\nBut more of reverence in us dwell;\\nThat mind and soul, according well,\\nMay make one music as before.\\n—Alfred Lord T ennyson,In Memoriam\\nWhether there be knowledge, it shall vanish away.\\n—The first epistle of Paul the apostle to the Corinthians, Chapter 13\\nWhat is all knowledge too but recorded experience,\\nand a product of history; of which therefore,\\nreasoning and belief, no less than action and passion,'), Document(metadata={}, page_content='are essential materials?\\n—Thomas Carlyle, Critical and Miscellaneous Essays\\nSo it is in travelling; a man must carry knowledge with him, if he would bring\\nhome knowledge.\\n—Samuel Johnson\\n17.1 Introduction\\nHuman beings use representations for the world around them all the time.\\nOne example is the use of language. Consider the following sentence:\\nThe cat sat on the mat.'), Document(metadata={}, page_content='466 CHAPTER 17 Advanced Knowledge Representation\\nThis sentence may seem trite, but it has real meaning to us. The word “cat”\\nrepresents a four-legged feline creature. The word “sat” represents an action\\nand tells us something about when that action took place. The word “mat”\\nrepresents another object, and the word “on” represents a relationship\\nbetween objects. What the word “the” represents is hard to define, but\\nclearly each word in a sentence, taken individually and grouped with other'), Document(metadata={}, page_content='words, conveys meaning to a person who reads, hears, or speaks the words.\\nAnother representation we use regularly is that of images, or signs. Note\\nthat there is a significant difference between the audible representation of a\\nword when it is spoken compared with the visible representation when it is\\nwritten down. We use a vast number of signs, symbols, and images in our\\neveryday lives, including the following:\\n■ letters and numbers\\n■ mathematical equations\\n■ road signs'), Document(metadata={}, page_content='■ mathematical equations\\n■ road signs\\n■ photographs of people, places, and things\\n■ caricatures and cartoons\\n■ alarms and other audible signals\\nThe list is endless.\\nThe human mind uses some form of representation for all concepts,\\nwhich enables us to understand such abstract ideas as “happiness, ” “late-\\nness,” and “common sense.” In this way, even a human baby is able to\\nunderstand the connection between the sound “woof!, ” the cartoon char-'), Document(metadata={}, page_content='acter Snoopy, and a dog. We use some kind of internal representation for\\na dog that allows us to associate those three different concepts together\\nin some way.\\nClearly this internal representation has a lot to do with our ability to think,\\nto understand, and to reason, and it is no surprise, therefore, that much of\\nArtificial Intelligence research is concerned with finding suitable represen-\\ntations for problems.\\nThroughout this book, we have considered representations and how they'), Document(metadata={}, page_content='can be manipulated to solve problems. Representations we have consid-\\nered include:'), Document(metadata={}, page_content='17.1 Introduction 467\\n■ propositional and predicate calculus\\n■ semantic nets\\n■ search trees\\n■ frames\\nMost of the methods we have examined are dependent on a suitable repre-\\nsentation being chosen. It is impossible to solve a problem using genetic\\nalgorithms, planning, or classifier systems without first selecting an appro-\\npriate representation for the problem.\\nIn this chapter, we consider a number of methods of representing knowl-'), Document(metadata={}, page_content='edge, as well as exploring extensions to some of the representations we have\\nexplored elsewhere.\\nA number of the sections in this chapter build on ideas presented in earlier\\nchapters—in particular, Chapter 7 on logic, Chapter 9 on rules and expert\\nsystems, and Chapters 15 and 16 on planning.\\nThe chapter starts by discussing the ideas of representation, semantics, and\\ninterpretations and tries to explain why these are so important to Artificial\\nIntelligence.'), Document(metadata={}, page_content='Intelligence.\\nIt then introduces a number of specific representational methods—the\\nblackboard architecture, scripts, and the Copycat architecture—and illus-\\ntrates how each of them is used.\\nThis chapter then concentrates on nonclassical logics, starting with a\\ndetailed discussion of nonmonotonic logics and nonmonotonic reasoning\\nmethods. The methods explained in this discussion include default reason-\\ning, truth maintenance systems, the closed world assumption, circumscrip-'), Document(metadata={}, page_content='tion, and abductive reasoning. This chapter also examines two methods for\\ndealing with uncertainty: the Dempster–Shafer theory and certainty factors.\\nThis chapter also expands on the discussion of situation calculus from\\nChapter 15 by explaining event calculus, temporal logic, and mental situa-\\ntion calculus, all of which are used to represent data in worlds that are sub-\\nject to change.\\nThis chapter has a brief discussion of the important steps in knowledge'), Document(metadata={}, page_content='engineering, an idea that was first introduced in Chapter 9, and also briefly\\nexplains why case-based reasoning (introduced in Chapter 16) is useful.'), Document(metadata={}, page_content='468 CHAPTER 17 Advanced Knowledge Representation\\n17.2 Representations and Semantics\\nMany representations involve some kind of language. We have seen, for\\nexample, propositional calculus and predicate calculus in Chapter 7, which\\nare languages used to represent and reason with logical statements; the lan-\\nguage of mathematics enables us to represent complex numeric relation-\\nships; programming languages such as Java and C++ use objects, arrays,'), Document(metadata={}, page_content='and other data structures to represent ideas, things, and numbers.\\nHuman beings use languages such as English to represent objects and more\\ncomplex notions. Human language is rather different from the languages\\nusually used in Artificial Intelligence, as we shall see in Chapter 20. In par-\\nticular, although human languages are able to express an extremely wide\\nrange of concepts, they tend to be ambiguous—a sentence can have more'), Document(metadata={}, page_content='than one meaning, depending on the time and place it is spoken, who said\\nit, and what was said before it. Human languages are also very efficient: it is\\npossible to express in a few words ideas that took thousands of years for\\nhumans to develop (for example, the words existentialism, solipsism, and\\nmathematics).\\nWhen considering any representational language, it is vital to consider the\\nsemantics of the language (i.e., what expressions in the language mean or\\nwhat they represent).'), Document(metadata={}, page_content='what they represent).\\nIn some ways, despite its tendency for ambiguity, human language is very\\nexplicit—each sentence has a meaning that can be determined without any\\nexternal information. The sentence “the cat sat on the mat, ” for example,\\nhas a fairly specific meaning (although, it does not specify which cat or\\nwhich mat).\\nIn contrast, sentences in a language such as predicate calculus need to have\\nan interpretation provided. For example, we might write\\n∀xP (x) → Q(x)'), Document(metadata={}, page_content='∀xP (x) → Q(x)\\nThis sentence might have a number of interpretations, depending on our\\nchoice of meaning for P and Q. For example, we could interpret it as mean-\\ning “all men are mortal. ” An inference engine that manipulates such sen-\\ntences does not need to know the meanings of the sentences, but if the\\nsentences are being used to reason about the real world and to form plans,\\nthen of course the interpretations must be carefully chosen.'), Document(metadata={}, page_content='17.3 The Blackboard Architecture 469\\n17.3 The Blackboard Architecture\\nThe blackboard architecture is a method for structured knowledge repre-\\nsentation that was invented in the 1970s by H. Penny Nii (Nii 1986) for a\\nsystem called HEARSAY-II. HEARSAY-II contained an index of computer\\nscience papers, about which it was able to retrieve information in response\\nto spoken queries by users.\\nIn Chapters 3 and 9, we saw the difference between reasoning forward from'), Document(metadata={}, page_content='a start state, applying rules or actions until a goal is reached, and working\\nbackward from a goal, seeing which rules or actions could lead to the goal\\nstate, and then selecting additional actions that satisfy the preconditions of\\nthose rules, and so on, until the start state is reached.\\nEach of these approaches has its advantages and is particularly useful when\\napplied to certain problems. In other situations, it is more appropriate to'), Document(metadata={}, page_content='use an opportunistic reasoning model, where rules can be applied forward\\nor backward at different times, in whatever order most effectively solves the\\ncurrent problem. Opportunistic reasoning applies well to planning (which\\nwas discussed in Part 5 of this book), but in this section we are going to\\nexamine how it is used by blackboard systems to effectively represent and\\nuse specific domain knowledge.\\nIn Chapters 3 and 9, we examined a structured knowledge representation'), Document(metadata={}, page_content='system based on frames. Each frame contains information about an object,\\nand frames are linked to other frames by relations that express the ways in\\nwhich the objects relate to each other. As we saw, this representation uses\\nthe idea of inheritance to provide an efficient way to represent the ways in\\nwhich one object shares properties with another object.\\nAlso in Chapter 9, we examined production systems, which use rules to'), Document(metadata={}, page_content='represent expert knowledge about a domain. Similarly, blackboard systems\\nare also used to represent and manipulate expert domain knowledge. The\\nidea behind blackboard systems is that disparate knowledge from different\\nexpert sources can be combined by providing a central database—the\\nblackboard—on which the experts (known as knowledge sources ) can\\n“write” information. Because the blackboard is shared, one knowledge\\nsource can see facts appear as another knowledge source puts them there,'), Document(metadata={}, page_content='and it can thus deduce new facts and add them to the blackboard. In this'), Document(metadata={}, page_content='470 CHAPTER 17 Advanced Knowledge Representation\\nKnowledge Source 1\\nKnowledge Source 2\\nKnowledge Source n\\nBlackboard\\nFigure 17.1\\nA simple blackboard\\narchitecture\\nway, a number of knowledge sources can be used together to solve a com-\\nplex problem, but each knowledge expert does not need to know from\\nwhere the data on the blackboard came.\\nA simple blackboard architecture is illustrated in Figure 17.1.\\nBecause the blackboard system uses opportunistic reasoning, the various'), Document(metadata={}, page_content='knowledge sources do not need to take turns to act. Each knowledge source\\ncan proactively examine the blackboard and add new data to it when it feels\\nthat it has something useful to contribute to the solution of the problem. In\\npractice, there is usually a central control mechanism that determines when\\neach knowledge source can interact with the blackboard, but it would be\\nequally possible to have each knowledge source an independent agent,\\nallowed to make its own decisions about when to act.'), Document(metadata={}, page_content='Nii (1986) compared this approach with a group of people solving a jigsaw\\npuzzle on a large blackboard. Each person has a number of pieces of the\\npuzzle, and when a person notices an opportunity to place one of his or her\\npieces on the board, he or she does so. The people involved do not need to\\ncommunicate with each other, and no one needs to tell the individuals\\nwhen to place their pieces on the board—they can each act independently\\nand autonomously.'), Document(metadata={}, page_content='and autonomously.\\nNii extends the analogy by supposing that the room has a monitor, who is\\na person able to control who is allowed to visit the blackboard and when.\\nNow only one person is allowed to place a piece on the blackboard at a\\ntime, and the monitor has complete authority to decide who can do so.\\nThe jigsaw puzzle analogy is helpful, but it does not quite describe the real\\nuse of blackboard systems. Each person solving the jigsaw puzzle has differ-'), Document(metadata={}, page_content='ent pieces of the puzzle, but they all have the same kind of domain knowl-\\nedge. The idea behind blackboard systems is that experts with entirely'), Document(metadata={}, page_content='17.3 The Blackboard Architecture 471\\ndifferent types of knowledge can work together to solve a single problem.\\nIn the next section, we explore in more detail the architecture of the black-\\nboard system, and in the section after that we see how the blackboard sys-\\ntem works in practice by considering the HEARSAY-II system.\\n17.3.1 Implementation\\nAs has already been suggested, the particular implementation of black-\\nboard system that is used can depend on the problem that is being solved,'), Document(metadata={}, page_content='and also on the computer systems that are available. We will now look at\\nsome of the key elements of real implementations of blackboard systems.\\nThe knowledge sources used in a blackboard system are entirely independ-\\nent. This means that by use of appropriate interfaces, a blackboard system\\ncan use a number of different representations for its knowledge sources.\\nTypically, knowledge sources are represented as rules or procedures. It is'), Document(metadata={}, page_content='also possible to represent the information in a language such as first-order\\npredicate calculus.\\nThe only interaction that occurs between the different knowledge sources is\\nthrough the blackboard data structure. The blackboard can contain items\\nof data, partial solutions to the problem, and finally, a complete solution.\\nThese data are usually arranged hierarchically, so that each level in the hier-\\narchy represents a different level of abstraction of the problem. In'), Document(metadata={}, page_content='HEARSAY, for example, the levels represent aspects such as\\n1. the digital audio signal\\n2. the phonemes that make up the entire signal\\n3. the syllables that can be constructed from the phonemes\\n4. the words that can be constructed from the syllables\\n5. the complete sentence\\nEach knowledge source looks at data in the level(s) that are appropriate for\\nit and places data onto levels that are appropriate. For example, one knowl-'), Document(metadata={}, page_content='edge source might have the ability to extract phonemes from an audio sig-\\nnal, in which case it would need only to examine data at Level 1 and would\\nneed only to add data at Level 2.\\nTypically, the blackboard system has a control mechanism that determines\\nwhich knowledge source should act next, based on the most recent changes\\nthat have occurred in the database. Hence, if in the example given above a'), Document(metadata={}, page_content='472 CHAPTER 17 Advanced Knowledge Representation\\nnew set of phonemes has been determined, the control module might\\nselect a knowledge source that has the ability to analyze phonemes to act\\nnext. In making this choice, the control module is said to be choosing the\\nfocus of attention of the system. At any one time, the system’s focus of\\nattention is directed at one knowledge source, or one piece of data, or a pair\\nthat consists of a knowledge source and a piece of data.'), Document(metadata={}, page_content='The overall strategy of the blackboard system is determined by the control\\nmodule and which ordering it uses when choosing the point of focus.\\nHence, this choice is clearly of particular importance.\\n17.3.2 HEARSAY\\nHEARSAY was designed as a system that would combine phonology, syn-\\ntax, semantics, and a contextual understanding of words in order to under-\\nstand human speech. In Chapter 20, we learn more about systems that\\nunderstand spoken words, but in this section we will briefly examine how'), Document(metadata={}, page_content='the blackboard architecture was applied to the problem.\\nIn the HEARSAY-II architecture, there were a number of knowledge\\nsources, each of which understood a different aspect of the sounds gener-\\nated when a human user would speak into the system’s microphone. The\\ncontext knowledge source has knowledge about the world, which it is able\\nto use to disambiguate words such as “their” and “there. ” This problem is\\ndiscussed in more detail in Chapter 20.'), Document(metadata={}, page_content='discussed in more detail in Chapter 20.\\nOne advantage of using the blackboard architecture for this problem is that\\na number of different knowledge sources could in fact be applied at each\\nstage—in particular, for example, in determining which word is being spo-\\nken, a number of different possible solutions might be generated by differ-\\nent modules at one level, and a higher level would later disambiguate\\n(using context, for example) and select the correct word. In this way, the'), Document(metadata={}, page_content='sound can be analyzed in a number of different ways in parallel to ensure\\nthat the best solution is obtained.\\n17.4 Scripts\\nA script (also known as a schema) is a data structure that is used as a struc-\\ntured representation for a situation that can be broken down into a\\nsequence of events. Scripts are often used in natural language processing,\\nwhich is discussed in more detail in Chapter 20.'), Document(metadata={}, page_content='17.4 Scripts 473\\nThe idea behind scripts is that for a given situation (such as buying food in\\na supermarket or attending a job interview) there is a finite set of knowl-\\nedge that is needed to understand what is said, and thus to determine how\\nto act and what to say. A script is a data structure that represents a very spe-\\ncific situation (such as buying apples from a fruit market).\\nThe script contains knowledge about the situation (such as the fact that in'), Document(metadata={}, page_content='order to buy an apple, one must pay the market seller, and that apples are\\ngood to eat unless they are rotten). A script has a set of entry conditions,\\nwhich state the preconditions necessary for a script to be used (e.g., to use\\nthe apples script, the story must start with someone near a fruit market),\\nand results, which occur as a result of running through the situation\\ndescribed by the script.\\nA script also encodes reasons: that is, why one engages in the situation'), Document(metadata={}, page_content='described by the script. This enables a script-based system to understand\\nmotivations (e.g., why a person would want to buy an apple).\\nA story can be understood by matching elements in the story to appropri-\\nate parts of the script. In this way, the script-based system can answer ques-\\ntions whose answers are not explicitly stated in the story.\\nSchank (1975) proposes a script for understanding stories about restau-\\nrants. His script includes a number of roles, or types of people that might'), Document(metadata={}, page_content='be involved, including customer, waitress, and chef. The script includes\\ninformation about reasons or why a customer might want to eat at a restau-\\nrant (clearly, hunger has something to do with this, as does money).\\nThe script is then broken down into a set of episodes, such as “entering, ”\\n“ordering, ” “eating, ” and “leaving. ”\\nEach episode is represented in the script data structure by a number of\\nrelated actions that the various people might perform.'), Document(metadata={}, page_content='Let us consider the following short story:\\nFred went to his favorite restaurant. The food was not as good as usual.\\nOn his way home, he realized he had left his wallet behind.\\nThe script system is able to match entities described in the story with its roles\\n(Fred is the customer, for example). Although the story does not mention a\\nwaitress or a chef, the script system knows that they would have been involved.\\nThe script system would also be able to answer questions such as “did Fred eat'), Document(metadata={}, page_content='at the restaurant?” even though the answer is not explicitly stated in the story.'), Document(metadata={}, page_content='474 CHAPTER 17 Advanced Knowledge Representation\\nA script is necessarily extremely specific. As we see in Chapter 20, more\\ngeneral systems for understanding language are extremely complex. How-\\never, in many situations it is possible to use scripts to understand natural\\nlanguage, provided the available scenarios are sufficiently restricted.\\n17.5 Copycat Architecture\\nThe Copycat architecture was invented by Melanie Mitchell in 1993. The'), Document(metadata={}, page_content='motivation behind the Copycat system was an interest in solving problems\\nby analogy, such as the following problem:\\nhat is to head as glove is to what?\\nOf course, the answer to this problem is obvious, but for computer pro-\\ngrams to make such analogies is not easy. The Copycat system invented by\\nMitchell works on textual analogies, such as the following:\\nabc → abd\\nHence,\\ntuv → ?\\nIn fact, there are a number of possible answers to this problem, depending'), Document(metadata={}, page_content='on the approach you choose to take. The answer most people will give\\nwould be “tuw” because they will have noted that in the first line, the third\\nletter in the group of three has been replaced by the letter that comes\\nimmediately after it in the alphabet (its successor, in other words). How-\\never, the following might also be a reasonable answer:\\ntud\\nT o solve such problems, the Copycat system uses a nondeterministic\\nmethod, such that when run repeatedly with the same problem it generates'), Document(metadata={}, page_content='different answers.\\nThe architecture of the Copycat system consists of the following components:\\n■ the W orkspace\\n■ the Slipnet\\n■ the Coderack'), Document(metadata={}, page_content='17.5 Copycat Architecture 475\\nF\\nE\\nD\\nC\\nB\\nA\\nGH I RSTU\\nV\\nW\\nX\\nY\\nZ\\nLetter category\\nopposite\\nfirst last\\nFigure 17.2\\nA simplified diagram of\\nthe slipnet in the Copycat\\nsystem\\nThe workspace is a data structure similar to a blackboard or to the message\\nlists used in classifier systems (see Chapter 13). It contains the input data\\n(such as “abc, ”“abd, ” and “tuv, ” for the problem given above) and is used as\\na working memory when solving problems. Eventually, it contains the\\nanswer that has been found.'), Document(metadata={}, page_content='answer that has been found.\\nThe slipnet is a network that contains a number of concepts. Each letter is\\nrepresented as a concept, as are ideas such as “opposite, ” “predecessor, ”\\n“sameness, ”“right, ” and “left. ” Each concept in the slipnet has an activation\\nlevel that indicates how relevant the concept is to the current problem. As a\\nproblem is being solved, the activation levels change.\\nThe slipnet can be thought of as the system’s long-term memory. It stores'), Document(metadata={}, page_content='information that the system has built up about the nature of objects and\\nconcepts, and the relationships between those concepts. The slipnet can\\nchange over time, as the system solves problems.\\nFigure 17.2 shows a simplified version of the slipnet used by the Copycat\\nsystem. Label nodes are included in the network that show, for example,\\nwhich concepts are “opposite” to each other. In other words, concepts are\\nused to show the relationship between concepts within the slipnet.'), Document(metadata={}, page_content='The coderack contains a number of agents, or codelets. Each codelet\\nembodies a relationship between objects—such as “the b in abc is the suc-\\ncessor of a. ” The higher a concept’s activation level is, the more codelets will\\nbe assigned to working with that concept.'), Document(metadata={}, page_content='476 CHAPTER 17 Advanced Knowledge Representation\\nAn important concept in the Copycat system is slippage. Slippage is the\\nidea that allows Copycat to find analogies that are not necessarily directly\\napparent. For example, consider the following problem:\\nabc → abd\\niijjkk → ?\\nA solution to this problem would be iijjll, which is found by relating the\\nfirst letter in abc to the first group of identical letters ( ii) in iijjkk. This'), Document(metadata={}, page_content='means that the rule has changed from “replace the last letter with its succes-\\nsor” to “replace the last group of identical letters with their successor. ” This\\nkind of change is slippage and is vital to solving analogy problems.\\nThe final part of the Copycat architecture is the idea of temperature.A s\\nwith simulated annealing (Chapter 5), temperature represents the degree of\\ndisorder in the system. The greater the temperature, the further from a'), Document(metadata={}, page_content='solution the system is, and the more random its codelets are allowed to be.\\nThe Copycat system starts with the problem representation in its work-\\nspace and with a reasonably high temperature.\\nAs it works, it builds up relationships in its workspace, such as “the letters\\nabc form a group where each letter is the successor of the letter to its left. ” It\\nthen tries to form correspondences between objects. For example, it might\\nform a correspondence between the entire group of letters abc and the'), Document(metadata={}, page_content='group ijk because they have a similar structure.\\nCopycat works by forming a rule that explains how to transform one object\\ninto another. For example, its rule might be “replace the rightmost letter by\\nits successor. ” This rule is adapted as Copycat works, in order to produce a\\nrule that will work with the problem object.\\nAs the system runs, the temperature lowers until it falls below a probabilis-\\ntic threshold, at which point it has reached a solution and can stop.'), Document(metadata={}, page_content='T o fully understand the Copycat system, it is well worth trying the online\\ndemonstration system.\\n17.6 Nonmonotonic Reasoning\\nAs was explained in Section 7.18, propositional logic and predicate logic\\nare monotonic reasoning systems. This means that if a conclusion C can be\\nderived from a set of expressions, S, then any number of additional expres-'), Document(metadata={}, page_content='17.6 Nonmonotonic Reasoning 477\\nsions being added to S cannot change the truth value of C, provided the\\nexpressions in S remain consistent.\\nIn other words, a monotonic reasoning system that stores facts about the\\nworld can deduce new facts from its existing facts but would never have\\ncause to delete or modify an existing fact (unless the world changed).\\nHence, the number of facts the system stores will increase monotonically.'), Document(metadata={}, page_content='In real life, reasoning tends not to be so straightforward. For example, you\\nmight know that dogs like to eat meat, and that Fido is a dog. Hence, you\\nconclude that Fido will like to eat meat. If you are later informed that Fido\\nis a vegetarian dog, you will need to change your conclusion. This kind of\\nreasoning is called nonmonotonic reasoning.\\nA nonmonotonic reasoning system needs to be able to deal with the fact\\nthat conclusions change as new facts are introduced and, hence, that its'), Document(metadata={}, page_content='knowledge is not always certain because later facts may contradict it. In this\\ncontext, we often use the word “belief” rather than “fact” to describe items\\nof data that the system stores or deduces about the world.\\nIn this section, we introduce a number of systems and principles that are\\nused to deal with nonmonotonic reasoning situations.\\n17.6.1 Nonmonotonic Logic with the Modal Operator, M\\nOne way to reason in nonmonotonic situations is to use nonmonotonic'), Document(metadata={}, page_content='logic. This involves augmenting the predicate calculus with a modal opera-\\ntor, M, which is used to represent the idea that a statement is consistent\\nwith all our beliefs. Hence, we might write\\n∀x bird (x) ∧ M flies (x) → flies (x)\\nThis can be read as follows:“for allx,i fx is a bird and it is consistent with our be-\\nliefs to believe thatx can fly, thenx can fly.” In other words, most birds can fly.\\nWe would consider M flies (x) to be false if we already knew that the bird'), Document(metadata={}, page_content='was dead and, in addition, we knew that dead birds could not fly. Note that\\nwe have just described a nonmonotonic logic, which is used for nonmonot-\\nonic reasoning.\\n17.6.2 Default Reasoning\\nDefault reasoning is another form of nonmonotonic reasoning that\\ninvolves assuming certain statements to be true, unless there is some clear'), Document(metadata={}, page_content='478 CHAPTER 17 Advanced Knowledge Representation\\nevidence to the contrary. This is a form of reasoning that people employ all\\nthe time. For example, a car might drive past you too fast for you to see\\nwhether it has a driver or not. It would be reasonable for you to assume that\\nthe car has a driver, unless you happen to know that it is a remote-con-\\ntrolled car, or you saw the driver jump out previously. This is default rea-'), Document(metadata={}, page_content='soning, as it assumes certain facts by default, unless they are disproved by\\nsome other facts.\\nA notation similar to that described in Section 17.6.1 is used for default logic:\\nCar (x)\\n∧ :Has_Driver (x) → Has_Driver (x)\\nThis is a default rule, which states, in default logic notation, that if x is a\\ncar, and it is consistent with our beliefs to believe that x has a driver, then\\nwe can conclude that x does indeed have a driver.'), Document(metadata={}, page_content='we can conclude that x does indeed have a driver.\\nSimilarly, the sentence above could be read as “if x is a car and there’s no\\nreason to suppose that x does not have a driver, then conclude that x does\\nhave a driver.”\\n17.6.3 Truth Maintenance Systems\\nA truth maintenance system (or TMS) stores information about how each\\nbelief was derived, as well as the beliefs themselves.\\nTruth maintenance systems are used in situations where belief revision is'), Document(metadata={}, page_content='important. In other words, situations in which the system’s beliefs need to\\nchange over time, as new facts come to light.\\nThe justification-based truth maintenance system (JTMS) was proposed by\\nJon Doyle in 1979.\\nThe JTMS stores reasons or justifications for beliefs, where a reason con-\\nsists of a pair of sets, such that the belief is true if the statements in the first\\nset (known as the in set) are all true, and the statements in the second set'), Document(metadata={}, page_content='(known as the out set) are all false. For example, the beliefQ might have the\\nfollowing reason:\\n({P, R}, {\\n¬S})\\nThis means that if P and R are both true, and ¬S is false, then we can\\ndeduce that Q is true. If we use this reason to conclude that Q is true, and\\nlater discover that ¬S is true, then we must retract our earlier conclusion.\\nThe JTMS uses a network of nodes, where each node represents a belief\\n(which can either be a simple statement such as “Fido is a dog” or a rule'), Document(metadata={}, page_content='17.6 Nonmonotonic Reasoning 479\\nsuch as modus ponens, or “all dogs like to eat meat”). The JTMS also stores\\njustifications for each belief.\\nThe JTMS does not carry out logical operations on beliefs (such as ∧, ∨,\\nand →) because these operations can be carried out by a problem-solving\\nsystem external to the JTMS. Similarly, the JTMS does not need to under-\\nstand the meanings of its beliefs. This kind of logical interpretation is car-'), Document(metadata={}, page_content='ried out by the problem-solving system. The JTMS simply ensures that as\\nnew beliefs are added to the system, the existing beliefs remain consistent.\\nThe JTMS is able to create new nodes, to add or retract justifications for\\nnodes, and can mark a node as a contradiction if it is informed by the prob-\\nlem solver that that is the case.\\nThe system either believes or does not believe in the statement represented\\nby each node, and so a node is described as being either in (the system'), Document(metadata={}, page_content='believes in it) or out (if the system does not believe in it). Of course, these\\nbeliefs can change, as new information is presented to the system and as it\\nmakes new arguments.\\nA node is considered to be contradictory if it represents a belief that is now\\nbelieved to be untrue. When such a contradiction is determined, the JTMS\\nmust use this information to retract beliefs that it had formed based (directly\\nor indirectly) on the incorrect belief. This retraction is done using depend-'), Document(metadata={}, page_content='ency-directed backtracking(also called nonchronological backtracking—\\nsee Section 5.17). Dependency-directed backtracking in this case simply\\nmeans working back from the contradictory node to find assumptions that led\\nto the contradiction. These assumptions are retracted, until a minimal combi-\\nnation of retractions is found to ensure that the contradiction disappears.\\nAn alternative to the JTMS is the assumption-based truth maintenance sys-'), Document(metadata={}, page_content='tem, or ATMS. An ATMS is very similar to a JTMS, but rather than repre-\\nsenting a complete statement of the system’s beliefs at any given time, it\\nincludes information about all possible beliefs, or all possible worlds. Each\\nnode has associated with it a set of premises or assumptions that can be\\nused to make the node true. Hence, a node might have the following\\nassumptions associated with it:\\n({P}, {Q})\\nThis would mean that the node would be true if P is true, or it would be'), Document(metadata={}, page_content='true if Q is true. A node that has an empty set associated with it is neces-\\nsarily true, which means that it does not depend on other assumptions.'), Document(metadata={}, page_content='480 CHAPTER 17 Advanced Knowledge Representation\\n17.6.4 Closed-World Assumption\\nThe closed-world assumption (also known as negation by failure, partic-\\nularly as used by PROLOG) is an assumption used by systems that any fact\\nnot specifically known to be true is not true. For example, if a system uses a\\ndatabase of facts, and a particular fact is not included in the database, then\\nthat fact is assumed to be false.\\nThe open-world assumption is the inverse of this: that any fact not explic-'), Document(metadata={}, page_content='itly contained within the database is assumed to be true. Note that one sig-\\nnificant difference between STRIPS and ADL, two planning methods\\ndescribed in Chapter 16, is that STRIPS uses the closed-world assumption,\\nwhereas ADL uses the open-world assumption.\\nClearly, systems that use the closed-world assumption (or the open-world\\nassumption) must use nonmonotonic reasoning because they make\\nassumptions that may later prove to be false.'), Document(metadata={}, page_content='assumptions that may later prove to be false.\\nPROLOG uses the closed-world assumption, which means that if a fact is\\nnot contained within its database, then it is assumed to be false.\\n17.6.5 The Ramification Problem\\nThe ramification problem is similar to the frame problem, described in\\nChapter 15, which concerns the difficulty of needing to define all facts that\\ndo not change when an action is performed. The ramification problem'), Document(metadata={}, page_content='concerns the additional consequences of actions that might not be imme-\\ndiately obvious. For example, if a robot picks up a block, and a fly has\\nlanded on the block, then the robot will also be picking up the fly. The ram-\\nification problem is the problem of determining how to deal with such\\npotentially highly complex consequences.\\n17.6.6 Circumscription\\nMcCarthy (1980) proposed a form of nonmonotonic reasoning, which he\\ncalled circumscription. Circumscription was designed to deal, like the'), Document(metadata={}, page_content='closed-world assumption, with situations in which not every possible fact\\nis stated or denied.\\nMcCarthy imagined someone attempting to solve the missionaries and\\ncannibals problem (see Section 3.9.1), which involves having a group of\\nmissionaries and cannibals cross a river without the cannibals eating the'), Document(metadata={}, page_content='17.6 Nonmonotonic Reasoning 481\\nmissionaries. McCarthy imagined a person trying to solve this question by\\nasking questions such as “Does the boat leak?” or “Is there a bridge?”\\nCircumscription allows us to modify a first-order predicate calculus\\nexpression to show that no facts are true other than those stated in the\\nexpression.\\nBy applying circumscription in the problem of the cannibals and mission-\\naries, we can conclude that any facts not explicitly stated in the problem'), Document(metadata={}, page_content='specification are not true.\\nThe circumscription of predicate P in an expression E is written\\nE(/H9278)\\n∧∀ x (/H9278(x) → P(x)) → ∀x (P(x) → /H9278(x))\\nwhere /H9278(x) is the result of substituting all occurrences of P with /H9278in E.\\nLet us consider a simple example from the blocks world:\\nE = IsBlock (A) ∧ IsBlock (B) ∧ IsBlock (C)\\nHere the predicate IsBlock is used to indicate that an object is a block.\\nWe can circumscribe the predicate IsBlock in E as follows:'), Document(metadata={}, page_content='First, we note that E(/H9278) is the following expression:\\n/H9278(A)\\n∧ /H9278(B) ∧ /H9278(C)\\nHence, the circumscription of IsBlock in E is\\n/H9278(A) ∧ /H9278(B) ∧ /H9278(C) ∧∀ x (/H9278(x) → IsBlock(x)) →\\n∀x (IsBlock (x) → /H9278(x))\\nNow to see what this really means, let us make the following substitution:\\n/H9278(x) ≡ (x = A ∨ x = B ∨ x = C)\\nClearly, /H9278(A) will become (A = A ∨ A = B ∨ A = C), which is true, and sim-'), Document(metadata={}, page_content='ilarly for /H9278(B) and /H9278(C). Hence, these parts can be eliminated from the\\nexpression (since TRUE ∧ A = A).\\nThis results in the following expression:\\n∀x ((x = A ∨ x = B ∨ x = C) → IsBlock(x)) → ∀x (IsBlock (x) →\\n(x = A ∨ x = B ∨ x = c))\\nNow, we can use our original expression:\\nE = IsBlock (A) ∧ IsBlock (B) ∧ IsBlock (C)'), Document(metadata={}, page_content='482 CHAPTER 17 Advanced Knowledge Representation\\nHence, (x = A ∨ x = B ∨ x = C) → IsBlock (x) is clearly true. Since\\nTRUE → A = A\\nWe can thus eliminate the left-hand side of the first implication, to give the\\nfollowing expression:\\n∀x (IsBlock (x) → (x = A ∨ x = B ∨ x = C ))\\nIn other words, not only are A, B, and C blocks, but there is nothing else in\\nthe world that can be called a block.\\nNote that if we now add an additional expression to E,\\nIsBlock (D)'), Document(metadata={}, page_content='IsBlock (D)\\nthe circumscribed expression we derived above is no longer true. We can\\ninstead, derive the following new circumscribed expression:\\n∀x (IsBlock (x) → (x = A ∨ x = B ∨ x = C ∨ x = D))\\nThis is a property of a nonmonotonic reasoning system: adding a new fact\\nnegates conclusions that have been logically deduced.\\n17.6.7 Abductive Reasoning\\nRecall the modus ponens rule from Section 7.11.4, which is written as follows:'), Document(metadata={}, page_content='This tells us that if A is true, and we know that A implies B, then we can\\ndeduce B.\\nAbductive reasoning is based on a modified version of modus ponens,\\nwhich while not logically sound, is nevertheless extremely useful:\\nThis tells us that if we observe that B is true, and we know that A implies B,\\nthen it is sensible to see A as a possible explanation for B.\\nFor example, consider the case where B represents “Fred is not at work” and'), Document(metadata={}, page_content='A represents “Fred is sick. ” If we know that when Fred is sick he does not\\ncome to work, and we also know that Fred is not at work, then we use\\nabductive reasoning to conclude that Fred is sick. This might not be the\\ncase, as he may be on holiday or at a conference, but the point of abductive\\nBA B\\nA\\n→\\nAA B\\nB\\n→'), Document(metadata={}, page_content='17.6 Nonmonotonic Reasoning 483\\nreasoning is that it provides a “good-enough” explanation for a phenome-\\nnon, which can be retracted later, if a preferable explanation is determined.\\nIn other words, abductive reasoning is nonmonotonic.\\n17.6.8 The Dempster–Shafer Theory\\nThe Dempster–Shafer theory of evidence is used to discuss the degree of\\nbelief in a statement. A degree of belief is subtly different from probability.\\nFor example, suppose that a barometer tells you that it is raining outside'), Document(metadata={}, page_content='and that you have no other way to determine whether this is the case or not\\nand no knowledge about the reliability of the barometer.\\nUsing probability theory, we might suppose that there is a 0.5 chance that the\\nbarometer is right, in which case the probability that it is raining would be 0.5.\\nHowever, using the Dempster–Shafer theory, we would start by stating that in\\nfact we have no knowledge about whether it is raining or not, and so we write\\nBel (Raining) = 0'), Document(metadata={}, page_content='Bel (Raining) = 0\\nSince we also have no knowledge about whether it is not raining, we can\\nalso write\\nBel (\\n¬Raining) = 0\\nNote that Bel (A) and Bel (¬A) do not need to sum to 1.\\nNow let us further suppose that we have determined that the barometer is\\n80% accurate.\\nHence, we can modify our belief as follows:\\nBel (Raining) = 0.8\\nThis tells us that because the barometer says it is raining, we have a belief of\\n0.8 that it is in fact raining. At this point, we still have the following:'), Document(metadata={}, page_content='Bel (¬Raining) = 0\\nBecause the barometer is telling us that it is raining, we do not have any rea-\\nson to believe that it is not raining. Note again the difference between this\\nnotation and normal probabilistic notation, where P (Raining) and\\nP (\\n¬Raining) must sum to 1.\\nWe also define the plausibility of a statement, X, as follows:\\nPl (X) = 1 /H11002Bel (¬X)'), Document(metadata={}, page_content='484 CHAPTER 17 Advanced Knowledge Representation\\nHence, we can define a range for X, which is [ Bel (X), Pl (X)]. For the\\nexample above, our range is\\n[0.8, 1]\\nThe narrower this range is, the more evidence we have, and the more cer-\\ntain we are about our belief. That is to say, if we have a belief range of [0, 1],\\nthen we really do not know anything. If we have a belief range of [0.5, 0.5],\\nthen we are certain that the probability of the proposition is 0.5. Hence, if'), Document(metadata={}, page_content='we have a wide range, then we know that we need to seek more evidence.\\nLet us now suppose that we have a second barometer, which is 75% accu-\\nrate, and which is also saying that it is raining outside. How does this affect\\nour belief? Dempster (1968) proposed a rule for combining beliefs of this\\nkind, which is applied as follows.\\nThe probability that both barometers are reliable is\\n0.75 /H110030.8\\n= 0.6\\nThe probability that both are unreliable is\\n0.25 /H110030.2\\n= 0.05'), Document(metadata={}, page_content='0.25 /H110030.2\\n= 0.05\\nHence the probability that at least one of the barometers is reliable is\\n1 /H110020.05\\n= 0.95\\nThus, we can assign the following belief range to the belief that it is raining:\\n[0.95, 1]\\nOnce again, we have no reason to believe that it is not raining, and so the\\nplausibility of the statement “it is raining” is 1. If we receive some evidence\\nthat it is not raining (e.g., if we cannot hear any rain), then we might mod-\\nify this value.'), Document(metadata={}, page_content='ify this value.\\nLet us now suppose that the second barometer says that it is not raining,\\nwhile the first barometer continues to say that it is raining.\\nNow, it cannot be the case that both barometers are reliable because they\\ndisagree with each other. The probability that the first barometer is reliable\\nand that the second is unreliable is'), Document(metadata={}, page_content='17.6 Nonmonotonic Reasoning 485\\n0.8 /H110030.25\\n= 0.2\\nSimilarly, the probability that the second is reliable and the first unreliable is\\n0.75 /H110030.2\\n= 0.15\\nThe probability that neither is reliable is\\n0.2 /H110030.25\\n= 0.05\\nDempster’s rule now lets us calculate the belief that it is raining. We can cal-\\nculate the posterior probability that it is raining, given that at least one of\\nthe barometers is unreliable as follows:'), Document(metadata={}, page_content='the barometers is unreliable as follows:\\nSimilarly, the probability that it is not raining, given that at least one of the\\nbarometers is unreliable is\\nHence, our belief that it is raining is Bel (Raining) = 0.5, and the plausibil-\\nity of this belief is 1 /H11002Bel(\\n¬Raining) = 1 /H110020.375 = 0.625. Hence, our belief\\ncan be expressed as the range\\n[0.5, 0.625]\\n17.6.9 MYCIN and Certainty Factors\\nIn Chapter 9, we introduced expert systems or production systems and'), Document(metadata={}, page_content='briefly mentioned MYCIN, which was a system developed at Stanford Uni-\\n01 5\\n02 01 5 00 5\\n01 5\\n04\\n0 375\\n.\\n.. .\\n.\\n.\\n.\\n++\\n=\\n=\\n02\\n02 01 5 00 5\\n02\\n04\\n05\\n.\\n.. .\\n.\\n.\\n.\\n++\\n=\\n='), Document(metadata={}, page_content='486 CHAPTER 17 Advanced Knowledge Representation\\nversity in the 1980s for medical diagnosis. MYCIN was designed to help\\ndoctors select the correct antimicrobial agent to treat a patient, based on\\ninformation about the patient’s symptoms.\\nMYCIN uses abductive reasoning and backward chaining to estimate,\\nbased on a set of evidence concerning the patient’s symptoms, which bacte-\\nria is most likely to be causing the illness.\\nMYCIN uses certainty factors to represent degrees of belief: much as the'), Document(metadata={}, page_content='Dempster–Shafer theory uses the Bel notation, certainty factors represent\\nthe degree of belief or disbelief, where the two do not necessarily sum to 1,\\nas they would in classical logic.\\nWe use M\\nB(H|E) to represent the measure of belief of hypothesis H,g i v e n\\nevidence E, and MD(H|E) to represent the measure of disbelief of hypothe-\\nsis H, given evidence E.\\nBecause a particular piece of evidence either supports or contradicts a'), Document(metadata={}, page_content='hypothesis, either MB(H|E) or MD(H|E) must be zero for any H and E.\\nWe now define the certainty factor,CF(H|E) as follows:\\nCF(H|E) = MB(H|E) /H11002MD(H|E)\\nThis value ranges from /H110021 to 1, where a high negative value indicates that\\nthe evidence gives a strong confidence that the hypothesis is false, and a\\nhigh positive value indicates that the evidence gives a strong confidence\\nthat the hypothesis is true.\\nEach production rule used by MYCIN has a certainty factor associated with'), Document(metadata={}, page_content='it. The following is a simplified example of one of MYCIN’s rules:\\nIF: The patient has meningitis\\nAND: The patient has no serious skin infection\\nAND: The infection is bacterial\\nTHEN: The infection could be caused by staphylococcus-coag-pos (0.75)\\nOR: streptococcus-group-a (0.5)\\nThis rule is of the form\\nIF A ∧ B ∧ C ∧ ... N THEN H1 (P1) ∨ H2 (P2) ∨ ... ∨ Hn (Pn)'), Document(metadata={}, page_content='17.7 Reasoning about Change 487\\nwhere A ... N are the observed evidence,H1 ... Hn are the possible hypothe-\\nses to explain the evidence, andPi is the certainty factor associated withHi.\\nCertainty factor algebra is used to combine the certainty factors of rules\\nwith the certainty factors of the evidence to determine how certain the\\nhypotheses are.\\nWhen a rule has a conjunction of premises, as in the example rule above,\\nthe minimum of the certainty factors of the premises is used as the cer-'), Document(metadata={}, page_content='tainty factor. If the rule has a disjunction of premises, then the maximum\\nof the certainty factors is used.\\n17.7 Reasoning about Change\\nAs we saw in Chapter 7, the classical propositional and predicate calculi\\nprovide us with a way to reason about an unchanging world. Most real-\\nworld problems involve a dynamic world, in which other people (or agents)\\neffect changes, where the world itself changes, and where robotic agents can\\nmove themselves and thus change their environment proactively.'), Document(metadata={}, page_content='In Chapter 15, we briefly introduced the situation calculus that enables us\\nto use a notation such as the following:\\n∃x(In(Robot,x,S\\n1) ∧ In(cheese,x,S1))\\nThis sentence says that in situation S1, the robot is in the same room as\\nthe cheese.\\nIn this section we will explore two alternatives to the situation calculus:\\nevent calculus and temporal logic.\\n17.7.1 Temporal Logic\\nAn early system for dealing with change was temporal logic , a form of'), Document(metadata={}, page_content='modal logic. T emporal logic extends predicate calculus with a set of modal\\noperators, which are usually defined as follows:\\nP means from now on, P will be true\\n/H17003P means that at some point in the future, P will be true'), Document(metadata={}, page_content='488 CHAPTER 17 Advanced Knowledge Representation\\nCompare these with the modal operators presented in Chapter 7, where the\\nsame symbols were used to indicate “necessarily” and “possibly. ” In tempo-\\nral logic, the symbols are read as “henceforth” and “eventually. ”\\nLinear time temporal logic defines two other operators: “until” and “in the\\nnext time interval, ” which are usually written\\nQµP means that Q is true until P is true\\n/H11034P means that P will be true in the next time interval'), Document(metadata={}, page_content='A number of other operators are also sometimes used, including\\nP awaits Q means that Q is true until P is true, or if P is never true,\\nthen Q is true forever (this contrasts with “until, ” which\\nimplicitly assumes that P will at some point be true)\\nSofar P means that P has been true until now\\nOnce P means that P was true at some time in the past\\nP precedes Q means that P occurred before Q\\nThese temporal operators implicitly assume that there is a concept of time,'), Document(metadata={}, page_content='which is broken down into intervals. In particular, the /H11034operator indicates\\nthat some expression will be true in the next time interval. T emporal logic\\ndoes not require the lengths of these time intervals to be defined, although\\nclearly for it to be applied to real-world problems a mapping needs to be\\ndefined. In linear time temporal logic there is a finite set of states, such that\\neach state has a unique successor. Hence, the logic cannot reason about'), Document(metadata={}, page_content='multiple possible futures. This is possible with an alternative form of tem-\\nporal logic: computation tree logic (CTL—also known as branching time\\ntemporal logic), which reasons about time in the form of a tree, with states\\nrepresented by nodes in the tree. Because each state can have more than one\\nsuccessor, it is possible in this logical system to reason about several possi-\\nble future outcomes.\\nCTL provides methods for reasoning about paths through the tree. For'), Document(metadata={}, page_content='example, it is possible to create expressions such as “a path exists in whichP\\nis true” or “a path exists in which eventuallyP is true for all successor states.”\\nThere also exist modal operators similar to the “necessarily” and “possibly”\\noperators presented in Chapter 7, which state, for example, “P is true in all\\npossible future states” or “P is true in at least one possible future state. ”'), Document(metadata={}, page_content='17.7 Reasoning about Change 489\\n17.7.2 Using Temporal Logic\\nT emporal logic can be used in a number of applications. It is used, for\\nexample, in specification and verification of software programs and can\\nalso be used to verify the behavior of other systems, such as elevators. It can\\nalso be used to reason about problems that cannot otherwise be reasoned\\nabout using classical logics.\\nA system being defined by temporal logic has three main sets of conditions\\nthat define its behavior:'), Document(metadata={}, page_content='that define its behavior:\\n1. Safety conditions define behaviors that should never occur (such\\nas the elevator being on two floors at once).\\n2. Liveness conditions specify what the system should do—for exam-\\nple, if someone pushes the button on the first floor, then the eleva-\\ntor should move toward that floor.\\n3. Fairness conditions define the behavior of the system in nondeter-\\nministic situations. For example, if the elevator is stationary on the'), Document(metadata={}, page_content='second floor, and someone pushes the button on the first floor at\\nthe same time that someone else pushes the button on the third\\nfloor, the system must decide which direction to move the elevator.\\nWe will now examine an example of temporal logic being used to specify a\\nproblem. The dining philosophers problem is defined as follows:\\nA number of philosophers are sitting around a round table, eating\\nspaghetti and cogitating. There are six philosophers, six plates, and six'), Document(metadata={}, page_content='forks. Each philosopher has a plate in front of him or her, and there is a fork\\nbetween each pair of philosophers. For a philosopher to eat spaghetti, he or\\nshe must use two forks. Hence, only three philosophers can be eating at any\\none time. When a philosopher is not eating, he or she is thinking.\\nWe will use the notation eating(i) to indicate that philosopher i is eating\\nand thinking(i) to indicate that philosopher i is thinking.\\nThe safety properties for this problem are defined as follows:'), Document(metadata={}, page_content='Each philosopher cannot be eating and thinking at the same time:\\n¬(eating (i) ∧ thinking (i))\\nEach philosopher is always either eating or thinking:\\n(eating (i) ∨ thinking (i))'), Document(metadata={}, page_content='490 CHAPTER 17 Advanced Knowledge Representation\\nParty\\nstarts\\nParty\\nendsspace\\ntime\\nFigure 17.3\\nThe space–time event that\\nis a party at Tom’ s house\\nIf one philosopher is eating, then the next philosopher cannot be eating:\\n¬(eating (i) ∧ eating (i + 1))\\nWe can also define the liveness properties of the system as follows:\\nIf a philosopher is eating now, then at some point in the future he or she\\nwill be thinking:\\n(eating (i) → /H17003thinking (i))'), Document(metadata={}, page_content='(eating (i) → /H17003thinking (i))\\nSimilarly, if a philosopher is thinking now, then at some point in the future\\nhe or she will be eating:\\n(thinking (i) → /H17003eating (i))\\nNote that in this notation, unlike situation calculus, there is no mention of\\nexplicit states. This is not necessary with temporal logic, which is one rea-\\nson for using it in preference to situation calculus.\\n17.7.3 Event Calculus\\nAn alternative method for reasoning about properties that vary over time is'), Document(metadata={}, page_content='the event calculus. Event calculus is concerned mainly with fluents. A flu-\\nent is a function that varies with time. For example, if a ball is dropped\\nfrom a first-story window, then the ball’s speed and height are both fluents.'), Document(metadata={}, page_content='17.7 Reasoning about Change 491\\nThe event calculus also uses the notion of an event, which is a period of\\ntime bounded by a start and a finish point. Events can also be thought of as\\ntaking place in the real world and so have a space dimension as well as a\\ntime dimension. For example, the event called “the party at T om’s house”\\nhas a start and stop time, and takes place in a finite space, as shown in Fig-\\nure 17.3.\\nThe event calculus uses a number of predicates:\\nHappens (e, t)\\nStarts (e, f, t)'), Document(metadata={}, page_content='Happens (e, t)\\nStarts (e, f, t)\\nEnds (e, f, t)\\nwhere f is a fluent, e is an event, and t is a variable of time.\\nHappens (e, t) means that event e happens at time t. In fact, t can be a func-\\ntion of time, and thus this predicate can be used to express the fact that an\\nevent (e) takes place over a period of time, defined by the function t.\\nStarts (e, f, t) means that the evente causes fluent f to hold immediately after'), Document(metadata={}, page_content='time t, and similarly,Ends (e, f, t) means that evente stops fluentf at time t.\\nA further predicate lets us state that fluent f was beginning at the start of\\nthe time period we are considering:\\nInitially (f)\\nFor example, let us consider the event in which a ball drops from a height of\\n10 meters to the ground. For this example, we will assume that time starts at\\nthe moment the ball is dropped, and we will consider the following fluents:\\nf\\n1 means the ball is motionless'), Document(metadata={}, page_content='f\\n1 means the ball is motionless\\nf2 means the ball is falling\\nf3 means the ball is on the floor\\nWe will also consider the following events:\\ne1 is the event that the ball is dropped\\ne2 is the event that the ball hits the floor\\nHence, we can start with the following expression:\\nInitially (f1)'), Document(metadata={}, page_content='492 CHAPTER 17 Advanced Knowledge Representation\\nbecause the ball starts out motionless.\\nNext we can say\\nHappens (e1, t1)\\nwhich tells us that the ball is dropped at time t1.\\nWe can also define the causal relationships involved in this scenario, by say-\\ning the following:\\nStarts (e1, f2, t1)\\nEnds (e1, f1, t1)\\nFinally, we can define the consequences of the ball hitting the floor:\\nHappens (e2, t2)\\nEnds (e2, f2, t2)\\nStarts (e2, f3, t2)\\nStarts (e2, f1, t2)'), Document(metadata={}, page_content='Starts (e2, f3, t2)\\nStarts (e2, f1, t2)\\nAn additional predicate is used to express a period of time over which\\nsomething occurs:\\nT (e, i)\\nThis means that event e took place throughout the interval defined by i.F o r\\nexample, we might say\\nT (Dropping (Ball), T oday)\\nwhich would mean that the ball started dropping on or before the stroke of\\nmidnight this morning and continued to drop for the entire day.\\nIt might be more useful to express the idea that the ball was dropping at'), Document(metadata={}, page_content='some time today, for which we use the E predicate:\\nE (Dropping (Ball), T oday)\\n17.7.4 Mental Situation Calculus\\nThe situation calculus and event calculus are used to describe events and\\ntheir effects on the world. It is also useful to consider the effects that events\\nhave on an agent’s beliefs about the world. For this, we use mental situa-\\ntion calculus.'), Document(metadata={}, page_content='17.7 Reasoning about Change 493\\nThe following functions are used:\\nHolds (P, S) means that proposition P holds in situation S\\nBelieves (P) means that the agent believes proposition P\\nHence, we might write:\\nHolds (Believes (Fly (Pigs)), S)\\nThis means that it is true in situationS that the agent believes that pigs can fly.\\nWe also use a number of functions based around the idea of knowledge. It\\nis convenient to write all of these using the same symbol:\\nKnows (P)'), Document(metadata={}, page_content='Knows (P)\\nIn fact, this can have a number of different meanings depending on the\\nnature of P.\\nFor example,\\nHolds (Knows (¬Knows (P)), S)\\nmeans that it is true in situationS that the agent knows that it does not know\\nP,w h e r eP is some individual concept (such as the whereabouts of the piece\\nof cheese for which the robot is searching, or T om’s telephone number).\\nAdditionally, the Knows function can be used to refer to knowledge about\\npropositions:\\nHolds (Knows (Fly (Pigs)), S)'), Document(metadata={}, page_content='propositions:\\nHolds (Knows (Fly (Pigs)), S)\\nThis means that it is true in situation S that the agent knows that pigs can\\nfly. Note that in this notation we are treating Fly (Pigs) as a fluent, which\\nmay vary over time: It may be true at the moment that pigs can fly, but\\ntomorrow they may forget how.\\nWe can extend the Believes function to allow it to express the idea that a\\nbelief exists for an interval of time:\\nBelieves (P, i)'), Document(metadata={}, page_content='Believes (P, i)\\nwhich means that the agent believes proposition P during the entirety of\\nthe interval defined by i.\\nWe can also treat knowledge and beliefs as fluents. For example, we might\\nwant to say\\nT(Believes (Fly (Pigs), Y esterday), T oday)'), Document(metadata={}, page_content='494 CHAPTER 17 Advanced Knowledge Representation\\nwhich means it is true (for the whole of) today that throughout yesterday\\nthe agent believed that pigs could fly.\\nEvents can occur that change an agent’s beliefs. For this purpose, we define\\na point fluent as defining the fact that an event takes place at some\\nmoment in time. We write\\nOccurs (e, S)\\nto state that event e occurs in situation S.\\nWe can then define a new function:\\nLearns (P)\\nwhich means that the agent learns proposition P.\\nHence,'), Document(metadata={}, page_content='Hence,\\nOccurs (Learns (P), S) → Holds (F (Knows (P)), S)\\nF (P) means that P will be true in the future. Hence, this sentence means\\nthat if in situation S the agent learns proposition P, then it is true that the\\nagent will know proposition P at some future time.\\n17.8 Knowledge Engineering\\nKnowledge engineering was introduced in Chapter 9, in the context of\\nexpert systems. In fact, knowledge engineering is an essential part of many\\nArtificial Intelligence systems.'), Document(metadata={}, page_content='Artificial Intelligence systems.\\nAll systems that are based on propositional calculus, predicate calculus, sit-\\nuation calculus, event calculus, temporal logic, and other such languages\\nare primarily designed to manipulate knowledge. For those systems to per-\\nform useful tasks, knowledge needs to be gathered that can be entered into\\nthe system. Of course, in some systems, knowledge is gathered by an\\nautonomous agent, and no knowledge engineering is necessary. In many'), Document(metadata={}, page_content='Artificial Intelligence systems today, this is not the case, and a knowledge\\nengineer is an essential component of the system.\\nThe knowledge engineer’s task is to gather knowledge (knowledge acquisi-\\ntion) about the problem space and to convert this knowledge into a form\\nusable by the system (e.g., into first-order predicate calculus). The knowl-\\nedge engineer must also consider the level of detail to use. For example, in'), Document(metadata={}, page_content='defining the properties of a building, it might be considered sufficient to'), Document(metadata={}, page_content='17.9 Case-Based Reasoning 495\\nsimply say Building (P) to define P as representing a building. It might also\\nbe more useful to include details such as HasWindows (P, 6), HasFloors (P,\\n2), and HasRoof (P). Alternatively, it might make more sense to define these\\nproperties for all buildings:\\n∀x Building (x) → HasWindows (x) ∧ HasFloors (x) ∧ HasRoof (x)\\nThe knowledge engineer might then choose to include detail about the\\nnature of buildings in terms of bricks, wood, and steel, and might further'), Document(metadata={}, page_content='include details about the physical nature of these materials. In some cases,\\nthis detail might be superfluous. In other words, it is important for the\\nknowledge engineer to select the correct level of detail to include in the\\nknowledge base that is being built.\\nThe important principle is to select predicates, functions, and constants\\nthat match the problem to be solved. If a system is being designed to deter-\\nmine the best layout of windows in a building, where the desired answer is'), Document(metadata={}, page_content='simply the number of windows to include on each wall, then having a con-\\nstant to represent each brick in the building would be unnecessary.\\n17.9 Case-Based Reasoning\\nCase-based reasoning was briefly introduced in Chapter 16, where it was\\ndiscussed in the context of planning. Case-based reasoning involves reusing\\npreviously identified solutions to solve new problems and is often used in\\nexpert systems, as well as in other types of systems, such as the checkers-'), Document(metadata={}, page_content='playing system developed by Samuel (see Chapter 6).\\nA case-based reasoning system uses a memory that can store solutions to past\\nproblems, along with information about whether each solution was successful\\nor not. Such a system must therefore have the ability to look up a new prob-\\nlem, in order to find a previous case that was similar or identical to the current\\nproblem. Once such a case is found, the solution that was applied is modified'), Document(metadata={}, page_content='in order to apply it directly to the current problem. This solution is then stored\\nin the memory, along with information about whether it succeeded or failed.\\nFor a case-based system to function adequately, the representation of cases\\nmust be carefully considered. The details that are used to index each case\\nneed to be relevant and also must be able to distinguish the case from other,\\ndissimilar cases. The notion of similarity is important: what features mark'), Document(metadata={}, page_content='two cases as similar? This is not always obvious, and the features that are\\nused to define each case must be carefully selected.'), Document(metadata={}, page_content='496 CHAPTER 17 Advanced Knowledge Representation\\nCase-based systems make the task of knowledge acquisition relatively straight-\\nforward: the knowledge engineer simply needs to obtain examples of prob-\\nlems and their solutions (cases), which are entered into the system’s memory.\\nCases can be stored in a number of formats. For example, each case can be\\ndefined simply as a vector of the features that define the case and its solu-'), Document(metadata={}, page_content='tion. Alternatively, each case can be stored as a set of situated action rules (as\\nused in Brooks’ subsumption architecture, which is described in Chapter\\n19), each of which represents a solution to a particular situation (problem).\\nCase-based reasoning can be a very efficient way for a system to learn to solve\\nproblems, by examining its performance at solving past problems. As the sys-\\ntem encounters more cases, it becomes better able to solve new problems. Of'), Document(metadata={}, page_content='course, as the database of cases becomes larger, it becomes slower at retrieving\\ncases from the database, and so there is a trade-off between performance and\\nefficiency. It is possible to avoid this problem by only storing the most suc-\\ncessful solutions and “forgetting” solutions that were less successful. Samuel’s\\ncheckers program used this idea to remember only the “best” positions.\\n17.10 Chapter Summary\\n■ Knowledge representation is vital to Artificial Intelligence and has'), Document(metadata={}, page_content='been used extensively throughout this book.\\n■ The blackboard architecture is a structured knowledge representa-\\ntion that uses opportunistic reasoning to combine inputs from a\\nnumber of knowledge sources.\\n■ Scripts are used to represent situations (such as going to a restau-\\nrant) that often conform to a particular pattern.\\n■ The Copycat architecture is used to solve analogy problems of the\\nform “A is to B as C is to what?”\\n■ Classical logic is monotonic, which means that as new facts are'), Document(metadata={}, page_content='added to a database, old conclusions are never contradicted. Many\\nreal-life situations require nonmonotonic reasoning.\\n■ The modal operator M is used to represent the idea that a proposi-\\ntion is consistent with our current beliefs.\\n■ Default reasoning uses assumptions about default values for cer-\\ntain variables, unless evidence to the contrary is found.'), Document(metadata={}, page_content='17.11 Review Questions 497\\n■ Truth maintenance systems are used to ensure that the facts con-\\ntained in a system’s database are consistent, in a nonmonotonic\\nreasoning environment.\\n■ The closed-world assumption is the assumption that any statement\\nnot explicitly known to be true is false.\\n■ The ramification problem is an extension of the frame problem.\\nThe ramification problem is the problem of dealing with small but\\npotentially significant side effects of actions.'), Document(metadata={}, page_content='potentially significant side effects of actions.\\n■ Circumscription is a form of nonmonotonic reasoning that enables\\nus to deduce which facts are false, based on a limited set of statements.\\n■ Abductive reasoning involves determining a possible explanation\\nfor an observed phenomenon and is widely used by people and\\nArtificial Intelligence systems.\\n■ The Dempster–Shafer theory provides a way to reason about\\ndegrees of belief.\\n■ MYCIN uses certainty factors to reason about degrees of certainty.'), Document(metadata={}, page_content='■ T emporal logic is an extension of first-order predicate calculus,\\nwhich uses a set of modal operators to reason about change.\\n■ Event calculus is similar to situation calculus, but reasons about\\nfinite events.\\n■ Mental situation calculus allows us to reason about beliefs and\\nknowledge, and how they change over time.\\n■ Knowledge engineering is a vital element of many Artificial Intelli-\\ngence systems.\\n■ Case-based reasoning allows a system to learn from previous solu-'), Document(metadata={}, page_content='tions to problems, in order to solve new problems.\\n17.11 Review Questions\\n17.1 Explain why the blackboard architecture is an effective way to\\ncombine information from a number of knowledge sources.\\nDescribe the main components of the blackboard architecture.\\n17.2 Explain what kinds of problems the Copycat architecture can solve.'), Document(metadata={}, page_content='498 CHAPTER 17 Advanced Knowledge Representation\\n17.3 Explain what is meant by nonmonotonic reasoning, and explain\\nwhy it is so important in Artificial Intelligence. Explain the differ-\\nence between the terms nonmonotonic reasoning and nonmonotonic\\nlogic.\\n17.4 Explain the purpose of a truth maintenance system.\\n17.5 Explain what is meant by abductive reasoning. Explain your views\\non its usefulness in solving the following types of problems:\\na. solving logical puzzles\\nb. medical diagnosis'), Document(metadata={}, page_content='a. solving logical puzzles\\nb. medical diagnosis\\nc. controlling the behavior of a robotic agent\\nd. understanding spoken human language\\n17.6 Compare and contrast the Dempster–Shafer theory and certainty\\nfactors.\\n17.7 Explain the idea behind temporal logic. What kinds of problems is\\nit useful for solving? Give three examples.\\n17.8 Can semantic networks be used to represent anything that can be\\nrepresented using temporal logic? Explain your answer.'), Document(metadata={}, page_content='17.9 Explain what is meant by knowledge engineering, and why it is\\nuseful for systems other than expert systems.\\n17.10 What is case-based reasoning? From which attributes of human\\nintelligence do you think it is derived? Describe the last time you\\nused case-based reasoning in your normal life.\\n17.12 Exercises\\n17.1 Download the Copycat demonstration applet, the details for which\\ncan be found on the Internet using any search engine. Examine the'), Document(metadata={}, page_content='following problems, and observe how the Copycat system solves\\nthem. In each case, produce three suitable solutions yourself before\\nyou see what solutions Copycat comes up with. How often does it\\nfind the same solutions as you do?\\na. AABB is to AACC as JJKK is to what?\\nb. ABB is to ABCC as JKK is to what?\\nc. AABC is to AABD as IJKK is to what?'), Document(metadata={}, page_content='17.12 Exercises 499\\nd. BCD is to BCE as JFFWWW is to what?\\ne. A is to Z as EFG is to what?\\nf. FSF is to SFS as ABBBC is to what?\\n17.2 Use temporal logic to describe the following situation:\\nThere are three barbers in the shop. Each barber can shave either of\\nthe other two barbers but cannot shave himself. If a barber is not\\nshaving, then he sits and reads the newspaper. If a customer arrives\\nand a barber is free, then he will shave that customer. If a customer'), Document(metadata={}, page_content='arrives and no barber is free, then the customer will sit and read the\\npaper until a barber is free. Each barber needs to be shaved once a\\nday.\\n17.3 Devise a representation for the following statement:\\nYesterday, Bob went to the cinema, and he saw the filmTitanic.A f t e r -\\nward, he went straight home, with thoughts of the film going through\\nhis head. Angela went to the cinema at the same time and saw the film\\nThe Lord of the Rings. After the film, Angela went for a swim.'), Document(metadata={}, page_content='Now add sufficient facts to the knowledge base you have created to\\nenable an Artificial Intelligence system to answer the following\\nquestions:\\nDid Bob meet Angela yesterday?\\nDid Bob and Angela leave the cinema at the same time?\\nDid Bob and Angela spend time together after the films?\\nDid Bob enjoy the film?\\nY ou will need to add basic facts to the knowledge base such as:\\nLord of the Rings is longer than Titanic.\\nLord of the Rings started at the same time as Titanic.'), Document(metadata={}, page_content='Some of these facts will be common sense, and others you will\\nneed to invent to give reasonable answers to the questions.\\nHow do you use the facts in the knowledge base to derive answers\\nto the questions?'), Document(metadata={}, page_content='500 CHAPTER 17 Advanced Knowledge Representation\\n17.13 Further Reading\\nLuger (1995) provides an excellent range of papers on the subject of Artifi-\\ncial Intelligence in general, and in particular it has a number of papers that\\nare relevant to this chapter. Russell and Norvig (1995) have a great deal of\\ncoverage of knowledge representation and knowledge engineering, prima-\\nrily in the context of intelligent agents.\\nAdditional references for MYCIN are contained in the Further Reading sec-'), Document(metadata={}, page_content='tion of Chapter 9 of this book.\\nNonmonotonic Reasoning, by Grigoris Antoniou (1997 – MIT Press)\\nA Logical Theory of Nonmonotonic Inference and Belief Change , by Alexan-\\nder Bochman (2001 – Springer V erlag)\\nNonmonotonic Reasoning : An Overview , by Gerhard Brewka, Jürgen Dix,\\nand Kurt Konolige (1995 – Cambridge University Press)\\nNonmonotonic Reasoning : From Theoretical Foundation to Efficient Compu-\\ntation, by G. Brewka (1991 – Cambridge University Press)'), Document(metadata={}, page_content='A Generalization of Bayesian Inference, by A. P . Dempster (1968 - in Journal\\nof the Royal Statistical Society)\\nA Truth Maintenance System, by Jon Doyle (1979 – in Computation & Intel-\\nligence – Collected Readings, edited by George F. Luger, The MIT Press)\\nProbabilistic Interpretations for Mycin’s Certainty Factors, by O. Heckerman\\n(1986 – in Uncertainty in Artificial Intelligence , edited by L. N. Kanal and\\nJ. F. Lemmer, Elsevier Science Ltd., pp. 167–196)'), Document(metadata={}, page_content='J. F. Lemmer, Elsevier Science Ltd., pp. 167–196)\\nHandbook of Logic in Artificial Intelligence and Logic Programming: Non-\\nmonotonic Reasoning and Uncertain Reasoning , edited by Dov M. Gab-\\nbay, J. A. Robinson, and Christopher John Hogger (1994 – Oxford\\nUniversity Press)\\nCase-Based Reasoning, by Janet Kolodner (1993 – Morgan Kaufmann)\\nCase-Based Reasoning: Experiences, Lessons, and Future Directions, edited by\\nDavid B. Leake (1996 –AAAI Press)'), Document(metadata={}, page_content='David B. Leake (1996 –AAAI Press)\\nFor the Sake of the Argument : Ramsey Test Conditionals, Inductive Infer-\\nence and Nonmonotonic Reasoning , by Isaac Levi (1996 – Cambridge Uni-\\nversity Press)'), Document(metadata={}, page_content='17.13 Further Reading 501\\nNonmonotonic Logic: Context-Dependent Reasoning, by V . W. Marek and M.\\nTruszczynski (1993 – Springer V erlag)\\nCircumscription: A Form of Non-Monotonic Reasoning , by John McCarthy\\n(1980 – in Computation & Intelligence – Collected Readings , edited by\\nGeorge F. Luger, The MIT Press)\\nA Production System Version of the Hearsay-II Speech Understanding System,\\nby Donald McCracken (1981 - UMI Research)'), Document(metadata={}, page_content='by Donald McCracken (1981 - UMI Research)\\nBlackboard Systems: The Blackboard Model of Problem Solving and the Evolu-\\ntion of Blackboard Architectures, by H. Penny Nii (1986 – inComputation &\\nIntelligence – Collected Readings, edited by George F. Luger, The MIT Press)\\nSoft Computing in Case Based Reasoning, edited by Sankar K. Pal, Tharam S.\\nDillon, and Daniel S. Y eung (2000 – Springer V erlag)\\nInside Case-Based Reasoning , by Christopher K. Riesbeck and Roger C.'), Document(metadata={}, page_content='Schank (1989 – Lawrence Erlbaum)\\nChange, Choice and Inference: A Study of Belief Revision and Nonmonotonic\\nReasoning, by Hans Rott (2002 – Oxford University Press)\\nThe Structure of Episodes in Memory , by Roger C. Schank (1975 – in Com-\\nputation & Intelligence – Collected Readings, edited by George F. Luger, The\\nMIT Press)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='18CHAPTER\\nFuzzy Reasoning\\nAnd new philosophy calls all in doubt,\\nThe element of fire is quite put out;\\nThe sun is lost, and th’earth, and no man’s wit\\nCan well direct him, where to look for it.\\n—John Donne, An Anatomy of the World\\nTo be, or not to be: that is the question.\\n—William Shakespeare, Hamlet\\nI used to love mathematics for its own sake, and I still do, because it allows for\\nno hypocrisy and no vagueness, my two bêtes noires.\\n—Henri Beyle Stendahl, La Vie d’Henri Brulard'), Document(metadata={}, page_content='—Henri Beyle Stendahl, La Vie d’Henri Brulard\\n18.1 Introduction\\nThis chapter introduces the idea of fuzzy sets and fuzzy logic. The chapter\\nexplains how fuzzy sets are defined and explains how linguistic variables,\\nfuzzy operators, and hedges are applied. It also explains the concepts of\\nfuzzy logic and how they can be applied in solving real-world problems.\\nThis chapter explains how fuzzy expert systems can be built, as well as neuro-'), Document(metadata={}, page_content='fuzzy systems, which are a cross between neural networks and fuzzy systems.'), Document(metadata={}, page_content='504 CHAPTER 18 Fuzzy Reasoning\\n18.2 Bivalent and Multivalent Logics\\nIn classical logic, which is often described as Aristotelian logic, there are\\ntwo possible truth values: propositions are either true or false. Such systems\\nare known as bivalent logics because they involve two logical values.\\nThe logic employed in Bayesian reasoning and other probabilistic models is\\nalso bivalent: each fact is either true or false, but it is often unclear whether'), Document(metadata={}, page_content='a given fact is true or false. Probability is used to express the likelihood that\\na particular proposition will turn out to be true.\\nOne early multivalent logic was used to reason about the Uncertainty Prin-\\nciple, used in quantum physics. This logic had three values: true, false, and\\nundetermined.\\nAn extension of this three-valued logic is to consider 0 to represent false, 1\\nto represent true, and to use real numbers between 0 and 1 to represent\\ndegrees of truth.'), Document(metadata={}, page_content='degrees of truth.\\nNote that this is not the same as probability: if a fact has a probability value\\nof 0.5, then it is as likely to be true as it is to be false, but in fact it will only\\nbe either true or false. If in a multivalent logic we have a proposition that\\nhas a logical value of 0.5, we are saying something about the degree to\\nwhich that statement is true. In probability theory we are dealing with\\nuncertainty (at the moment we don’t know whether the proposition will be'), Document(metadata={}, page_content='true or false, but it will definitely either be true or false—not both, not nei-\\nther, and not something in between), but with multivalent logic we are cer-\\ntain of the truth value of the proposition; it is just vague—it is neither true\\nnor false, or it is both true and false.\\nAlthough this kind of logic may sound absurd, in this chapter we will see\\nhow it can be put to practical use and indeed how multivalent logics, and in'), Document(metadata={}, page_content='particular fuzzy logic, have become an extremely important part of Artifi-\\ncial Intelligence.\\n18.3 Linguistic Variables\\nIn fuzzy set theory and fuzzy logic, we make great use of linguistic vari-\\nables. A linguistic variable is a concept such as “height, ” which can have a\\nvalue from a range of fuzzy values including “tall,” “short,” and “medium.”\\nThe linguistic variable “height” may be defined over the universe of dis-'), Document(metadata={}, page_content='18.4 Fuzzy Sets 505\\n1\\n0\\n4 ft 8 ft Height\\nDegree of\\nmembersip\\nof the fuzzy\\nset of tall\\npeople\\nFigure 18.1\\nChart showing the mem-\\nbership function for the\\nfuzzy set of tall people\\ncourse from 2 feet up to 8 feet. As we will see, the values “tall, ” “short, ” and\\n“medium” define subsets of this universe of discourse.\\n18.4 Fuzzy Sets\\nFuzzy logic is used to reason about fuzzy sets. Fuzzy sets contrast with the\\nsets used in traditional set theory, which are sometimes known as crisp'), Document(metadata={}, page_content='sets. A crisp set can be defined by the values that are contained within it. A\\nvalue is either within the crisp set, or it is not. For example, the set of natu-\\nral numbers is a crisp set: 1, 2, 3, 4, and so on are natural numbers and so\\nare definitely members of the set of natural numbers. Numbers such as 0.2,\\n101.101, and /H9266are definitely not members of the set of natural numbers.\\nOn the other hand, let us consider the set of tall people. Bill is 7 feet tall,'), Document(metadata={}, page_content='and so it is pretty clear that he is included in the set of tall people. John is\\nonly 4 feet tall, and so most would say that he is not included in the set.\\nWhat about Jane, who is 5 feet 10 inches tall? Some would certainly say she\\nis tall, but others would say she is not.\\nThe fuzzy set of tall people contains Bill, and it also contains Jane, and it\\neven contains John. Each is a member of the set to some degree and is not a'), Document(metadata={}, page_content='member of the set to some degree. This can be seen in the chart in Figure\\n18.1, which shows the degree of membership that a person of a given height\\nhas in the fuzzy set of tall people.\\nThis definition of a fuzzy set is extremely natural and fits much better with\\nthe way people really talk about things. It is very common to say of some-\\none that she is “fairly tall” or “not very tall” but actually quite unusual to use'), Document(metadata={}, page_content='the unqualified descriptions “tall” or “not tall. ” This is because each person'), Document(metadata={}, page_content='506 CHAPTER 18 Fuzzy Reasoning\\nBaby Child Adult Teenager\\nDegree of\\nmembership\\n1\\n0\\n0 100 Age\\nFigure 18.2\\nGraph showing member-\\nship of the fuzzy sets baby,\\nchild, teenager, and adult\\nhas his or her own idea of what tall means, and in fact our definitions of tall\\nare not precise—if we were asked to define a group of people as either tall\\nor not tall, and then asked to repeat the exercise, we might well classify one\\nperson as tall on the first occasion and as not tall on the second occasion.'), Document(metadata={}, page_content='This is modeled very clearly in the fuzzy set, which defines each person as\\nbeing both tall and not tall, to some extent.\\nY ou may recall from Section 7.20 that we defined the law of the excluded\\nmiddle, which is a fundamental rule of classical logic, and which states that\\na proposition must either be true or false: it cannot be both true and false,\\nand it is not possible for a statement to be neither true nor false. This is the'), Document(metadata={}, page_content='basis of Aristotelian logic, but as we will see, in fuzzy logic, a statement can\\nbe both true and false, and also can be neither true nor false. Whereas in\\nclassical logic we can state axioms such as\\nA\\n∨¬ A = TRUE\\nA ∧¬ A = FALSE\\nin fuzzy logic these do not hold—A ∨¬ A can be, to some extent, false, and\\nA ∧¬ A can to some extent be true: the law of the excluded middle does not\\nhold in fuzzy logic.\\nThe idea of the intersection between crisp sets is easy to understand: if an'), Document(metadata={}, page_content='item is in set A and is also in set B, then it is in the intersection of sets A and\\nB. Similarly, we can define an intersection between fuzzy sets. Consider the\\nfuzzy sets whose membership functions are shown in Figure 18.2.\\nFigure 18.2 shows the membership functions for the fuzzy sets baby, child,\\nteenager, and adult. Note that there are intersections between baby and'), Document(metadata={}, page_content='18.4 Fuzzy Sets 507\\nchild, between child and teenager, and between teenager and adult. Note\\nthat at some age, let us say 12, a person might be defined as all of the fol-\\nlowing: a child, not a child, a teenager, and not a teenager. Our definitions\\nof the sets do not allow a person to be a child and an adult at the same time,\\nbut we could easily redefine the sets such that a person could be to some\\nextent a child and at the same time to some extent an adult.\\n18.4.1 Fuzzy Set Membership Functions'), Document(metadata={}, page_content='18.4.1 Fuzzy Set Membership Functions\\nA fuzzy set A is defined by its membership function, MA.\\nFor example, we might define the membership functions for the fuzzy sets\\nB and C (baby and child) as follows:\\nSimilarly, we could define membership functions for fuzzy setsT (teenager)\\nand A (adult). Note that there is nothing special about these functions—\\nthey have been chosen entirely arbitrarily and reflect a subjective view on'), Document(metadata={}, page_content='the part of the author. Different functions could very well be chosen for\\nM\\nB(x) and MC(x), which would equally reasonably define those sets.\\nT o represent a fuzzy set in a computer, we use a list of pairs, where each pair\\nrepresents a value and the fuzzy membership value for that value. Hence,\\nwe write the fuzzy set A as\\nA = {(x\\n1, MA(x1) ) ,...,( xn, MA(xn))}\\nFor example, we might define B, the fuzzy set of babies as follows:\\nB = {(0, 1), (2, 0)}'), Document(metadata={}, page_content='B = {(0, 1), (2, 0)}\\nThis can also be thought of as representing the x and y coordinates of two\\npoints on the line, which represents the set membership function, as shown in\\nFigure 18.2. Similarly, we could define the fuzzy set of children,C, as follows:\\nC = {(1, 0), (7, 1), (8, 1), (14, 0)}\\nMx\\nx for x\\nfor x\\nMx\\nx for x\\nfor x and x\\nx for x\\nB\\nC\\n( ) = −≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n( ) =\\n− ≤\\n>≤\\n− >\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n1 2 2\\n02\\n1\\n6 7\\n17 8\\n14\\n6 8'), Document(metadata={}, page_content='508 CHAPTER 18 Fuzzy Reasoning\\n18.4.2 Fuzzy Set Operators\\nTraditional set theory (developed by Georg Cantor in the 19th century)\\nuses a number of operators that can be applied to sets A and B:\\nNot A the complement of A, which contains the elements that are\\nnot contained in A\\nA ∩ B the intersection of A and B, which contains those elements\\nthat are contained in both A and B\\nA ∪ B the union of A and B, which contains all the elements of A\\nand all the elements of B'), Document(metadata={}, page_content='and all the elements of B\\nWe can think of these as being related to the logical operators, ¬, ∧, and ∨.\\nNaturally, the set “Not A” is the same as ¬A. The intersection of A and B is\\nthe same as the conjunction of A and B: A ∧ B. Similarly, the union of A\\nand B is the same as the disjunction of A and B: A ∨ B.\\nAs a result, the set operators are commutative, associative, and distributive,\\nas we would expect, and they obey DeMorgan’s laws:\\n¬(A ∪ B) = ¬A ∩ ¬B\\n¬(A ∩ B) = ¬A ∪ ¬B'), Document(metadata={}, page_content='¬(A ∪ B) = ¬A ∩ ¬B\\n¬(A ∩ B) = ¬A ∪ ¬B\\nWe can define similar operators for fuzzy sets. The complement of fuzzy set\\nA, whose membership function is MA is defined as\\nM¬A(x) = 1 /H11002MA(x)\\nThus, we could define the set of not-babies, ¬B, as follows:\\nM¬B(x) = 1 /H11002MB(x)\\nSo,\\n¬B = {(0, 0), (2, 1)\\nSimilarly, we can define ¬C:\\n¬C = {{(1, 1), (7, 0), (8, 0), (14, 1)}\\nFor each x, we have defined M¬C(x) as being 1 /H11002MC(x).\\nWe can now define fuzzy intersection of two sets as being the minimum of'), Document(metadata={}, page_content='the fuzzy membership functions for the sets. That is,\\nMA ∩ B (x) = MIN (MA (x), MB (x))\\nSo, for example, let us determine the intersection of B and C, babies and\\nchildren:'), Document(metadata={}, page_content='18.4 Fuzzy Sets 509\\nRecall that we define B and C as follows:\\nB = {(0, 1), (2, 0)}\\nC = {(1, 0), (7, 1), (8, 1), (14, 0)}\\nT o determine the intersection, we need to have the sets defined over the\\nsame values; hence, we augment set B:\\nB = {(0, 1), (1, 0.5), (2, 0), (7, 0), (8, 0), (14, 0)}\\nSimilarly, we augment C:\\nC = {(0, 0), (1, 0), (2, 0.166), (7, 1), (8, 1), (14, 0)}\\nNow we can find the intersection, by using\\nMB ∩ C (x) = MIN (MB (x), MC (x))'), Document(metadata={}, page_content='MB ∩ C (x) = MIN (MB (x), MC (x))\\n∴ B ∩ C = {(0, 0), (1, 0), (2, 0), (7, 0), (8, 0), (14, 0)}\\nBut this has not worked! Clearly we need to define the set using values that\\nwill correctly define the ranges. In other words, we can correctly define B ∩\\nC as follows:\\nB ∩ C = {(1, 0), (1.75, 0.125), (2, 0)}\\nwhere 1.75 was used as the value for x. This was determined by calculating\\nthe value of x for which MB(x) = MC(x).\\nLet us consider for a moment what the intersection of two fuzzy sets actually'), Document(metadata={}, page_content='means. As we said previously,B ∩ C can be thought of as being similar to B\\n∧ C. If a person is in the setB ∩ C, then she isboth a baby And a child. So the\\nintersection of two sets is the set of elements that belong to both those two\\nsets, or the elements that belong to the conjunction of the two sets.\\nSimilarly, we can define the union of two fuzzy sets A and B as follows:\\nM\\nA ∪ B (x) = MAX (MA (x), MB (x))\\nHence, the union of the fuzzy sets of babies and children is as follows:'), Document(metadata={}, page_content='B ∪ C = {(0, 1), (1.75, 0.25), (7, 1), (8, 1), (14, 0)}\\nAgain, recall that the union B ∪ C is similar to the disjunction B ∨ C.A  p e r -\\nson who belongs to the set B ∪ C is either a baby Or a child.\\nLet us consider one final fuzzy set operator—containment.\\nIn traditional set theory, if crisp set A contains crisp set B, then this means\\nthat all elements of set B are also elements of set A. In other words, the'), Document(metadata={}, page_content='510 CHAPTER 18 Fuzzy Reasoning\\nA\\nDegree of\\nmembership\\n1\\n0\\n0 100 Age\\nP\\nFigure 18.3\\nMembership functions for\\nthe fuzzy sets adults (A)\\nand pensioners (P)\\nunion A ∪ B = A and the intersection A ∩ B = B. In this case,B is said to be\\na subset of A, which is written A ⊂ B.\\nT o see how fuzzy subsets work, let us consider a new fuzzy set, P, which is\\nthe fuzzy set of pensioners. We will define this set by the following mem-\\nbership function:'), Document(metadata={}, page_content='bership function:\\nLet us suppose in this case that we are considering the universe of people to\\nrange over the ages between 0 and 100 (not to exclude people over the age\\nof 100, but simply to make the mathematics a little simpler).\\nIn Figure 18.3 we can see the membership functions for A and P.\\nThe intersection of A and P, A ∩ P, can be seen clearly from this diagram to\\nbe P.H e n c e ,P is a subset of A,o r  A ⊂ P.\\nThe definition of fuzzy containment is as follows:\\nB ⊂ A iff'), Document(metadata={}, page_content='B ⊂ A iff\\n∀x (MB (x) ≤ MA (x))\\nIn other words,B is a fuzzy subset ofA if B’s membership function is always\\nsmaller than (or equal to) the membership function for A.\\n18.4.3 Hedges\\nA hedge is a fuzzy set qualifier, such as “very,” “quite,” “extremely,” or\\n“somewhat. ” When one of these qualifiers is applied to a fuzzy set, such as\\n“tall people, ” we produce a new set. For example, by applying the “very”\\nMx\\nfor x\\nx for xp ( ) =\\n≤\\n− >\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n05 5\\n55\\n45 55'), Document(metadata={}, page_content='18.5 Fuzzy Logic 511\\nhedge to “tall people, ” we produce a subset of “tall people” called “very tall\\npeople. ” Similarly we can produce a new subset of “quite tall people” or\\n“somewhat tall people. ”\\nThe meanings of these hedges are fairly subjective, as are the meanings of\\nfuzzy sets themselves. However, it is usual to use a systematic mathematic\\ndefinition for the hedges so that they can be applied logically.\\nOften a hedge is applied by raising the set’s membership function to an'), Document(metadata={}, page_content='appropriate power. For example, it is common to consider the “very” hedge\\nto square the value of the membership function. For example, if M\\nA is the\\nmembership function for fuzzy set A of tall people, then the membership\\nfunction for VA, the fuzzy set of very tall people is\\nMVA (x) = (MA (x))2\\nSimilarly, we can define hedges such as “quite, ”“somewhat, ” and “extremely, ”\\nas raising the membership function to powers of 1.3, 0.5, and 4, respectively.'), Document(metadata={}, page_content='Hence, if Jane has a fuzzy membership value of the “tall people” set of 0.6,\\nthen she has a membership value of “very tall people” of 0.6 2 = 0.36; a\\nmembership value of “quite tall people” of 0.6 1.3 = 0.515; a membership\\nvalue of “somewhat tall people” of 0.60.5 = 0.775; and a membership value\\nof “extremely tall people” of 0.64 = 0.1296.\\nNote that while hedges such as “very, ”“extremely, ” and “quite” define a sub-\\nset of a fuzzy set, hedges such as “somewhat” or “more or less” expand the'), Document(metadata={}, page_content='set to which they are applied. A person who is not at all tall, for example,\\nmight be defined as being, to some extent, “somewhat tall. ”\\n18.5 Fuzzy Logic\\nFuzzy logic is a form of logic that applies to fuzzy variables. Fuzzy logic is non-\\nmonotonic, in the sense that if a new fuzzy fact is added to a database, this fact\\nmay contradict conclusions that were previously derived from the database.\\nWe have already seen that the functions MAX and MIN can be used with'), Document(metadata={}, page_content='fuzzy sets to calculate the intersection and union of two fuzzy sets. Simi-\\nlarly, the same functions can be used in fuzzy logic to calculate the disjunc-\\ntion or conjunction of two fuzzy variables.\\nEach fuzzy variable can take a value from 0 (not at all true) to 1 (entirely\\ntrue) but can also take on real values in between. Hence, 0.5 might indicate\\n“somewhat true, ” or “about as true as it is false. ”'), Document(metadata={}, page_content='512 CHAPTER 18 Fuzzy Reasoning\\nIf A and B are fuzzy logical values, then we can define the logical connec-\\ntives ∧ and ∨ as follows:\\nA ∨ B ≡ MAX (A, B)\\nA ∧ B ≡ MIN (A, B)\\nSimilarly, we can define negation as follows:\\n¬A ≡ 1 /H11002A\\nRecall from Chapter 7 that we can define any binary logical connective\\nusing just ¬ and ∧ . Hence, we can define any fuzzy logic connective using\\njust MIN and the function f (x) = 1 /H11002x.\\nClearly, we cannot write a complete truth table for a fuzzy logical connec-'), Document(metadata={}, page_content='tive because it would have an infinite number of entries. We can, however,\\nproduce a fuzzy truth table for a finite set of input values. For example, we\\ncould consider the set {0, 0.5, 1}, which would be used in a multivalent logic\\nthat had three logical values. Hence,\\nABA  ∨ B\\n000\\n0 0.5 0.5\\n011\\n0.5 0 0.5\\n0.5 0.5 0.5\\n0.5 1 1\\n101\\n1 0.5 1\\n111\\nWe could similarly draw up truth tables for \\n∧ and the other logical connec-\\ntives. Consider the following, which is the three-valued truth table for ¬:'), Document(metadata={}, page_content='A ¬A\\n01\\n0.5 0.5\\n10'), Document(metadata={}, page_content='18.5 Fuzzy Logic 513\\nNote in particular that ifA = 0.5, then A = ¬A. The extent to which A is true\\nis the same as the extent to which it is false. This is a fundamental aspect of\\nfuzzy logic and is a feature that would be entirely anathema to the thinking\\nof most classical logicians.\\nNow let us look at defining fuzzy logical implication, or →. Recall from\\nChapter 7 that in classical logic → is defined by the following:\\nA → B\\n≡¬ A ∨ B'), Document(metadata={}, page_content='A → B\\n≡¬ A ∨ B\\nHence, it would seem natural to define fuzzy implication as follows:\\nA → B ≡ MAX ((1 /H11002A), B)\\nLet us now examine the truth table for this function:\\nABA → B\\n001\\n0 0.5 1\\n011\\n0.5 0 0.5\\n0.5 0.5 0.5\\n0.5 1 1\\n100\\n1 0.5 0.5\\n111\\nIt is interesting to note that using this definition of implication, 0.5 → 0 =\\n0.5. This is somewhat counterintuitive because we would expect 0.5 → 0 =\\n0. Also, we have the counterintuitive statement that 0.5 → 0.5 = 0.5,'), Document(metadata={}, page_content='whereas we would expect 0.5 → 0.5 = 1.\\nAs a result of this, a number of alternative definitions for fuzzy implication\\nhave been proposed. One such definition is known as Gödel implication,\\nwhich is defined as follows:\\nA → B\\n≡ (A ≤ B) ∨ B\\nUsing this definition, we can draw up an alternative fuzzy truth table for →\\nover three logical values as follows:'), Document(metadata={}, page_content='514 CHAPTER 18 Fuzzy Reasoning\\nABA → B\\n001\\n0 0.5 1\\n011\\n0.5 0 0\\n0.5 0.5 1\\n0.5 1 1\\n100\\n1 0.5 0.5\\n111\\nThis table seems more intuitive.\\nNow let us consider modus ponens, the logical rule we saw in Section 7.11.4:\\nIn fuzzy logic this rule also holds. We will now examine, by drawing up a\\ntruth table, whether it also holds for three-valued fuzzy logic.\\nABA → B( A ∧ (A → B)) → B\\n001 1\\n0 0.5 1 1\\n011 1\\n0.5 0 0 1\\n0.5 0.5 1 0.5\\n0.5 1 1 1\\n100 1\\n1 0.5 0.5 1\\n111 1\\nAA B\\nB\\n→'), Document(metadata={}, page_content='18.6 Fuzzy Logic as Applied to Traditional Logical Paradoxes 515\\nWe have drawn up this truth table using our original, less satisfactory defi-\\nnition of →, and as a result, we have found that modus ponens does not\\nquite hold. If A = 0.5 and B = 0.5, then we have\\n(A ∧ (A → B)) → B = 0.5\\nAssuming we want modus ponens to hold, then this is not satisfactory\\nbecause we would want to obtain\\n(A ∧ (A → B)) → B = 1\\nIf we draw up the equivalent truth table but use Gödel implication, then we'), Document(metadata={}, page_content='find that each row in the truth table has a final value of 1, as we would\\nexpect, and thus modus ponens holds.\\n18.6 Fuzzy Logic as Applied to Traditional Logical Paradoxes\\nThere are a number of well-known paradoxes in classical logic: problems\\nthat cannot be solved using propositional logic because they lead to a con-\\nclusion that contradicts one or more of the premises. For example, Rus-\\nsell’s paradox can be stated as follows:\\nA barber, who himself has a beard, shaves all men who do not shave'), Document(metadata={}, page_content='themselves. He does not shave men who shave themselves.\\nWe now ask the following question: Who shaves the barber? If he shaves\\nhimself, then according to the second sentence in the statement above, he\\ncannot shave himself. But if he does not shave himself, then the first sen-\\ntence above tells us that he does shave himself.\\nThis paradox exemplifies the law of the excluded middle—the problem\\narises due to the fact that we cannot have A\\n∧¬ A. In fuzzy logic, this prob-'), Document(metadata={}, page_content='∧¬ A. In fuzzy logic, this prob-\\nlem does not exist, and Russell’s paradox is not a paradox: the barber both\\nshaves himself and does not shave himself.\\nSimilarly, consider another commonly discussed paradox:\\n“All Cretans are liars, ” said the Cretan.\\nIf the Cretan is a liar, as his claim would suggest, then his claim cannot be\\nbelieved, and so he is not a liar. But if he is not a liar, then he is telling the\\ntruth, and all Cretans are liars. But because he is a Cretan, he must therefore'), Document(metadata={}, page_content='be a liar. Again, this is a paradox that can be resolved by using fuzzy logical'), Document(metadata={}, page_content='516 CHAPTER 18 Fuzzy Reasoning\\nvalues, instead of the two logical values “true” and “false. ” The Cretan’s\\nstatement is true and false, to some extent, at the same time.\\nThis makes perfect sense: when the Cretan says that all Cretans are liars, it\\nis unlikely that he is really speaking of every single Cretan. It is also unlikely\\nthat he really means that every Cretan lies every time he opens his mouth.\\nHence, his statement has a fuzzy truth value somewhere below 1, but some-\\nwhere above 0.'), Document(metadata={}, page_content='where above 0.\\n18.7 Fuzzy Rules\\nWe will now consider fuzzy rules, which are the fuzzy equivalent of the\\nrules we used in Chapter 9, when we considered expert systems.\\nThe rules we saw in Chapter 9 had the following form:\\nIF A THEN B\\nA fuzzy rule has the form\\nIF A = x then B = y\\nIn fact, to be more precise, a fuzzy rule can take the following form:\\nIF A op x then B = y\\nWhere op is some mathematical operator (such as =, >, or <)\\nHence, we might have fuzzy rules such as the following:'), Document(metadata={}, page_content='IF temperature > 50 then fan speed = fast\\nIF height = tall then trouser length = long\\nIF study time = short then grades = poor\\nBy using fuzzy inference, which is explained in the next section, an expert\\nsystem can be built based around fuzzy rules such as these.\\n18.8 Fuzzy Inference\\nAn alternative to Gödel implication called Mamdani implication (or Mam-\\ndani inference) is often used in fuzzy systems. Mamdani inference allows a'), Document(metadata={}, page_content='system to take in a set of crisp input values (from a set of sensors or inputs\\nfrom a human operator, for example) and apply a set of fuzzy rules to those\\nvalues, in order to derive a single, crisp, output value or action recommen-\\ndation. Mamdani inference was invented by Professor Ebrahim Mamdani\\nin the 1970s and was used by him to control a steam engine and boiler.'), Document(metadata={}, page_content='18.8 Fuzzy Inference 517\\nWe will now examine a simple example to see how this form of reason-\\ning works.\\nLet us suppose that we are designing a simple braking system for a car,\\nwhich is designed to cope when the roads are icy and the wheels lock.\\nThe rules for our system might be as follows:\\nRule 1 IF pressure on brake pedal is medium\\nTHEN apply the brake\\nRule 2 IF pressure on brake pedal is high\\nAND car speed is fast\\nAND wheel speed is fast\\nTHEN apply the brake'), Document(metadata={}, page_content='AND wheel speed is fast\\nTHEN apply the brake\\nRule 3 IF pressure on brake pedal is high\\nAND car speed is fast\\nAND wheel speed is slow\\nTHEN release the brake\\nRule 4 IF pressure on brake pedal is low\\nTHEN release the brake\\nT o apply these rules, using Mamdani inference, the first step is to fuzzify\\nthe crisp input values.\\nT o do this, we need first to define the fuzzy sets for the various linguistic\\nvariables we are using.\\nFor this simple example, we will assume that brake pressure is measured'), Document(metadata={}, page_content='from 0 (no pressure) to 100 (brake fully applied). We will define brake pres-\\nsure as having three linguistic values: high ( H), medium (M), and low (L),\\nwhich we will define as follows:\\nH = {(50, 0), (100, 1)}\\nM = {(30, 0), (50, 1), (70, 0)}\\nL = {(0, 1), (50, 0)}\\nFigure 18.4 shows the membership functions for these three fuzzy sets.\\nLet us suppose that the pressure value in a given situation is in fact 60. This\\ncorresponds to fuzzy membership values for the three sets of'), Document(metadata={}, page_content='518 CHAPTER 18 Fuzzy Reasoning\\n1\\n0 100\\nPressure\\nM\\nHL\\nFigure 18.4\\nGraph showing member-\\nship functions for fuzzy\\nvariable pressure\\nML(60) = 0\\nMM(60) = 0.5\\nMH(60) = 0.2\\nSimilarly, we must consider the wheel speed. We will define the wheel speed\\nas also having three linguistic values: slow, medium, and fast. We will define\\nthe membership functions for these values for a universe of discourse of\\nvalues from 0 to 100:\\nS = {(0, 1), (60, 0)}\\nM = {(20, 0), (50, 1), (80, 0)}\\nF = {(40, 0), (100, 1)}'), Document(metadata={}, page_content='F = {(40, 0), (100, 1)}\\nIf the wheel speed is in fact 55, then this gives us membership values as follows:\\nM\\nS(55) = 0.083\\nMM(55) = 0.833\\nMF(55) = 0.25\\nFor the sake of simplicity, we will define the linguistic variable car speed\\nusing the same linguistic values ( S, M, and F for slow, medium, and fast),\\nusing the same membership functions. Clearly, in a real system, the two\\nwould be entirely independent of each other.\\nLet us suppose now that the car speed is 80, which gives us the following'), Document(metadata={}, page_content='membership values:\\nM\\nS(80) = 0'), Document(metadata={}, page_content='18.8 Fuzzy Inference 519\\nMM(80) = 0\\nMF(80) = 0.667\\nWe now need to apply these fuzzy values to the antecedents of the sys-\\ntem’s rules.\\nRule 1, taken on its own, tells us that the degree to which we should apply\\nthe brake is the same as the degree to which the pressure on the brake pedal\\ncan be described as “medium. ”\\nWe saw above that the pressure is 60 and that M\\nM(60) = 0.5. Hence, Rule 1\\ngives us a value of 0.5 for the instruction “Apply the brake. ”\\nRule 2 uses an AND:'), Document(metadata={}, page_content='Rule 2 uses an AND:\\nIF pressure on brake pedal is high\\nAND car speed is fast\\nAND wheel speed is fast\\nTHEN apply the brake\\nThe membership functions for the three parts of the antecedent are\\nM\\nH(60) = 0.2\\nMF(80) = 0.667\\nMF(55) = 0.25\\nUsually, the conjunction of two or more fuzzy variables is taken to be the\\nminimum of the various membership values. Hence, the antecedent for\\nRule 2 in this case has the value 0.2. Thus, Rule 2 is giving us a fuzzy value\\nof 0.2 for “Apply the brake.”'), Document(metadata={}, page_content='of 0.2 for “Apply the brake.”\\nSimilarly, we evaluate Rules 3 and 4:\\nRule 3 M\\nH(60) = 0.2\\nMF(80) = 0.667\\nMS(55) = 0.083\\nHence, Rule 3 gives a value of 0.083 for “Release the brake. ”\\nRule 4 ML(60) = 0\\nHence, Rule 4 gives us a fuzzy value of 0 for “Release the brake. ”\\nNow we have four fuzzy values: 0.5 and 0.2 for “Apply the brake” and 0.083\\nand 0 for “Release the brake. ”'), Document(metadata={}, page_content='520 CHAPTER 18 Fuzzy Reasoning\\n1\\n0 100\\nPressure\\nAR\\nFigure 18.5\\nMembership functions for\\n“Apply the brake” (A) and\\n“Release the brake” (R)\\nWe now need to combine these values together.\\nFirst, let us see what we mean by “Apply the brake” and “Release the brake. ”\\nFigure 18.5 shows fuzzy membership functions for “Apply the brake” ( A)\\nand “Release the brake” (R), which show the degree of pressure the brake\\nshould apply to the wheel for each value of these variables.'), Document(metadata={}, page_content='T o put that another way, the x-axis of the graph in Figure 18.5 shows the\\npressure applied by the brake to the wheel, and they-axis shows the degree to\\nwhich “Apply the brake” and “Release the brake” are true (M[A] and M[R]).\\nT o apply the rules, we first need to decide how to combine the differing val-\\nues for each of the two fuzzy variables. We have 0.2 and 0.5 for “Apply the\\nbrake” and 0.083 and 0 for “Release the brake. ” We could sum the values or'), Document(metadata={}, page_content='take the minimum or take the maximum. The appropriate combination\\nwill depend on the nature of the problem being solved. In this case it makes\\nsense to sum the values because the separate rules are giving different rea-\\nsons for applying or releasing the brakes, and those reasons should com-\\nbine together cumulatively.\\nHence, we end up with a value of 0.7 for “Apply the brake” and 0.083 for\\n“Release the brake. ”\\nThe next step is to clip the membership functions of the two variables to'), Document(metadata={}, page_content='these values, as is shown in Figure 18.6.'), Document(metadata={}, page_content='18.8 Fuzzy Inference 521\\n1\\n0.7\\n0.083\\n0 100\\nPressure\\nAR\\nFigure 18.6\\nShowing how the fuzzy\\nvalues for the antecedents\\nof the rules are applied to\\nthe consequents\\nIn Figure 18.6, the membership function for A has been clipped at 0.7, and\\nthe membership function for R has been clipped at 0.083. The resulting\\nshape is the shaded area under the two clipped lines and shows the com-\\nbined fuzzy output of the four rules.\\nT o use this fuzzy output, a crisp output value must now be determined'), Document(metadata={}, page_content='from the fuzzy values. This process of obtaining a crisp value from a set of\\nfuzzy variables is known as defuzzification. This can be done by obtaining\\nthe center of gravity (or centroid) of the shaded shape shown in Figure\\n18.6.\\nThe formula for the center of gravity, C, is as follows:\\nwhere M\\nA(x) is the membership function illustrated by the shaded area in\\nFigure 18.6.\\nIn fact, the center of gravity should really be calculated as a continuous'), Document(metadata={}, page_content='integral, but if we use a discrete sum over a reasonable selection of values,\\nwe can obtain an answer that is close enough. In fuzzy systems it is not usu-\\nally necessary to be accurate to several decimal places, but rather to obtain\\na value in the right range.\\nC Mx x\\nMx\\nA\\nA\\n= ( )\\n( )\\n∑\\n∑'), Document(metadata={}, page_content='522 CHAPTER 18 Fuzzy Reasoning\\nHence, we can calculate the center of gravity of the shaded shape in Figure\\n18.6 as follows:\\nHence, the crisp output value for this system is 68.13, which can be trans-\\nlated into the pressure applied by the brake to the wheel in the car.\\n18.9 Fuzzy Expert Systems\\nExpert systems, or production systems, are described in more detail in\\nChapter 9. An expert system consists of a set of rules that are developed in'), Document(metadata={}, page_content='collaboration with an expert. Expert systems are used, for example, for\\nmedical diagnosis. Traditional expert systems use crisp logical values to\\ndetermine a diagnosis or a recommendation based on a set of evidence. In\\nmany ways, this is a fine way to apply the expert’s knowledge. On the other\\nhand, most expert decisions are not black and white. An expert who is pre-\\nsented with a patient with one set of symptoms will not usually be able to'), Document(metadata={}, page_content='provide a diagnosis with absolute certainty but will have a strong feeling\\nabout a diagnosis based on the weight of evidence.\\nHence, applying fuzzy logic to these rules seems like a natural way to progress.\\nThe fuzzy expert system can be built by choosing a set of linguistic vari-\\nables appropriate to the problem and defining membership functions for\\nthose variables. Rules are then generated based on the expert’s knowledge'), Document(metadata={}, page_content='and using the linguistic variables. The fuzzy rules can then be applied as\\ndescribed above using Mamdani inference.\\nLet us now look at a simple example of how a fuzzy expert system can be\\nbuilt from an expert’s knowledge.\\nWe will consider an imaginary medical system designed to recommend a\\ndose of quinine to a patient or doctor based on the likelihood that that\\npatient might catch malaria while on vacation.\\nCreating the fuzzy expert system will involve the following steps:'), Document(metadata={}, page_content='C = ×( ) +×( ) +×( ) +×( ) ++ × ( )\\n++ ++ +\\n==\\n5 0 83 10 0 1 15 0 15 20 0 2 100 1\\n00 8 3 01 01 5 02 1\\n717 666\\n10 533 68 13\\n.. ..\\n.. . .\\n.\\n. .\\nK\\nK'), Document(metadata={}, page_content='18.9 Fuzzy Expert Systems 523\\n1. Obtain information from one or more experts.\\n2. Define the fuzzy sets.\\n3. Define the fuzzy rules.\\nT o use the fuzzy expert system, we will use the following steps:\\n1. Relate observations to the fuzzy sets.\\n2. Evaluate each case for all fuzzy rules.\\n3. Combine information from the rules.\\n4. Defuzzify the results.\\n18.9.1 Defining the Fuzzy Sets\\nHaving obtained suitable information from our experts, we must start by\\ndefining the fuzzy sets.'), Document(metadata={}, page_content='defining the fuzzy sets.\\nIn this case, we will use the following fuzzy sets, or linguistic variables:\\n■ average temperature of destination (T)\\n■ average humidity of destination (H)\\n■ proximity to large bodies of water (P)\\n■ industrialization of destination (I)\\nNote that this is a purely imaginary example for the purposes of illustration\\nand explanation and has no real bearing on the way that malaria is pre-\\nvented or treated!'), Document(metadata={}, page_content='vented or treated!\\nAs well as defining the linguistic variables, we need to give each one a range\\nof possible values. For this example, we will assume that each has just two\\nvalues: T emperature, humidity, and industrialization can be high ( H) or\\nlow (L), and proximity to water can be near (N) or far (F).\\nT o represent the fuzzy membership functions, we will use the notation M\\nAB\\n(x), where A is the variable (T, H, P,o r  I) and B is the value (H, L, N,o r  F).'), Document(metadata={}, page_content='For example, MHL is the membership function for the fuzzy subset\\ndescribed as “humidity low. ”\\nThe crisp values that we will allow as inputs will range from 0 to 100 for\\ntemperature, humidity, and industrialization, and from 0 to 50 for proxim-\\nity to water.'), Document(metadata={}, page_content='524 CHAPTER 18 Fuzzy Reasoning\\nWe will define the membership functions for each of these fuzzy subsets\\nusing the following equations:\\nMx\\nfor x\\nx for x\\nfor x\\nIL ( ) =\\n<\\n− ≤<\\n≥\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n11 0\\n20\\n10 20\\n02 0\\n 10\\nMx\\nfor x\\nx for x\\nfor x\\nIH ( ) =\\n<\\n− ≤<\\n≥\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n01 0\\n10\\n10 20\\n12 0\\n 10\\nMx\\nfor x\\nx for x\\nfor x\\nPF ( ) =\\n<\\n− ≤<\\n≥\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n01 0\\n10\\n30 40\\n14 0\\n 10\\nMx\\nfor x\\nx for x\\nfor x\\nPN ( ) =\\n<\\n− ≤<\\n≥\\n\\uf8f1\\n\\uf8f2\\n\\uf8f4\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f4\\n11 0\\n40\\n30 40\\n04 0\\n 10\\nMx x\\nMx x\\nHH\\nHL\\n( ) =\\n( ) =−\\n100\\n1 100\\nMx\\nx for x\\nfor x\\nMx'), Document(metadata={}, page_content='HH\\nHL\\n( ) =\\n( ) =−\\n100\\n1 100\\nMx\\nx for x\\nfor x\\nMx\\nx for x\\nfor x\\nTH\\nTL\\n( ) =\\n− ≥\\n<\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n( ) = −≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n25\\n75 25\\n02 5\\n1 75 75\\n07 5'), Document(metadata={}, page_content='18.9 Fuzzy Expert Systems 525\\n1\\n0 1007525\\nTemperature\\nHL\\nFigure 18.7\\nGraph showing member-\\nship function for tempera-\\nture (high and low)\\n1\\n0 10050\\nHumidity\\nHL\\nFigure 18.8\\nGraph showing member-\\nship function for humidity\\n(high and low)\\nFigures 18.7 to 18.10 show graphs for all of these fuzzy membership functions.\\nWe need to define one more fuzzy set, which is the set used to describe the\\noutput of the system. In this case, our system will prescribe a dose of qui-'), Document(metadata={}, page_content='nine which can take on one of three values:\\nvery low dose (V)\\nlow dose (L)\\nhigh dose (H)'), Document(metadata={}, page_content='526 CHAPTER 18 Fuzzy Reasoning\\n1\\n0 504010\\nProximity to Water\\nNF\\nFigure 18.9\\nGraph showing member-\\nship function for proximity\\nto water (near and far)\\n1\\n0 10010 20\\nIndustrialization\\nLH\\nFigure 18.10\\nGraph showing member-\\nship function for industri-\\nalization (high and low)\\nWe will define three membership functions for these three fuzzy sets as follows:\\nMx\\nx for x\\nfor x\\nQL ( ) =\\n− ≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n50\\n50 50\\n05 0\\nMx\\nx for x\\nfor x\\nQV ( ) =\\n− ≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n10\\n10 10\\n01 0'), Document(metadata={}, page_content='18.9 Fuzzy Expert Systems 527\\n1\\n0 10010 40 50\\nQuinine dose\\nL\\nV\\nH\\nFigure 18.11\\nMembership functions for\\nquinine dose values; V\\n(very low), L (low), and H\\n(high).\\nGraphs for these three membership functions are shown in Figure 18.11.\\n18.9.2 Defining Fuzzy Rules\\nThe second step in creating our fuzzy expert system is to define a set of\\nfuzzy rules.\\nThese rules, unlike those used by traditional expert systems, are expressed in'), Document(metadata={}, page_content='vague English terms and do not define cut-off points or thresholds, but rather\\nuse subjective terms such as “high” and “low. ” This maps more naturally to the\\nway an expert would express his or her knowledge and makes the process of\\nconverting that knowledge into rules far simpler and less prone to error.\\nOur rules are defined as follows:\\nRule 1 IF temperature is high\\nAND humidity is high\\nAND proximity to water is near\\nAND industrialization is low\\nTHEN quinine dose is high\\nMx\\nfor x'), Document(metadata={}, page_content='THEN quinine dose is high\\nMx\\nfor x\\nx for xQH ( ) =\\n≤\\n− >\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n04 0\\n40\\n60 40'), Document(metadata={}, page_content='528 CHAPTER 18 Fuzzy Reasoning\\nRule 2: IF industrialization is high\\nTHEN quinine dose is low\\nRule 3: IF humidity is high\\nAND temperature is high\\nAND industrialization is low\\nOR proximity to water is near\\nTHEN quinine dose is high\\nRule 4: IF temperature is low\\nAND humidity is low\\nTHEN quinine dose is very low\\nThese rules may not be the best way to express this information, but they\\nwill suffice for this example.\\n18.9.3 Relating Observations to Fuzzy Sets'), Document(metadata={}, page_content='18.9.3 Relating Observations to Fuzzy Sets\\nWe are now ready to make use of our fuzzy expert system.\\nWe will examine five sets of data, for five individuals, each of whom is trav-\\neling to a country that is at risk from malaria.\\nThe crisp data are as follows:\\ntemperature = {80, 40, 30, 90, 85}\\nhumidity = {10, 90, 40, 80, 75}\\nproximity to water = {15, 45, 20, 5, 45}\\nindustrialization = {90, 10, 15, 20, 10}\\nHence, for example, person three is traveling to an area where the average'), Document(metadata={}, page_content='temperature is 30, the humidity is 40, the distance to water is 20, and the\\nlevel of industrialization is 25.\\nWe must now convert these crisp values into fuzzy membership values.\\nThis can be done by simply applying the relevant fuzzy membership func-\\ntions to each of the values. For example, let us look at some of the calcula-\\ntions for the first person:\\nT emperature = 80.\\nThese membership functions were defined as:'), Document(metadata={}, page_content='18.9 Fuzzy Expert Systems 529\\nSo,\\nMTH(80) = (80 – 25) / 75 = 0.733\\nMTL(80) = 0\\nSimilarly, we obtain the following membership function values:\\nMHH(10) = 10 / 100 = 0.1\\nMHL(10) = 1 /H11002(10 / 100) = 0.9\\nMPN(15) = (40 /H1100215) / 30 = 0.833\\nMPF(15) = (15 /H1100210) / 30 = 0.167\\nMIH(90) = 1\\nMIL(90) = 0\\nIn a similar fashion, we can obtain membership values for the other four\\ntravelers, which results in the following:\\nMTH = {0.733, 0.2, 0.067, 0.867, 0.8}\\nMTL = {0, 0.467, 0.6, 0, 0}'), Document(metadata={}, page_content='MTL = {0, 0.467, 0.6, 0, 0}\\nMHH = {0.1, 0.9, 0.4, 0.8, 0.75}\\nMHL = {0.9, 0.1, 0.6, 0.2, 0.25}\\nMPN = {0.833, 0, 0.667, 1, 0}\\nMPF = {0.167, 1, 0.333, 0, 1}\\nMIH = {1, 0, 0.5, 1, 0}\\nMIL = {0, 1, 0.5, 0, 1}\\nNote that for all of the fuzzy sets apart from temperature, the two possible\\nvalues sum to 1 in every case. For example, for the third person, member-\\nship of “high humidity” is 0.4, and membership of “low humidity” is 0.6.'), Document(metadata={}, page_content='This relationship does not always have to hold for fuzzy sets, and in this\\ncase it does not hold for temperature.\\nMx\\nx for x\\nfor x\\nTL ( ) = −≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n1 75 75\\n07 5\\nMx\\nx for x\\nfor x\\nTH ( ) =\\n− ≥\\n<\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n25\\n75 25\\n02 5'), Document(metadata={}, page_content='530 CHAPTER 18 Fuzzy Reasoning\\n18.9.4 Evaluating Each Case for the Fuzzy Rules\\nWe now have a set of fuzzy inputs that can be applied to the antecedents of\\nthe rules.\\nFor example, let us examine traveler number 1. The values are as follows:\\nMTH = 0.733\\nMTL = 0\\nMHH = 0.1\\nMHL = 0.9\\nMPN = 0.833\\nMPF = 0.167\\nMIH = 1\\nMIL = 0\\nRule 1, written with the appropriate fuzzy membership values for person 1,\\nis as follows:\\nIF temperature is high (0.733)\\nAND humidity is high (0.1)'), Document(metadata={}, page_content='AND humidity is high (0.1)\\nAND proximity to water is near (0.833)\\nAND industrialization is low (0)\\nTHEN quinine dose is high (0)\\nRecall that to apply the fuzzy AND operator, we take the minimum value of\\nthe antecedents. In this case, therefore, the rule fires with value 0, which\\nmeans it does not fire at all.\\nWe will now apply the remaining rules in the same way:\\nRule 2 IF industrialization is high (1)\\nTHEN quinine dose is low (1)\\nIn this case, the rule fires with fuzzy strength of 1.'), Document(metadata={}, page_content='Rule 3 IF humidity is high (0.1)\\nAND temperature is high (0.733)\\nAND industrialization is low (0)\\nOR proximity to water is near (0.833)\\nTHEN quinine dose is high (0.1)'), Document(metadata={}, page_content='18.9 Fuzzy Expert Systems 531\\nNote that in this case, the rule has an OR clause. This is calculated by taking\\nthe maximum value of its arguments, which in this case is 0.833. Hence, the\\noverall result of the rule is the minimum of 0.1, 0.733, and 0.833, which is 0.1.\\nRule 4 IF temperature is low (0)\\nAND humidity is low (0.9)\\nTHEN quinine dose is very low (0)\\nWe can use this method for all of the five sets of input data and obtain\\nresults as follows:\\nRule 1 (high dose): {0, 0, 0.067, 0, 0}'), Document(metadata={}, page_content='Rule 1 (high dose): {0, 0, 0.067, 0, 0}\\nRule 2 (low dose): {1, 0, 0.5, 1, 0}\\nRule 3 (high dose): {0.1, 0.2, 0.067, 0.8, 0.75}\\nRule 4 (very low dose): {0, 0.1, 0.6, 0, 0}\\nIn this case, to combine Rules 1 and 3, which each give values for the “high\\ndose” fuzzy set, we will take the maximum value, thus obtaining the follow-\\ning values for “high dose” from these two rules:\\nhigh dose: {0.1, 0.2, 0.067, 0.8, 0.75}\\n18.9.5 Defuzzification'), Document(metadata={}, page_content='18.9.5 Defuzzification\\nWe now need to defuzzify the outputs to obtain a crisp dosage recommen-\\ndation for each traveler.\\nLet us examine this process for traveler 1:\\nTraveler 1 obtained the following three fuzzy outputs:\\nvery low dose (V): 0\\nlow dose (L): 1\\nhigh dose (H): 0.1\\nT o defuzzify this output, we use the clipping operation described in Section\\n18.8. This clipping is shown graphically in Figure 18.12.\\nIn this case, we clip the V set to value 0, which means it is effectively not'), Document(metadata={}, page_content='used at all. We clip the L set to value 1, which means it is not clipped, and\\nwe clip the H set to value 0.1.The shaded area in Figure 18.12 is the com-\\nbined result of the three fuzzy sets, and obtaining the centroid of this\\nshaded shape will give us the crisp output value, which is the recommenda-\\ntion for dosage for this traveler.'), Document(metadata={}, page_content='532 CHAPTER 18 Fuzzy Reasoning\\n1\\n0 10010 40 50\\nQuinine dose\\nL\\nV\\nH\\nFigure 18.12\\nClipped fuzzy set for the\\noutput for traveler 1\\nC = (0.9 /H110035) + (0.8 /H1100310) + (0.7 /H1100315) + (0.6 /H1100320) + (0.5 /H1100325) + (0.4 /H1100330) + \\n(0.3 /H1100335) + (0.2 /H1100340) + (0.1 /H1100345) + (0.1 /H1100350) + (0.1 /H1100355) + (0.1 /H1100360) + \\n(0.1 /H1100365) + (0.1 /H1100370) + (0.1 /H1100375) + (0.1 /H1100380) + (0.1 /H1100385) + (0.1 /H1100390) + \\n(0.1 /H1100395) + (0.1 /H11003100)'), Document(metadata={}, page_content='(0.1 /H1100395) + (0.1 /H11003100)\\n0.9 + 0.8 + 0.7 + 0.6 + 0.5 + 0.4 + 0.3 + 0.2 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 \\n+ 0.1 + 0.1 + 0.1 + 0.1 + 0.1\\n= 165 / 5.6\\n= 29.46\\nRecall that we define the center of gravity as follows:\\nWe will sum over values of x, which increase in increments of 5. A more\\naccurate result could be obtained using smaller increments, but for this\\nexample, increments of 5 will give sufficient accuracy.\\nHence,\\nC Mx x\\nMx\\nA\\nA\\n= ( )\\n( )\\n∑\\n∑'), Document(metadata={}, page_content='Hence,\\nC Mx x\\nMx\\nA\\nA\\n= ( )\\n( )\\n∑\\n∑\\nThus, the recommended dose for traveler 1 is 29.46.\\nWe will now defuzzify the results for traveler 3.'), Document(metadata={}, page_content='18.9 Fuzzy Expert Systems 533\\n1\\n0.6\\n0.5\\n0.067\\n0 10010 40 50\\nQuinine dose\\nL\\nV\\nH\\nFigure 18.13\\nClipped fuzzy set for the\\noutput for traveler 3\\nC = (0.6 /H110035) + (0.5 /H1100310) + (0.5 /H1100315) + (0.5 /H1100320) + (0.5 /H1100325) + (0.4 /H1100330) + \\n(0.3 /H1100335) + (0.2 /H1100340) + (0.1 /H1100345) + (0.067 /H1100350) + (0.067 /H1100355) + (0.067 /H1100360) +\\n(0.067 /H1100365) + (0.067 /H1100370) + (0.067 /H1100375) + (0.067 /H1100380) + (0.067 /H1100385) +'), Document(metadata={}, page_content='(0.067 /H1100390) + (0.067 /H1100395) + (0.067 /H11003100)\\n0.6 + 0.5 + 0.5 + 0.5 + 0.5 + 0.4 + 0.3 + 0.2 + 0.1 + 0.067 + 0.067 + 0.067 + 0.067 + 0.067 + \\n0.067 + 0.067 + 0.067 + 0.067 + 0.067 + 0.067\\n= 128 / 4.3\\n= 29.58\\nTraveler 3 had the following results:\\nvery low dose (V): 0.6\\nlow dose (L): 0.5\\nhigh dose (H): 0.067\\nFigure 18.13 shows the result of using these values to clip the three fuzzy\\nsets, H, L, and V.\\nThe centroid of the shaded area shown in Figure 18.13 is calculated as fol-\\nlows:'), Document(metadata={}, page_content='lows:\\nUsing this same method, dosages can also be calculated for the other four\\ntravelers. This is left as an exercise for the reader.'), Document(metadata={}, page_content='534 CHAPTER 18 Fuzzy Reasoning\\n18.10 Fuzzy Systems That Learn\\nThe fuzzy systems we have seen so far are static: once the fuzzy sets and\\nrules are set up, they do not change. As new inputs are presented to them,\\nthey do not learn from those inputs. This makes sense because the rules\\nthat we have given to the systems are designed by experts and, so, should\\nnot need to change. On the other hand, we have already said that the rules'), Document(metadata={}, page_content='from the experts are subjective and vague. When one expert says that a dial\\nshould be set to “high, ” another expert might say it should be set to “very\\nhigh, ” but mean the same crisp setting.\\nFuzzy systems are designed to be able to cope with this kind of vagueness\\nand inaccuracy, and tend to produce good results regardless. However, in\\nsome situations it makes more sense to allow the fuzzy system to adapt. In\\nChapter 11, we saw how neural networks use a system based on the neural'), Document(metadata={}, page_content='structures in human brains to learn how to deal with new problems. These\\nsystems are able to adapt—to learn how to deal with situations that they\\nhave not previously encountered and, in extreme cases, are able to learn to\\nsurvive when the environment in which they operate changes.\\nWe will now look at how fuzzy logic can be used in combination with neu-\\nral networks to produce fuzzy systems that are able to adapt and learn.\\n18.10.1 Neuro-fuzzy Systems'), Document(metadata={}, page_content='18.10.1 Neuro-fuzzy Systems\\nA neuro-fuzzy system is a neural network that learns to classify data using\\nfuzzy rules and fuzzy classifications (fuzzy sets). A neuro-fuzzy system has\\nadvantages over fuzzy systems and traditional neural networks: A tradi-\\ntional neural network is often described as being like a “black box, ” in the\\nsense that once it is trained, it is very hard to see why it gives a particular\\nresponse to a set of inputs. This can be a disadvantage when neural net-'), Document(metadata={}, page_content='works are used in mission-critical tasks where it is important to know why\\na component fails.\\nFuzzy systems and neuro-fuzzy systems do not have this disadvantage.\\nOnce a fuzzy system has been set up, it is very easy to see which rules fired\\nand, thus, why it gave a particular answer to a set of inputs. Similarly, it is\\npossible with a neuro-fuzzy system to see which rules have been developed\\nby the system, and these rules can be examined by experts to ensure that'), Document(metadata={}, page_content='they correctly address the problem.'), Document(metadata={}, page_content='18.10 Fuzzy Systems That Learn 535\\nT\\nH\\nQ\\nLayer 1 Layer 2 Layer 3 Layer 4 Layer 5\\nFigure 18.14\\nTypical layout of a five-\\nlayer neuro-fuzzy network\\nTypically, a fuzzy neural network is a five-layer feed-forward network. The\\nfive layers are as follows:\\n1 input layer—receives crisp inputs\\n2 fuzzy input membership functions\\n3 fuzzy rules\\n4 fuzzy output membership functions\\n5 output layer—outputs crisp values\\nFigure 18.14 shows the typical layout of such a network.'), Document(metadata={}, page_content='The network in Figure 18.14 has two crisp inputs, T and H, and produces\\none crisp output, Q. It has five layers, whose functions are as follows:\\nThe first layer, the input layer, simply passes its crisp input values to the\\nnext layer in the network.\\nThe second layer contains information about the various fuzzy sets that are\\nbeing used to map the crisp inputs. In other words, it fuzzifies the inputs in\\nthe same way that we fuzzified our inputs in the examples in Section 18.9.3.'), Document(metadata={}, page_content='Typically, the neurons used in this second layer have triangular activation\\nfunctions, which represent the triangular membership functions of the\\nfuzzy sets, although any functions can be used.\\nThe third layer represents the fuzzy rules of the system. Each neuron in this\\nlayer represents a single fuzzy rule.'), Document(metadata={}, page_content='536 CHAPTER 18 Fuzzy Reasoning\\nTypically, the system would be set up with initial fuzzy rules built in, and the\\nnetwork would develop suitable weightings to give the best possible responses.\\nIn some cases, it is possible to start the system with no built-in rules, in which\\ncase the system learns its own rules and develops weights for them.\\nThe fourth layer in the network contains the neurons that represent the\\nmembership functions of the various possible outputs of the fuzzy rules (in'), Document(metadata={}, page_content='this case there are three possible outputs, and so three neurons in the\\nfourth layer).\\nThe fifth and final layer is the layer that combines and defuzzifies the various\\no u t p u t st op r o d u c eo n es i n g l ec r i s po u t p u tf o rt h en e t w o r ki nr e s p o n s et oa\\ngiven set of inputs. In this case, the network has just one final output, but it is\\npossible to have a fuzzy neural network that produces a number of outputs.'), Document(metadata={}, page_content='The connections between the layers have weights associated with them, and\\nusing the methods such as back-propagation, which are described in Chap-\\nter 11, the system is able to learn.\\nLet us now examine in detail the behavior of each of the levels of neurons\\nin the network to see how the entire network behaves.\\nWe will use the network shown in Figure 18.14 to learn a simple version of\\nthe fuzzy rules used in Section 18.9.2 for prescribing quinine. We will use'), Document(metadata={}, page_content='just two input variables: temperature (T) and humidity (H).\\n18.10.2 Layer 1: The Input Layer\\nThis layer simply passes the input values it receives (T and H in our exam-\\nple) to each of the neurons in the second layer. In fact, in our example this\\nlayer is set up so that input T is passed to the top two neurons of Layer 2,\\nand input H is passed to the bottom two neurons. This is because in this\\nexample each of the inputs has two possible values, which have different\\nmembership functions.'), Document(metadata={}, page_content='membership functions.\\n18.10.3 Layer 2: The Fuzzification Layer\\nThe neurons in Layer 2 represent the fuzzy membership functions for the\\ntwo inputs to the system. The top two neurons in this layer represent the\\nmembership functions for “high temperature” and “low temperature, ”\\nwhereas the bottom two represent “high humidity” and “low humidity. ” If\\nthe membership functions for “high humidity” and “high temperature”\\nwere the same, then these could be combined into one neuron.'), Document(metadata={}, page_content='18.10 Fuzzy Systems That Learn 537\\nEach neuron in this layer has an activation function (defined in Chapter\\n11) that is identical to the membership function it is representing. In this\\ncase, the activation functions will be the membership functions shown in\\nFigures 18.7 and 18.8.\\nHence, the output of each neuron (or the extent to which it fires) in this layer\\nis determined by applying the appropriate membership function to its inputs.\\n18.10.4 Layer 3: The Fuzzy Rule Layer'), Document(metadata={}, page_content='18.10.4 Layer 3: The Fuzzy Rule Layer\\nThe outputs of Layer 2 are the values that represent the extent to which\\neach of the inputs belongs to each of the fuzzy sets “high humidity, ” “low\\nhumidity, ” “high temperature,” and “low temperature. ”\\nThe way in which this layer usually works is that the various input values\\nare multiplied together to give the fuzzy intersection of the inputs. This\\nsingle input is then used as the antecedent of the rule, which determines the'), Document(metadata={}, page_content='extent to which the neuron fires.\\nThe network in this example will be using rules such as these:\\nIF T\\nH THEN QH AND QL\\nIF TH AND TL AND HH THEN QH AND QL\\nIF TL AND HH AND HL THEN QL AND QV\\nIF HL THEN QL AND QV\\n(These can be seen by examining the network shown in Figure 18.14 and\\nassuming that the neurons in Layer 2 are ordered from the top—TH, TL, HH,\\nHL—and that the neurons in Layer 4 are ordered from the top—QH, QL, QV).'), Document(metadata={}, page_content='These rules will also be modified by the weights of the network. In cases\\nwhere the system is initially set up so that each rule uses each of the inputs, the\\nweights adapt so that inputs that are not relevant to a given rule fade to zero.\\n18.10.5 Layer 4: The Output Membership Function Layer\\nEach neuron in Layer 4 represents the membership function of one of the\\nfuzzy outputs of the rules in Layer 3. In our example, the neurons thus rep-'), Document(metadata={}, page_content='resent the membership functions for “high quinine dose, ” “low quinine\\ndose, ” and “very low quinine dose. ” The activation functions for these neu-\\nrons thus match the membership functions shown in Figure 18.11.'), Document(metadata={}, page_content='538 CHAPTER 18 Fuzzy Reasoning\\n18.10.6 Layer 5: The Defuzzification Layer\\nEach output of the system has a neuron in the fifth layer. In our case, the\\nsystem outputs just one value—the dose of quinine to prescribe to the trav-\\neler. The single node in Layer 5 takes inputs from each of the output nodes\\nin Layer 4 and combines them together to form one crisp output. This is\\ncalculated, as explained in Section 18.8, by combining the clipped fuzzy'), Document(metadata={}, page_content='membership sets and determining the centroid of the shape that this com-\\nbined function describes. This centroid is the output value of the system.\\n18.10.7 How the System Learns\\nThe neuro-fuzzy system learns using the same techniques used by tradi-\\ntional neural networks. Learning is done by adjusting the weights of the\\nconnections between neurons in the network.\\nFor example, using back-propagation, a set of input training data is applied'), Document(metadata={}, page_content='to the system and the outputs compared with the correct outputs. The\\nerror between the outputs and the correct outputs is then fed back through\\nthe network to adjust the weights to improve the network’s performance\\nwith that set of training data. When this process is repeated, the network\\neventually converges on an optimal set of weights.\\nThe system can start with a set of rules that are a “blank canvas”—where all\\nnodes in one layer are connected to all nodes in the next layer. In this case,'), Document(metadata={}, page_content='the system learns its own rules from the training data and eliminates\\nunnecessary inputs and outputs from nodes by setting their weights to zero.\\nAlternatively, the system can be set up using input from an expert or\\nexperts. This information can be used to create suitable rules in much the\\nsame way as for a traditional fuzzy system, and the network will determine\\nthe optimal weights to use with those rules. Such systems are very robust'), Document(metadata={}, page_content='and can usually detect rules that have been entered erroneously. For exam-\\nple, if one expert gave the following rule:\\nIF T\\nH and HH then QH\\nand another expert gave the following rule:\\nIF TH and HH then QV\\nclearly, one of these experts is incorrect. The system would show this by set-\\nting all the weights for the wrong rule to zero because the training data\\nwould match the correct rule but would not match the incorrect rule\\n(assuming the training data are correct).'), Document(metadata={}, page_content='18.12 Review Questions 539\\n18.11 Chapter Summary\\n■ Bivalent logics are based on two truth values (true and false, usually).\\n■ Multivalent logics allow a range of possible values.\\n■ A linguistic variable is a word such as “height” that can be used to\\nrepresent a variable that can take a number of possible fuzzy values.\\n■ A fuzzy set is defined by its membership function.\\n■ A number of fuzzy operators can be applied to fuzzy sets, including\\nfuzzy intersection, fuzzy union, and fuzzy inverse.'), Document(metadata={}, page_content='■ A hedge such as “very” or “extremely” can be applied to linguistic\\nvariables.\\n■ Fuzzy logic defines how we reason about fuzzy variables.\\n■ Fuzzy rules can be defined that tell a fuzzy system how to behave\\nbased on the value of certain fuzzy inputs.\\n■ Fuzzy inference (such as Mamdani inference) allows a fuzzy system\\nto convert crisp input values into fuzzy variables and then to rea-\\nson about those variables, resulting in a single crisp output.'), Document(metadata={}, page_content='■ Fuzzy expert systems have several advantages over traditional,\\nnonfuzzy expert systems.\\n■ Neuro-fuzzy systems are fuzzy systems that use techniques from\\nneural networks in order to learn.\\n18.12 Review Questions\\n18.1 Explain the difference between bivalent and multivalent logics.\\nWhich type of logic are you more familiar with?\\n18.2 What is the law of the excluded middle? Argue against the need for\\nthis law.\\n18.3 What is a linguistic variable? Give 10 examples of linguistic vari-'), Document(metadata={}, page_content='ables that you might use to describe a building.\\n18.4 What are hedges? Give five hedges that apply to the linguistic vari-\\nables you gave in answer to question 18.3.\\n18.5 How do fuzzy sets differ from traditional sets? What is the connec-\\ntion between linguistic variables and fuzzy sets?\\n18.6 What is the connection between fuzzy sets and fuzzy logic?'), Document(metadata={}, page_content='540 CHAPTER 18 Fuzzy Reasoning\\n18.7 Explain carefully how fuzzy logic differs from traditional Aris-\\ntotelian logic.\\n18.8 Explain how Mamdani inference works.\\n18.9 Explain what is meant by Defuzzification. How is it performed?\\n18.10 What advantages would fuzzy expert systems have over traditional\\nexpert systems. Would they have any disadvantages?\\n18.11 What is a neuro-fuzzy system? How does it learn? Compare and\\ncontrast neuro-fuzzy systems with traditional neural networks.\\n18.13 Exercises'), Document(metadata={}, page_content='18.13 Exercises\\n18.1 Develop fuzzy rules to control a set of traffic lights at a four-way\\njunction. Assume that there are sensors at each junction that\\ndetermine how many cars are waiting and how long they have\\nbeen waiting. The fuzzy rules should control the lights to mini-\\nmize delay to all cars. Each junction has a traffic light that can be\\nred (stop) or green (go). Y ou can add additional lights to each\\njunction to control traffic moving in different directions: in other'), Document(metadata={}, page_content='words, you could allow traffic turning right to go, while traffic\\ngoing straight or turning left is required to stop. Y ou can allow\\nmore than one light to be green, as long as it cannot cause any\\naccidents. Implement the fuzzy rules in a fuzzy system in the pro-\\ngramming language of your choice.\\n18.2 Prove the following expressions using fuzzy logic:\\nA\\n∧ B → A\\nA ∧ B → ¬A\\nA ∧ B → A ∨ B\\n18.14 Further Reading\\nKosko (1993) provides a nontechnical introduction to the subjects covered'), Document(metadata={}, page_content='in this chapter. Of the main texts, Negnevitsky (2002) provides the greatest\\ncoverage of fuzzy logic and fuzzy systems, providing some excellent con-\\ncrete examples of how fuzzy systems work and how fuzzy techniques can be\\ncombined with other Artificial Intelligence methods, such as neural net-\\nworks and expert systems.'), Document(metadata={}, page_content='18.14 Further Reading 541\\nThe Fuzzy Systems Handbook: A Practitioner’s Guide to Building, Using, &\\nMaintaining Fuzzy Systems, by Earl Cox (1999 – Morgan Kaufmann)\\nFuzzy Logic for Business and Industry, by Earl Cox (2000 – Charles River Media)\\nAn Introduction to Fuzzy Control, by Dimiter Driankov, Hans Hellendoorn,\\nand M. Reinfrank (1996 – Springer V erlag)\\nComputational Intelligence: An Introduction , by Andries P . Engelbrecht\\n(2003 – John Wiley & Sons)'), Document(metadata={}, page_content='(2003 – John Wiley & Sons)\\nFuzzy Control: Synthesis and Analysis, edited by Shehu S. Farinwata, Dimi-\\ntar P . Filev, and Reza Langari (2000 – John Wiley & Sons)\\nFuzzy and Neural Approaches in Engineering , by J. Wesley Hines (1997 –\\nWiley Interscience)\\nApplications of Fuzzy Logic: Towards High Machine Intelligence Quotient\\nSystems, edited by Mohammad Jamshidi, Andre Titli, Lotfi Zadeh, and\\nSerge Boverie (1997 – Prentice Hall)\\nNeuro-Fuzzy and Soft Computing: A Computational Approach to Learning'), Document(metadata={}, page_content='and Machine Intelligence , by Jyh-Shing Roger Jang, Chuen-Tsai Sun, and\\nEiji Mizutani (1996 – Prentice Hall)\\nMultistage Fuzzy Control: A Model-Based Approach to Fuzzy Control and\\nDecision Making, by Janusz Kacprzyk (1997 – John Wiley & Sons)\\nFuzzy Thinking: The New Science of Fuzzy Logic , by Bart Kosko (1994 –\\nHyperion)\\nFuzzy Logic: The Revolutionary Computer Technology That Is Changing Our\\nWorld, by Daniel Mcneill (1994 – Simon & Schuster)'), Document(metadata={}, page_content='Uncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions,\\nby Jerry M. Mendel (2000 – Prentice Hall)\\nAn Introduction to Fuzzy Sets: Analysis and Design , by Witold Pedrycz and\\nFernando Gomide (1998 – MIT Press)\\nThe Importance of Being Fuzzy , by Arturo Sangalli (1998 – Princeton Uni-\\nversity Press)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='19CHAPTER\\nIntelligent Agents\\nAn active line on a walk, moving freely without a goal. A walk for a walk’s\\nsake. The agent is a point that shifts position.\\n—Paul Klee, Pedagogical Sketchbook\\nMy team had written a number of programs to control swarms of agents.\\nThese programs were modeled on behavior of bees. The programs had many\\nuseful characteristics. Because swarms were composed of many agents, the\\nswarm could respond to the environment in a robust way. Faced with new and'), Document(metadata={}, page_content='unexpected conditions, the swarm programs didn’t crash; they just sort of\\nflowed around the obstacles, and kept going.\\n—Michael Crichton, Prey\\nFor I also am a man set under authority, having under me soldiers, and I say\\nunto one, Go, and he goeth; and to another, Come, and he cometh; and to my\\nservant, Do this, and he doeth it.\\n—The Gospel according to St Luke, Chapter 7, Verse 8\\n19.1 Introduction\\nAn agent is an entity that is able to carry out some task, usually to help a'), Document(metadata={}, page_content='human user. Agents can be biologic (people or animals, for example),\\nrobotic, or computational. This chapter is primarily concerned with the\\nlatter type, in particular with software agents. A software agent is a com-\\nputer program designed to carry out some task on behalf of a user.'), Document(metadata={}, page_content='544 CHAPTER 19 Intelligent Agents\\nAs we will see, there are a number of ways in which software agents can be\\nbuilt and a number of properties that they can have. One property with\\nwhich we are particularly concerned is intelligence. We will discuss in\\nmore detail what is meant by intelligence, in the context of agents, in Sec-\\ntion 19.2.1.\\nThis chapter also introduces other important properties that agents may or\\nmay not have, including autonomy, benevolence, the ability to collaborate'), Document(metadata={}, page_content='(with other agents, for example), and the ability to learn.\\nA number of architectures that can be used to build agents are discussed.\\nThis chapter also introduces a number of types of agents, such as reactive\\nagents, interface agents , information agents , and multiagent systems ,\\nwhich use a number of agents together to solve a single problem.\\nFinally, the chapter briefly introduces the ideas behind robotic agents and'), Document(metadata={}, page_content='discusses a particular type of robot, known as a Braitenberg vehicle, which is\\nused to discuss the nature of intelligence and our interpretation of behavior.\\nIn many ways, the field of Artificial Intelligence as a whole can be seen as\\nthe study of methods that can be used to build intelligent agents. For exam-\\nple, the techniques discussed in Chapters 3 through 6 can be thought of as\\nmethods that intelligent agents can use to enable them to search or to play'), Document(metadata={}, page_content='games. Each of the methods explained in this book can be used by an intel-\\nligent agent or to build intelligent agent systems.\\n19.2 Properties of Agents\\n19.2.1 Intelligence\\nAn agent is a tool that carries out some task or tasks on behalf of a human. For\\nexample, a simple agent might be set up to buy a particular stock when its\\nprice fell below a particular level. A simple Internet search agent might be\\ndesigned to send queries to a number of search engines and collate the results.'), Document(metadata={}, page_content='Intelligent agents have additional domain knowledge that enables them to\\ncarry out their tasks even when the parameters of the task change or when\\nunexpected situations arise. For example, an intelligent agent might be\\ndesigned to buy books for a user on the Internet at the lowest possible\\nprice. The agent would need to be able to interact with a set of online book-\\nstores but would also need to be able to learn how to deal with new book-'), Document(metadata={}, page_content='stores or with individuals who were offering secondhand books. These'), Document(metadata={}, page_content='19.2 Properties of Agents 545\\nkinds of agents that perform tasks on behalf of people are called interface\\nagents, which are discussed in Section 19.5.\\nMany intelligent agents are able to learn, from their own performance,\\nfrom other agents, from the user, or from the environment in which they\\nare situated. The ways in which agents can learn have been covered in some\\ndetail in Part 4 of this book, and the way in which some of these ideas can'), Document(metadata={}, page_content='be applied by intelligent agents are introduced in Section 19.12.\\n19.2.2 Autonomy\\nIn addition to intelligence, an important feature of many intelligent agents\\nis autonomy—the ability to act and make decisions independently of the\\nprogrammer or user of the agent. For example, an intelligent buying agent\\nthat is designed to buy goods on behalf of a user needs to be able to make\\ndecisions about what items to purchase without checking back with the'), Document(metadata={}, page_content='user. This autonomy is what sets intelligent agents aside from many other\\nArtificial Intelligence techniques.\\n19.2.3 Ability to Learn\\nMany agents have an ability to learn. In other words, when presented with\\nnew information, such an agent is able to store that new information in a\\nuseful form. For example, agents can learn from a user by observing actions\\nor by being given instruction. We see how interface agents use these kinds'), Document(metadata={}, page_content='of learning in Section 19.5. Agents can also learn from other agents in mul-\\ntiagent systems, which are described in Section 19.8.\\nLearning allows agents to improve their performance at carrying out a par-\\nticular task over time. If a human user tells an agent that it has carried out\\na task poorly, it is useful for that agent to be able to learn from this experi-\\nence to avoid making the same mistakes in the future.\\n19.2.4 Cooperation'), Document(metadata={}, page_content='19.2.4 Cooperation\\nIn multiagent systems, agents usuallycooperate with each other. This coop-\\neration implies some form of social interaction between agents. For exam-\\nple, a buying agent may negotiate with selling agents to make purchases. As\\nhas been mentioned, agents can also learn from each other. T o use the buy-\\ning agent example again, a buying agent may be informed by another buy-\\ning agent of a new shopping portal that the agent may find useful.'), Document(metadata={}, page_content='546 CHAPTER 19 Intelligent Agents\\nOf course, it is also useful for agents to cooperate with the humans who use\\nthem. Although in most agent systems, this cooperation is in the form of\\nsimple inputs and instructions, the manner in which agents cooperate with\\npeople can be very important, as we see in Section 19.5 when we discuss\\ninterface agents.\\n19.2.5 Other Agent Properties\\nAgents can have a number of other properties. A versatile agent is one that'), Document(metadata={}, page_content='is able to carry out many different tasks. Most agents are benevolent,b u t\\nsome can be competitive or nonhelpful. Similarly, agents may be altruistic\\nor antagonistic. Some agents can have the ability to lie to other agents, or\\nto users, whereas other agents are always truthful (this property is known as\\nveracity).\\nOther properties of agents include the extent to which they can be trusted\\nwith delegated tasks and whether or not they degrade gracefully (i.e.,'), Document(metadata={}, page_content='when the agent encounters a new problem that it is unable to solve, does it\\nfail completely, or is it able to make some progress?).\\nAn agent’s mobility is defined by its ability to move about on the Internet\\nor another network.\\n19.3 Agent Classifications\\nAs has been discussed in Section 19.2, agents can be classified according to\\na number of parameters. We will now discuss a variety of types of agents\\nthat are classified according to these, and other, parameters.'), Document(metadata={}, page_content='The types of agents that we will look at are not mutually exclusive: an interface\\nagent can be reactive or utility based. It can also be versatile or nonversatile.\\nThe main classes of agents are defined as follows:\\n■ reactive agents\\n■ collaborative agents\\n■ interface agents\\n■ mobile agents\\n■ information-gathering agents'), Document(metadata={}, page_content='19.4 Reactive Agents 547\\nWe also look at the difference between reactive agents and goal-based and\\nutility-based agents, which are defined by the ways in which they are moti-\\nvated. Reactive agents simply respond to inputs they receive, whereas goal-\\nbased and utility-based agents have an ability to reason about their\\npositions and make decisions on the basis of that reasoning.\\nSome agents are hybrids, which exhibit properties of more than one of the'), Document(metadata={}, page_content='categories listed above. The eventual aim of most intelligent agent research\\nis to develop smart agents, which would be fully autonomous and able to\\nlearn and cooperate with other agents. Smart agents do not yet exist and are\\nnot covered by this book.\\n19.4 Reactive Agents\\nA simple reactive agent (also known as a reflex agent) is a production sys-\\ntem where inputs from the environment are compared with rules to deter-\\nmine which actions to carry out. In other words, reactive agents simply'), Document(metadata={}, page_content='react to events in their environment according to predetermined rules.\\nA simple example of a reactive agent is the automatic mail filter that many\\ne-mail systems now possess. This mail filter examines each e-mail as it\\narrives and compares it against a set of rules, or templates, and classifies it\\naccordingly. A common use for such systems is to reject so-called “junk\\nmail” or “spam. ” More complex systems are used to route e-mails within an'), Document(metadata={}, page_content='organization, so that a consumer can send an e-mail to a central mail\\naddress, and the system will determine to which department within the\\ncompany to send the mail, based on its contents.\\nIn the case of the e-mail–filtering agent, the environment is simply an e-\\nmail inbox and the contents of that inbox.\\nA reactive agent does not tend to perform well when its environment\\nchanges or when something happens that it has not been told about. For'), Document(metadata={}, page_content='example, an e-mail–filtering system might have problems when it receives\\nan e-mail that is entirely in Chinese. New rules can of course be written to\\ndeal with such situations, but it might be more desirable to have an agent\\nthat can learn to adapt to new situations.\\nA more complex reactive agent can be developed that combines inputs\\nfrom its environment with information about the state of the world and\\ninformation about how its actions affect the world.'), Document(metadata={}, page_content='548 CHAPTER 19 Intelligent Agents\\nHence, a scheduling system might be based on the e-mail–filtering agent\\nsystem, which assigns tasks to employees based on the content of e-mails as\\nthey arrive.\\nFor example, when an e-mail arrives from a customer, reporting a bug in\\nthe company’s software system, the agent might assign a task to the engi-\\nneering department to fix the bug. The agent would then wait for further\\ninformation from the engineering department. If it did not receive assur-'), Document(metadata={}, page_content='ance that the bug had been fixed within a reasonable amount of time, it\\nmight contact the engineering department again. The agent’s ability to do\\nthis derives from the fact that it is able to store information about the state\\nof the world (such as “engineering department working to fix bug number\\n36,234,120”) and about how its actions affect the state of the world (such as\\n“when I send this e-mail to engineering, they will start to work on fixing\\nthe bug”).'), Document(metadata={}, page_content='the bug”).\\nIf a subsequent e-mail arrives from a different customer, reporting the\\nsame bug, the agent would not need to report the bug again because it\\nknows that it has already reported it. Instead, it might reply to the customer\\nsaying something like\\nThank you for your email—we are already aware of this problem, and\\nour engineers are working to fix it now.\\n19.4.1 Goal-based Agents\\nGoal-based agents are more complex than reactive agents. Rather than fol-'), Document(metadata={}, page_content='lowing a predetermined set of rules, a goal-based agent acts to try to\\nachieve a goal. This is often done by using search (see Part 2) or planning\\n(see Part 5).\\nA goal-based agent might, for example, be given the goal of finding pages\\non the Internet that are of interest to an Artificial Intelligence researcher.\\nThe agent will be designed so that it is capable of carrying out actions (such\\nas loading a web page, examining it, and following links from one web page'), Document(metadata={}, page_content='to another). It is also able to identify when it has reached a goal (for exam-\\nple, by matching the pages it finds against a set of keywords whose presence\\nindicates relevance to Artificial Intelligence).\\nThis goal based agent would search the Internet looking for pages that\\nmatched its criteria and would presumably report those pages to its owner\\nor to a client. This kind of agent does not take into account how efficiently'), Document(metadata={}, page_content='19.4 Reactive Agents 549\\nit is searching or how relevant the pages are that it is finding. In other\\nwords, its aim is simply to satisfy its goal; it does not take into account how\\nwell it has satisfied the goal or how efficiently. Utility-based agents, which\\nare described in the next section, use these concepts to attempt to provide\\nbetter results and in a more efficient manner.\\n19.4.2 Utility-based Agents\\nA utility-based agent is similar to a goal-based agent, but in addition to'), Document(metadata={}, page_content='attempting to achieve a set of goals, the utility-based agent is also trying to\\nmaximize some utility value . The utility value can be thought of as the\\nhappiness of the agent, or how successful it is being. It may also take into\\naccount how much work the agent needs to do to achieve its goals.\\nLet us return to our example from the previous section of an agent that\\nsearches for pages on the Internet that are of interest to Artificial Intelli-\\ngence researchers.'), Document(metadata={}, page_content='gence researchers.\\nThe utility-based agent can use knowledge about the Internet to follow the\\nmost worthwhile paths from one page to another. In other words, it can use\\nheuristic-based search techniques to minimize the amount of time it\\nspends examining pages that are not of interest and to maximize the likeli-\\nhood that if an interesting page exists, it will be found (this combines\\nsearch concepts from Chapters 4 and 5 with information retrieval tech-'), Document(metadata={}, page_content='niques, which are discussed in Chapter 20).\\nThe techniques we saw in Chapter 6 for game-playing systems can also be\\nused as part of a utility-based agent. In this case, the agent’s utility function\\nis based on how successful it is at playing the game, and its goal is to maxi-\\nmize this utility function by winning the game.\\n19.4.3 Utility Functions\\nA utility function maps a set of states to the set of real numbers. In other'), Document(metadata={}, page_content='words, given a particular state of the world, an agent is able to use its utility\\nfunction to derive a score, or utility value, that tells it how “happy” it is in\\nthat state or how successful it has been if it reaches that state.\\nThe static board evaluators that we saw in Chapter 6 are an example of\\na utility function that is used to evaluate a single position in a board\\ngame.'), Document(metadata={}, page_content='550 CHAPTER 19 Intelligent Agents\\nBy searching through a tree of possible future states, based on available\\nactions, and selecting a path that maximizes the utility function through-\\nout the tree, a utility-based agent is able to achieve its goals effectively and\\nefficiently.\\nFor example, our Artificial Intelligence research agent might assign a high\\nutility value to pages that are written in English and that appear to be writ-\\nten by a reliable source.'), Document(metadata={}, page_content='ten by a reliable source.\\nThe idea of utility is closely related to the idea of rationality. An agent that\\nbehaves rationally is one that attempts to maximize its utility function. This\\nutility function may not seem rational to all observers, although a rational\\nagent might be programmed to lose at chess as spectacularly as possible. By\\nlosing a game, this agent maximizes its utility function and so, contrary to\\nappearance, it is behaving rationally.'), Document(metadata={}, page_content='appearance, it is behaving rationally.\\nThis model of utility is based on economics theory. One utility function for\\npeople is money. In general, people tend to prefer to have more money\\nrather than less money. It is not as simple as this though. We might assume\\nthat the utility function for a human relating to money (ignoring other\\naspects of life) is simply based on the amount of money that that person\\nhad. This is contradicted by an experiment carried out in 1982 by psychol-'), Document(metadata={}, page_content='ogists, Tversky and Kahneman. In their experiment, they offered subjects\\ntwo consecutive choices:\\n1. A or B:\\nA = 80% chance of winning $4000\\nB = 100% chance of winning $3000\\n2. C or D:\\nC = 20% chance of winning $4000\\nD = 25% chance of winning $3000\\nMost subjects choose A, rather than B; and C, rather than D. Let us consider\\nthe utility of these choices. In the choice between A and B, we have an 80%\\nchance of winning $4000 or a 100% chance of winning $3000. The'), Document(metadata={}, page_content='expected values of these two choices are\\nE(A) = 0.8 /H110034000 = 3200\\nE(B) = 1.0 /H110033000 = 3000'), Document(metadata={}, page_content='19.5 Interface Agents 551\\nHence, the most rational choice, using a simple utility function, would be to\\nselectA rather thanB. For the choice betweenC and D, the expected values are\\nE(C) = 0.2 /H110034000 = 800\\nE(D) = 0.25 /H110033000 = 750\\nSo in this choice, most people make the more rational decision on the basis\\nof the simple utility function. What this experiment tells us is that people\\nhave much more complex utility functions than we might assume.'), Document(metadata={}, page_content='Similarly, utility-based intelligent agents usually need sophisticated utility\\nfunctions. In the case of a chess playing agent, for example, a utility func-\\ntion based solely on the number of pieces each player has would not be suf-\\nficient. A utility function based on which player wins is fine, but as we saw\\nin Chapter 6, this does not help the agent to play the game because the\\nsearch tree is usually too large for the agent to reach a position where one\\nplayer has won.\\n19.5 Interface Agents'), Document(metadata={}, page_content='player has won.\\n19.5 Interface Agents\\nAn interface agent can be thought of as a personal assistant. Interface\\nagents are typically autonomous agents, capable of learning in order to\\ncarry out tasks on behalf of a human user. Typically, interface agents col-\\nlaborate with the user, but do not need to collaborate with other agents;\\nalthough in some cases, interface agents can learn by seeking advice from\\nother agents.\\nA typical example of an interface agent is a tool that is used to help a user'), Document(metadata={}, page_content='learn to use a new software package. Such an agent has the ability to observe\\nwhat the user does and make suggestions for better ways to perform those\\ntasks. It is also able to assist the user in carrying out complex tasks, possibly\\nlearning as it does so. Interface agents can thus take instructions from users\\nand can also learn from feedback from users about whether they are doing a\\ngood job or not, in order to perform better in future.'), Document(metadata={}, page_content='It is often useful for repetitive tasks to be delegated to an interface agent.\\nThe interface agent can learn how to carry out the task by observing the\\nuser and then is able to repeat the task as required.\\nKozierok and Maes (1993) describe an interface agent that is able to assist a\\nuser with scheduling meetings on a calendar. The agent is able to arrange\\nmeetings with other people and is also able to accept, reject, and rearrange'), Document(metadata={}, page_content='552 CHAPTER 19 Intelligent Agents\\nmeetings on behalf of the user. By observing the user’s behavior, it is able to\\nlearn, for example, that the user does not like to book meetings on Friday\\nafternoons and so is able to avoid such meetings.\\nA number of tools exist that filter Usenet postings and new articles for a\\nuser. These tools can typically be trained by example: a user can show\\nexamples of interesting articles, and examples of uninteresting articles and'), Document(metadata={}, page_content='the agent can learn to identify interesting articles and present those to the\\nuser, while avoiding uninteresting ones.\\n19.6 Mobile Agents\\nMobile agents are those capable of “moving” from one place to another. In\\nthe case of mobile robots, this literally means moving in physical space. In\\nthe case of mobile software agents, this mobility usually refers to the Inter-\\nnet or other network. An agent that is not mobile is static.'), Document(metadata={}, page_content='Mobile agents travel from one computer to another, gathering information\\nand performing actions as needed on the basis of that information. A com-\\nputer virus can be thought of as a form of mobile agent, although most\\nviruses are not intelligent, merely autonomous. That is, they are able to act\\nwithout being given direct instruction from a human, but they do not\\nadapt intelligently to their surroundings—they simply follow a fixed set of'), Document(metadata={}, page_content='rules that tells them how to infect a computer and how to reproduce.\\nFor mobile agents to run on remote computers, a suitable environment\\nmust of course be provided that allows the agent to run on that machine.\\nAn example of a system that provides such an environment is T elescript,\\ndeveloped by General Magic. The Java programming language, developed\\nby Sun, can also be used for developing mobile agents.\\nThe idea that a mobile agent can be sent from one computer across the'), Document(metadata={}, page_content='Internet to run on another computer raises many security questions.\\nThe main advantages of mobile agents are in efficiency. An agent that has to\\ncommunicate with a number of remote servers and request large quantities\\nof information in order to make a decision uses a large amount of band-\\nwidth, which can be avoided if the agent is able to physically move to the\\nremote server and query it locally.'), Document(metadata={}, page_content='19.7 Information Agents 553\\nSimilarly, the mobile agent may be able to take advantage of superior com-\\nputing power or the existence of particular functional abilities at the\\nremote machine that are not present locally.\\nIn this way, mobile agents can be used to generate a distributed computing\\narchitecture, where computation takes place on multiple computers at\\narbitrary locations.\\nA further advantage of mobile agents is that they can carry out their tasks'), Document(metadata={}, page_content='asynchronously: the user can set a mobile agent off on a particular task and\\ncan then get on with other work, or maybe even switch the computer off.\\nWhen the user is ready to receive the results, the agent can be recalled.\\n19.7 Information Agents\\nInformation agents , also known as information-gathering agents ,a r e\\nusually used on the Internet and so are also sometimes called Internet\\nagents. An information agent is used to help a user find, filter, and classify'), Document(metadata={}, page_content='information from the vast array of sources available on the Internet.\\nInformation agents may be static or mobile. Some information agents are\\ncapable of learning, whereas the behavior of others is fixed. Additionally,\\ninformation agents can be collaborative or can work independently of\\nother agents. The distinctive feature of an information agent is the function\\nthat it provides, rather than the way it works.\\nThere is an overlap between information agents and other kinds of agents'), Document(metadata={}, page_content='described in this chapter. The interface agents described in Section 19.5,\\nwhich monitor Usenet postings or online news articles, are examples of\\ninformation agents.\\nInformation agents know how to search the Internet, usually using a num-\\nber of search tools. In this way, they are able to cover as much content as\\npossible and thus maximize their recall (see Chapter 20). The real chal-\\nlenge is usually precision. This is heavily dependent on the ability of the'), Document(metadata={}, page_content='agent to receive input instructions from the user. Some agents learn by\\nexample: the user shows the agent examples of pages that are relevant and\\npages that are not relevant, and the system learns to differentiate the two\\ngroups. Other agents are directed by keywords or more sophisticated infor-\\nmation retrieval techniques (see Chapter 20) to identify relevant material\\nfor the user.'), Document(metadata={}, page_content='554 CHAPTER 19 Intelligent Agents\\nThe Internet provides some unique challenges to these agents. Internet data\\nis very dirty: most of the information on the Internet is not organized in\\nany way; much of it includes misspellings, incorrect grammar, and incor-\\nrect facts. Additionally, the Internet is global in nature, and so material is\\navailable in almost every language.\\nThe sheer quantity of the data and the dirty nature of the data make it very'), Document(metadata={}, page_content='difficult for many information agent systems to provide adequate precision\\nin identifying relevant documents.\\nOf course, this is one of the reasons that information agents are so useful. It\\nis even harder for humans to locate the data they want than it is for the\\nagents. Agents have the advantage of speed and of being able to examine\\npages asynchronously, delivering results to a user, perhaps by e-mail, once\\nthey are available.'), Document(metadata={}, page_content='they are available.\\nMore sophisticated information agents are able to monitor the browsing\\nhabits of users to identify the kinds of material they are interested in and to\\nuse that information to improve the performance of future searches.\\n19.8 Multiagent Systems\\nIn many situations, simple reactive agents are sufficient. The fact that they\\ndo not have the ability to learn means that they are not suited to operating\\nin complex, dynamic environments. Also, because such an agent is based'), Document(metadata={}, page_content='on a set of rules, the number of tasks and situations that it can deal with is\\nlimited by the number of rules it has. In fact, most agents do not exist in\\nisolation.\\nMultiagent systemsare a common way of exploiting the potential power of\\nagents by combining many agents in one system. Each agent in a multiagent\\nsystem has incomplete information and is incapable of solving the entire\\nproblem on its own, but combined together, the agents form a system that'), Document(metadata={}, page_content='has sufficient information and ability to solve the problem. The system does\\nnot have a centralized control mechanism for solving the problem.\\nAn example of how many simple agents can combine together to produce\\ncomplex behavior can be seen by examining the way that ant colonies func-\\ntion. Each ant has very little intelligence and very little ability to learn.\\nTaken as a whole, however, the ant colony is able to deal with complex situ-\\nations and in some ways behaves as a single living entity.'), Document(metadata={}, page_content='19.8 Multiagent Systems 555\\nIn much the same way, many “dumb” agents can be combined together to\\nproduce a more intelligent system. For example, the legs of a robot might\\nbe controlled by a set of agents. Each leg is controlled by a simple reactive\\nrobot that has instructions for how to move the leg according to what the\\nleg encounters.\\nCommunication and collaboration are desirable properties of multiagent\\nsystems. Communication means, for example, that agents can inform each'), Document(metadata={}, page_content='other of changes in the environment or of new discoveries they have made.\\nCollaboration means that agents can work together to solve a common goal.\\nIn fact, multiagent systems often involve relatively simple interactions\\nbetween agents, and as we have seen with systems like Reynolds’ Boids (Chap-\\nter 13), the system as a whole is able to solve complex problems without the\\nindividual agents necessarily knowing anything about the overall problem.'), Document(metadata={}, page_content='Such emergent behavior is a valuable property of multiagent systems.\\nMultiagent systems can be given the ability to learn to solve new problems\\nusing genetic algorithms (see Chapter 14). In this way, robots have been\\nsuccessfully developed whose limbs are controlled by individual agents,\\neach of which has been developed using a genetic algorithm. The robots are\\nable to walk in a way that mimics the locomotion of insects (Gary Parker\\n1997, 1998).'), Document(metadata={}, page_content='1997, 1998).\\nAgents in a multiagent system can be collaborative or competitive. Agents\\ndesigned to play chess against other agents would clearly be competitive,\\nwhereas agents that traverse the Internet searching for specific material\\nmay find it advantageous to cooperate with other similar agents.\\nAn agent team is a group of agents that collaborate together to achieve\\nsome common goal. It is often the case that an agent team consists of'), Document(metadata={}, page_content='agents that operate in different ways and have different goals to accomplish.\\nFor example, a team of agents might be used to arrange travel for a busi-\\nnessman: one agent might book flights, another agent arranges hotel\\naccommodation, a third agent arranges meetings with business associates,\\nwhile a fourth agent arranges meals and entertainments.\\nIn some situations, these agents will be competing with other agents, bid-\\nding for purchases, but the agents within the team will cooperate with each'), Document(metadata={}, page_content='other (e.g., the meal-booking agent will inform the meeting booking agent\\nif it changes its restaurant bookings, which might affect a meeting that has\\nbeen arranged in that restaurant).'), Document(metadata={}, page_content='556 CHAPTER 19 Intelligent Agents\\n19.9 Collaborative Agents\\nCollaborative agent systems are multiagent systems in which the agents\\ncollaborate with each other to accomplish goals. This property, of cooper-\\nating to achieve a common goal, is known as benevolence.\\nCollaborative agents typically do not have the ability to learn, although\\nsome have simple learning abilities. As with multiagent systems, the idea is\\nthat a combination of many simple agents can solve a problem that each'), Document(metadata={}, page_content='agent individually would not be able to solve.\\nCollaborative agent systems are able to take advantage of their parallel\\nnature in order to solve problems faster than would otherwise be possible.\\nThey are also more reliable than traditional systems because additional\\nagents can be added to provide redundancy: if one agent fails, or provides\\nincorrect information, this will not affect the overall performance of the\\nsystem because other agents will provide corrective information.'), Document(metadata={}, page_content='19.10 Agent Architectures\\nIn this section, we will look at a number ofarchitecturesthat can be used to\\nbuild intelligent agents. The architecture of an agent is the way in which its\\nvarious processing modules are connected together and the way in which\\nthose modules are connected to the environment in which the agent operates.\\n19.10.1 Subsumption Architecture\\nThere are a number of architectures suitable for reactive agents. One of the'), Document(metadata={}, page_content='most commonly used is Brooks’subsumption architecture (Brooks 1985).\\nThe subsumption architecture is a layered architecture that was designed\\nfor implementing physical robots, which does not involve any centralized\\nintelligence or control mechanism.\\nThe agent in this architecture has a set of inputs, a possible set of actions,\\nand a layered set of modules, each of which is designed to control some\\naspect of the agent’s behavior. Each layer is able to inhibit the behavior of\\nlayers below it.'), Document(metadata={}, page_content='layers below it.\\nThe modules are augmented finite state machines (AFSMs), which are\\nsimilar to the finite state automata we saw in Chapter 13. AFSMs are often\\nbased on production rules, as used by expert systems, which take the form\\ninput → action'), Document(metadata={}, page_content='19.10 Agent Architectures 557\\nInputs\\nInputs\\nInputs Actions\\nActions\\nActions\\nEXPLORE\\nWANDER\\nAVOID OBSTACLES Figure 19.1\\nA three-layer subsumption\\narchitecture\\nThese rules are called situated action rules or situation action rules\\nbecause they map situations to actions. An agent that uses such rules is said\\nto be situated, in that it is affected by where it is in its environment.\\nAn AFSM is triggered when its inputs exceed a threshold. Each AFSM also'), Document(metadata={}, page_content='has inhibitor inputs that can prevent it from triggering.\\nRather than having a centralized representation, the subsumption architec-\\nture relies on lower-level modules that combine together. From these com-\\nbined modules emerges intelligent behavior.\\nA simple subsumption architecture is shown in Figure 19.1.\\nThis architecture was proposed by Brooks as a control mechanism for a\\nrobot. Each layer in the architecture is designed to handle one type of behav-'), Document(metadata={}, page_content='ior: exploring, wandering, or avoiding obstacles. The modules actasynchro-\\nnously, but each module can affect the behavior of the other modules.\\nThe WANDER module will take into account the instructions generated by\\nthe A VOID OBSTACLES module, but it is also able to suppress the instruc-\\ntions generated by the A VOID OBSTACLES module, in order to ensure that\\nwhile avoiding collisions, the robot still wanders around. This is to ensure'), Document(metadata={}, page_content='that the robot does not simply focus on avoiding obstacles to the exclusion\\nof everything else.\\nMore important than wandering, for this robot, is exploration. Hence, the\\nEXPLORE module is able to suppress instructions from the WANDER\\nmodule to ensure that the robot continues to explore new territory, rather\\nthan simply wandering aimlessly.\\nFurther layers can be added to the architecture to generate more sophisti-\\ncated behavior—for example, Brooks describes a system that is able to'), Document(metadata={}, page_content='wander around among desks in an office, looking for empty drink cans.\\nThis system has an architecture with additional layers for identifying drink\\ncans, identifying desks, and so on (Brooks 1993).'), Document(metadata={}, page_content='558 CHAPTER 19 Intelligent Agents\\n19.10.2 BDI Architectures\\nBDI architectures, or Belief Desire Intention architectures, are based on\\nthe three concepts of belief, desire, and intention. A belief is a statement\\nabout the environment that the agent considers to be true. BDI agents have\\na set of beliefs that are similar to the set of facts contained in a rule-based\\nproduction system. A desire is a goal state that the agent would like to'), Document(metadata={}, page_content='reach, and the agent’s intentions are the plans it has for how to behave in\\norder to achieve its desires.\\nAn agent can have an intention to carry out a particular action, in which\\ncase it will probably do so. Alternatively, an agent can have an intention to\\nbring about a particular state.\\nWhen an agent commits to carrying out a particular action, or achieving a\\nparticular goal, it ‘promises’ that it will do so. Hence, a BDI agent has a set'), Document(metadata={}, page_content='of beliefs that lead it to establish a set of desires. T o achieve its desires, the\\nBDI agent considers a number of options and commits to one or more of\\nthem. These options now become the agent’s intentions.\\nIntentions persist until the goals are achieved, or until it becomes unrea-\\nsonable to continue to attempt to achieve them (e.g., if it becomes obvious\\nthat the goals can never be achieved or if new beliefs are developed that\\nlead the agent to change its desires).'), Document(metadata={}, page_content='lead the agent to change its desires).\\nA bold agent is one that establishes a set of intentions and then aims to\\ncarry them out without ever stopping to consider whether it should change\\nits intentions. A cautious agent is one that considers its intentions continu-\\nally. Kinny and Georgeff (1991) found that bold agents perform better than\\ncautious agents in worlds where the environment does not change very fre-\\nquently and that cautious agents perform better than bold agents in worlds'), Document(metadata={}, page_content='that change quickly.\\n19.10.3 Other Architectures\\nA number of other agent architectures exist. Logic-based agents apply rules\\nof logical deduction to a symbolic representation of their environment.\\nThe state of such an agent is usually represented using first-order predi-\\ncates, and its behavior is determined by a set of deduction rules, usually\\nexpressed in first-order predicate logic.'), Document(metadata={}, page_content='19.10 Agent Architectures 559\\nLayer n\\nLayer 2\\nLayer 1\\nInputs\\nInputs\\nOutputs and\\nactions\\nOutputs and\\nactions\\nHorizontal Architecture Vertical Architecture\\n. . .\\nLayer n\\nLayer 2\\nLayer 1\\n. . .\\nFigure 19.2\\nHorizontal and vertical\\nagent architectures\\ncompared\\nIn contrast to logic-based architectures, purely reactive agents do not per-\\nform any symbol manipulation and rely on a simple mapping from inputs\\nto actions.\\nA number of layered architectures exist other than the subsumption archi-'), Document(metadata={}, page_content='tecture. The subsumption architecture is an example of a horizontal lay-\\nered architecture, where each layer receives inputs and contributes to the\\nactions and outputs of the agent. In a vertical layered architecture, input is\\npassed to one layer, which then passes information on to a further layer.\\nActions and outputs are eventually produced by the final layer. These two\\narchitecture types are illustrated in Figure 19.2.\\nT ouringMachines is an example of a horizontal architecture, which is'), Document(metadata={}, page_content='based on three layers:\\n■ Reactive layer: This layer uses situation rules to react to changes in\\nthe agent’s environment.\\n■ Planning layer: This layer uses a library of plans (called schemas)\\nto determine the behavior of the agent, in order to achieve particu-\\nlar goals. In most situations, this is the layer that decides the main\\nbehavior of the agent.\\n■ Modeling layer:This layer contains a model of the agent and any other\\nagents in the world, in order to avoid conflicts with other agents.'), Document(metadata={}, page_content='InteRRaP is an example of a vertical layered architecture, which has three\\nlayers with very similar functions to the layers of the T ouringMachines\\narchitecture. Each layer in the InteRRap architecture has a database of rele-\\nvant knowledge: the reactive layer has a database of knowledge about the\\nworld the agent inhabits; the planning layer has a database of planning'), Document(metadata={}, page_content='560 CHAPTER 19 Intelligent Agents\\nknowledge that contains information about the agent’s plans; the coopera-\\ntion layer (similar to the modeling layer in T ouringMachines) has social\\nknowledge about the other agents and their interactions.\\nIn the T ouringMachines architecture, each layer interacts with the environ-\\nment, directly receiving inputs and producing actions and outputs. In the\\nInteRRap architecture, only the bottom layer (the reactive, behavior layer)'), Document(metadata={}, page_content='interacts directly with the world. If it is unable to deal with a particular sit-\\nuation, it passes the information on to the next layer, the planning layer.\\nSimilarly, if this layer cannot deal with the current situation, it passes the\\ninformation on to the final layer, the cooperation layer. Outputs are passed\\nback to the behavior layer, which turns them into actions or outputs.\\n19.11 Accessibility\\nWhen playing a game such as chess, each player knows what position he'), Document(metadata={}, page_content='will be in after making any given move. What he does not usually know is\\nwhat move his opponent will make and, thus, what position he will reach\\nafter his opponent’s move.\\nIn some cases an agent’s state after carrying out a particular action can be\\ndeterministically predicted. In many situations, however, this is not the\\ncase, and the outcome is unpredictable, or stochastic. Given that an agent\\nusually has a certain degree of knowledge about the world and the way its'), Document(metadata={}, page_content='actions affect its state, we can make certain predictions. For example, an\\nagent can say that if it is in state S\\n1 and it takes action A, then it will move\\ninto state S2 with probability p. These probabilities are contained within a\\ntransition model, which enables the agent to make predictions about what\\neffect its actions will have on it and its environment.\\nIf an agent is able to determine all relevant facts about the environment in'), Document(metadata={}, page_content='which it operates, then that environment is described as being accessible.I f\\nit is inaccessible, then certain facts are hidden from the agent, although it\\nmay be able to deduce them by maintaining internal information about the\\nstate of the environment. For example, if an agent is in an environment in\\nwhich it is unable to determine the temperature, it may have a rule that says\\n“if you turn up the heating, the temperature will increase. ”'), Document(metadata={}, page_content='We could consider two types of agents that play chess. One agent might have\\nthe ability to examine the board at each move of the game and make deci-\\nsions about what move to make from that point. The agent does not have the'), Document(metadata={}, page_content='19.12 Learning Agents 561\\nability to remember moves that have been made in the past, and thus the\\nonly way it can determine the current position is by examining the board.\\nThis agent acts in an accessible environment because, at any given point, it\\nhas access to all the information it needs to be able to play the game. If we\\nimagine that this agent is playing a game where half of the board is covered\\nup, and it is unable to see what happens there, then we can see that the'), Document(metadata={}, page_content='agent would have great difficulties because it would have no way of deter-\\nmining what was happening on that side of the board apart from a few lim-\\nited facts it could deduce, such as “my king is on this side of the board, so I\\nknow I do not have a king on the other side of the board. ”\\nA different type of agent might play the game without any direct access to\\nthe board at all. This agent stores information about the moves that have'), Document(metadata={}, page_content='been made in the past and is able to use this information to determine the\\ncurrent position of the board. This agent would play equally well whether\\nthe board were entirely visible or entirely covered up.\\nThis agent operates in an inaccessible environment, but, in fact, because the\\nenvironment it operates in is entirely deterministic, it is able to derive com-\\nplete knowledge about the board at all times.\\nAn agent that played a game such as poker would need to be able to act in'), Document(metadata={}, page_content='an inaccessible, stochastic environment because the cards the opponent has\\nare neither visible nor deterministically allocated.\\nIn an accessible, stochastic environment, agents use Markov decision\\nprocesses (MDPs) to determine the best course of action. In an inaccessi-\\nble, stochastic environment, agents use partially observable Markov deci-\\nsion processes (POMDPs). Clearly, POMDPs must operate with far less\\ninformation and so tend to be more complex than MDPs.\\n19.12 Learning Agents'), Document(metadata={}, page_content='19.12 Learning Agents\\nMachine learning is covered in more detail in Part 4 of this book. An agent\\nthat is capable of learning (a learning agent) is able to acquire new knowl-\\nedge and skills and is able to use the new knowledge and skills to improve\\nits performance.\\nOne common way to provide agents with the ability to learn is to use neu-\\nral networks, which are covered in more detail in Chapter 11. A neural net-\\nwork is designed to learn in a similar manner to the way a human brain'), Document(metadata={}, page_content='562 CHAPTER 19 Intelligent Agents\\nlearns. Another method for enabling agents to learn is to use genetic algo-\\nrithms. One way to use genetic algorithms in this way is to have the genetic\\nalgorithm breed populations of agents, with the aim of breeding a highly\\nsuccessful agent. Another way is to have each agent use a genetic algorithm\\nto develop suitable strategies for dealing with particular problems.\\n19.12.1 Multiagent Learning'), Document(metadata={}, page_content='19.12.1 Multiagent Learning\\nMultiagent systems are often required to solve problems in dynamic and\\nunpredictable environments. In these circumstances, a learning ability is\\nparticularly important because the environment can change too quickly for\\npredetermined behaviors to be effective.\\nMultiagent learning can in many ways be more impressive than the learn-\\ning carried out by individual agents. Each agent in a learning multiagent'), Document(metadata={}, page_content='system can learn independently of the other agents and can also learn from\\nthe other agents.\\nIn this way, the agents can explore multiple potential strategies in parallel,\\nand when one agent discovers a particularly effective strategy, it can pass\\nthis knowledge on to other agents. For this reason, when the environment\\nchanges, multiagent learning systems are able to adapt much more quickly\\nthan nonlearning systems, or even individual learning agents.'), Document(metadata={}, page_content='In centralized learning , the agents learn on an individual and distinct\\nbasis, whereas in decentralized learning , the actions of the individual\\nagents lead to the whole system learning. The classifier systems described in\\nChapter 13 are an example of a decentralized multiagent learning system,\\nwhere each rule can be thought of as a separate agent, and where the whole\\nsystem learns by experience how best to solve a problem.\\n19.13 Robotic Agents'), Document(metadata={}, page_content='19.13 Robotic Agents\\nThe agents described in this chapter so far have been software agents—they\\nexist only in a virtual world. Robotic agents,o r  robots, are artificial agents\\nthat exist physically in the real world.\\nMobile robotic agents controlled by Brooks’ subsumption architecture\\nhave been briefly described in Section 19.10.1.\\nRobotic agents operate in an inaccessible, stochastic environment. The real\\nworld has many properties that make the tasks of robotic agents much'), Document(metadata={}, page_content='19.14 Braitenberg Vehicles 563\\nharder than those of many software agents. An ability to deal with uncer-\\ntainty is clearly important, as is robustness in the face of extremely unpre-\\ndictable and potentially dangerous environments.\\nRobots have been designed that build cars, using robotic arms and con-\\nveyer belts.\\nMore sophisticated are the robots that are designed to explore other planets\\nand collect samples for scientific analysis. Such robots, of course, require'), Document(metadata={}, page_content='autonomy: they cannot be controlled directly by human input because they\\nwould be too far away from the earth. One important aspect of such robots\\nis their ability to walk: this involves not just knowing how to move legs in\\nsuch a way as to move forward, but also how to navigate over hills and\\nrocks, around pot-holes and through valleys. Agents such as Atilla and\\nGenghis, designed by the MIT Mobot Lab (Mobot means “mobile robot”),\\nhave these abilities and are modeled on insects.'), Document(metadata={}, page_content='have these abilities and are modeled on insects.\\nGenghis has six legs and a number of sensors that enable it to determine\\ncertain facts about its inaccessible environment. The interesting thing\\nabout Genghis is that nobody ever told it how to walk or steer around\\nobstacles. Its brain consists of 57 augmented finite state machines, each of\\nwhich is responsible for a simple piece of behavior, such as lifting a leg or\\nwandering. Using these AFSMs and feedback from its sensors, Genghis was'), Document(metadata={}, page_content='able to learn to walk from the experience of trying and failing to do so.\\n19.14 Braitenberg Vehicles\\nBraitenberg vehicles were invented by a neuroscientist, Valentino Braiten-\\nberg, in the 1980s. Braitenberg vehicles are imaginary robots used by Brait-\\nenberg in thought experiments on the nature of intelligence. There are 14\\ndifferent classes of vehicles, ranging from extremely simple to fairly com-\\nplex. We will consider just the six simplest types.'), Document(metadata={}, page_content='Even the simplest of his vehicles can exhibit interesting behaviors and tell\\nus a great deal about our assumptions concerning intelligence and thought.\\nThe simplest type of Braitenberg vehicle, known as vehicle 1, simply has\\none motor and a sensor. The sensor is wired directly to the motor, such that\\nthe more of whatever the sensor is designed to sense there is, the faster the\\nmotor turns. For example, if the sensor were a light sensor, then the motor'), Document(metadata={}, page_content='would turn faster when the sensor could detect more light.'), Document(metadata={}, page_content='564 CHAPTER 19 Intelligent Agents\\nFigure 19.3\\nTwo varieties of Braiten-\\nberg vehicles type 2, seen\\nfrom above\\nThe behavior of this vehicle is very simple: the more light there is, the faster\\nit moves. It would normally move in a straight line, although imperfections\\nin its environment (such as friction and obstacles) might cause it to deviate.\\nThe second type of Braitenberg vehicle has two sensors and two motors.\\nThe motors and sensors are placed symmetrically around the vehicle, as'), Document(metadata={}, page_content='shown in Figure 19.3.\\nIn the first vehicle shown in Figure 19.3, the left-hand sensor (the sensors\\nare on the front of the vehicle) is connected to the left-hand motor, and the\\nright-hand sensor to the right-hand motor. In the second vehicle shown,\\nthe sensors and motors are connected the other way around. The first vehi-\\ncle will tend to move away from the source that its sensors detect, whereas\\nthe second vehicle will move toward it.'), Document(metadata={}, page_content='the second vehicle will move toward it.\\nThese vehicles can be thought of as timid (the one that moves away from\\nthe source) and bold (the one that moves toward the source).\\nLet us now consider a type of the timid vehicle, which has a sensor for\\nproximity and where its motors have a built-in tendency to move even\\nwithout any stimulation to the sensors. When placed in a simple maze, this\\nvehicle will navigate through the maze without bumping into the walls.'), Document(metadata={}, page_content='Clearly, apparently complex behavior can emerge from very simple con-\\ncepts. This timid vehicle was certainly not designed to traverse a maze, and\\nit does not have any knowledge of mazes or the world. An observer who did\\nnot know how the vehicle worked might conclude that it relied on a very\\nsophisticated form of Artificial Intelligence.\\nIt is interesting to note at this point some of the words that we have been\\nusing to describe agents: timid, bold, cautious, and so on. There is a ten-'), Document(metadata={}, page_content='dency to anthropomorphize the behaviors of agents, which is at least partly\\ndue to the impression that agents can give of having almost human-like\\nintelligence.\\nThe third type of vehicle is similar to the second type except that the sen-\\nsors are wired in such a way that they inhibit the motors: the more stimula-'), Document(metadata={}, page_content='19.15 Chapter Summary 565\\ntion they receive, the slower the motors turn. These types of vehicles will\\ntend to move toward a source of stimulation but will end up near the\\nsource, either facing it or turned away from it, depending on which way its\\nsensors are wired to the motors.\\nBraitenberg vehicles can have more than one type of sensor—for example,\\na vehicle might have light sensors and proximity detectors for objects.\\nThese sensors can be connected to motors in different ways, producing'), Document(metadata={}, page_content='more and more complex behaviors.\\nThe fourth type of Braitenberg vehicle has a nonlinear relationship\\nbetween input to the sensors and the speed of the motors. For example,\\none of these vehicles might move slowly toward a light source and\\nspeed up as it gets closer, then slow down again as it gets very close to\\nthe source.\\nThe fifth type of vehicle has a primitive memory that can be used to store\\ninformation about events that happened in the past.'), Document(metadata={}, page_content='The sixth type of Braitenberg vehicle is evolved using artificial evolution, as\\ndescribed in Chapters 13 and 14.\\nBraitenberg vehicles teach us the following principle, which Braitenberg\\ncalled the principle of “Uphill Analysis and Downhill Invention”: It is easier\\nto invent something than to analyze it. Fully functioning Braitenberg vehicles\\ncan be built using easily available components, and yet their behavior can be\\nextremely complex and, in some cases, impossible to analyze or explain.'), Document(metadata={}, page_content='19.15 Chapter Summary\\n■ An agent is an entity that carries out a task on behalf of a human user.\\n■ A software agent is an agent that exists solely as a computer program.\\n■ Intelligent agents have more knowledge or understanding of their\\nenvironment than simple agents and are able to use this intelli-\\ngence to carry out their tasks more effectively.\\n■ Autonomous agents are able to carry out their tasks without direct\\ninput from a human.'), Document(metadata={}, page_content='input from a human.\\n■ Some agents are able to learn from their user, from other agents,\\nfrom the environment, or by observing the consequences of their\\nown actions.'), Document(metadata={}, page_content='566 CHAPTER 19 Intelligent Agents\\n■ Reactive agents simply react to the environment they are in, using\\nsituated action rules, which provide an action for each situation.\\n■ Goal-based agents seek to achieve some goal, whereas utility-based\\nagents seek to maximize some utility function.\\n■ Interface agents are automated personal assistants.\\n■ Mobile agents are able to travel over a network, such as the Internet.\\n■ An information agent collects information (often from the Inter-'), Document(metadata={}, page_content='net) on behalf of its owner.\\n■ Multiagent systems use a number of agents that usually collaborate\\ntogether to achieve some common goal.\\n■ The subsumption architecture is an example of a vertically layered\\narchitecture for controlling robots.\\n■ BDI architectures use beliefs, desires, and intentions to control agents.\\n■ An accessible environment is one in which all necessary facts are\\navailable to the agent. Many agents must be able to operate in inac-'), Document(metadata={}, page_content='cessible environments and often in stochastic ones, where the\\nchanges in the environment are unpredictable.\\n■ Robotic agents operate in the real world.\\n19.16 Review Questions\\n19.1 “A computer virus is a kind of intelligent agent. ” Discuss this state-\\nment. Consider the various agent properties that have been dis-\\ncussed in this chapter. Which of these properties do computer\\nviruses have?\\n19.2 Explain what is meant by the following terms in the context of agents:\\n■ intelligence\\n■ autonomy'), Document(metadata={}, page_content='■ intelligence\\n■ autonomy\\n■ learning\\n■ collaboration\\n■ utility\\n19.3 Explain the idea behind the BDI architecture. Why do you think\\nthis architecture is particularly appealing to human researchers?'), Document(metadata={}, page_content='19.18 Further Reading 567\\n19.4 Explain the nature of the first six types of Braitenberg vehicles.\\nDiscuss how these vehicles can help us to understand the nature of\\nintelligence.\\n19.5 Think of a real-world interface agent. Discuss to what extent this\\nagent has autonomy, learning abilities, and intelligence.\\n19.6 What do Braitenberg vehicles teach us about intelligence? Do you\\nthink the intelligence given to Braitenberg vehicles could be put to\\nsome practical use?'), Document(metadata={}, page_content='some practical use?\\n19.7 In Michael Crichton’s novel, Prey, he postulates a multiagent system\\nconsisting of millions of tiny robotic agents. The system evolves over\\na period of days to develop human-like intelligence, and a belligerent\\ndesire to destroy life. Discuss how plausible you think this idea is, in\\nthe context of the subjects introduced in this chapter.\\n19.17 Exercises\\n19.1 Implement an intelligent agent system to carry out a simple task'), Document(metadata={}, page_content='for you in the programming language of your choice.\\n19.2 Investigate a software agent that comes with your computer, or\\nfind one that you can download for free. Explore its limitations\\nand its capabilities. T o what extent would you describe it as “intel-\\nligent”? What simple improvements would you suggest for the\\nagent? Which of the following properties does the agent exhibit:\\n■ intelligence\\n■ autonomy\\n■ ability to learn\\n■ cooperation\\n■ benevolence\\n■ veracity'), Document(metadata={}, page_content='■ cooperation\\n■ benevolence\\n■ veracity\\nT o what extent would it still be useful if it did not have the proper-\\nties that it does have? Which of the above properties might be given\\nto the agent to improve it? How would it be improved?\\n19.18 Further Reading\\nSeveral texts cover the subject of Artificial Intelligence from the perspective\\nof Artificial Agents—in particular, Russell and Norvig (1995) and Pfeifer'), Document(metadata={}, page_content='568 CHAPTER 19 Intelligent Agents\\nand Scheier (1999). Weiss (1999) provides an excellent exploration of mul-\\ntiagent systems.\\nBrooks’ subsumption architecture was introduced in A Robust Layered\\nControl System For a Mobile Robot (from IEEE Journal of Robotics and\\nAutomation, RA-2, April, pp. 14–23), and was also published as MIT AI\\nMemo 864 (1985).\\nBraitenberg (1986) provides a fascinating description of his vehicles, as well'), Document(metadata={}, page_content='as providing an absorbing philosophical argument. A good practical expla-\\nnation of Braitenberg’s vehicles is also found in Pfeifer and Scheier (2000)\\nBehavior-Based Robotics, by Ronald C. Arkin (1998 – MIT Press)\\nSoftware Agents, edited by Jeffrey M. Bradshaw (1997 – AAAI Press)\\nVehicles: Experiments in Synthetic Psychology , by Valentino Braitenberg\\n(1986 – MIT Press)\\nIntelligent Agents for Mobile and Virtual Media , edited by Rae Earnshaw,'), Document(metadata={}, page_content='John Vince, and Margaret A. Arden (2002 – Springer V erlag)\\nCommitment and Effectiveness of Situated Agents , by D. Kinny and M.\\nGeorgeff (1991 – in Proceedings of the Twelfth International Joint Conference\\non Artificial Intelligence, pp. 82–88)\\nBraitenberg Creatures, by David W. Hogg, Fred Martin, and Mitchel Resnick\\n(1991 – originally published as Epistemology and Learning Memo #13)\\nA Learning Interface Agent for Scheduling Meetings , by R. Kozierok and P .'), Document(metadata={}, page_content='Maes (1993 – in Proceedings of the ACM-SIGCHI International Workshop on\\nIntelligent User Interfaces)\\nEvolutionary Robotics: The Biology, Intelligence, and Technology of Self-Orga-\\nnizing Machines, by Stefano Nolfi and Dario Floreano (2000 – MIT Press)\\nEvolving Hexapod Gaits Using a Cyclic Genetic Algorithm , by Gary Parker\\n(1997 – in Proceedings of the IASTED International Conference on Artificial\\nIntelligence and Soft Computing, pp. 141–144)'), Document(metadata={}, page_content='Intelligence and Soft Computing, pp. 141–144)\\nGenerating Arachnid Robot Gaits with Cyclic Genetic Algorithms , by Gary\\nParker (1998 - in Genetic Programming III, pp. 576–583)\\nMetachronal Wave Gait Generation for Hexapod Robots , by Gary Parker\\n(1998 – in Proceedings of the Seventh International Symposium on Robotics\\nwith Applications)'), Document(metadata={}, page_content='19.18 Further Reading 569\\nUnderstanding Intelligence, by Rolf Pfeifer and Christian Scheier (2000 –\\nMIT Press)\\nLayered Learning in Multiagent Systems: A Winning Approach to Robotic\\nSoccer, by Peter Stone (2000 – MIT Press)\\nMultiagent Systems: A Modern Approach to Distributed Artificial Intelligence,\\nedited by Gerhard Weiss (1999 – MIT Press)\\nIntroduction to MultiAgent Systems , by Michael Wooldridge (2002 – John\\nWiley & Sons)'), Document(metadata={}, page_content='Wiley & Sons)\\nStrategic Negotiation in Multiagent Environments , by Sarit Kraus (2001 –\\nMIT Press)\\nIntelligent Information Agents: The Agentlink Perspective (Lecture Notes in\\nComputer Science, 2586) , edited by Matthias Klusch, Sonia Bergamaschi,\\nand Pete Edwards (2003 – Springer V erlag)\\nAn Introduction to AI Robotics, by Robin R. Murphy (2000 – MIT Press)\\nReal-Time and Multi-Agent Systems, by Ammar Attoui (2000 – Springer V erlag)'), Document(metadata={}, page_content='Understanding Agent Systems, edited by Mark D’Inverno and Michael Luck\\n(2001 – Springer V erlag)\\nAgent Technology: Foundations, Applications, and Markets , edited by\\nNicholas R. Jennings and Michael J. Wooldridge (1998 – Springer V erlag)\\nSocially Intelligent Agents - Creating Relationships with Computers and\\nRobots, edited by Kerstin Dautenhahn, Alan H. Bond, Lola Canamero, and\\nBruce Edmonds (2002 – Kluwer Academic Publishers)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='20CHAPTER\\nUnderstanding Language\\nPhilosophy is a battle against the bewitchment of our intelligence by means\\nof language.\\n—Ludwig Wittgenstein,Philosophische Untersuchungen\\nLanguage is a form of human reason, and has its reasons which are\\nunknown to man.\\n—Claude Lévi-Strauss, La Pensée Sauvage\\nI linger yet with nature, for the night\\nHath been to me a more familiar face\\nThan that of man; and in her starry shade\\nOf dim and solitary loveliness\\nI learned the language of another world.'), Document(metadata={}, page_content='I learned the language of another world.\\n—Lord Byron, Manfred\\n20.1 Introduction\\nThis chapter explores several techniques that are used to enable humans to\\ninteract with computers via natural human languages.\\nNatural languages are the languages used by humans for communication\\n(among other functions). They are distinctly different from formal lan-\\nguages, such as C++, Java, and PROLOG. One of the main differences,\\nwhich we will examine in some detail in this chapter, is that natural lan-'), Document(metadata={}, page_content='guages are ambiguous, meaning that a given sentence can have more than'), Document(metadata={}, page_content='572 CHAPTER 20 Understanding Language\\none possible meaning, and in some cases the correct meaning can be very\\nhard to determine. Formal languages are almost always designed to ensure\\nthat ambiguity cannot occur. Hence, a given program written in C++ can\\nhave only one interpretation. This is clearly desirable because otherwise the\\ncomputer would have to make an arbitrary decision as to which interpreta-\\ntion to work with.\\nIt is becoming increasingly important for computers to be able to under-'), Document(metadata={}, page_content='stand natural languages. T elephone systems are now widespread that are\\nable to understand a narrow range of commands and questions to assist\\ncallers to large call centers, without needing to use human resources.\\nAdditionally, the quantity of unstructured textual data that exists in the\\nworld (and in particular, on the Internet) has reached unmanageable pro-\\nportions. For humans to search through these data using traditional tech-'), Document(metadata={}, page_content='niques such as Boolean queries or the database query language SQL is\\nimpractical. The idea that people should be able to pose questions in their\\nown language, or something similar to it, is an increasingly popular one.\\nOf course, English is not the only natural language. A great deal of research\\nin natural language processing and information retrieval is carried out in\\nEnglish, but many human languages differ enormously from English. Lan-'), Document(metadata={}, page_content='guages such as Chinese, Finnish, and Navajo have almost nothing in com-\\nmon with English (although of course Finnish uses the same alphabet).\\nHence, a system that can work with one human language cannot necessar-\\nily deal with any other human language.\\nIn this section we will explore two main topics. First, we will examine natu-\\nral language processing, which is a collection of techniques used to enable\\ncomputers to “understand” human language. In general, they are con-'), Document(metadata={}, page_content='cerned with extracting grammatical information as well as meaning from\\nhuman utterances but they are also concerned with understanding those\\nutterances, and performing useful tasks as a result.\\nTwo of the earliest goals of natural language processing were automated trans-\\nlation (which is explored in this chapter) and database access. The idea here\\nwas that if a user wanted to find some information from a database, it would'), Document(metadata={}, page_content='make much more sense if he or she could query the database in her language,\\nrather than needing to learn a new formal language such as SQL.\\nInformation retrieval is a collection of techniques used to try to match a\\nquery (or a command) to a set of documents from an existing corpus of'), Document(metadata={}, page_content='20.2 Natural Language Processing 573\\ndocuments. Systems such as the search engines that we use to find data on\\nthe Internet use information retrieval (albeit of a fairly simple nature).\\n20.2 Natural Language Processing\\nIn dealing with natural language, a computer system needs to be able to\\nprocess and manipulate language at a number of levels.\\n1. Phonology. This is needed only if the computer is required to\\nunderstand spoken language. Phonology is the study of the sounds'), Document(metadata={}, page_content='that make up words and is used to identify words from sounds. We\\nwill explore this in a little more detail later, when we look at the\\nways in which computers can understand speech.\\n2. Morphology. This is the first stage of analysis that is applied to\\nwords, once they have been identified from speech, or input into\\nthe system. Morphology looks at the ways in which words break\\ndown into components and how that affects their grammatical sta-'), Document(metadata={}, page_content='tus. For example, the letter “s” on the end of a word can often either\\nindicate that it is a plural noun or a third-person present-tense\\nverb.\\n3. Syntax. This stage involves applying the rules of the grammar from\\nthe language being used. Syntax determines the role of each word in\\na sentence and, thus, enables a computer system to convert sen-\\ntences into a structure that can be more easily manipulated.\\n4. Semantics.This involves the examination of the meaning of words and'), Document(metadata={}, page_content='sentences. As we will see, it is possible for a sentence to be syntactically\\ncorrect but to be semantically meaningless. Conversely, it is desirable\\nthat a computer system be able to understand sentences with incorrect\\nsyntax but that still convey useful information semantically.\\n5. Pragmatics. This is the application of human-like understanding to\\nsentences and discourse to determine meanings that are not imme-\\ndiately clear from the semantics. For example, if someone says,'), Document(metadata={}, page_content='“Can you tell me the time?” , most people know that “yes” is not a\\nsuitable answer. Pragmatics enables a computer system to give a\\nsensible answer to questions like this.\\nIn addition to these levels of analysis, natural language processing systems\\nmust apply some kind of world knowledge. In most real-world systems, this'), Document(metadata={}, page_content='574 CHAPTER 20 Understanding Language\\nworld knowledge is limited to a specific domain (e.g., a system might have\\ndetailed knowledge about the Blocks World and be able to answer questions\\nabout this world). The ultimate goal of natural language processing would\\nbe to have a system with enough world knowledge to be able to engage a\\nhuman in discussion on any subject. This goal is still a long way off.\\nWe will now look at the individual stages of analysis that are involved in'), Document(metadata={}, page_content='natural language processing.\\n20.2.1 Morphological Analysis\\nIn studying the English language, morphology is relatively simple. We have\\nendings such as -ing, -s, and -ed, which are applied to verbs; endings such as\\n-s and -es, which are applied to nouns; we also have the ending -ly, which\\nusually indicates that a word is an adverb. We also have prefixes such as\\nanti-, non-, un-, and in-, which tend to indicate negation, or opposition. We'), Document(metadata={}, page_content='also have a number of other prefixes and suffixes that provide a variety of\\nsemantic and syntactic information.\\nIn practice, however, morphologic analysis for the English language is not\\nterribly complex, particularly when compared with agglutinative languages\\nsuch as German, which tend to combine words together into single words\\nto indicate combinations of meaning.\\nMorphologic analysis is mainly useful in natural language processing for'), Document(metadata={}, page_content='identifying parts of speech (nouns, verbs, etc.) and for identifying which\\nwords belong together. In English, word order tends to provide more of this\\ninformation than morphology, however. In languages such as Latin, word\\norder was almost entirely superficial, and the morphology was extremely\\nimportant. Languages such as French, Italian, and Spanish lie somewhere\\nbetween these two extremes.\\nAs we will see in the following sections, being able to identify the part of'), Document(metadata={}, page_content='speech for each word is essential to understanding a sentence. This can\\npartly be achieved by simply looking up each word in a dictionary, which\\nmight contain for example the following entries:\\n(swims, verb, present, singular, third person)\\n(swimmer, noun, singular)\\n(swim, verb, present, singular, first and second persons)'), Document(metadata={}, page_content='20.2 Natural Language Processing 575\\n(swim, verb, present plural, first, second, and third persons)\\n(swimming, participle)\\n(swimmingly, adverb)\\n(swam, verb, past)\\nClearly, a complete dictionary of this kind would be unfeasibly large. A more\\npractical approach is to include information about standard endings, such as:\\n(-ly,a d v e r b )\\n(-ed, verb, past)\\n(-s, noun, plural)\\nThis works fine for regular verbs, such as walk, but for all natural languages'), Document(metadata={}, page_content='(except Esperanto, the human-invented language) there are large numbers\\nof irregular verbs, which do not follow these rules. V erbs such as to be and\\nto do are particularly difficult in English as they do not seem to follow any\\nmorphologic rules.\\nThe most sensible approach to morphologic analysis is thus to include a set\\nof rules that work for most regular words and then a list of irregular words.\\nFor a system that was designed to converse on any subject, this second list'), Document(metadata={}, page_content='would be extremely long. Most natural language systems currently are\\ndesigned to discuss fairly limited domains and so do not need to include\\nover-large look-up tables.\\nIn most natural languages, as well as the problem posed by the fact that\\nword order tends to have more importance than morphology, there is also\\nthe difficulty of ambiguity at a word level. This kind of ambiguity can be\\nseen in particular in words such as trains, which could be a plural noun or'), Document(metadata={}, page_content='a singular verb, and set, which can be a noun, verb, or adjective. We will see\\nlater how parsers are designed to overcome these difficulties.\\n20.2.2 BNF\\nIn Section 20.2.4, we look at the methods that are available forparsing a piece\\nof text. Parsing involves mapping a linear piece of text onto a hierarchy that\\nrepresents the way the various words interact with each other syntactically.\\nFirst, we will look at grammars, which are used to represent the rules that'), Document(metadata={}, page_content='define how a specific language is built up.'), Document(metadata={}, page_content='576 CHAPTER 20 Understanding Language\\nMost natural languages are made up of a number of parts of speech, mainly\\nthe following:\\n■ verb\\n■ noun\\n■ adjective\\n■ adverb\\n■ conjunction\\n■ pronoun\\n■ article\\nIn fact it is useful when parsing to combine words together to form syntac-\\ntic groups. Hence, the words,a dog, which consist of an article and a noun,\\ncan also be described as a noun phrase . A noun phrase is one or more\\nwords that combine together to represent an object or thing (material or'), Document(metadata={}, page_content='otherwise) that can be described by a noun. Hence, the following are valid\\nnoun phrases:\\n■ Christmas\\n■ the dog\\n■ that packet of chips\\n■ the boy who had measles last year and nearly died\\n■ my favorite color\\nNote that a noun phrase is not a sentence—it is part of a sentence.\\nSimilarly, we have verb phrases. A verb phrase is one or more words that\\nrepresent an action. The following are valid verb phrases:\\n■ swim\\n■ eat that packet of chips\\n■ walking'), Document(metadata={}, page_content='■ swim\\n■ eat that packet of chips\\n■ walking\\nA simple way to describe a sentence is to say that it consists of a noun\\nphrase and a verb phrase. Hence, for example:\\nThat dog is eating my packet of chips.\\nIn this sentence,that dog is a noun phrase, and is eating my packet of chips is\\na verb phrase. Note that the verb phrase is in fact made up of a verb phrase,'), Document(metadata={}, page_content='20.2 Natural Language Processing 577\\nis eating, and a noun phrase, my packet of chips. In the next section, we will\\nexplore this idea in more detail and see how it enables us to build a parse\\ntree to identify the syntactic structure of a sentence.\\nA language is defined partly by its grammar. The rules of grammar for a\\nlanguage such as English can be written out in full, although it would be a\\ncomplex process to do so. T o allow a natural language processing system to'), Document(metadata={}, page_content='parse sentences, it needs to have knowledge of the rules that describe how a\\nvalid sentence can be constructed.\\nThese rules are often written in what is known as Backus–Naur form (also\\nknown as Backus normal form—both names are abbreviated as BNF).\\nBNF is widely used by computer scientists to define formal languages such as\\nC++ and Java. We can also use it to define the grammar of a natural language.\\nA grammar specified in BNF consists of the following components:'), Document(metadata={}, page_content='1. T erminal symbols.Each terminal symbol is a symbol or word that\\nappears in the language itself. In English, for example, the terminal\\nsymbols are our dictionary words such as the, cat, dog, and so on.\\nIn formal languages, the terminal symbols include variable names\\nsuch as x, y, and so on, but for our purposes we will consider the\\nterminal symbols to be the words in the language.\\n2. Nonterminal symbols. These are the symbols such as noun, verb'), Document(metadata={}, page_content='phrase, and conjunction that are used to define words and phrases\\nof the language. A nonterminal symbol is so-named because it is\\nused to represent one or more terminal symbols.\\n3. The start symbol. The start symbol is used to represent a complete\\nsentence in the language. In our case, the start symbol is simply\\nsentence, but in first-order predicate logic, for example, the start\\nsymbol would be expression.\\n4. Rewrite rules. The rewrite rules define the structure of the gram-'), Document(metadata={}, page_content='mar. Each rewrite rule details what symbols (terminal or nonter-\\nminal) can be used to make up each nonterminal symbol.\\nLet us now look at rewrite rules in more detail.\\nWe saw above that a sentence could take the following form:\\nnoun phrase verb phrase'), Document(metadata={}, page_content='578 CHAPTER 20 Understanding Language\\nWe thus write the following rewrite rule:\\nSentence → NounPhrase VerbPhrase\\nThis does not mean that every sentence must be of this form, but simply\\nthat a string of symbols that takes on the form of the right-hand side can be\\nrewritten in the form of the left-hand side. Hence, if we see the words\\nThe cat sat on the mat\\nwe might identify that the cat is a noun phrase and that sat on the mat is a'), Document(metadata={}, page_content='verb phrase. We can thus conclude that this string forms a sentence.\\nWe can also use BNF to define a number of possible noun phrases. Note\\nhow we use the “|” symbol to separate the possible right-hand sides in BNF:\\nNounPhrase → Noun\\n| Article Noun\\n| Adjective Noun\\n| Article Adjective Noun\\nSimilarly, we can define a verb phrase:\\nVerbPhrase → Verb\\n| Verb NounPhrase\\n| Adverb Verb NounPhrase\\nThe structure of human languages varies considerably. Hence, a set of rules'), Document(metadata={}, page_content='like this will be valid for one language, but not necessarily for any other lan-\\nguage. For example, in English it is usual to place the adjective before the\\nnoun (black cat, stale bread), whereas in French, it is often the case that the\\nadjective comes after the noun (moulin rouge).\\nThus far, the rewrite rules we have written consist solely of nonterminal\\nsymbols. Rewrite rules are also used to describe the parts of speech of indi-\\nvidual words (or terminal symbols):\\nNoun → cat\\n| dog'), Document(metadata={}, page_content='Noun → cat\\n| dog\\n| Mount Rushmore\\n| chickens'), Document(metadata={}, page_content='20.2 Natural Language Processing 579\\nVerb → swims\\n| eats\\n| climbs\\nArticle → the\\n| a\\nAdjective → black\\n| brown\\n| green\\n| stale\\nThese rules form a lexicon of the language, which details which words are\\navailable and which parts of speech they are.\\n20.2.3 Grammars\\nWe have briefly looked at the ways in which grammars can be described.\\nLet us now examine the types of grammars that exist.\\nNoam Chomsky invented a hierarchy of grammars. The hierarchy consists\\nof four main types of grammars.'), Document(metadata={}, page_content='of four main types of grammars.\\nThe simplest grammars are used to define regular languages . A regular\\nlanguage is one that can be described or understood by a finite state\\nautomaton. Such languages are very simplistic and allow sentences such as\\n“aaaaabbbbbb.” Recall that a finite state automaton consists of a finite\\nnumber of states, and rules that define how the automaton can transition\\nfrom one state to another.\\nA finite state automaton could be designed that defined the language that'), Document(metadata={}, page_content='consisted of a string of one or more occurrences of the letter a.H e n c e ,t h e\\nfollowing strings would be valid strings in this language:\\naaa\\na\\naaaaaaaaaaaaaaaaa\\nRegular languages are of interest to computer scientists, but are not of great\\ninterest to the field of natural language processing because they are not\\npowerful enough to represent even simple formal languages, let alone the'), Document(metadata={}, page_content='580 CHAPTER 20 Understanding Language\\nmore complex natural languages. Sentences defined by a regular grammar\\nare often known as regular expressions.\\nThe grammar that we defined above using rewrite rules is a context-free\\ngrammar. It is context free because it defines the grammar simply in terms\\nof which word types can go together—it does not specify the way that\\nwords should agree with each. For example, the grammar defined in Sec-'), Document(metadata={}, page_content='tion 20.2.2 allows the following sentence, which is grammatically correct\\n(although not necessarily semantically):\\nA stale dog climbs Mount Rushmore.\\nIt also, however, allows the following sentence, which is not grammati-\\ncally correct:\\nChickens eats.\\nA context-free grammar can have only at most one terminal symbol on the\\nright-hand side of its rewrite rules. Rewrite rules for a context-sensitive\\ngrammar, in contrast, can have more than one terminal symbol on the'), Document(metadata={}, page_content='right-hand side. This enables the grammar to specify number, case, tense,\\nand gender agreement. Each context-sensitive rewrite rule must have at least\\nas many symbols on the right-hand side as it does on the left-hand side.\\nRewrite rules for context-sensitive grammars have the following form:\\nA X B → A Y B\\nwhich means that in the context of A and B, X can be rewritten as Y.E a c h\\nof A, B, X, and Y can be either a terminal or a nonterminal symbol.'), Document(metadata={}, page_content='Context-sensitive grammars are most usually used for natural language\\nprocessing because they are powerful enough to define the kinds of gram-\\nmars that natural languages use. Unfortunately, they tend to involve a\\nmuch larger number of rules and are a much less natural way to describe\\nlanguage, making them harder for human developers to design than con-\\ntext-free grammars.\\nThe final class of grammars in Chomsky’s hierarchy consists of recursively'), Document(metadata={}, page_content='enumerable grammars (also known as unrestricted grammars). A recur-\\nsively enumerable grammar can define any language and has no restric-\\ntions on the structure of its rewrite rules. Such grammars are of interest to\\ncomputer scientists but are not of great use in the study of natural language\\nprocessing.'), Document(metadata={}, page_content='20.2 Natural Language Processing 581\\nSentence\\nadjective nounarticle verb article noun\\nNoun phrase\\nVerb phraseNoun phrase\\nThe black cat the road crossed\\nFigure 20.1\\nParse tree for the sentence\\n“the black cat crossed the\\nroad”\\n20.2.4 Parsing: Syntactic Analysis\\nAs we have seen, morphologic analysis can be used to determine to which\\npart of speech each word in a sentence belongs. We will now examine how\\nthis information is used to determine the syntactic structure of a sentence.'), Document(metadata={}, page_content='This process, in which we convert a sentence into a tree that represents the\\nsentence’s syntactic structure, is known as parsing.\\nParsing a sentence tells us whether it is a valid sentence, as defined by our\\ngrammar (for this section, we will assume that we are working with the\\nEnglish language and that the grammar we are using is English grammar).\\nIf a sentence is not a valid sentence, then it cannot be parsed.\\nParsing a sentence involves producing a tree, such as that shown in Figure'), Document(metadata={}, page_content='20.1, which shows the parse tree for the following sentence:\\nThe black cat crossed the road.\\nThis tree shows how the sentence is made up of a noun phrase and a verb\\nphrase. The noun phrase consists of an article, an adjective, and a noun.\\nThe verb phrase consists of a verb and a further noun phrase, which in turn\\nconsists of an article and a noun.\\nParse trees can be built in a bottom-up fashion or in a top-down fashion.\\nBuilding a parse tree from the top down involves starting from a sentence'), Document(metadata={}, page_content='and determining which of the possible rewrites for Sentence can be applied\\nto the sentence that is being parsed. Hence, in this case, Sentence would be\\nrewritten using the following rule:\\nSentence → NounPhrase VerbPhrase'), Document(metadata={}, page_content='582 CHAPTER 20 Understanding Language\\nThen the verb phrase and noun phrase would be broken down recursively\\nin the same way, until only terminal symbols were left.\\nWhen a parse tree is built from the top down, it is known as aderivation tree.\\nT o build a parse tree from the bottom up, the terminal symbols of the sen-\\ntence are first replaced by their corresponding nonterminals (e.g., cat is\\nreplaced by noun), and then these nonterminals are combined to match the'), Document(metadata={}, page_content='right-hand sides of rewrite rules. For example, the and road would be com-\\nbined using the following rewrite rule:\\nNounPhrase → Article Noun\\nIn the next section we examine a practical example of a parser and see\\nh o wi tw o r k s .\\n20.2.5 Transition Networks\\nA transition network is a finite state automaton that is used to represent a\\npart of a grammar. A transition network parser uses a number of these\\ntransition networks to represent its entire grammar. Each network repre-'), Document(metadata={}, page_content='sents one nonterminal symbol in the grammar. Hence, in the grammar for\\nthe English language, we would have one transition network for Sentence,\\none for Noun Phrase, one for Verb Phrase, one for Verb, and so on.\\nFigure 20.2 shows the transition network equivalents for three produc-\\ntion rules.\\nIn each transition network, S1 is the start state, and the accepting state, or\\nfinal state, is denoted by a heavy border. When a phrase is applied to a tran-'), Document(metadata={}, page_content='sition network, the first word is compared against one of the arcs leading\\nfrom the first state. If this word matches one of those arcs, the network\\nmoves into the state to which that arc points. Hence, the first network\\nshown in Figure 20.2, when presented with a Noun Phrase, will move from\\nstate S1 to state S2.\\nIf a phrase is presented to a transition network and no match is found from\\nthe current state, then that network cannot be used and another network'), Document(metadata={}, page_content='must be tried. Hence, when starting with the phrase the cat sat on the mat ,\\nnone of the networks shown in Figure 20.2 will be used because they all\\nhave only nonterminal symbols, whereas all the symbols in the cat sat on the\\nmat are terminal. Hence, we need further networks, such as the ones shown\\nin Figure 20.3, which deal with terminal symbols.'), Document(metadata={}, page_content='20.2 Natural Language Processing 583\\nVerb\\nVerb\\nNoun\\nNoun\\nNoun\\nNoun\\nNounPhrase\\nSentence → NounPhrase VerbPhrase\\nVerbPhrase → Verb\\n | Verb Noun\\nNounPhrase → Noun\\n| Article Noun\\n| Article Adjective Noun\\nVerbPhrase\\nS2\\nS2\\nS3\\nS1\\nS2S1 S3\\nS3\\nS4\\nS1\\nVerbPhrase\\nNounPhrase\\nSentence\\nTransition NetworkProduction Rule\\nArticle\\nAdjective\\nFigure 20.2\\nTransition network equiva-\\nlents for three rewrite\\nrules\\ncat\\nmat\\nthe\\na\\nsat\\nNoun → cat\\n | mat\\nArticle → the\\nVerb → sat\\n | a\\nS1\\nS2S1\\nS2\\nS2\\nS1\\nNoun\\nArticle\\nVerb'), Document(metadata={}, page_content='| a\\nS1\\nS2S1\\nS2\\nS2\\nS1\\nNoun\\nArticle\\nVerb\\nTransition NetworkProduction Rule\\nFigure 20.3\\nTransition network equiva-\\nlents for three rewrite\\nrules that represent termi-\\nnal symbols'), Document(metadata={}, page_content='584 CHAPTER 20 Understanding Language\\nTransition networks can be used to determine whether a sentence is gram-\\nmatically correct, at least according to the rules of the grammar the net-\\nworks represent.\\nParsing using transition networks involves exploring a search space of pos-\\nsible parses in a depth-first fashion.\\nLet us examine the parse of the following simple sentence:\\nA cat sat.\\nWe begin in state S1 in the Sentence transition network. T o proceed, we'), Document(metadata={}, page_content='must follow the arc that is labeled NounPhrase. We thus move out of the\\nSentence network and into the NounPhrase network.\\nThe first arc of the NounPhrase network is labeled Noun. We thus move\\ninto the Noun network. We now follow each of the arcs in the Noun net-\\nwork and discover that our first word, A, does not match any of them.\\nHence, we backtrack to the next arc in the NounPhrase network. This arc is\\nlabeled Article, so we move on to the Article transition network. Here, on'), Document(metadata={}, page_content='examining the second label, we find that the first word is matched by the\\nterminal symbol on this arc. We therefore consume the word, A, and move\\non to state S2 in the Article network. Because this is a success node, we are\\nable to return to the NounPhrase network and move on to state S2 in this\\nnetwork. We now have an arc labeled Noun.\\nAs before, we move into the Noun network and find that our next word,cat,\\nmatches. We thus move to state S4 in the NounPhrase network. This is a'), Document(metadata={}, page_content='success node, and so we move back to the Sentence network and repeat the\\nprocess for the VerbPhrase arc.\\nIt is possible for a system to use transition networks to generate a deriva-\\ntion tree for a sentence, so that as well as determining whether the sentence\\nis grammatically valid, it parses it fully to obtain further information by\\nsemantic analysis from the sentence. This can be done by simply having the\\nsystem build up the tree by noting which arcs it successfully followed.'), Document(metadata={}, page_content='When, for example, it successfully follows the NounPhrase arc in the Sen-\\ntence network, the system generates a root node labeled Sentence and an arc\\nleading from that node to a new node labeled NounPhrase. When the sys-\\ntem follows the NounPhrase network and identifies an article and a noun,\\nthese are similarly added to the tree. In this way, the full parse tree for the\\nsentence can be generated using transition networks.'), Document(metadata={}, page_content='20.2 Natural Language Processing 585\\nParsing using transition networks is simple to understand, but is not neces-\\nsarily as efficient or as effective as we might hope for. In particular, it does\\nnot pay any attention to potential ambiguities or the need for words to\\nagree with each other in case, gender, or number. In the next section, we\\nexamine augmented transition networks, which are a more sophisticated\\nparsing tool.\\n20.2.6 Augmented Transition Networks'), Document(metadata={}, page_content='20.2.6 Augmented Transition Networks\\nAn augmented transition network , or ATN, is an extended version of a\\ntransition network. ATNs have the ability to apply tests to arcs, for example,\\nto ensure agreement with number. Thus, an ATN for Sentence would be as\\nshown in Figure 20.2, but the arc from node S2 to S3 would be conditional\\non the number of the verb being the same as the number for the noun.\\nHence, if the noun phrase were three dogs and the verb phrase were is blue,'), Document(metadata={}, page_content='the ATN would not be able to follow the arc from node S2 to S3 because the\\nnumber of the noun phrase (plural) does not match the number of the\\nverb phrase (singular). In languages such as French, checks for gender\\nwould also be necessary.\\nThe conditions on the arcs are calculated by procedures that are attached to\\nthe arcs. The procedure attached to an arc is called when the network\\nreaches that arc. These procedures, as well as carrying out checks on agree-'), Document(metadata={}, page_content='ment, are able to form a parse tree from the sentence that is being analyzed.\\n20.2.7 Chart Parsing\\nParsing using transition networks is effective, but not the most efficient\\nway to parse natural language. One problem can be seen in examining the\\nfollowing two sentences:\\n1. Have all the fish been fed?\\n2. Have all the fish.\\nClearly these are very different sentences—the first is a question, and the sec-\\nond is an instruction. In spite of this, the first three words of each sentence are'), Document(metadata={}, page_content='the same. When a parser is examining one of these sentences, it is quite likely\\nto have to backtrack to the beginning if it makes the wrong choice in the first\\ncase for the structure of the sentence. In longer sentences, this can be a much\\ngreater problem, particularly as it involves examining the same words more\\nthan once, without using the fact that the words have already been analyzed.'), Document(metadata={}, page_content='586 CHAPTER 20 Understanding Language\\n0 1 2 3 4The cat eats big 5 6fisha\\nFigure 20.4\\nThe initial chart for the\\nsentence The cat eats a big\\nfish\\nAnother method that is sometimes used for parsing natural language is\\nchart parsing . In the worst case, chart parsing will parse a sentence of n\\nwords in O(n3) time. In many cases it will perform better than this and will\\nparse most sentences in O(n2) or even O(n) time.\\nIn examining sentence 1 above, the chart parser would note that the wordstwo'), Document(metadata={}, page_content='children form a noun phrase. It would note this on its first pass through the\\nsentence and would store this information in achart, meaning it would not\\nneed to examine those words again on a subsequent pass, after backtracking.\\nThe initial chart for the sentenceThe cat eats a big fishis shown in Figure 20.4.\\nFigure 20.4 shows the chart that the chart parse algorithm would start with\\nfor parsing the sentence. The chart consists of seven vertices, which will'), Document(metadata={}, page_content='become connected to each other by edges. The edges will show how the\\nconstituents of the sentence combine together.\\nThe chart parser starts by adding the following edge to the chart:\\n[0, 0, Ta r g e t→ • Sentence]\\nThis notation means that the edge connects vertex 0 to itself (the first two\\nnumbers in the square brackets show which vertices the edge connects).\\nTa r g e tis the target that we want to find, which is really just a placeholder to'), Document(metadata={}, page_content='enable us to have an edge that requires us to find a whole sentence. The\\narrow indicates that in order to make what is on its left-hand side ( Ta r g e t)\\nwe need to find what is on its right-hand side (Sentence). The dot (•) shows\\nwhat has been found already, on its left-hand side, and what is yet to be\\nfound, on its right-hand side. This is perhaps best explained by examining\\nan example.\\nConsider the following edge, which is shown in the chart in Figure 20.5:'), Document(metadata={}, page_content='[0, 2, Sentence → NounPhrase • VerbPhrase]\\nThis means that an edge exists connecting nodes 0 and 2. The dot shows us\\nthat we have already found a NounPhrase (the cat) and that we are looking'), Document(metadata={}, page_content='20.2 Natural Language Processing 587\\n0 1 2 3 4The cat eats big 5 6fisha\\n[0, 2, Sentence → NounPhrase • VerbPhrase]\\nFigure 20.5\\nPartial chart for the sen-\\ntence The cat eats a big\\nfish, showing the edge [0,\\n2, Sentence→ NounPhrase\\n• VerbPhrase]\\nfor a VerbPhrase. Once we have found the VerbPhrase, we will have what is\\non the left-hand side of the arrow—that is, a Sentence.\\nThe chart parser can add edges to the chart using the following three rules:'), Document(metadata={}, page_content='1. If we have an edge [ x, y, A → B • C], which needs to find a C, then\\nan edge can be added that supplies that C (i.e., the edge [x, y, C →\\n• E]), where E is some sequence of terminals or nonterminals\\nwhich can be replaced by a C).\\n2. If we have two edges, [ x, y, A → B • C D] and [y, z, C → E •}, then\\nthese two edges can be combined together to form a new edge: [ x,\\nz, A → B C • D].\\n3. If we have an edge [ x, y, A → B • C], and the word at vertex y is of'), Document(metadata={}, page_content='type C, then we have found a suitable word for this edge, and so we\\nextend the edge along to the next vertex by adding the following\\nedge: [y, y + 1, A → B C •].\\nLet us now see how this works, by examining the example of the sentence\\nshown in Figure 20.4: The cat eats a big fish.\\nWe start with the edge [0, 0,Ta r g e t→ • Sentence], which means that to find\\nour target, we must first find a sentence.\\nUsing rule 1 above, we can add the following edge to the chart:'), Document(metadata={}, page_content='[0, 0, Sentence → • NounPhrase VerbPhrase]\\nThis means we must now find a NounPhrase and a VerbPhrase.\\nWe now apply rule 1 again, to try to find a suitable NounPhrase, which\\ninvolves adding the following edge:\\n[0, 0, NounPhrase → • Article NounPhrase]'), Document(metadata={}, page_content='588 CHAPTER 20 Understanding Language\\nblack\\ncat matSat on\\nFigure 20.6\\nA semantic net representa-\\ntion for the sentence The\\nblack cat sat on the mat\\nNow we are able to apply rule 3 because the word at the end of this edge\\n(from vertex 0 to vertex 0) is the, which is an Article. (This would be deter-\\nmined by looking the word up in a lexicon.) Hence, we can now add the\\nfollowing edge:\\n[0, 1, NounPhrase → Article • NounPhrase]\\nNow we are looking for another NounPhrase, so we use rule 1 again to add'), Document(metadata={}, page_content='the following edge:\\n[0, 1, Noun Phrase → • Noun]\\nWe can now use rule 3 again because the next word is indeed a Noun,t o  a d d\\nthe following edge to the chart:\\n[0, 2, NounPhrase → Noun •}\\nThis process now continues, until we have reached an edge in which we\\nhave found everything we need. In this example, the final edge will be\\n[0, 6, Sentence → NounPhrase VerbPhrase•}\\nT o build a parse tree from the chart, we modify rule 2 so that when it com-'), Document(metadata={}, page_content='bines two edges together, it stores in the new edge information about the\\ntwo edges that were combined to form it (the children edges). Then when\\nthe parse has completed, we can obtain the parse tree directly from the\\nedges of the tree by starting from the first edge and recursively examining\\nthe children edges of each node.\\n20.2.8 Semantic Analysis\\nHaving determined the syntactic structure of a sentence, the next task of\\nnatural language processing is to determine the meaning of the sentence.'), Document(metadata={}, page_content='Semantics is the study of the meaning of words, and semantic analysis is\\nthe analysis we use to extract meaning from utterances.\\nSemantic analysis involves building up a representation of the objects and\\nactions that a sentence is describing, including details provided by adjectives,\\nadverbs, and prepositions. Hence, after analyzing the sentenceThe black cat\\nsat on the mat, the system would use a semantic net such as the one shown in'), Document(metadata={}, page_content='Figure 20.6 to represent the objects and the relationships between them.'), Document(metadata={}, page_content='20.2 Natural Language Processing 589\\nA more sophisticated semantic network is likely to be formed, which\\nincludes information about the nature of a cat (a cat is an object, an ani-\\nmal, a quadruped, etc.) that can be used to deduce facts about the cat (e.g.,\\nthat it likes to drink milk).\\nIn fact, semantic analysis is most useful in disambiguating sentences, as we\\nsee in the next section.\\n20.2.9 Ambiguity and Pragmatic Analysis\\nOne of the main differences between natural languages and formal lan-'), Document(metadata={}, page_content='guages like C++ is that a sentence in a natural language can have more than\\none meaning. This is ambiguity—the fact that a sentence can be inter-\\npreted in different ways depending on who is speaking, the context in\\nwhich it is spoken, and a number of other factors.\\nWe will briefly examine some of the more common forms of ambiguity\\nand look at ways in which a natural language processing system can make\\nsensible decisions about how to disambiguate them.'), Document(metadata={}, page_content='Lexical ambiguity occurs when a word has more than one possible mean-\\ning. For example, a bat can be a flying mammal or a piece of sporting\\nequipment. The word set is an interesting example of this because it can be\\nused as a verb, a noun, an adjective, or an adverb. Determining which part\\nof speech is intended can often be achieved by a parser in cases where only\\none analysis is possible, but in other cases semantic disambiguation is\\nneeded to determine which meaning is intended.'), Document(metadata={}, page_content='needed to determine which meaning is intended.\\nSyntactic ambiguity occurs when there is more than one possible parse of a\\nsentence. The sentence Jane carried the girl with the spade could be inter-\\npreted in two different ways, as is shown in the two parse trees in Figure 20.7.\\nIn the first of the two parse trees in Figure 20.7, the prepositional phrase\\nwith the spade is applied to the noun phrase the girl, indicating that it was'), Document(metadata={}, page_content='the girl who had a spade that Jane carried. In the second sentence, the\\nprepositional phrase has been attached to the verb phrase carried the girl ,\\nindicating that Jane somehow used the spade to carry the girl.\\nSemantic ambiguity occurs when a sentence has more than one possible\\nmeaning—often as a result of a syntactic ambiguity. In the example shown in\\nFigure 20.7 for example, the sentenceJane carried the girl with the spade, the'), Document(metadata={}, page_content='sentence has two different parses, which correspond to two possible mean-\\nings for the sentence. The significance of this becomes clearer for practical\\nsystems if we imagine a robot that receives vocal instructions from a human.'), Document(metadata={}, page_content='590 CHAPTER 20 Understanding Language\\nSentence\\nVerbNoun\\nNounPhrase NounPhrase\\nNoun\\nPhrase\\nPrepositional\\nPhrase\\nVerbPhrase\\ncarried the girlJane with the spade\\nSentence\\nVerbNoun\\nNounPhrase VerbPhrase\\nNoun\\nPhrase\\nPrepositional\\nPhrase\\nVerbPhrase\\ncarried the girlJane with the spade\\nFigure 20.7\\nTwo possible parse trees for the sentence Jane carried the girl with the spade\\nReferential ambiguity occurs when we use anaphoric expressions, or pro-'), Document(metadata={}, page_content='nouns to refer to objects that have already been discussed. An anaphora\\noccurs when a word or phrase is used to refer to something without naming\\nit. The problem of ambiguity occurs where it is not immediately clear which\\nobject is being referred to. For example, consider the following sentences:\\nJohn gave Bob the sandwich. He smiled.\\nIt is not at all clear from this who smiled—it could have been John or Bob.\\nIn general, English speakers or writers avoid constructions such as this to'), Document(metadata={}, page_content='avoid humans becoming confused by the ambiguity. In spite of this, ambi-\\nguity can also occur in a similar way where a human would not have a\\nproblem, such as\\nJohn gave the dog the sandwich. It wagged its tail.\\nIn this case, a human listener would know very well that it was the dog that\\nwagged its tail, and not the sandwich. Without specific world knowledge,\\nthe natural language processing system might not find it so obvious.'), Document(metadata={}, page_content='A local ambiguity occurs when a part of a sentence is ambiguous; however,\\nwhen the whole sentence is examined, the ambiguity is resolved. For exam-\\nple, in the sentenceThere are longer rivers than the Thames, the phraselonger\\nrivers is ambiguous until we read the rest of the sentence,than the Thames.\\nAnother cause of ambiguity in human language is vagueness. As we saw in\\nChapter 18, when we examined fuzzy logic, words such as tall, high, and fast'), Document(metadata={}, page_content='20.2 Natural Language Processing 591\\nare vague and do not have precise numeric meanings. A natural language\\nprocessing system may have no problem syntactically analyzing the sen-\\ntence The car is very fast , but it needs a good deal of world knowledge to\\nunderstand exactly what this sentence means. Of course, it will have differ-\\nent meanings to different people and in different circumstances: a normal\\nAmerican driver might interpret it as meaning that the car is traveling (or'), Document(metadata={}, page_content='can travel) faster than 70 miles per hour. A German, used to traveling on\\nthe Autobahn, might consider 70 miles per hour to be very slow and might\\ninterpret the sentence as meaning that the car could travel over 130 mph.\\nHumans use a number of other constructions, such as metaphor (as in he\\nran like the wind) and metonymy (using a part of an object to describe the\\nwhole, as in the suit sat next to me ). We tend to take these forms of speech'), Document(metadata={}, page_content='for granted and do not need to carry out much additional thought to\\nunderstand what is meant by them. Clearly, for a computer system this is\\nnot so easy.\\nThe process by which a natural language processing system determines which\\nmeaning is intended by an ambiguous utterance is known as disambigua-\\ntion. Disambiguation can be done in a number of ways. One of the most\\neffective ways to overcome many forms of ambiguity is to use probability.'), Document(metadata={}, page_content='This can be done using prior probabilities or conditional probabilities. Prior\\nprobability might be used to tell the system that the word bat nearly always\\nmeans a piece of sporting equipment. Conditional probability would tell it\\nthat when the wordbat is used by a sports fan, this is likely to be the case, but\\nthat when it is spoken by a naturalist it is more likely to be a winged mammal.\\nContext is also an extremely important tool in disambiguation. Consider\\nthe following sentences:'), Document(metadata={}, page_content='the following sentences:\\nI went into the cave. It was full of bats.\\nI looked in the locker. It was full of bats.\\nIn each case, the second sentence is the same, but the context provided by\\nthe first sentence helps us to choose the correct meaning of the word “bat”\\nin each case.\\nDisambiguation thus requires a good world model, which contains knowl-\\nedge about the world that can be used to determine the most likely meaning\\nof a given word or sentence. The world model would help the system to'), Document(metadata={}, page_content='understand that the sentenceJane carried the girl with the spadeis unlikely to'), Document(metadata={}, page_content='592 CHAPTER 20 Understanding Language\\nmean that Jane used the spade to carry the girl because spades are usually used\\nto carry smaller things than girls. The challenge, of course, is to encode this\\nknowledge in a way that can be used effectively and efficiently by the system.\\nThe world model needs to be as broad as the sentences the system is likely\\nto hear. For example, a natural language processing system devoted to\\nanswering sports questions might not need to know how to disambiguate'), Document(metadata={}, page_content='the sporting bat from the winged mammal, but a system designed to\\nanswer any type of question would.\\n20.3 Machine Translation\\nOne of the early goals of natural language processing was to build a system\\nthat could translate text from one human language to another. Behind this\\nattempt is an implicit assumption that human languages are like codes: in\\nother words, a word in one language is simply a code for a real-world\\nobject, emotion, action, place, etc., and can therefore be exchanged for the'), Document(metadata={}, page_content='code in another language for the same thing. Clearly this works to some\\nextent: translating the world cheval from French into English can be\\nachieved by simply looking it up in a dictionary.\\nIt is much harder to translate entire sentences, for many of the reasons that\\nhave been given above for the difficulty of natural language processing in\\ngeneral. In particular, machine translation is not possible simply using syn-\\ntactic and lexical analysis: a knowledge of the world that is being discussed'), Document(metadata={}, page_content='is also essential, in order to disambiguate the text that is being translated. It\\nmay be, in some cases, that the text can be translated directly, ignoring the\\nambiguity, and creating a similarly ambiguous sentence in the target lan-\\nguage. This does not always work, however: the word bat in English has (at\\nleast) two meanings, but there is no single word in French that has both of\\nthose meanings. Hence, for a system to translate that word from English to'), Document(metadata={}, page_content='French, it must first determine which of the meanings is intended.\\nMachine translation systems have been developed, but at present the best\\nresults they can achieve are inadequate for most uses. One way in which they\\ncan be used is in combination with a human translator. The machine is able\\nto provide a rough translation, and the human then tidies up the resultant\\ntext, ensuring that ambiguities have been handled correctly and that the'), Document(metadata={}, page_content='translated text sounds natural, as well as being grammatically correct.'), Document(metadata={}, page_content='20.3 Machine Translation 593\\n20.3.1 Language Identification\\nA similar, but easier problem to machine translation is that of language\\nidentification. There are many thousands of human languages in the world,\\nand several hundred that are widely used today. Many of these are related to\\neach other, and so can be easily confused. For an English speaker who\\nknows no Italian or Spanish, those two languages can sometimes appear\\nsimilar, for example. A system that can identify which language is being'), Document(metadata={}, page_content='used in a piece of text is thus very useful. It is also particularly useful in\\napplying textual analysis of all kinds to documents that appear on the Inter-\\nnet. Because pages on the Internet often have no indication of which lan-\\nguage is being used, an automated system that is analyzing such documents\\nneeds to have the ability first to determine which language is being used.\\nOne way to determine the language of a piece of text would be to have a'), Document(metadata={}, page_content='complete lexicon of all words in all languages. This would clearly provide\\naccurate results, but is likely to be impractical to develop for a number of\\nreasons. The lexicon would be enormous, of course, and it would be very\\ndifficult to ensure that all words were really included.\\nThe acquaintance algorithm is a commonly used method for language\\nidentification that uses n-grams.A n  n-gram is simply a collection of n let-'), Document(metadata={}, page_content='ters, but detailed statistics exist that indicate the likelihood of a particular\\nset of letters occurring in any given language. Hence, for example, the tri-\\ngrams ing, and, the, ent, and ant probably indicate that a document is in\\nEnglish. When the acquaintance algorithm is presented with sufficient text\\n(usually a few hundred to a thousand words is sufficient), it is able to iden-\\ntify the language with a surprisingly high degree of accuracy.'), Document(metadata={}, page_content='The acquaintance algorithm is trained by being presented with text in each\\nlanguage that it is expected to identify. The system then calculates a vector\\nfor each language based on the training data. This vector stores informa-\\ntion about how many times each n-gram occurs in that language. When a\\ndocument in an unknown language is presented to the algorithm, it calcu-\\nlates a similar vector for this document and compares it with the vectors it'), Document(metadata={}, page_content='has calculated for the training data. The vector that is closest indicates\\nwhich language is being used in the document.\\nOne advantage of this approach is that it is easy to tell how certain the algo-\\nrithm is about a particular document. A score is calculated for a document'), Document(metadata={}, page_content='594 CHAPTER 20 Understanding Language\\nfor each possible language, and the language with the highest score is\\nselected. If the highest score is very much higher than the second highest\\nscore, this indicates a high degree of certainty. Conversely, if the top two or\\nthree scores are similar, then the algorithm is less certain, and there are one\\nor more other possibilities that might need to be examined.\\n20.4 Information Retrieval'), Document(metadata={}, page_content='20.4 Information Retrieval\\nInformation retrieval involves matching the text contained in a query or a\\ndocument to a set of other documents. Often, the task involves finding the\\ndocuments from a corpus of documents that are relevant to a user’s query.\\nInformation retrieval was briefly introduced in Chapter 12, where we saw\\nhow Bayes’ theorem can be used to produce a system that is effective at\\nmatching documents to a query and thus retrieving relevant documents'), Document(metadata={}, page_content='from a corpus in response to a user request.\\nThe idea behind information retrieval is that if a user enters a query such as\\nwhat is the capital of Sri Lanka?, then a good approach to finding the answer\\nis to find a document that contains all (or some) of the words contained in\\nthe query. In fact, words such as what, is, the, and of would normally be\\nstripped from the query (using a stop list, which contains words that are to\\nbe stripped from all queries) before processing, and the information'), Document(metadata={}, page_content='retrieval system would locate the documents that contained the words cap-\\nital, Sri, and Lanka.\\nThe corpus of documents is clearly very important. As has already been dis-\\ncussed, ambiguities in the query text can be avoided if the corpus is a very\\nspecific one. Information retrieval systems tend not to deal well with ambi-\\nguity because they are usually not given any world knowledge but are sim-\\nply designed to perform statistical analysis of words in order to pick out'), Document(metadata={}, page_content='suitable responses to a query.\\nAs well as providing responses to a query, information retrieval can be used\\nto find other documents that are similar to a given document. This provides\\na “more like this” function that many search engines use, which enables a\\nuser to say “I like this web site—find me other ones that are similar. ”\\nThe main concept used in information retrieval is known as TF-IDF,\\n(T erm Frequency – Inverse Document Frequency).'), Document(metadata={}, page_content='20.4 Information Retrieval 595\\nUsually, a TF-IDF value is calculated for each of a set of words, and the\\nresultant values are placed in a vector, which represents a document or\\npiece of text (such as a query).\\nThe inverse document frequency (IDF) of a wordW is calculated as follows:\\nWhere |D| is the number of documents in the corpus; DF (W) is the docu-\\nment frequency of W, which is the number of documents in the corpus\\nthat contain the word W.'), Document(metadata={}, page_content='that contain the word W.\\nThe term frequency of word W in document D is written TF ( W, D) and\\nrepresents the number of times the word W occurs in document D.\\nThe TF-IDF vector is the product of the TF and IDF values for a set of\\nwords for a particular document:\\nTF-IDF (D, W\\ni) = TF(Wi, D) /H11003IDF (Wi)\\nLet us now consider why this calculation makes sense. The inverse docu-\\nment frequency is designed to give a large value for infrequent words and a'), Document(metadata={}, page_content='low value for frequent words. It is important that this calculation is done\\nusing the number of occurrences in the appropriate corpus. In some cases,\\nthe corpus can be representative of the English language as a whole, in\\nwhich case no assumptions are being made about the nature of the subjects\\nbeing searched for.\\nIn many other cases, however, the corpus should be representative of a par-\\nticular subject area. Hence, if the corpus were a set of documents about'), Document(metadata={}, page_content='New Y ork City, then the word elephant would be relatively infrequent and\\nwould thus produce a relatively high IDF value. Conversely, words such as\\ntaxi, building, New York, and streets would be relatively common and so\\nwould receive relatively low IDF values.\\nLet us consider the following query that is put to an information retrieval\\nsystem using a corpus of documents about New Y ork City:\\nWhen did an elephant walk through the streets of New York?'), Document(metadata={}, page_content='First, the stop words would be stripped, leaving the following words:\\nElephant walk through streets New York\\nIDF W D\\nDF W( ) =\\n( )\\nlog'), Document(metadata={}, page_content='596 CHAPTER 20 Understanding Language\\nAn IDF value would now be calculated for each word in the query. The\\nword elephant would certainly receive the highest score, and the words\\nthrough, streets, New, and Yor kwould all obtain very low scores.\\nNow, an index of the corpus of documents is consulted to obtain all docu-\\nments that contain all (or some) of the words contained in the query. One\\ntechnique here would be to require the least common word (elephant) to be'), Document(metadata={}, page_content='in all documents, but to allow documents to have some combination of one\\nor more of the other query words. Hence, the user would effectively be\\nmaking the following Boolean query:\\n“Elephant” and (“walk” or “through” or “streets” or “New” or “Y ork”)\\nAt this point, the TF part of TF-TDF comes into play. For each document\\nthat is retrieved, a TF-IDF value is calculated for the words in the query,\\nproducing a vector of six values for each document.'), Document(metadata={}, page_content='The idea behind the TF calculation is that if a document mentions the\\nword elephant 10 times, then it is much more likely to be relevant to this\\nquery than a document that mentions it just once. On the other hand, a\\ndocument that mentions the word elephant just once is still more likely to\\nbe relevant than a document that does not mention the word elephant at\\nall, even if it has all the other words in the query several times.'), Document(metadata={}, page_content='The most common behavior for an information retrieval system in response\\nto a query such as this is to return one or more of the most relevant docu-\\nments that were obtained from the corpus. The relevance of a document can\\nbe obtained by obtaining the magnitude of its TF-IDF vector.\\nIt is also possible to show a document to a corpus and ask it to find the most\\nsimilar documents in the corpus. In some cases, queries are considered to'), Document(metadata={}, page_content='be documents and are treated in this way. In this case, a TF-IDF vector is\\ncalculated for the query document and is also calculated for each document\\nin the corpus. The most relevant documents are deemed to be those whose\\nTF-IDF vectors are closest to the vector of the query document.\\n20.4.1 Stemming\\nIf a user enters the query “where are elephants?” , it would clearly be foolish\\nfor the system to reject a document that contains several occurrences of the'), Document(metadata={}, page_content='20.4 Information Retrieval 597\\nword elephant simply because it does not contain the word elephants\\nexactly as used in the query.\\nStemming is often applied in information retrieval systems to avoid this\\nproblem. Stemming simply involves removing common stems such as -ing,\\n-s, and -ed from words. In this way, the word swimming will be stemmed to\\nswim and will match swims, swimmers, and so on. It will not usually be able\\nto match swam or swum because these are irregular forms.'), Document(metadata={}, page_content='The most commonly used stemmer is Porter’s stemmer , which is an\\nextremely simple algorithm that has in some cases been shown to improve\\nthe performance of information retrieval systems.\\nPorter’s stemmer is explained in detail in Spärck Jones and Willett (1997).\\nThe following is a brief description of the algorithm. Each step is carried\\nout in turn on each word. The algorithm also includes conditions relating\\nto word length so that words such as sing are not stemmed, whereas words'), Document(metadata={}, page_content='such as hissing are. The algorithm is also careful to differentiate between\\nsingle and double letters, ensuring that hopping is stemmed to hop, and\\nhissing is stemmed to hiss.\\n1. -s is removed, and -sses is converted to ss (hence, caresses is\\nstemmed to caress).\\n2. -ed, -ing are removed. After -ed is removed, an -e is added if the\\nword now ends in -at, -bl,o r  -iz, ensuring that grated, disabled, and\\nrealized are correctly stemmed to grate, disable, and realize rather'), Document(metadata={}, page_content='than grat, disabl, and realiz.\\n3. -y is converted to -i. This seems like a strange step, but ensures that\\nfly and flies are considered to be the same word because they are\\nboth stemmed to fli.\\n4. A number of specific rules are now applied such as:\\n-ATIONAL → ATE\\n-IVENESS → IVE\\n-BILITI → BLE\\n5. Endings such as -ative, -ful, -ness, -able, and -er are removed, and\\nendings such as -icative, -iciti, and -ical are converted to -ic.\\n6. -e is removed.'), Document(metadata={}, page_content='598 CHAPTER 20 Understanding Language\\n7. Double letters at the end of some words (based on length) are con-\\nverted to single letters—for example, controll is converted to con-\\ntrol. This ensures that the following words are all considered to be\\nthe same: controlling, control, controlled, controllable.\\nThe aim of stemming is to ensure that a query word will match other words\\nwith the same meaning that differ only in endings in the corpus. Hence, it'), Document(metadata={}, page_content='is desirable not necessarily that the stemmed words are real words, but that\\nwhen two words are stemmed they become the same if they really are the\\nsame word. Hence, the words flying, fly, and flies all stem to the nonword fli.\\nThe fact that fli is not a word does not matter because the aim is to match\\nthese words together in query and documents, not to show the stemmed\\nwords to the user.\\n20.4.2 Precision and Recall\\nThe success of an information retrieval system can be measured using two'), Document(metadata={}, page_content='metrics: precision and recall. If a system has 100% precision, it means that\\nwhen it says that a particular document is relevant, then it is guaranteed to\\nbe correct. Lower precision means that it will wrongly classify some docu-\\nments as being relevant (false positives).\\nFor a system to have 100% recall, it must be guaranteed to find all relevant\\ndocuments within a corpus in response to a particular query. Lower recall\\nmeans that the system will fail to identify some documents as being rele-'), Document(metadata={}, page_content='vant (false negatives).\\nIn general, for most information retrieval techniques, precision and recall\\nare in opposition to each other, meaning that when the system’s precision\\nincreases it does so at the expense of recall, and vice-versa. This is intuitive:\\nthe only way to get 100% recall in most real-world situations is to be very\\nrelaxed about which documents are classified. In other words, a great deal\\nof documents must be classified as being relevant to ensure that all relevant'), Document(metadata={}, page_content='documents are found. Inevitably, this will mean that some irrelevant docu-\\nments will be found as well.\\nSimilarly, to obtain 100% precision it is necessary to return very few doc-\\numents, meaning that some documents that are in fact relevant will not\\nbe returned.\\nA perfect information retrieval system would be one that achieved 100%\\nrecall and 100% precision over a given corpus and a given set of queries.'), Document(metadata={}, page_content='20.5 Chapter Summary 599\\nSuch an information retrieval system is highly unlikely to ever be devel-\\noped for most reasonable problems. One of the main causes of this diffi-\\nculty is the complexity of human language, and the ambiguities it presents,\\nas discussed above.\\nInformation retrieval systems, unlike natural language processing systems,\\ndo not tend to take into account grammatical structures and thus are poor\\nat, for example, noticing that “ the city that used to be the capital of Sri'), Document(metadata={}, page_content='Lanka” is no longer the capital of Sri Lanka.\\n20.5 Chapter Summary\\n■ Morphologic analysis involves examining the structure of individ-\\nual words.\\n■ BNF (Backus–Naur form or Backus normal form) is used to define\\nthe rules that make up a grammar for a language.\\n■ Grammars define the syntactic rules and structures of a language.\\n■ Parsing (or syntactic analysis) uses the grammar of a language to\\ndetermine the structure of a sentence or utterance, in order to'), Document(metadata={}, page_content='derive further information (such as meaning) from the words.\\n■ Semantic analysis involves examining the meaning of words\\nand phrases.\\n■ Ambiguity is a common problem with natural language processing\\nsystems. It can be dealt with to some extent by pragmatic analysis.\\n■ Machine translation involves presenting a piece of text in one\\nhuman language, which a computer program is then expected to\\ntranslate into another human language. Although a great deal of'), Document(metadata={}, page_content='work has been carried out in this field, the success predicted in the\\n1950s has yet to be achieved.\\n■ Determining the language of a piece of text can be done by exam-\\nining the occurrence of particular trigrams within the text.\\n■ Information retrieval (IR) involves producing a response to a user\\nquery by selecting relevant documents from a corpus of docu-\\nments (such as the Internet).\\n■ An IR system that achieves 100% precision can guarantee that any'), Document(metadata={}, page_content='document it returns is relevant to the query.'), Document(metadata={}, page_content='600 CHAPTER 20 Understanding Language\\n■ An IR system that achieves 100% recall can guarantee that if a rele-\\nvant document exists for a query, then it will find it.\\n■ Achieving 100% recall and 100% precision is the goal of most infor-\\nmation retrieval systems and one that has not yet been achieved.\\n20.6 Review Questions\\n20.1 Explain what is meant by Natural Language Processing . Why is it\\nsuch a difficult subject?\\n20.2 Explain the role of each of the following in Natural Language Pro-\\ncessing:'), Document(metadata={}, page_content='cessing:\\nmorphology\\nsyntax\\nsemantics\\npragmatics\\ngrammars\\n20.3 What is BNF? Why is it used to describe grammars?\\n20.4 How are transition networks used to represent a grammar?\\n20.5 Explain the difficulties involved in machine translation.\\n20.6 What is information retrieval? How does it differ from natural lan-\\nguage processing?\\n20.7 What are precision and recall? How are they related? Explain why it\\nis not usually possible to have 100% precision and 100% recall in'), Document(metadata={}, page_content='the same system. Can you imagine a scenario in which it would be\\npossible to achieve 100% precision and 100% recall?\\n20.7 Exercises\\n20.1 Examine the BNF definition of the syntax of a programming lan-\\nguage such as C++, BASIC, or Java. What differences are immedi-\\nately obvious compared with the BNF for a human language\\ngrammar? Are there any similarities?\\n20.2 Implement Porter’s stemming algorithm in the programming lan-\\nguage of your choice. Y ou will need to find a full description of the'), Document(metadata={}, page_content='algorithm. Y ou can find this in Porter (1980), Spärck Jones (1997),'), Document(metadata={}, page_content='20.8 Further Reading 601\\nor online. Apply the algorithm to a dictionary of words, such as the\\none that comes with most UNIX implementations. Then allow a\\nuser to enter a word, and have the system look this word up and say\\n“yes” if it is present in its stemmed form in the dictionary or “no” if\\nit is not. For example, if the dictionary contains swim, fish, and\\ncheese, then it should say “yes” to swimming, fishing, and cheeses but\\nno to chocolate and swam.'), Document(metadata={}, page_content='no to chocolate and swam.\\n20.3 Find a book or web site that defines a set of rules for English gram-\\nmar or the grammar of another human language. Express all of the\\nrules in BNF and as transition networks. What problems do you\\nencounter? Are there any rules that you cannot represent in either\\nsystem or in both systems?\\n20.4 Find a machine translation service online. Have it translate a piece\\nof text in a language with which you are not familiar into English.'), Document(metadata={}, page_content='What errors does the translator introduce? Can you determine any\\nsophisticated features based on the translation it produces?\\n20.5 Implement a language identification system in the programming\\nlanguage of your choice. Y ou should start by selecting a number of\\nlanguages (four or five should do). Y ou should have a suitable\\nquantity of typical material in each language—about 1000 words\\nin each language would be plenty. First, write an algorithm that'), Document(metadata={}, page_content='determines the most common 100 trigrams in each language. Now\\nbuild these data into a program that uses it to determine the lan-\\nguage of unseen text. Produce an alternative version of the soft-\\nware that calculates a frequency vector using all (26 * 26 * 26)\\ntrigrams. How does this system perform compared with the first\\none you produced in terms of accuracy and efficiency?\\n20.8 Further Reading\\nNatural language processing is briefly covered by most of the standard'), Document(metadata={}, page_content='texts; information retrieval is less well covered. Spärck Jones and Willett\\n(1997) provide an excellent coverage of the topics of information retrieval,\\nwith papers from a number of researchers in the field.\\nNatural Language Understanding, by James Allen (1995 – Addison Wesley)\\nModern Information Retrieval , by Ricardo Baeza-Y ates and Berthier\\nRibeiro-Neto (1999 – Addison Wesley)'), Document(metadata={}, page_content='602 CHAPTER 20 Understanding Language\\nPlan Recognition in Natural Language Dialogue , by Sandra Carberry (1990\\n– MIT Press)\\nCross-Language Information Retrieval , edited by Gregory Grefenstette\\n(1998 – Kluwer Academic Publishing)\\nFoundations of Computational Linguistics: Human-Computer Communica-\\ntion in Natural Language, by Roland R. Hausser (2001 – Springer V erlag)\\nInformation Retrieval, by William R. Hersh (2002 – Springer V erlag)'), Document(metadata={}, page_content='Spoken Language Processing: A Guide to Theory, Algorithm and System\\nDevelopment, by Xuedong Huang, Alex Acero, Hsiao-Wuen Hon, and Raj\\nReddy (2001 – Prentice Hall)\\nNatural Language Processing and Knowledge Representation: Language for\\nKnowledge and Knowledge for Language , edited by Lucja M. Iwanska and\\nStuart C. Shapiro (2000 – AAAI Press)\\nText-Based Intelligent Systems: Current Research and Practice in Information\\nExtraction and Retrieval , edited by Paul Schafran Jacobs (1992 – Lawrence'), Document(metadata={}, page_content='Erlbaum Assoc.)\\nSpeech and Language Processing: An Introduction to Natural Language Process-\\ning, Computational Linguistics and Speech Recognition, by Dan Jurafsky, James\\nH. Martin, Keith Vander Linden, and Nigel Ward (2000 – Prentice Hall)\\nIntelligent Multimedia Information Retrieval , edited by Mark T. Maybury\\n(1997 – AAAI Press)\\nComputational Linguistics , by T ony McEnery (1992 – Coronet Books –\\nout of print)\\nText Information Retrieval Systems , by Charles T. Meadow, Bert R. Boyce,'), Document(metadata={}, page_content='and Donald H. Kraft (2000 – Academic Press)\\nNatural Language Processing for Online Applications: Text Retrieval, Extrac-\\ntion, and Categorization , by Peter Jackson and Isabelle Moulinier (2002 –\\nJohn Benjamins Publishing Company)\\nSpotting and Discovering Terms through Natural Language Processing ,b y\\nChristian Jacquemin (2001 – MIT Press)\\nFoundations of Statistical Natural Language Processing , by Christopher D.\\nManning and Hinrich Schütze (1999 – MIT Press)'), Document(metadata={}, page_content='Manning and Hinrich Schütze (1999 – MIT Press)\\nReadings in Machine Translation , edited by Sergei Nirenburg, Harold L.\\nSomers, and Y orick A. Wilks (2002 – MIT Press)'), Document(metadata={}, page_content='20.8 Further Reading 603\\nNatural Language Processing , by Fernando C. N. Pereira and Barbara J.\\nGrosz (1994 – MIT Press)\\nAn Algorithm for Suffix Stripping , by M. F. Porter (1980 – in Spärck Jones\\nand Willett 1997)\\nFundamentals of Speech Recognition , by Lawrence Rabiner and Biing-\\nHwang Juang (1993 – Pearson Education)\\nEvolutionary Language Understanding , by Geoffrey Sampson (1996 –\\nContinuum)\\nEvaluating Natural Language Processing Systems: An Analysis and Review,b y'), Document(metadata={}, page_content='Karen Spärck Jones, Julia R. Galliers (1996 – Springer V erlag)\\nReadings in Information Retrieval , edited by Karen Spärck Jones and Peter\\nWillett (1997 – Morgan Kaufmann)\\nTranslation Engines: Techniques for Machine Translation, by Arturo Trujillo\\n(1999 – Springer V erlag)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='21CHAPTER\\nMachine Vision\\nThe Lord looseth men out of prison: the Lord giveth sight to the blind.\\n—Psalm 146, V erse 7\\nYou see, but you do not observe.\\n—Sir Arthur Conan Doyle, The Adventures of Sherlock Holmes\\nAll the mighty world\\nOf eye and ear, both what they half create,\\nAnd what they perceive.\\n—Sir William Wordsworth\\n21.1 Introduction\\nThe vision system in mammals (such as human beings) is one of the most\\nremarkable systems in the natural world. Without vision, it can be argued'), Document(metadata={}, page_content='that human beings would not have reached their current levels of technical\\nachievement, and indeed that none of the creatures alive today would have\\nbeen able to evolve successfully without vision.\\nProviding the ability for computer systems, agents, or robots to perceive the\\nworld visually is clearly highly desirable.\\nIn this chapter, we look at the techniques that are used to enable computers\\nto “see” the real world, in much the same way that we do.'), Document(metadata={}, page_content='606 CHAPTER 21 Machine Vision\\nThis chapter explains how the Canny method uses convolution to detect\\nedges in images. It also explains how an image recognition system can then go\\non to segment the image and thus determine what objects are being viewed.\\nThis chapter presents a popular method that is used for face recognition and\\nalso discusses the importance of textures in computer vision systems.\\n21.2 Human Vision\\nIn this section, we briefly describe the structure and function of the compo-'), Document(metadata={}, page_content='nents that make up the mammalian visual system and, in particular, the\\nhuman visual system. Understanding how humans see is vital to understand-\\ning how it can be possible to enable computers to perceive in a similar way.\\nFigure 21.1 shows a simplified diagram of the human vision system.\\nThe most important parts of the human visual system are the eyes and the\\nbrain—in particular, the part of the brain that is associated with vision is\\nthe visual cortex.'), Document(metadata={}, page_content='the visual cortex.\\nThe eye is the device that captures light that has bounced off nearby\\nobjects. This is achieved by a lens that focuses the light onto the retina,\\nwhich is a screen at the back of the eye containing millions of photorecep-\\ntors. Photoreceptors are cells that are sensitive to light. There are two types\\nof photoreceptors: rods and cones.\\nRod cells are highly sensitive and so respond well in situations where there'), Document(metadata={}, page_content='is little light, but they have a low level of acuity, meaning that the images\\nthey transmit to the brain are less detailed and “fuzzier” than those trans-\\nmitted by the cones. Additionally, rods do not have the ability to recognize\\ndifferences in color.\\nCones, on the other hand, are relatively insensitive and so only respond well\\nwhen presented with high levels of light, but they have a high level of acu-\\nity and are able to recognize differences in colors. The cone cells are mainly'), Document(metadata={}, page_content='situated in the center of the retina, whereas the cones are mainly situated\\naround the edges. This explains why most of our vision in normal, well-lit\\ncircumstances takes place in the center of our field of vision (the corre-\\nsponding area of the retina is called the fovea), whereas at night, our\\nperipheral vision is more important. Y ou will notice, for example, that on a\\ndark night, you can often see stars out of the corner of your eye, but if you'), Document(metadata={}, page_content='turn your eye to look at those stars, they seem to disappear.'), Document(metadata={}, page_content='21.2 Human Vision 607\\nRIGHT EYE\\nOPTIC  NERVE\\nRETINA\\nOPTIC CHIASM\\nOPTIC TRACT\\nLATERAL\\nGENICULATE\\nNUCLEUS\\nOPTIC RADIATIONS\\nVISUAL CORTEX\\nLEFT EYE\\nLEFT OF\\nVISUAL FIELD\\nRIGHT OF\\nVISUAL FIELD\\nCENTER OF\\nVISUAL FIELD\\nFigure 21.1\\nA diagram of the human\\nbrain, showing the mam-\\nmalian system\\nSignals from the photoreceptors in the retina are passed via the optic nerve\\nto the lateral geniculate nucleus (LGN) and also to the superior collicu-\\nlus. The main pathway is the one to the LGN.'), Document(metadata={}, page_content='lus. The main pathway is the one to the LGN.\\nThe nerves that travel from the right eye go to the left-hand side of the\\nbrain, and the nerves from the left eye go to the right-hand side of the brain.\\nThe point where the optic nerves cross over each other is theoptic chiasm.\\nFrom the LGN, the signals are carried to the visual cortex by the optic radi-\\nations. This is done in such a way that if both eyes can see a point in the'), Document(metadata={}, page_content='field of view, then the signals corresponding to this point from the two eyes\\nwill arrive at the same part of the brain. It is as a result of this that we are\\nable to perceive a three-dimensional depth to the world that we see. If you'), Document(metadata={}, page_content='608 CHAPTER 21 Machine Vision\\nFigure 21.2\\nA picture of a plant and a\\nmagnified view of a rec-\\ntangular region of this\\nsame photograph. The\\nmagnified region has been\\ntaken from the area high-\\nlighted in the lower right-\\nhand side of the\\nphotograph.\\nshut one eye you will find that it is much harder to accurately perceive\\ndepth. For example, if you hold out a pen in one hand, shut one eye, and\\nthen try to place the cap on the pen with the other hand, you will find it'), Document(metadata={}, page_content='much harder to do than if you have both eyes open. This is due to the fact\\nthat we have binocular (“two eyes”),stereoscopic vision.\\n21.3 Image Processing\\nIn this section, we introduce the main techniques that are used in computer\\nvision systems to process images. The process of image recognition can be\\nbroken down into the following main stages:\\n■ image capture\\n■ edge detection\\n■ segmentation\\n■ three-dimensional segmentation\\n■ recognition and analysis'), Document(metadata={}, page_content='■ recognition and analysis\\nImage capture can be performed by a simple camera (or pair of cameras, to\\ngive stereoscopic vision), which converts light signals from a scene to elec-\\ntrical signals, much as the human visual system does.\\nHaving obtained these light signals, which are simply a set of 1s and 0s\\n(assuming a black and white system—if color is being used then each pixel,\\nor picture element, would be represented by a number indicating that\\npixel’s color).'), Document(metadata={}, page_content='pixel’s color).\\nFor example, look at the images shown in Figure 21.2.'), Document(metadata={}, page_content='21.3 Image Processing 609\\nFigure 21.3\\nA photograph of a hand\\nIn the second image in Figure 21.2, you can see the individual grey-scale\\npixels that made up a portion of the original photograph. Each pixel takes\\non one of a number of possible grey-scale values, often from 0 to 255. Color\\nimages are broken down in the same way, but with varying colors instead of\\ngrey scales. When a computer receives an image from an image sensor'), Document(metadata={}, page_content='(such as a camera), this is the form it receives—a set of pixels. We see in this\\nchapter how these pixels can be interpreted to give the computer an under-\\nstanding of what it is perceiving.\\n21.3.1 Edge Detection\\nThe first stage of analysis, once an image has been obtained, is to determine\\nwhere the edges are in the image. This idea has a sound biological basis, and\\nthere is evidence that edge detection is an important part of the mam-'), Document(metadata={}, page_content='malian visual system. Because objects in the real world almost all have solid\\nedges of one kind or another, detecting those images is the first stage in the\\nprocess of determining which objects are present in a scene.\\nConsider the photograph shown in Figure 21.3 and the image shown in\\nFigure 21.4, which shows the edges detected from this photograph.\\nNote that in Figure 21.4, the edges of the hand have been clearly picked out'), Document(metadata={}, page_content='because these are the highest contrast edges in the photograph. Less clear are\\nthe edges of the fencing from the background, although these are also visible.\\nThe reason that edge detection is useful in mammalian vision is that in\\nmost situations a predator (or prey) can be seen to be contrasted sharply\\nwith its background. Hence, noting the edges in the field of vision will\\nenable an animal to quickly recognize other important animals near it.'), Document(metadata={}, page_content='This, of course, explains why camouflage is such a popular technique in the\\nanimal kingdom. A photo of a brown moth sitting on the brown bark of a'), Document(metadata={}, page_content='610 CHAPTER 21 Machine Vision\\nFigure 21.5\\nAn object illustrating sur-\\nface orientation disconti-\\nnuities (edges between\\nfaces of an object) (These\\nedges are shown in bold.)\\nFigure 21.4\\nThe edges from the photo-\\ngraph in Figure 21.2\\ntree will have very few edges, and they will not be easy to detect, compared\\nwith the edges in an image such as the photograph in Figure 21.3.\\nThere are a number of types of edges that can appear in a visual scene. The'), Document(metadata={}, page_content='edges we can see in Figure 21.4 are mostly depth discontinuities, which are\\nedges that represent the differences in depths between parts of the image.\\nIn this case, most of the edges represent the difference in depth between the\\nhand and the fencing behind it.\\nWhen viewing a three-dimension object such as a block on a table (as\\nshown in Figure 21.5) there are surface orientation discontinuities\\n(marked in bold lines in Figure 21.5), which represent edges between faces'), Document(metadata={}, page_content='of the same object—in other words, such an edge appears because the\\nobjects on either side of the edge are facing different directions.\\nThere are two other types of edges, which are caused by differences in color\\n(or texture) on a single surface ( surface reflectance discontinuities ) and\\nby shadows cast by objects (illumination discontinuities).\\nAll of these types of edges can be (and are) used in image recognition sys-'), Document(metadata={}, page_content='tems to determine the position and nature of objects within a visual field.'), Document(metadata={}, page_content='21.3 Image Processing 611\\n21.3.2 Convolution and the Canny Edge Detector\\nThe simplest way to find edges in an image is to differentiate the image.\\nAreas of consistent color will produce low differentials, and edges that are\\nareas of greatest change will produce greater differentials.\\nUnfortunately, because real images contain a great deal of noise, differenti-\\nation does not work well as an edge detection method because the noise'), Document(metadata={}, page_content='produces extremely high differentials in areas where there is really no edge.\\nA more effective method of edge detection is to use convolution.\\nThe convolution of two discrete functions f(a, b) and g(a, b) is defined\\nas follows:\\nThe convolution of continuous functions f(a, b) and g( a, b) is defined\\nas follows:\\nThe idea of using convolution is to eliminate the effects of noise by\\nsmoothing the image. One way to smooth an image is to convolve it with\\nthe following Gaussian function:'), Document(metadata={}, page_content='the following Gaussian function:\\nAfter convolving the image with the Gaussian function, the resultant can be\\ndifferentiated to determine where the edges are. In fact, it is possible to\\neliminate a step from this process because it can be shown that convolution\\nwith G\\n/H9268(x) and then differentiating the result is the same as convolution\\nwith the differential of G/H9268(x), which is defined as follows:\\nHence, to detect edges in an image, we can convolve the image with G/H11032/H9268(x)'), Document(metadata={}, page_content='and obtain the peaks in the resultant. The peaks will correspond to the\\n′ ( ) = − −\\nGx x e\\nx\\nσ\\nπσ\\nσ\\n2\\n3\\n2\\n2 2\\nGx e\\nx\\nσ\\nπσ\\nσ( ) =\\n−1\\n2\\n2\\n2 2\\nfab g ab fab g a u b v d u d v,, , ,( ) ∗ ( ) = ( ) −−( )\\n−∞\\n∞\\n−∞\\n∞\\n∫∫\\nfab g ab fab g a u b v\\nvu\\n,, , ,( ) ∗ ( ) = ( ) −−( )\\n=−∞\\n∞\\n=−∞\\n∞\\n∑∑'), Document(metadata={}, page_content='612 CHAPTER 21 Machine Vision\\nedges in the image. In doing so, we are using G/H11032/H9268(x) as a filter because we\\nare filtering out everything except the edges in the image.\\nUnfortunately, this method only works for one-dimensional strips of an\\nimage. It will detect an edge in a single line of pixels taken from an image,\\nwhich is useful, but not enough for detecting edges in real images.\\nT o detect edges that might be at any angle in an image, we need to convolve\\nthe image with two filters:'), Document(metadata={}, page_content='the image with two filters:\\nFilter 1: G/H11032\\n/H9268(x) G/H9268(y)\\nFilter 2: G/H11032/H9268(y) G/H9268(x)\\nThe image is convolved with each of these filters, and the results are\\nsquared and added together:\\n(I(x, y) * G/H11032/H9268(x) G/H9268(y))2 + (I(x, y) * G/H11032/H9268(y) G/H9268(x))2\\nwhere I(x, y) is the value of the pixel at location (x, y) in the image.\\nPeaks in the resultant then correspond to edges in the image. Pixels that are'), Document(metadata={}, page_content='considered to be edges are joined with adjacent pixels that are also edges in\\norder to determine the shape and location of the entire edge. This method,\\nknown as the Canny edge detector, produces edges such as the ones shown\\nin Figure 21.4.\\n21.3.3 Segmentation\\nOnce the edges have been detected in an image, this information can be\\nused to segment the image into homogeneous areas. In this case, when we\\nsay that an area of an image is homogeneous, we mean that its color or'), Document(metadata={}, page_content='intensity of shading does not vary dramatically—in other words, there are\\nno edges within the area.\\nThere are other methods available for segmenting an image, apart from\\nusing edge detection. One simple method is thresholding. Thresholding\\ninvolves finding the color of each pixel in an image and then considering\\nadjacent pixels to be in the same area as long as their color is similar\\nenough. This is very similar to edge detection but is used to segment the'), Document(metadata={}, page_content='image, rather than just to find the edges in the image. This is different\\nbecause edge detection will not necessarily produce continuous edges and\\nwill, therefore, not necessarily divide the image into more than one area. In\\nthe photograph shown in Figure 21.3, there are clearly a number of distinct'), Document(metadata={}, page_content='21.3 Image Processing 613\\nFigure 21.6\\nA line drawing of a simple\\nblocks world\\nsegments, but the edges detected in Figure 21.4 divide the image into only\\none or two segments.\\nA similar method for segmenting images is splitting and merging . Split-\\nting involves taking an area that is not homogeneous and splitting it into\\ntwo or more smaller areas, each of which is homogeneous. Merging\\ninvolves taking two areas (e.g., two individual pixels) that are the same as'), Document(metadata={}, page_content='each other, and adjacent to each other, and combining them together into a\\nlarger area. This provides a sophisticated iterative approach to segmenting\\nan image that is often far more reliable than simple thresholding.\\n21.3.4 Classifying Edges in Line Drawings\\nOnce a computer vision system has extracted the edges from an image, it\\nhas something that is rather similar to a line drawing. The line drawings in\\nFigure 21.6 are illustrative of the kind of representations a system might'), Document(metadata={}, page_content='have in observing a simple blocks world.\\nSuch illustrations are easy for us to interpret. For a computer system to\\nunderstand what it is observing, it needs to first classify the edges in the\\ndiagram it has produced.\\nThere are three types of edges:\\n■ A convex edge is an edge between two faces that are at an angle of\\nmore than 180/H11034from each other.\\n■ A concave edge is an edge between two faces that are at an angle of\\nless than 180/H11034from each other.'), Document(metadata={}, page_content='less than 180/H11034from each other.\\n■ Where only one of the two faces that are joined by an edge is visible\\nin the image, the edge is an occluding edge. (An occluding edge is\\na depth discontinuity.)\\nIn Figure 21.7, the edges have been labeled as convex, concave, or occluding\\nusing the traditional notation of + for a convex edge,/H11002for a concave edge,\\nand an arrow for an occluding edge. The direction of the arrow on an'), Document(metadata={}, page_content='614 CHAPTER 21 Machine Vision\\n+\\n+ + ++\\n+\\n+\\n+\\n–\\n–\\n–\\n–\\n–\\n–\\n+\\n+\\n–\\n–\\nFigure 21.7\\nSimple blocks world line\\ndrawing with edges\\nlabeled as convex (+),\\nconcave (/H11002), and\\noccluding (arrow)\\noccluding edge is such that the visible surface is on the right of the direc-\\ntion of the arrow.\\nHaving determined which type each edge in the image is, the system can\\nmake further assessments about the nature, shape, and relative position of\\nthe objects in the picture. Now we need a method for determining which'), Document(metadata={}, page_content='type each edge is.\\nFirst, if we assume that all objects in our image are polyhedral (i.e., all the\\nedges are straight and all surfaces are flat), then we can make the following\\nassumption: A single line will have the same type (convex, concave, or\\noccluding) for its entire length. If you look carefully at Figure 21.7, you will\\nsee that this is the case. No line starts out as concave and ends up convex, or\\nany other combination. This is because the type of an edge is determined'), Document(metadata={}, page_content='by the angle of the faces that the edge joins, and if all lines are straight and\\nall faces flat, then this angle cannot change. (T o see that this is true, try to\\nimagine a polyhedral object where the angle between two faces varies over\\nthe edge that joins them).\\nIn the 1970s, Huffman (1971) showed that further assumptions could be\\nmade that would help in the analysis of line drawings of polyhedral shapes.\\nIf one considers a vertex at which a number of edges meet, it can be shown'), Document(metadata={}, page_content='that there are only a few possible combinations of edges that can make up\\nthat vertex.\\nMost vertices form a point of connection for three flat faces. Such vertices\\nare called trihedral vertices. There are only 16 possible arrangements of\\nedges that can make up trihedral vertices in the real world, and these are\\nshown in Figure 21.8.\\nAs you can see from Figure 21.8, there are a number of labelings of a trihe-\\ndral vertex that are simply not possible.'), Document(metadata={}, page_content='21.4 Using Texture 615\\n+\\n++\\n+\\n+\\n++\\n++\\n–\\n––\\n–\\n–\\n––\\n–+\\n––\\nFigure 21.8\\nThe 16 possible ways to\\nlabel trihedral vertices\\nAs a result, in analyzing a scene such as the one shown in Figure 21.6, a\\ncomputer vision system can use Huffman’s 16 trihedral vertices as con-\\nstraints to limit the possible labelings for the diagram.\\nThe Waltz algorithm does so by selecting a possible label for one junction,\\nfrom the list shown in Figure 21.8, and then moving onto an adjacent junc-'), Document(metadata={}, page_content='tion and attempting to apply a labeling to this junction. If none is possible,\\nthe algorithm backtracks and tries a different labeling for a previous junc-\\ntion. Hence, the method applies depth-first search to the structure until a\\nlabeling is found that ensures that all junctions have valid labelings.\\nIn some cases, there will be more than one possible labeling. In other words,\\nthe image is ambiguous. In such cases, additional information must be used.'), Document(metadata={}, page_content='Often shading information can be used to provide additional constraints.\\n21.4 Using Texture\\nT exture is a vital aspect of the visual world. It helps us to identify a wide\\nrange of facets of what we see—it does not just tell us the materials that\\nthings are made of; it also gives us information about movement and shapes.'), Document(metadata={}, page_content='616 CHAPTER 21 Machine Vision\\nFigure 21.9\\nFour different textures\\nT exture, in the visual sense, can be defined as the pattern that we perceive\\non the surface of an object or in an area. The photos in Figure 21.9 show a\\nvariety of textures.\\nClearly, the textures of the images in Figure 21.9 show us that we are look-\\ning at grass, pebbles, clouds, and roofing tiles. As we will see, the textures\\nalso tell us a great deal more than this.\\n21.4.1 Identifying Textures'), Document(metadata={}, page_content='21.4.1 Identifying Textures\\nT o make use of texture information from an image, a computer system must\\nfirst analyze the image and determine the nature of its texture or textures.\\nThe simplest type of texture is the texture we usually expect to find on\\nblocks in the simple blocks world—this is a completely plain, vanilla tex-\\nture, which could be described as textureless, or smooth. In dealing with\\nthe blocks world, we tend to assume that our blocks are textureless and'), Document(metadata={}, page_content='therefore pay no attention to texture. In fact, of course, in a real blocks'), Document(metadata={}, page_content='21.4 Using Texture 617\\nworld, the blocks must be made of something (wood, metal, plastic) and\\nmust therefore have a texture.\\nIn examining a blocks world scene, texture usually is not terribly important\\nbecause the shapes are so simple that determining their position, orienta-\\ntion, and so on can be done using edge detection and simple mathematical\\nalgorithms. In more complex environments, a system must make use of\\ntexture to make these kinds of analyses.'), Document(metadata={}, page_content='texture to make these kinds of analyses.\\nThere are a number of statistical methods that can be used to categorize a\\nparticular texture in an image. We will now examine one such method,\\nbased on the use of cooccurrence matrices.\\nThe idea of this method is to determine the relationships between pixels in\\nthe image of particular intensities. We will represent our image as a matrix\\nof pixel values, which for this example will range from 0 to 4. Let us define'), Document(metadata={}, page_content='a matrix P, which we will use for this example as the matrix representing\\nthe intensity values of the grey pixels in our image:\\nClearly,P defines a rather uninteresting image, but for a real image the matrix\\nwould be significantly larger and would have a greater range of values.P will\\nsuffice for us to illustrate this statistical method for analyzing textures.\\nEach pixel in P is defined as P(x, y), so that for example:\\nP(0, 0) = 1\\nP(1, 1) = 3\\nP(3, 2) = 3'), Document(metadata={}, page_content='P(0, 0) = 1\\nP(1, 1) = 3\\nP(3, 2) = 3\\nWe will now define a new matrix,D, which is defined as follows:\\nD(m, n) is the number of pairs of pixels in P for which\\nP(i, j) = m\\nP(i + /H9254i, j + /H9254j) = n\\nwhere i and j are any pixels in P, and /H9254i and /H9254j are small increments defined\\nfor this particular matrix D.\\nP =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\uf8fa\\n1032\\n2301\\n1413\\n3224'), Document(metadata={}, page_content='618 CHAPTER 21 Machine Vision\\nIn other words, D defines how likely it is that any two pixels a particular\\ndistance apart (/H9254i and /H9254j) will have a particular pair of values.\\nWe will see how this works for our matrix.\\nLet us first define (/H9254i, /H9254j) = (1, 1). In other words, we are interested in pairs\\nof pixels that are diagonally one pixel apart.\\nWe can now define the matrix D.\\nD(0, 0) is equal to the number of pairs of pixels inP that are (1, 1) apart, and'), Document(metadata={}, page_content='which are both valued 0. Looking atP, we can see that there is one such pair:\\nP(1, 0) = 0\\nP(2, 1) = 0\\nHence, D(0, 0) = 1.\\nD(1, 0) is equal to the number of pairs of pixels in P that are (1, 1) apart\\nand which are values 1 and 0, respectively. Y ou should be able to see that no\\nsuch pairs exist in P.H e n c e ,D(1, 0) = 0.\\nWe can define the whole of matrix D in the same way. D is a 5 /H110035 matrix\\nbecause there are five possible pixel values in P:'), Document(metadata={}, page_content='Let us see now the difference when we apply this method to a different matrix:\\nClearly this matrix represents an image where the texture is far more\\nnoticeable than in the previous matrix, P. We should expect to see this rep-\\nresented in the corresponding matrix D\\n1, which is defined as follows:\\nP1\\n1043\\n2104\\n3210\\n4321\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\uf8fa\\nD =\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n10010\\n00111\\n00001\\n01000\\n00100'), Document(metadata={}, page_content='21.4 Using Texture 619\\nFinally, let us examine the following matrix:\\nThis matrix, P2, is clearly similar to P1, but reflected about the Y-axis. Let us\\nsee how D2 turns out:\\nThe difference between D2 and D1 is as we would expect. The values in D1\\nare on the main diagonal, whereas in D2 they are on two minor diagonals.\\nThis reflects the relationship between the vector (/H9254i, /H9254j) = (1, 1) and the tex-\\ntures in the images.'), Document(metadata={}, page_content='tures in the images.\\nOne extension to this method is to assume that we should not distinguish\\nbetween D(m, n) and D(n, m). Hence, we produce the cooccurrence\\nmatrix, C, which is defined as follows:\\nC = D + DT\\nWhere DT is the transposition of matrix D.\\nC has the property that C(m, n) = C(n, m) because\\nC(m, n)=  D(m, n) + D(n, m)\\n= D(n, m) + D(m, n)\\n= C(n, m)\\nD2\\n00300\\n00020\\n00001\\n10000\\n02000\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\nP2\\n3401\\n4012\\n0123\\n1234\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\uf8fa\\nD1\\n20000\\n03000\\n00200'), Document(metadata={}, page_content='=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\uf8fa\\nD1\\n20000\\n03000\\n00200\\n00010\\n00001\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa'), Document(metadata={}, page_content='620 CHAPTER 21 Machine Vision\\nThis matrix, C, gives us a useful statistical analysis of the texture contained\\nwithin the image, relative to the vector ( /H9254i, /H9254j) that we have chosen. By\\nusing a number of different ( /H9254i, /H9254j) vectors, we can determine more infor-\\nmation about the texture.\\n21.4.2 Structural Texture Analysis\\nAn alternative to the statistical approach is to analyze textures using a\\nstructural approach, based on units of texture called texels. A texel is a sin-'), Document(metadata={}, page_content='gle texture element or a piece of image that is repeated throughout the\\nimage to produce the texture. Looking at the final photo in Figure 21.9, of\\nroofing tiles, we can see that this texture is made up of a single tile, repeated\\nover the image. This tile is the texel. Note that in fact, due to perspective\\nand other distortions, the texels will not all be identical (e.g., perspective\\nwill cause them to have different shapes and sizes, and distortions will also'), Document(metadata={}, page_content='be caused by mapping the texture onto a curved surface).\\nT exel analysis thus involves searching for repeated components within an\\nimage, taking into account distortions such as elongation, rotation, com-\\npression, and so on. As we see in the next sections, once the texture has\\nbeen determined in this way, this information can be used to determine a\\nnumber of useful properties about the object whose texture we are\\nlooking at.\\n21.4.3 Determining Shape and Orientation from Texture'), Document(metadata={}, page_content='Usually, when examining a texture, we are able to use information gained\\nfrom the texture to determine the shape of the surface. This assumes, of\\ncourse, that the picture involves a surface. This is certainly the case for three\\nof the images in Figure 21.9 (the pebbles, the grass, and the tiles are all\\nplaced on some surface). For the fourth image, of clouds, the texture tells us\\nsomething about the “surface” of the clouds, although this is somewhat'), Document(metadata={}, page_content='illusory because the clouds are not solid. Still, the techniques used by com-\\nputer vision systems will draw mostly the same conclusions about the shape\\nand the “surface” of the clouds that we would when examining that picture.\\nNotice how the variation of the shape of the bricks in the photo in Figure\\n21.10 enables us to see, without any external help, that the wall in the pic-\\nture is curved. The main reason that we can tell that the wall is curved is the'), Document(metadata={}, page_content='foreshortening of the bricks on the right-hand side of the picture, suggest-'), Document(metadata={}, page_content='21.4 Using Texture 621\\nFigure 21.10\\nA section of curved wall,\\nshowing how surface\\nshape can be determined,\\nto some extent, from the\\ntexture\\nFigure 21.11\\nAn illustration showing\\nhow the texture on the\\nsurface of a golf ball helps\\nus to understand the\\nshape of the ball\\ning that they are further away, and seen from a sharper angle, than those on\\nthe left-hand side of the picture.\\nIn a similar way, a photograph of a golf ball can be used to determine the'), Document(metadata={}, page_content='shape of the golf ball, by examining the way in which the small circular\\nindentations on the surface of the ball vary in apparent shape. See the illus-\\ntration in Figure 21.11, for example.\\nThese circles on the surface of the ball, and the bricks in Figure 21.10, are\\nthe texels that make up the texture, which we use to determine the shape of\\nthe object in the picture.\\nAlthough the image in Figure 21.11 is flat, it is translated in our minds into'), Document(metadata={}, page_content='a sphere because that is the simplest explanation for the way the circles dis-\\ntort as they get farther from the center of the image.'), Document(metadata={}, page_content='622 CHAPTER 21 Machine Vision\\nFigure 21.12\\nA photograph of a section\\nof wall. The angle between\\nthe perpendicular to this\\nwall and the camera can\\nbe determined by examin-\\ning the distortion of the\\ntexels (in this case, the\\nindividual bricks).\\nY -axis\\nX-axis\\nZ-axis\\nP\\nImage plane\\nSurface Normal (n)\\nσ\\nτ\\nFigure 21.13\\nShowing how slant (/H9268) and\\ntilt (/H9270) are measured.\\nA simple way to extract shape from a texture of this kind is to assume that'), Document(metadata={}, page_content='each texel is flat (which with a curved surface is probably not true but is a rea-\\nsonable approximation). By determining the extent and direction of distor-\\ntion of a given texel, the slant of the texel can be determined and a\\nperpendicular line projected from it. Once perpendiculars have been deter-\\nmined for all the texels in an image, the surface shape has been determined.\\nIn the same way, the orientation of a flat surface, such as the section of wall'), Document(metadata={}, page_content='shown in Figure 21.12, can be determined.\\nA similar, though less obviously intuitive, method can be applied to tex-\\ntures such as those shown in Figure 21.9. In these cases, the orientation of\\nthe surfaces beneath the grass, pebbles, and roofing tiles would be deter-\\nmined, as would the apparent shape of the clouds.\\nWhen determining orientation, we are interested in two factors: slant and\\ntilt. Slant and tilt are measured between a vector perpendicular to the sur-'), Document(metadata={}, page_content='face of an object and the z- and x-axes. This is illustrated in Figure 21.13.'), Document(metadata={}, page_content='21.5 Interpreting Motion 623\\nSlant, which is usually written as the Greek letter sigma— /H9268, is measured\\nbetween the surface normal (the vector n, in Figure 21.13), which is per-\\npendicular to the object we are observing at the point we are interested in,\\nand the z-axis. This is shown in Figure 21.13.\\nWe measure tilt (often written as the Greek letter tau— /H9270) as the angle\\nbetween the x-axis and the projection, p, of the normal vector n onto the'), Document(metadata={}, page_content='plane of the image. In other words, the tilt is an apparent angle, determined\\nby the position and orientation of the viewer.\\nIn the diagram in Figure 21.13, we are measuring slant and tilt of a specific\\npoint on the surface of a sphere. This point is the point on the surface from\\nwhich the normal vector, n, has been measured.\\n21.5 Interpreting Motion\\nOne of the most important aspects of mammalian vision is the ability to'), Document(metadata={}, page_content='detect (and thus react to) motion. For hunters, it is important to be able to\\nspot prey and follow it as it attempts to flee, and for the prey, it is important\\nto detect the hunter as quickly as possible. In a world full of confusing\\nvisual information, most animals (including humans) use motion to pro-\\nvide additional information about what is being seen.\\nSimilarly, for an agent that has the ability of vision, it is important to be\\nable to detect motion.'), Document(metadata={}, page_content='able to detect motion.\\nThere are two main types of motion that an observer is interested in—\\nmotion of other objects and the apparent motion of the environment\\ncaused by the observer’s own motion.\\nWe will start with the latter type of motion—the apparent motion caused\\nby the movement of the camera or other image capture device.\\nThe photograph in Figure 21.14, for example, was taken using a camera on\\na moving train. A subsequent photo, taken a second later, would show that'), Document(metadata={}, page_content='the buildings, trees, and other objects in the photograph had apparently\\nmoved. Of course, this is in fact due to the fact that the train, and therefore\\nthe camera, has moved. This apparent motion in an image is known as\\noptical flow , and the vectors that define the apparent motion make up\\nwhat is known as the motion field. Some of these vectors have been drawn\\nonto the photograph in Figure 21.14. The photograph was taken from the'), Document(metadata={}, page_content='624 CHAPTER 21 Machine Vision\\nFigure 21.14\\nA photograph taken from a\\ntrain, illustrating the idea\\nof the motion field. Some\\nof the motion field vectors\\nhave been drawn in as\\narrows, moving away from\\nthe camera.\\nback of the train, so the vectors show that the objects are apparently mov-\\ning away from the camera.\\nThe direction of the motion field will clearly depend on the direction in\\nwhich the camera is moving. If the photograph in Figure 21.14 had been'), Document(metadata={}, page_content='taken from a car crossing a level crossing, the arrows would have gone hor-\\nizontally across the image, for example, instead of heading toward the van-\\nishing point, the point toward which perspective causes all parallel lines in\\nthe image to converge.\\nHence, by examining a sequence of images taken from a moving camera, if\\nthe direction and speed of the optical flow can be determined, then this can\\nprovide information about the direction and speed of travel of the camera,'), Document(metadata={}, page_content='relative to the background.\\nIt is possible to estimate the nature of the motion field, and thus the optical\\nflow, in a sequence of images by comparing the features of the images. First,\\nwe assume that the objects in a sequence of images will not themselves\\nchange, and so any changes that occur to them are caused by the movement\\nof the camera. This will clearly not apply if there are moving objects (e.g.,'), Document(metadata={}, page_content='21.6 Making Use of Vision 625\\ncars, people, animals) in the image, but it will still apply to the majority of the\\nfeatures within most images, and the anomalies can be dealt with separately.\\nBy computing common points in a sequence of images, we can thus calcu-\\nlate the optical flow vectors and thus determine the speed of motion of the\\ncamera. This technique can also be applied in cases where the camera is still\\nand is capturing a sequence of images of a moving or rotating object.'), Document(metadata={}, page_content='21.6 Making Use of Vision\\nWe have thus far described techniques that can enable a computer system\\nto extract information from a visual scene that has been recorded on a\\ndevice such as a camera. We will now look at ways in which this informa-\\ntion can be used for practical purposes.\\nImages such as the one shown in Figure 21.14 might be used to control the\\nmotion of a vehicle. In this case, the visual information could be used to'), Document(metadata={}, page_content='control the speed of travel of the train. If another train or other obstacle\\nappeared on the tracks in front of the train, the brakes could be applied, for\\nexample. A more complex system could control the motion of a car, which\\ncould be designed to negotiate traffic, stop at traffic lights, and avoid pedes-\\ntrians, other vehicles, and the sidewalk.\\nOne of the most common uses of machine vision in robotic agents is to\\nidentify objects in the agent’s path. In simple cases, these objects will be'), Document(metadata={}, page_content='limited to blocks of various shapes and sizes, but in real-world systems, the\\nobjects could be almost anything.\\nThe main task is therefore to map the image that has been received to an\\ninternal representation of an object. The method that is usually used\\ndepends on the principle that there are some properties of any object that\\nare invariant. In other words, whatever angle you view the object from,\\nwhatever lighting conditions it is in, whatever changes occur to its shape,'), Document(metadata={}, page_content='the invariant properties will remain constant.\\nIn the case of a sphere, this is fairly simple: the sphere will appear in a two-\\ndimensional image as a circle under almost any conditions. This is of course\\ncomplicated by the possibility of other objects obscuring the object we are\\nlooking at and also by the complications of texture. A sphere with a checker-\\nboard pattern on it might look rather different when placed against a'), Document(metadata={}, page_content='checkerboard background than if it were placed against a plain background.'), Document(metadata={}, page_content='626 CHAPTER 21 Machine Vision\\nThe method that is usually used to identify objects is known as the parts\\ndecomposition method, which involves breaking an object into its constituent\\nparts and then attempting to identify those parts in the image. For example, a\\ncat could be broken down into a head, eyes, mouth, tail, legs, fur, etc.\\nAnother method, which does not work so well with cats, but works well\\nwith more rigid objects, is to assume that an object will look the same once'), Document(metadata={}, page_content='a set of transformations (e.g., rotation, translation, and increase or decrease\\nin size) are applied. Hence, if an image is found that looks somewhat like a\\ncube, but not exactly the same as the image of a cube that the system has\\nbeen trained with, the image can be transformed using rotation, transla-\\ntion, and resizing, until it matches more closely the training image. This\\nmethod is known as the alignment method. This method thus involves'), Document(metadata={}, page_content='finding an object whose internal representation matches that being seen in\\nthe image after applying one or more allowable transformations.\\nHaving identified an object, the agent can use its behavior model (defined\\nperhaps using the subsumption architecture—see Chapter 19) to deter-\\nmine what to do—it may need to move toward the object to examine it in\\nmore detail, it may want to avoid the object, or it may want to pick it up. If'), Document(metadata={}, page_content='the agent cannot detect objects using vision, then it can be very difficult for\\nit to decide what to do. Hence, agents that need to interact with objects in\\nthe real world in any way more complex than simply moving past them\\nneed to be able to receive some kind of visual input from the world and\\nthen to analyze that visual information.\\n21.7 Face Recognition\\nOne very popular area of computer vision at present is the study of automatic'), Document(metadata={}, page_content='face recognition. This problem is an excellent example of the kinds of prob-\\nlems that Artificial Intelligence techniques are usually applied to: it is a prob-\\nlem that humans find so simple that we take it for granted, yet it is a problem\\nthat traditional computer science has found almost impossible to solve.\\nThe difficulties with automatic face recognition are numerous. First of all,\\nthe conditions in which a face can be seen, such as lighting, distance from'), Document(metadata={}, page_content='camera to face, and angle, can dramatically alter the appearance of the face.\\nThis problem is faced with most object recognition systems. Face recogni-\\ntion is further complicated by the fact that human faces are so flexible and\\nso capable of being altered. Facial expressions are one complexity, but peo-'), Document(metadata={}, page_content='21.7 Face Recognition 627\\nple also are able to grow beards; cut or grow their hair; wear glasses, sun-\\nglasses, hats, and earrings; and grow older, all of which can significantly\\naffect the appearance of a face.\\nAs a result, identifying invariant properties of a given face is an important\\nfirst step in automating the process of face recognition. These properties\\nneed to be invariant regardless of distance, angle, orientation, and lighting,'), Document(metadata={}, page_content='but also regardless of what has happened to the face—whether it is wearing\\nglasses, whether it has its eyes shut or open, and so on.\\nOne early approach to face recognition was to identify particular facial fea-\\ntures, such as eyes, nose, mouth, eyebrows, and so on, and to store informa-\\ntion about the relative positions of those features. These features could be\\ncompared in a new face to determine if it is one that has been seen before.'), Document(metadata={}, page_content='This method works in some circumstances, but is not particularly robust.\\nOne problem with this method is that it assumes that the best way to tell\\nthe difference between two faces is to note the locations of the features such\\nas eyes, mouth, and so on. This might not be the case.\\nThis observation led to another face recognition method that uses eigen-\\nfaces. Eigenfaces are based on the idea of principle component analysis .\\nPrinciple component analysis is an important idea in computer science.'), Document(metadata={}, page_content='The idea is that to learn to recognize any type of items of data, the best way\\nis to determine the features of the data that vary most from one item to\\nanother. This idea was applied, for example, in Chapter 20, when we saw\\nthat the way in which a system could search for responses to a query from a\\ncorpus of text was to treat the words that are most infrequent within the\\ncorpus as being the most important in queries. In the same way, if we look'), Document(metadata={}, page_content='at a selection of ten faces and note that the position of the tip of the nose\\nrelative to the end of the chin is the feature that varies the most, then this is\\na principle component and should be treated as an important feature for\\nidentifying faces. This is the idea behind eigenfaces.\\nThe eigenfaces are the components chosen to represent the faces in the\\ntraining set. These features are chosen as being the features that provide the'), Document(metadata={}, page_content='greatest differentiation between the faces in the training set and thus pro-\\nvide the greatest likelihood of giving a correct match when presented with\\na new face.\\nThe eigenfaces can be viewed graphically and tend to look like morphed\\nimages of several faces, which is indeed what they are. When attempting to'), Document(metadata={}, page_content='628 CHAPTER 21 Machine Vision\\nmatch a single face, a number of these eigenfaces are combined together,\\nand it is the manner in which these faces are combined together that is used\\nto identify which face is being viewed.\\nThis method can also be thought of in terms of vectors. The eigenfaces are\\nthe vectors (or eigenvectors) that form the principle components of the\\ntraining data, and thus define the face space. When a new face is examined,'), Document(metadata={}, page_content='it is constructed as a sum of some combination of the eigenvectors, and this\\nvector is compared with the vectors that were already calculated for the\\nfaces in the training data. The face with the closest vector is the match.\\nUsually a number of faces are used for each person in the training set, with\\na variety of expressions and with different additional elements such as hats,\\nglasses, and so on. In this way, the eigenfaces method provides a very robust'), Document(metadata={}, page_content='way of recognizing faces. In experimentation, this method has given an\\naccuracy of over 90% at matching faces from a small database of training\\nimages (around 50 images).\\n21.8 Chapter Summary\\n■ The mammalian vision system is a remarkably sophisticated system,\\nand most computer vision systems are based to some extent on it.\\n■ Edge detection is often the first stage in image-processing systems.\\nEdge detection involves determining the location of high-fre-'), Document(metadata={}, page_content='quency areas of an image, which usually correspond to edges or\\nchanges in depth in the image.\\n■ Convolution is used by the Canny edge detector to find edges in\\nimages.\\n■ Once edges have been detected in an image, the image is usually\\nsegmented into homogeneous areas, which correspond roughly to\\nparticular objects or textures.\\n■ Edges in a three-dimensional line drawing can be classified using\\nthe Waltz algorithm.\\n■ T exture can be used to provide a great deal of information about a'), Document(metadata={}, page_content='scene, such as shape and orientation.\\n■ Detecting and interpreting motion is an important part of the\\nmammalian vision system and is also useful in many computer'), Document(metadata={}, page_content='21.10 Exercises 629\\nimage recognition systems (particularly in robotic agents that need\\nto navigate in the real world).\\n■ Face recognition is one of the hardest problems of image recogni-\\ntion, but one in which a great deal of success has already been\\nachieved. One popular method is to use eigenfaces, which are\\nbased on the idea of principle component analysis.\\n21.9 Review Questions\\n21.1 Why does it make sense to model computer vision systems on the'), Document(metadata={}, page_content='human vision system? Can you think of any suitable alternative\\nmodels?\\n21.2 Why is edge detection used as an early stage of computer vision?\\nCan you think of any situations in which edge detection methods\\nwould fail completely?\\n21.3 Explain how convolution is used to detect edges in images.\\n21.4 What is the purpose of segmentation?\\n21.5 Explain the purpose of the Waltz algorithm. Describe in detail how\\nit works.\\n21.6 Why is texture so important for computer vision systems? What dif-'), Document(metadata={}, page_content='ferences would there be if the world had no texture and all objects\\nwere smooth and uniformly colored? Would this make it easier or\\nharder for machine vision systems? What if there were no shadows\\nand everything was uniformly illuminated from all sides as well?\\n21.7 How do computer vision systems make use of motion?\\n21.8 Explain the way that eigenfaces are used in face recognition systems.\\n21.10 Exercises\\n21.1 Apply the Waltz algorithm using pen and paper to the three-'), Document(metadata={}, page_content='dimensional scene shown in Figure 21.6. Is it possible that you will\\nend up with a different labeling from that shown in Figure 21.7? If\\nyour answer was yes, how could this happen? If your answer was\\nno, why not, and can you imagine any situation in which there is\\nmore than one possible labeling for such an image?'), Document(metadata={}, page_content='630 CHAPTER 21 Machine Vision\\n21.2 Implement an edge detection system in the programming language\\nof your choice. Y ou will need first to find a way to obtain pixel data\\nfrom an image and convert this into a two-dimensional array of\\npixels. Y our edge detection system should be capable of outputting\\nan image showing the edges.\\n21.11 Further Reading\\nThere are many books available on the subject of image recognition, com-\\nputer vision, face recognition, and other related subjects. Shapiro (2001)'), Document(metadata={}, page_content='and Forsyth (2002) both provide excellent coverage of the subject. Nalwa\\n(1993) provides a very readable introduction. Hoffman (1998) provides a\\ndifferent perspective, with a cognitive scientist’s view of human vision.\\n2D Object Detection and Recognition: Models, Algorithms, and Networks ,b y\\nY ali Amit (2002 – MIT Press)\\nIntelligent Machine Vision: Techniques, Implementations and Applications ,\\nby Bruce G. Batchelor and Frederick M. Waltz (2001 – Springer V erlag)'), Document(metadata={}, page_content='Advances in Image Understanding: A Festschrift for Azriel Rosenfeld , edited\\nby Kevin W. Bowyer and Narendra Ahuja (1996 – Wiley IEEE Press)\\nNeural Networks for Vision and Image Processing, edited by Gail A. Carpen-\\nter and Stephen Grossberg (1992 – MIT Press)\\nMachine Vision: Theory, Algorithms, Practicalities , by E. R. Davies (1996 –\\nAcademic Press)\\nThree-Dimensional Computer Vision, by Olivier Faugeras (1993 – MIT Press)\\nComputer Vision: A Modern Approach, by David A. Forsyth and Jean Ponce'), Document(metadata={}, page_content='(2002 – Prentice Hall)\\nDynamic Vision: From Images to Face Recognition , by Shaogang Gong and\\nStephen J. McKenna (2000 – Imperial College Press)\\nComputer and Robot Vision (Volume II), by Robert M. Haralick and Linda\\nG. Shapiro (2002 – Pearson Education)\\nVisual Intelligence: How We Create What We See , by Donald D. Hoffman\\n(1998 – W. W. Norton & Company)\\nRobot Vision, by Berthold K. Horn (1986 – McGraw Hill Higher Education)\\nMachine Vision, by Ramesh Jain, Rangachar Kasturi, and Brian G. Schunck'), Document(metadata={}, page_content='(1995 –McGraw Hill)'), Document(metadata={}, page_content='21.11 Further Reading 631\\nComputer Vision and Fuzzy Neural Systems , by Arun D. Kulkarni (2001 –\\nPrentice Hall)\\nA Guided Tour of Computer Vision , by Vishvjit S. Nalwa (1993 – Addison\\nWesley)\\nFeature Extraction in Computer Vision and Image Processing ,b y  M a r k  S .\\nNixon and Alberto Aguado (2002 – Butterworth-Heinemann)\\nAlgorithms for Image Processing and Computer Vision, by J. R. Parker (1996 –\\nJohn Wiley & Sons)\\nLearning-Based Robot Vision, edited by Josef Pauli (2001 – Springer V erlag)'), Document(metadata={}, page_content='Computer Vision, by Linda G. Shapiro and George C. Stockman (2001 –\\nPrentice Hall)\\nImage Processing: Analysis and Machine Vision , by Milan Sonka, Vaclav\\nHlavac, and Roger Boyle (1998 – Brooks Cole)\\nIntroductory Techniques for 3-D Computer Vision, by Emanuele Trucco and\\nAlessandro V erri (1998 – Prentice Hall)\\nHuman Face Recognition Using Third-Order Synthetic Neural Networks ,b y\\nOkechukwu A. Uwechue and Abhijit S. Pandya (1997 – Kluwer Academic\\nPublishers)'), Document(metadata={}, page_content='Publishers)\\nFace Recognition: From Theory to Applications , by Harry Wechsler (1998 –\\nSpringer V erlag)'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='Glossary\\nThis glossary includes definitions and descriptions of the most important\\nterms used in this book.\\nA\\nAbduction\\nA *nonmonotonic form of reasoning that helps us to find plausible expla-\\nnations for observed phenomena.\\nAccepting state\\nA *state in a *finite state machine that represents a “yes” response.\\nAcquaintance algorithm\\nA vector-based approach to identifying languages using *n-grams.\\nAction description language (ADL)\\nA more expressive variation of the *STRIPS planning language.'), Document(metadata={}, page_content='Activation function\\nThe function applied to the inputs of a *neuron in a *neural network. The\\noutput of this function is compared with the *activation level to determine\\nif the neuron fires.\\nActivation level\\nThe level of output that a *neuron must reach in order to fire.'), Document(metadata={}, page_content='634 Glossary\\nActivity product rule\\nThe rule used in *neural networks that use *Hebbian learning to determine\\nhow the *weights between *neurons change.\\nAdmissibility\\nA *heuristic method is defined as admissible if it never overestimates the\\ncost of getting from a given *state to a *goal state.\\nAdversarial methods\\nMethods used by game-playing systems to search a *game tree for a *path\\nthat will lead to a win over an opponent.\\nAgent'), Document(metadata={}, page_content='that will lead to a win over an opponent.\\nAgent\\nAn entity (usually a software entity) that exists to assist humans in carrying\\nout some task or solving some problem. Types of agents include *software\\nagents, *interface agents, *mobile agents, and *information agents.\\nAgent team\\nA group of *agents that collaborate together to reach a common goal.\\nAlphabet\\nThe set of symbols available for a logical system.\\nAlpha–beta pruning'), Document(metadata={}, page_content='Alpha–beta pruning\\nA method used to make searching a *game tree more efficient. It relies on\\nthe principle that if a part of a game tree is going to give a bad result, it is\\nnot worth further examining that part of the tree.\\nAmbiguity\\nThe problem that an *utterance in a *human language can have more than\\none possible meaning. Types of ambiguity include lexical, semantic, syntac-\\ntic, referential, and local.\\nAncestor'), Document(metadata={}, page_content='tic, referential, and local.\\nAncestor\\nAn ancestor, a, of a *node,n, in a *tree is a node that is further up a *path in\\nthe tree than n. n is a *descendant of a.\\nAnd-goal\\nAnd-goals are *subgoals in a *goal tree that must all be satisfied in order to\\nsatisfy the goal tree.'), Document(metadata={}, page_content='Glossary 635\\nAnd-node\\nA *node that represents an *and-goal.\\nAnd–or tree\\nSee *goal tree.\\nAntecedent\\nThe part of the *rule that comes before the *implication. In the rule A → B,\\nA is the antecedent. See *consequent.\\nArtificial Intelligence\\nThe subject of this book. See Chapters 1 to 21 for more information. See\\nalso *weak AI, *strong AI.\\nArtificial Life\\nMethods modeled on life that often use *emergent behavior to find better\\nsolutions to problems than can be found using traditional methods.'), Document(metadata={}, page_content='Artificial neural network\\nSee *neural network.\\nAssociativity\\nA property of mathematical and logical operators. Operator o is associative\\nif A o (B o C) ≡ (A o B) o C.\\nAtomic action\\nIndividual actions used by *planning systems. An atomic action can consist\\nof a number of smaller actions, but it is treated as one indivisible action for\\nthe purpose of constructing a plan.\\nAtomic formula\\nA *well-formed formula of the form P(x1, x2, x3, ...,x n).\\nAttractor network\\nSee *recurrent network.'), Document(metadata={}, page_content='Attractor network\\nSee *recurrent network.\\nAugmented finite state machine (AFSM)\\nA type of *finite state machine that uses *situated action rules. AFSMs are\\nused in Brooks’s *subsumption architecture.'), Document(metadata={}, page_content='636 Glossary\\nAugmented transition network (ATN)\\nA type of *transition network that is used for *parsing sentences. An ATN\\nhas *procedures and tests that are attached to its arcs.\\nAutoassociative memory\\nA type of memory that can recognize an object but cannot associate one\\nobject or piece of data with another. A *Hopfield network is an autoasso-\\nciative memory. See *heteroassociative memory.\\nAutonomy\\nA property of *agents. An autonomous agent has the ability to act inde-'), Document(metadata={}, page_content='pendently to some extent of its owner or programmer. This is often a\\ndesired property for *intelligent agents.\\nAxon\\nThe part of a *neuron in the human brain that provides output to other\\nneurons via a *synapse.\\nB\\nBackpropagation\\nA method used to modify the *weights in a multilayer *neural network.\\nErrors at the output layer are fed back through the network, correcting the\\nweights. Over a number of iterations, this usually leads to a network that'), Document(metadata={}, page_content='gives mostly correct responses to the *training data.\\nBackus–Naur form (BNF)\\nA language used to define the grammar of *formal and *informal lan-\\nguages. BNF uses *terminal and *nonterminal symbols, and *rewrite rules\\nthat express how sentences can be legally built up out of terminal symbols.\\nBackward chaining\\nSee *goal-driven search.\\nBayesian belief network\\nAn acyclic directed *graph, where the *nodes in the graph represent evi-'), Document(metadata={}, page_content='dence or hypotheses, and where an edge that connects two nodes represents\\na dependence between those two nodes. Each node is labeled with a set of\\nprobabilities that express how that node depends on other nodes.'), Document(metadata={}, page_content='Glossary 637\\nBayes’ optimal classifier\\nA system that uses *Bayes’ theorem to learn to classify data. It can be shown\\nthat this classifier is optimal, which means that it provides the best possible\\nmechanism for classifying data.\\nBayes’ theorem\\nBayes’ theorem can be used to calculate the probability that a certain event\\nwill occur or that a certain proposition is true, given that we already know\\na related piece of information. It is written as follows:\\nBelief desire intention architecture (BDI)'), Document(metadata={}, page_content='Belief desire intention architecture (BDI)\\nAn architecture used by *agents that uses beliefs about the world and\\ndesires to develop intentions about how the agent should behave.\\nBidirectional associative memory (BAM)\\nA *neural network used to associate items from one set with items in\\nanother set.\\nBinary operator\\nA logical or mathematical operator that takes two arguments, such as logi-\\ncal and (\\n∧) and logical or (∨). See *unary operator.\\nBivalent logic'), Document(metadata={}, page_content='Bivalent logic\\nA logical system that has two *truth values. Classical logic is bivalent\\nbecause a logical expression can either be true or false. See *multivalent\\nlogic and *fuzzy logic.\\nBlackboard architecture\\nA method for structure knowledge representation that combines informa-\\ntion from a number of knowledge sources (such as human experts) in\\norder to solve a problem.\\nBlind search method\\nA *search method that does not use *heuristics. Also known as *uninformed'), Document(metadata={}, page_content='search. *Depth-first search and *breadth-first search are blind search methods.\\nPB A\\nPA B PB\\nPA( ) = ( ) ⋅ ( )\\n( )'), Document(metadata={}, page_content='638 Glossary\\nBlocks world\\nA scenario that is used to explain planning techniques. The blocks world\\nconsists of a table with a number of blocks, which are usually cubes. The\\nblocks world has very simple properties, and there are usually a limited set\\nof actions that can be taken to interact with the world, such as “pick up”\\nand “put on.”\\nBottom up\\nAn approach to solving problems that involves first solving the smaller sub-\\nproblems, and repeatedly combining these solutions together until a com-'), Document(metadata={}, page_content='plete solution is found. See *top down.\\nBounded lookahead\\nA method used when searching *game trees that involves cutting off\\n*search when a specified depth in the tree is reached. This is particularly\\nuseful in games such as chess or Go that have very deep search trees.\\nBound variable\\nA bound variable in a logical expression is one that has been quantified\\nwithin the same scope. For example, in the expression \\n∀x(x → y), x is'), Document(metadata={}, page_content='∀x(x → y), x is\\nbound because it is quantified by the ∀ *quantifier. See *free variable.\\nBraitenberg vehicle\\nA type of *robotic vehicle that is used in thought experiments to study the\\nnature of intelligence. Braitenberg vehicles range from very simple robots\\nthat follow or avoid light to more complex systems. None of them have any\\nreal intelligence, but they often display behavior that appears intelligent.\\nBranch\\nA connection between two *nodes within a *tree.\\nBranching factor'), Document(metadata={}, page_content='Branching factor\\nA node within a tree has a branching factor of n if that node has n *chil-\\ndren. In a tree that has a branching factor of n, all nodes (apart from *leaf\\nnodes) have n children.\\nBreadth-first search\\nA *blind, *exhaustive search method that visits all *nodes at a given depth\\nbefore moving on to the next depth in the *tree.'), Document(metadata={}, page_content='Glossary 639\\nBrute-force search\\nA *search method that examines every *path in a *search tree until it finds\\na goal. See *exhaustive search.\\nBucket-brigade algorithm\\nA method used by *classifier systems that assigns blame and credit to indi-\\nvidual components within the system.\\nBuilding-block hypothesis\\nA consequence of the *schema theorem, which can be stated as: “*Genetic\\nalgorithms manipulate short, low-order, high-fitness *schemata in order to\\nfind optimal solutions to problems.”\\nC'), Document(metadata={}, page_content='find optimal solutions to problems.”\\nC\\nCandidate elimination\\nA *learning method that uses *version spaces to learn to classify data. The\\nmethod uses two sets of hypotheses, which start out as the most general\\npossible hypothesis and the most specific hypothesis. On successive itera-\\ntions these hypotheses converge until a match is found.\\nCanny edge detector\\nAn *edge detection method based on *convolution.\\nCase-based planning'), Document(metadata={}, page_content='Case-based planning\\nA case-based planning system stores the *plans it formulates for solving\\nproblems and is able to reuse whole plans or parts of plans to solve similar\\nproblems in the future. Case-based planners use *case-based reasoning to\\nsolve problems.\\nCase-based reasoning\\nSee *case-based planning.\\nCausal link\\nIn *partial order planning, a causal link is a link between an action and one\\nor more conditions, which shows that that action causes the conditions to'), Document(metadata={}, page_content='become true. See *protected link.\\nCellular automaton\\nA set of cells that live or die according to a set of rules. A cellular automaton\\nof sufficient complexity can reproduce itself. See *Conway’s Life.'), Document(metadata={}, page_content='640 Glossary\\nCenter of gravity\\nSee *centroid.\\nCentroid\\nThe point in a two-dimensional shape that has equal area on all sides of it.\\nThis is the *center of gravity of the shape.\\nCertainty factor\\nA representation of the degree of belief in a hypothesis. Used by *MYCIN.\\nChart parser\\nAn efficient method of *parsing natural language sentences that uses a\\nchart to store information about the sentence being parsed.\\nChinese room\\nA thought experiment that is used to claim that a computer is not capable'), Document(metadata={}, page_content='of thought in the same way that a human is. The experiment consists of a\\nroom with a person inside it who does not speak or understand any Chi-\\nnese. This person has a set of symbols and a set of rules for how to manip-\\nulate the symbols. A question in Chinese is passed into the room, and the\\nhuman uses the rules to construct an answer in Chinese. Although the\\nhuman clearly does not understand Chinese, the room as a whole appears'), Document(metadata={}, page_content='to have understood the question and given a sensible answer, thus display-\\ning the kinds of behavior that a computer might display when answering\\nquestions using Artificial Intelligence.\\nChomsky’ s hierarchy\\nA hierarchy of *grammars invented by Noam Chomsky, which includes\\n*regular grammars, *context-free grammars, *context-sensitive grammars,\\nand *recursively enumerable grammars.\\nChromosome\\nA representation of a single solution to a problem as used by a *genetic'), Document(metadata={}, page_content='algorithm. A chromosome is also a structure contained in biological cells\\nthat contains genetic information.\\nChronological backtracking\\nA method of backtracking (used by depth-first search) that backtracks to\\nthe next available *path that has not yet been taken. This contrasts with\\n*nonchronological backtracking, which can often be more efficient.'), Document(metadata={}, page_content='Glossary 641\\nCircumscription\\nA form of *nonmonotonic reasoning that is designed to deal with situa-\\ntions in which not all facts are either stated or denied. See *closed-world\\nassumption.\\nClass\\nA group of objects that is defined by some shared property. For example,\\nwe might consider the class of humans or the class of things with three\\nsides. An *object is an instantiation of a class.\\nClass frame\\nA *frame within a *frame system that represents a *class.\\nClassical logic'), Document(metadata={}, page_content='Classical logic\\nThe logical system based on that proposed by Aristotle. This system contrasts\\nwith nonclassical logics such as *nonmonotonic logics and *modal logics.\\nClassifier system\\nAn *expert system that uses *genetic algorithms and a *bucket-brigade\\nalgorithm to improve its ability to solve problems.\\nClause\\nA *sentence in *conjunctive normal form consists of a *conjunction of\\nclauses, where each clause is of the following form:\\nB\\n1 ∨ B2 ∨ B3 ∨ ... ∨ Bn'), Document(metadata={}, page_content='B\\n1 ∨ B2 ∨ B3 ∨ ... ∨ Bn\\nCLIPS (C Language Integrated Production System)\\nAn *expert system shell.\\nCloning\\nA reproductive method in *genetic algorithms that does not use\\n*crossover. A cloned offspring is an exact replica of its parent, apart from\\nany effects of *mutation.\\nClosed-world assumption\\nThe assumption that any fact not specifically known to be true must be\\nfalse. Also known as *negation by failure. The closed-world assumption is\\nused by *PROLOG.'), Document(metadata={}, page_content='642 Glossary\\nCoevolution\\nThe process whereby the evolution of two species is tightly connected. Usu-\\nally this applies to a predator species and a prey species. As the predator\\nspecies becomes better at catching the prey, the prey must evolve tech-\\nniques to enable it to escape. In turn this causes the predator to evolve new\\nabilities. Coevolution can often cause species to evolve much faster than\\nthey otherwise would and to reach levels of sophistication that would not'), Document(metadata={}, page_content='otherwise be possible. This was used successfully by Danny Hillis in devel-\\noping his ramps to solve problems.\\nCognitive psychology\\nA branch of psychology that studies the way in which the human brain\\nprocesses knowledge or data to solve problems.\\nCollaborative agent\\nAn *agent that is part of a *multiagent system and that cooperates with\\nother agents to achieve a common goal.\\nCollaborative filtering\\nA method used to determine an individual’s likes or dislikes by comparing'), Document(metadata={}, page_content='his or her past behavior with that of other individuals.\\nCombinatorial explosion\\nThe problem encountered when computers attempt to solve problems\\nwhose complexity grows *exponentially.\\nCombinatorial problem\\nA problem that involves assigning values to a number of variables in order\\nto find some optimal solution. The eight-queens problem is an example of\\na combinatorial problem.\\nCommutativity\\nA property of mathematical and logical operators. Operator o is commuta-\\ntive if a o b\\n≡ b o a.'), Document(metadata={}, page_content='tive if a o b\\n≡ b o a.\\nCompetitive learning\\nA form of *unsupervised learning used by *Kohonen maps.'), Document(metadata={}, page_content='Glossary 643\\nCompleteness\\nA property of *search methods. A search method is complete if it guaran-\\ntees that it will find a solution if one exists. Completeness is also a property\\nof logical systems: A logical system is complete if every *valid statement in\\nthe logic can be proved by applying the rules of deduction to the axioms.\\nBoth *propositional logic and *first-order predicate logic are complete. See\\n*soundness.\\nComplete path'), Document(metadata={}, page_content='*soundness.\\nComplete path\\nA *path in a *tree that leads from the *root node to a *goal node.\\nComposition\\nAn operator that can be combined to two *substitutions to produce a new\\n*substitution that is the same as applying the two original substitutions\\nconsecutively.\\nComputation tree logic (CTL)\\nA form of *temporal logic that uses a *tree to represent time.\\nConcave edge\\nAn edge in a two-dimensional line drawing that is between two faces that\\nare at an angle of less than 180/H11034from each other.'), Document(metadata={}, page_content='Concept learning\\nConcept learning involves learning to map from a set of input variables to a\\nBoolean value. Concept-learning systems can thus learn to determine\\nwhether or not an object meets a particular criterion based on a number of\\nthat object’s attributes.\\nConditional planning\\nA *planning method that does not start with complete information about\\nthe problem, so it allows for several possible results of each action.\\nConditional probability'), Document(metadata={}, page_content='Conditional probability\\nThe *probability that one fact will be true given that another fact is known\\nto be true. This is written P(A|B), which is read “the probability of A given\\nB” . See *posterior probability, *prior probability.'), Document(metadata={}, page_content='644 Glossary\\nConditional probability table\\nA table that shows the probabilities that one variable will be true given the\\npossible values of other variables on which it depends.\\nConflict\\nA situation that arises in multiple inheritance or in *rule-based systems in\\nwhich two contradictory pieces of data arise. For example, two *rules in a\\nrule-based system fire that recommend contradictory actions.\\nConflict resolution\\nThe methods used in a *rule-based system to decide which *rule to use'), Document(metadata={}, page_content='when a *conflict occurs. See *expert system, *metarule.\\nConjunction\\nThe conjunction of two logical variables is the logical and (\\n∧) of those two\\nvariables, written A ∧ B. A ∧ B is true if and only if A and B are both true.\\nSee *disjunction.\\nConjunctive normal form\\nAn expression is in conjunctive normal form if it consists of a *conjunction\\nof a set of *clauses. See *disjunctive normal form.\\nConsequent\\nThe part of a rule that comes after the implication. The consequent of a'), Document(metadata={}, page_content='rule in an *expert system represents the diagnosis or recommended action\\nthat is the consequence of the *antecedent.\\nConstant\\nA symbol in *first-order predicate calculus that names a specific object. A\\nconstant cannot be quantified as a variable can.\\nConstraint\\nA rule that dictates limitations on the possible values variables can take in\\nsolving a problem.\\nConstraint satisfaction problem\\nA problem in which a set of *constraints dictate possible values for vari-'), Document(metadata={}, page_content='ables. The eight-queens problem is an example of a constraint satisfaction\\nproblem. In this case, the constraint is that no two queens can be on the\\nsame row, column, or diagonal.'), Document(metadata={}, page_content='Glossary 645\\nContext-free grammar\\nA *grammar with *rewrite rules that can have at most one *terminal sym-\\nbol on the right-hand side. This type of grammar does not specify how\\nwords should agree with each other in case, number, or gender. See *con-\\ntext-sensitive grammar, *Chomsky’s hierarchy.\\nContext-sensitive grammar\\nA *grammar with *rewrite rules that can have more than one *terminal\\nsymbol on the right-hand side, and which can therefore specify rules con-'), Document(metadata={}, page_content='cerning the agreement of case, number, and gender. Context-sensitive\\ngrammars are often used for *natural language processing. See *context-\\nfree grammar, *Chomsky’s hierarchy.\\nContingent\\nA logical statement whose *truth value is not fixed, but varies depending\\non circumstances, is contingent. For example, A\\n∧ B is true if and only if\\nboth A and B are true. See *noncontingent, *interpretation.\\nContradiction\\nIf a logical system has two facts that disagree with each other, there is a con-'), Document(metadata={}, page_content='tradiction. For example, it would be a contradiction, in *classical logic, to\\nbelieve both A and\\n¬A.\\nConvex edge\\nAn edge between two faces that are at an angle of more than 180 /H11034from\\neach other.\\nConvolution\\nA mathematical operator used in *edge detection. The convolution of two\\ndiscrete functions f(a, b) and g(a, b) is defined as follows:\\nThe convolution of continuous functionsf(a, b) and g(a, b) is defined as fol-\\nlows:\\nApplying convolution to an image is one way to *smooth an image.'), Document(metadata={}, page_content='fab g ab fab g a u b v d u d v,, , ,( ) ∗ ( ) = ( ) −−( )\\n−∞\\n∞\\n−∞\\n∞\\n∫∫\\nfab g ab fab g a u b v\\nvu\\n,, , ,( ) ∗ ( ) = ( ) −−( )\\n=−∞\\n∞\\n=−∞\\n∞\\n∑∑'), Document(metadata={}, page_content='646 Glossary\\nConway’ s Life\\nA two-dimensional *cellular automaton that consists of a grid of cells. Each\\ncell can be either alive or dead, and rules are used to determine from one\\ngeneration to the next which cells will live, which will die, and which will\\ncome to life.\\nCo-occurrence matrix\\nA matrix used in *image recognition. The co-occurrence matrix, C,i s\\ndefined as follows:\\nC = D + D\\nT\\nWhere DT is the transposition of the matrix D.\\nCopycat architecture'), Document(metadata={}, page_content='Copycat architecture\\nA system designed to solve analogy problems such as “ ABC is to CBA as\\nDEF is to ???.”\\nCorpus\\nA body of text, usually used in *information retrieval problems.\\nCredit assignment\\nThe technique used to decide which parts of a system contributed to its\\nsuccess. This method is used by *classifier systems and other systems that\\nuse a *bucket-brigade algorithm. See *reinforcement learning, *winner\\ntakes all algorithm.\\nCrisp set'), Document(metadata={}, page_content='takes all algorithm.\\nCrisp set\\nAn ordinary, nonfuzzy set. Each item in the world either is or is not a mem-\\nber of a given crisp set. There is no idea of “degree of membership” of a\\ncrisp set. See *fuzzy set.\\nCrossover\\nAn operator used in *genetic algorithms that combines genetic informa-\\ntion from two *chromosomes to produce one or two offspring.\\nCYC\\nA *frame-based knowledge representation system that uses a database of mil-'), Document(metadata={}, page_content='lions of facts and *rules to make common-sense deductions about the world.'), Document(metadata={}, page_content='Glossary 647\\nCycle\\nA *path through a *semantic net or other *graph that visits the same *node\\ntwice. A *tree is a net that does not have any cycles.\\nD\\nData-driven search\\nA *search method that works from a start *state toward a goal state. See\\n*goal-driven search.\\nDeception\\nA problem that arises in *genetic algorithms, due to the use of building blocks.\\nDeception can be avoided by using *inversion or *messy genetic algorithms.\\nDecidability'), Document(metadata={}, page_content='Decidability\\nA logical system is decidable if it is possible to produce an algorithm that\\nwill determine whether any *well-formed formula is a *theorem. In other\\nwords, if a logical system is decidable, then a computer can be used to\\ndetermine whether logical expressions in that system are *valid or not.\\nDecision tree\\nA *tree in which each *node represents a question and the answers to the\\nquestion determine which *path is to be followed from that *node. *Leaf'), Document(metadata={}, page_content='nodes represent classifications determined by the decision tree. See *ID3,\\n*decision-tree induction.\\nDecision-tree induction\\nA method that learns to classify data by building a *decision tree based on\\nthe *training data.\\nDeduction\\nA process that applies a set of inference rules to a set of assumptions to lead\\nlogically to a conclusion. We write\\n{A\\n1, A2,..., An} ⊢C\\nwhere A1, A2,..., An are the assumptions, and C is the conclusion that can\\nbe deduced from them.\\nDefault reasoning'), Document(metadata={}, page_content='be deduced from them.\\nDefault reasoning\\nA form of *nonmonotonic reasoning that uses default rules to assume that\\ncertain facts are true unless there is evidence to contradict them. See\\n*closed-world assumption.'), Document(metadata={}, page_content='648 Glossary\\nDefault value\\nThe value that is assigned to a *slot in a *frame-based system unless it is\\noverridden.\\nDefining length\\nThe defining length of a *genetic algorithm *schema is defined as the dis-\\ntance between the first and last defined bits (bits that are not) in the schema.\\nDefuzzification\\nThe process whereby a crisp value can be obtained from the *fuzzy sets\\nderived by a *fuzzy system.\\nDemon\\nA *procedure in a *frame-based system that is run automatically when the'), Document(metadata={}, page_content='value in a particular *slot is changed.\\nDeMorgan’ s laws\\nA pair of complementary logical rules that can be expressed as follows:\\nA ∧ B ≡¬ (¬A ∨¬ B)\\nA ∨ B ≡¬ (¬A ∧¬ B)\\nDempster–Shafer theory\\nA method that is used to reason about degrees of belief in a theory.\\nDepth-first search\\nA *blind search method that follows one *path to its first *leaf node before\\n*backtracking chronologically to the next deepest choice. See *breadth-\\nfirst search.\\nDepth threshold'), Document(metadata={}, page_content='first search.\\nDepth threshold\\nA limit that is applied in *depth-first search that cuts search off at a speci-\\nfied depth. This avoids the problem that occurs when a *tree has a *path\\nthat is of infinite length, meaning that the search might never find a *goal.\\nDerivation tree\\nA *parse tree that is built from the *top down.\\nDescendant\\nA descendant, d,o fa  * n o d e ,n, in a *tree is a node that is further down a\\n*path in the tree than n. n is an *ancestor of a.'), Document(metadata={}, page_content='Glossary 649\\nDescribe and match\\nA method that uses a *decision tree to identify an object, by asking ques-\\ntions about the object.\\nDiagnosis\\nA process of explaining the cause of some observed phenomenon. Often\\nused in medicine to explain the cause of a patient’s symptoms.\\nDirected graph\\nA graph in which directions are attached to the edges between *nodes,\\nmeaning that if one edge exists between two nodes, it is only possible to'), Document(metadata={}, page_content='travel in one direction between those nodes, but if two edges exist between\\ntwo nodes, it is possible to travel in both directions between those nodes.\\nDiscontinuity\\nA line in a two-dimensional line drawing that has one plane on one side\\nand another plane on its other side. Discontinuities can be caused by two\\nfaces meeting, by perception of depth, by lighting, shadows, or *texture.\\nDisjunction\\nThe disjunction of two logical variables is the logical or (\\n∨) of those two vari-'), Document(metadata={}, page_content='∨) of those two vari-\\nables, writtenA ∨ B. A ∨ B is true if eitherA or B is true. See *conjunction.\\nDisjunctive normal form\\nAn expression is in disjunctive normal form if it consists of a *disjunction\\nof a set of *clauses. See *conjunctive normal form.\\nDiversity\\nA measure of the difference between chromosomes in a population.\\nDomain expert\\nA human expert who provides domain knowledge to an *expert system.\\nFor example, a medical expert system would have input from a number of'), Document(metadata={}, page_content='domain experts, most of whom would probably be doctors.\\nDualism\\nThe philosophical idea that mind and matter are the two distinct con-\\nstituents of the universe.'), Document(metadata={}, page_content='650 Glossary\\nDynamic planning\\n*Planning methods that take account of unforeseen circumstances by\\nallowing the execution of the plan to change in reaction to events and\\nchanges in the environment.\\nE\\nEdge\\nA line in a *graph that directly connects two *nodes. In *image recognition,\\nan edge is a perceived line that exists between two areas of different depth,\\n*texture, orientation, or color.\\nEdge detection\\nA method in *image recognition that locates the *edges in an image,'), Document(metadata={}, page_content='often by looking for areas of high frequency, which indicate a change in\\ncolor or *texture.\\nEffect axiom\\nIn *situation calculus, a rule that describes the effect of an action.\\nEffective branching factor\\nIf a search method expands n *nodes of a *search tree when solving a par-\\nticular problem, then the effective *branching factor ( b) of the *search is\\nthe branching factor of a *uniform tree that contains n nodes.\\nEmergent behavior'), Document(metadata={}, page_content='Emergent behavior\\nComplex behavior that emerges from an apparently simple system. Emer-\\ngent behavior usually involves a system developing some useful behavior\\nthat was not built in by its designer.\\nEntropy\\nThe extent to which a system is disordered. See *information gain, *ID3.\\nEpoch\\nA complete iteration of the training cycle of a *perceptron.\\nEquivalence\\nTwo logical expressions that must always have the same *truth value for all\\ninterpretations are equivalent. This is written A'), Document(metadata={}, page_content='interpretations are equivalent. This is written A\\n≡ B. For example, A ∧ B ≡\\nB ∧ A because whatever truth values are assigned to A and B, A ∧ B must\\nhave the same truth value as B ∧ A.'), Document(metadata={}, page_content='Glossary 651\\nError gradient\\nA measure that *neural networks employ in *backpropagation. The error\\ngradient for an output node k is defined as the *error value for this node\\nmultiplied by the derivative of the *activation function:\\nxk is the weighted sum of the input values to the node k.\\nError value\\nThe difference between the expected output of a *node in a *neural net-\\nwork and the actual output.\\nEvent calculus\\nA method for reasoning about entities that vary over time. See *situa-'), Document(metadata={}, page_content='tion calculus.\\nEvolution\\nThe biologic process by which species change over a number of genera-\\ntions. Evolution is modeled in many *Artificial Life methods, particularly\\nin *genetic algorithms.\\nEvolutionary programming\\nA method that evolves *finite state automata to find a solution to the prob-\\nlem of determining the next symbol in a finite sequence of symbols, a\\n1, a2,\\na3, a4, a5,..., an. See *genetic algorithm, *Artificial Life.\\nExcluded middle, law of'), Document(metadata={}, page_content='Excluded middle, law of\\nA law from Aristotelian logic that says that it is not possible to assert both A\\nand ¬A. Similarly, it states that either A must be true, or ¬A is true. These\\ncan be written as the two logically *equivalent statements:\\n¬(A ∧¬ A)\\nA ∨¬ A\\nSee *fuzzy logic, *classical logic.\\nExecution\\nThe process of carrying out the steps determined by a *planner for solving\\na problem.\\nδk\\nk\\nk\\nk\\ny\\nx e= ∂\\n∂ ⋅'), Document(metadata={}, page_content='652 Glossary\\nExecution monitoring\\nA method that is used during the *execution of a *plan to ensure that the\\nplan is still a sensible solution to the problem, by checking that the *pre-\\nconditions of the planned actions still hold.\\nExhaustive search\\nSee *brute-force search.\\nExistential quantifier\\nThe existential quantifier \\n∃ is read as “there exists, ” and is used to refer to\\nsome variable of which at least one must exist, but which is not explicitly\\ndefined. For example, we can write'), Document(metadata={}, page_content='defined. For example, we can write \\n∃x (P(x)), which can be read as “there\\nexists an x for which P(x) is true. ” See *universal quantifier, *first-order\\npredicate calculus.\\nExpectiminimax\\nA version of the *minimax algorithm that works with games of chance, by\\ntaking into account the *probability that each *path through the game tree\\nwill be taken.\\nExpert system\\nA system, usually built using a set of *rules, that uses expert knowledge to'), Document(metadata={}, page_content='solve problems and explain phenomena such as symptoms. See *expert sys-\\ntem shell, *production system.\\nExpert system shell\\nA toolkit that can be used to build *expert systems. *CLIPS is an example of\\nan expert system shell.\\nExponential growth\\nA function grows exponentially if its output grows as a function of some\\nvalue raised to the power of its input. For example,f(x) = 2\\nx is an exponen-\\ntial function. A problem whose complexity grows exponentially as the'), Document(metadata={}, page_content='problem grows is usually very hard to solve. See *NP-Complete, *combina-\\ntorial explosion.\\nF\\nFace recognition\\nMethods used to identify an individual by examining his or her face.'), Document(metadata={}, page_content='Glossary 653\\nFact\\nA *clause in *PROLOG that has no negative *literals and thus has nothing\\non the right-hand side of the implication, as in the following example:\\nA :-\\nFailure node\\nAn *or-node in a *goal tree that is also a *leaf node and is thus impossi-\\nble to solve.\\nFalse negative\\nIn *information retrieval, a result that is classified as not being of interest\\nbut which in fact is interesting is a false negative. The fewer false negatives'), Document(metadata={}, page_content='an information retrieval system gives, the higher its *recall. See *false posi-\\ntive, *precision.\\nFalse positive\\nIn *information retrieval, a result that is classified as being of interest but\\nwhich in fact is not interesting is a false positive. The fewer false positives an\\ninformation retrieval system gives, the higher its *precision. See *false neg-\\native, *recall.\\nFalsum\\nThe symbol ⊥ is called falsum, which is used to indicate an absurdity, or a\\n*contradiction.\\nFeasible region'), Document(metadata={}, page_content='*contradiction.\\nFeasible region\\nThe part of a *search space that contains possible solutions to the problem.\\nFeed-forward network\\nA *multilayer neural network with an input layer, an output layer, and one\\nor more hidden layers.\\nFilter\\nA function that, when applied to an image, removes all undesired parts of the\\nimage. For example, a filter might remove all non-edge regions from an image.\\nFinite state automaton (FSA)\\nA finite state automaton is a simple device that has a finite set of states and'), Document(metadata={}, page_content='an input string (often thought of as being on a tape, running through a\\ndevice that can read one symbol at a time). Each symbol that the finite state'), Document(metadata={}, page_content='654 Glossary\\nautomaton reads in is compared with a rule that dictates to which state to\\nmove from that state, with that input. After reading the entire input, the\\nfinite state machine is either in an accepting state, which means its answer\\nis “yes” to some question, or it is in some other state, in which case the\\nanswer is “no. ”\\nFirst-order predicate logic\\nA logical system in which *quantifiers can be applied to terms but not to'), Document(metadata={}, page_content='functions or predicates. See *propositional calculus, *propositional logic,\\n*classical logic, *monotonic logic, *nonmonotonic logic.\\nFitness\\nA *metric that is used to measure how successful a given *chromosome is at\\nsolving the *genetic algorithm’s problem. It is usually the case that a chro-\\nmosome with higher fitness will produce more offspring or have a greater\\nchance of reproducing than a chromosome with lower fitness.\\nFluent\\nA function that varies with time.\\nFoothill\\nA *local maximum.'), Document(metadata={}, page_content='Foothill\\nA *local maximum.\\nForgetting factor\\nA value used in *Hebbian learning systems to reduce the *weights of *nodes.\\nFormal language\\nA language such as PROLOG or C++, as compared with a *natural language.\\nForward chaining\\nSee *data-driven search.\\nForward pruning\\nA method used by game-playing systems that involves cutting off examina-\\ntion of the *game tree at a point where the position has become unaccept-\\nably poor for the computer.\\nFrame'), Document(metadata={}, page_content='ably poor for the computer.\\nFrame\\nIn a *frame system, a frame defines either a *class or an *instance of a class\\nand contains one or more *slots that hold values for attributes of that class\\nor instance.'), Document(metadata={}, page_content='Glossary 655\\nFrame axiom\\nA rule that states what aspects of the world do not change when an action\\ntakes place. See *effect axiom, *frame problem.\\nFrame problem\\nThe problem that it is usually easy to determine the effects of an action but\\noften very difficult to work out what does not change as a result of the\\naction. See *frame axiom, *ramification problem.\\nFrame system\\nA *semantic network that consists of a set of *frames, connected together\\nby relations.\\nFree variable'), Document(metadata={}, page_content='by relations.\\nFree variable\\nA free variable in a logical expression is one that has not been quantified\\nwithin the same scope. For example, in the expression ∀x(x → y), y is free\\nbecause it is not quantified by the ∀ *quantifier. See *bound variable.\\nFundamental memory\\nOne of the stable values of a *recurrent network.\\nFuzzification\\nThe process of converting a crisp input value into a fuzzy value.\\nFuzzy expert system\\nAn *expert system that uses *fuzzy logic rules.\\nFuzzy inference'), Document(metadata={}, page_content='Fuzzy inference\\nThe process by which a fuzzy system applies fuzzy rules to a set of crisp\\ninput values to derive a single crisp value. See *Mamdani inference.\\nFuzzy logic\\nAn alternative to *classical logic in which the *law of the excluded middle\\ndoes not apply and in which logical variables can take on any real number\\nvalue between 0 and 1. See *multivalent logic.\\nFuzzy reasoning\\nThe process of reasoning using fuzzy logic.'), Document(metadata={}, page_content='656 Glossary\\nFuzzy rule\\nA rule used by a *fuzzy expert system that takes the form\\nIF A op x then B = y\\nwhere op is some mathematical operator (such as “=, ” “>, ” or “<”).\\nFuzzy set\\nA set with a membership function that determines the extent to which any\\nitem is a member. See *crisp set.\\nG\\nGame tree\\nA *tree that represents the moves in a game. The *root node represents the\\nstate before any moves have been made, the *nodes in the tree represent'), Document(metadata={}, page_content='possible states of the game (or positions), and *edges in the tree represent\\nmoves in the game.\\nGene\\nA single unit of *genetic algorithm contained within a *chromosome.\\nGeneral problem solver (GPS)\\nA computer program invented in the 1950s that uses *means–ends analysis\\nto solve logical problems.\\nGeneralization\\nThe act of moving from an *instance of a *class to the class itself. Also\\nknown as the “is–a” relationship. For example, one can generalize from'), Document(metadata={}, page_content='Ronald Reagan to the class of presidents of the United States.\\nGenerate and test\\nA *search method that involves examining every *node in the *search space\\nuntil one is found that matches some criteria that describe the *goal state.\\nGenetic algorithm\\nA system that uses methods modeled on natural *evolution to solve com-\\nplex problems. See *chromosome, *gene.\\nGenetic programming\\nA method that evolves *LISP programs or *S-expressions. The method'), Document(metadata={}, page_content='searches through the *search space of possible S-expressions until it finds\\none that best solves the current problem.'), Document(metadata={}, page_content='Glossary 657\\nGenotype\\nThe genetic information represented by a set of *genes that make up an\\nindividual person or other biologic creature. See *phenotype.\\nGeometric progression\\nA sequence of numbers where each number in the progression is obtained\\nby multiplying the previous number by some constant.\\nGlobal maximum\\nThe best possible solution in a *search space. When the search space is rep-\\nresented as a curved surface, the global maximum is the highest peak in the'), Document(metadata={}, page_content='surface. See *foothill, *plateau, *ridge.\\nGoal\\nThe solution to a problem. See *goal node.\\nGoal-based agent\\nAn *agent that uses *search or *planning to achieve some *goal.\\nGoal-driven search\\nA search method that works from a goal state toward the start state. See\\n*data-driven search.\\nGoal node\\nThe *node (of which there may be more than one) in a *search tree that\\nrepresents the solution of the problem that is being solved. The aim of all\\n*search methods is to find a goal node. See *goal.'), Document(metadata={}, page_content='Goal reduction\\nSee *problem reduction, *goal tree.\\nGoal tree\\nA *tree that is used to represent the way in which a problem can be broken\\ndown into subgoals. Each *node in the tree represents a subgoal. See\\n*and–or tree, *and-node, *or-node, *success node, *failure node.\\nGödel implication\\nA form of logical *implication that is used in *fuzzy logic. It is defined\\nas follows:\\nA → B\\n≡ (A ≤ B) ∨ B'), Document(metadata={}, page_content='658 Glossary\\nGradient descent\\nA method used in training *backpropagation neural networks, which\\ninvolves descending the steepest part of the *error gradient.\\nGrammar\\nA set of rules that define the syntax and structure of a language. See\\n*Backus–Naur form, *context-sensitive grammar, *context-free grammar.\\nGraph\\nA data structure that consists of *nodes connected by *edges. See *tree,\\n*semantic net, *cycle.\\nGraphPlan\\nA *planning method that uses *planning graphs to solve problems that are'), Document(metadata={}, page_content='expressed in *STRIPS notation.\\nGround instance\\nA ground instance of a *clause is a version of that clause in which any variables\\nit contains have been replaced by *ground terms from the *Herbrand universe.\\nGround term\\nA *constant or *function that does not contain any *variables.\\nH\\nHalting Problem\\nThe problem of determining whether a given computer program will ever halt.\\nIt can be proved that no computer program can ever be written that can solve'), Document(metadata={}, page_content='the Halting problem. The proof is as follows: Imagine a program H, which\\nwhen given an argumentP, which is another program, determines whetherP\\nhalts or not. IfH determines thatP does halt, thenH enters an infinite loop and\\ntherefore never halts. IfH determines thatP does not halt, then it reports a pos-\\nitive response and exits. Now further imagine thatH is applied to itself. Let us\\nsuppose thatH determines thatH does halt. In that case, it will enter an infinite'), Document(metadata={}, page_content='loop and never halt. Hence, its assessment was wrong. Similarly, if it decides\\nthat H does not halt, then it halts, disproving its own assessment again.\\nHamming distance\\nThe Hamming distance measures the number of elements of two vectors\\nthat differ. The Hamming distance between two vectors,X and Y, is written\\n||X, Y||. For example,||(1,0,0,1),(1,0,1,1)|| = 1 because there is only one ele-\\nment of the vectors that differs.'), Document(metadata={}, page_content='Glossary 659\\nHEARSAY II\\nA system that used a *blackboard architecture and an index of computer\\nscience papers to answer spoken questions on the subject.\\nHebbian learning\\nA method of *unsupervised learning used in *neural networks. Hebbian\\nlearning is based on the idea that if two *neurons in a neural network are\\nconnected together, and they fire at the same time when a particular input\\nis given to the network, then the connection between those two neurons\\nshould be strengthened.\\nHedge'), Document(metadata={}, page_content='should be strengthened.\\nHedge\\nA fuzzy set qualifier, such as very, quite, extremely,o r  somewhat.\\nHerbrand base\\nThe Herbrand base of a set of *clauses, S, is defined as the set of\\n*ground atoms that can be obtained by replacing variables in S by\\nmembers of the *Herbrand universe for S. The Herbrand base of S is\\nwritten H\\nS(S).\\nHerbrand interpretation\\nA Herbrand Interpretation for a set of *clauses, S, is defined as a set of'), Document(metadata={}, page_content='assignments of true and false to the elements of the *Herbrand base,HS(S).\\nHerbrand universe\\nFor a set of *clauses, S, the Herbrand universe, HS, is defined as being the\\nset of constants that are contained within S and the set of functions in S\\napplied to those constants. See *ground term.\\nHeteroassociative memory\\nA type of memory that can associate one object or piece of data with another.\\nHeuristic\\nA rule or piece of information that is used to make *search or another'), Document(metadata={}, page_content='problem-solving method more effective or more efficient.\\nHeuristic evaluation function\\nA function that when applied to a *node gives a value that represents a\\ngood estimate of the distance of the node from the *goal.'), Document(metadata={}, page_content='660 Glossary\\nHeuristic repair\\nA method of solving *combinatorial problems that involves generating a\\nrandom solution to the problem and iterating toward a better solution by\\nmaking simple changes that reduce the number of errors.\\nHidden layer\\nA layer of *neurons within a *neural network that is between an input layer\\nand an output layer. The hidden layer usually carries out the calculations of\\nthe network.\\nHill climbing\\nAn *informed search method that acts by always moving toward a better'), Document(metadata={}, page_content='solution one step at a time, ensuring that every step improves the current\\nstate. Hill climbing is very susceptible to problems such as *ridges,\\n*foothills, and *plateaus.\\nHopfield network\\nA *recurrent neural network that usually uses the sign activation function:\\nHorizon problem\\nThis problem involves an extremely long sequence of moves in a game that\\nclearly lead to a strong advantage for one player, but where the sequence of'), Document(metadata={}, page_content='moves, although potentially obvious to a human player, takes more moves\\nthan can be examined by a computer using *bounded lookahead. Hence,\\nthe significant end of the sequence has been pushed over the horizon.\\nHorn clause\\nA *clause that has at most one positive *literal.\\nHuman language\\nOne of the many hundreds of languages spoken and written by human\\nbeings around the world, such as English, Japanese, Russian, Swahili, and\\nFrench. See *formal language, *natural language.\\nHybrid agent'), Document(metadata={}, page_content='Hybrid agent\\nAn *agent that exhibits properties of more than one agent type.\\nSign X for x\\nfor x( ) = +>\\n−<\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n10\\n10'), Document(metadata={}, page_content='Glossary 661\\nHypothesis\\nA possible explanation for a set of observed phenomena.\\nI\\nID3\\nA *decision tree induction algorithm that builds a decision tree from the\\ntop down. The nodes in the decision tree are selected by choosing features\\nof the *training data set that provide the most information about the data\\nand turning those features into questions.\\nImage capture\\nThe process of obtaining an image from a real-world scene so that some\\nkind of image processing can be carried out on the image.'), Document(metadata={}, page_content='Image recognition\\nThe process of identifying features and making informed decisions about a\\nscene by examining an image of the scene. See *edge detection, *segmentation.\\nImplication\\nA logical operator that is defined by the following *truth table:\\nABA → B\\nfalse false true\\nfalse true true\\ntrue false false\\ntrue true true\\nIndependence\\nIf the value of one variable does not in any way affect the value of another,\\nthen the two variables are said to be independent. Hence, for example, the'), Document(metadata={}, page_content='color of the sky is independent of my name, but the color of the sky is not\\nindependent of the time of day.\\nInductive bias\\nRestrictions imposed on a *learning method by its assumptions.'), Document(metadata={}, page_content='662 Glossary\\nInductive reasoning\\nReasoning about what will happen in the future based on evidence from\\nthe past. See *deduction, *abduction.\\nInference engine\\nThe part of an *expert system that controls the process of deriving conclu-\\nsions and recommended actions from a set of rules and facts.\\nInference rule\\nA rule that is used in the process of logical *deduction.\\nInformation agent\\nAn *agent that gathers information on behalf of a human user. Usually'), Document(metadata={}, page_content='information agents are used to gather information from the Internet and so\\nare also called *internet agents.\\nInformation gain\\nThe reduction in *entropy caused by some change in a system.\\nInformation retrieval\\nInformation retrieval involves matching the text contained in a query or a\\ndocument to a set of other documents. Often, the task involves finding the\\ndocuments from a *corpus of documents that are relevant to a user’s query.\\nInformed search method'), Document(metadata={}, page_content='Informed search method\\nA *search method that uses a *heuristic to ensure that it searches the\\n*search space as efficiently as it can.\\nInheritance\\nThe way in which one *class or *object can derive features from a *super-\\nclass from which it is itself derived.\\nInitial state\\nThe *state in which a problem-solving system starts to solve its problem.\\nInstance\\nAn instance of a *class is an *object that has properties of that class.\\nInstance constructor'), Document(metadata={}, page_content='Instance constructor\\nA *procedure that creates an *instance of a *class.'), Document(metadata={}, page_content='Glossary 663\\nInstance frame\\nA *frame within a *frame system that represents an *instance of a *class.\\nIntelligent agent\\n*Software agents are often referred to as intelligent agents, but an intelli-\\ngent agent is really an agent that has the particular property of intelligence.\\nThis means, for example, that it has the ability to reason independently\\nabout data. Many intelligent agents have the ability to learn. Another\\nimportant property of most intelligent agents is *autonomy.\\nInterface agent'), Document(metadata={}, page_content='Interface agent\\nAn *autonomous *intelligent agent that acts as a personal assistant for a user.\\nInternet agent\\nSee *information agent.\\nInterpretation\\nAn interpretation is a specific choice of assignment of values to a set of\\nvariables. A logical expression can be true under one interpretation and\\nfalse under another.\\nInversion\\nA *unary operator that reverses the order of a subset of the bits within a\\n*chromosome. Inversion is used to avoid the problem of *deception.\\nIrrevocability'), Document(metadata={}, page_content='Irrevocability\\nA *search method is irrevocable if it does not employ *backtracking—in\\nother words, it explores just one *path through a *search tree. *Hill climb-\\ning is an example of an irrevocable search method.\\nIterated local search\\nA *local search method that is repeated iteratively using different starting\\npoints in order to avoid the problem of *local maxima.\\nJ\\nJoint probability distribution (joint)\\nThe distribution of *probabilities of a combination of two logical variables.'), Document(metadata={}, page_content='For example, we could refer to the joint probability distribution of A ∧ B,\\nwhich would be represented by a table such as the following:'), Document(metadata={}, page_content='664 Glossary\\nA ¬A\\nB 0.11 0.09\\n¬B 0.63 0.17\\nK\\nKnowledge base\\nThe database of *rules used by an *expert system. The knowledge base con-\\ntains the data that represent the system’s knowledge about the domain.\\nKnowledge engineer\\nThe human being who is responsible for a large part of the creation of an\\n*expert system. Specifically the knowledge engineer is responsible for\\ninputting *metaknowledge into the system.\\nKohonen map\\nAn *unsupervised learning *neural network. A Kohonen map is capable of'), Document(metadata={}, page_content='classifying data into a set of classifications without being given the specific\\nclassifications in advance. This kind of classification is also known as auto-\\nmatic clustering.\\nL\\nLeaf node\\nA *node in a *tree that has no *successors. All *goal nodes are leaf nodes,\\nbut not all leaf nodes are goal nodes. In other words, it is possible to reach\\na leaf node at the end of a path in a tree without successfully finding a\\ngoal node.\\nLearning agent'), Document(metadata={}, page_content='goal node.\\nLearning agent\\nAn *agent that is capable of *learning and is therefore able to acquire new\\nknowledge and skills and is able to use the new knowledge and skills to improve\\nits performance. This learning is often carried out using a *neural network.\\nLexicon\\nA dictionary of words and other linguistic units that make up a language.\\nLife, game of\\nSee *Conway’s life.'), Document(metadata={}, page_content='Glossary 665\\nLikelihood\\nThe *probability that a given piece of evidence (E) would occur, given that\\na particular hypothesis (H) were true:\\nP (E | H)\\nThis is the likelihood of E,g i v e n  H, as compared with the probability of H\\ngiven E, which would be written P(H | E).\\nLinear threshold function\\nA step function used as an *activation function by artificial *neurons. The\\nlinear threshold function gives a value of 0 for inputs below a threshold (t)\\nand a value of 1 for inputs above t.'), Document(metadata={}, page_content='and a value of 1 for inputs above t.\\nLinearly separable function\\nA function that can be drawn in a two-dimensional graph such that a\\nstraight line can be drawn between the values so that inputs that are classi-\\nfied into one classification are on one side of the line, and inputs that are\\nclassified into the other are on the other side of the line. Logical-OR is a lin-\\nearly separable function, but exclusive-OR is not. *Perceptrons can only be\\nused to learn linearly separable functions.'), Document(metadata={}, page_content='used to learn linearly separable functions.\\nLinguistic variable\\nA fuzzy variable such asheight or age that is defined not in objective numer-\\nical terms but in terms of fuzzy values such as tall, short, old, and young.\\nLISP\\n(LISt Programming). A programming language widely used in Artificial\\nIntelligence research.\\nLiteral\\nOne of the basic symbols of *propositional logic. For example, in the fol-\\nlowing expression\\n¬A ∧ (B ∨ C)\\nthe literals are ¬A, B, and C.\\nLocal maximum'), Document(metadata={}, page_content='the literals are ¬A, B, and C.\\nLocal maximum\\nA peak within a *search space that is higher than all the points immediately\\naround it but is lower than the global maximum, which is the highest point\\nin the entire search space and the most desirable goal. Many search meth-\\nods are prone to identifying local maxima rather than *global maxima.'), Document(metadata={}, page_content='666 Glossary\\nT echniques such as *simulated annealing and *iterated local search are\\ndesigned to avoid this problem. See *local optimization.\\nLocal optimization\\nA method of solving *combinatorial problems that involves finding a *local\\nmaximum by moving toward a better solution in the *search space. See\\n*hill climbing.\\nLocal search\\nLocal search methods work by starting from some initial configuration\\n(usually random) and making small changes to the configuration until a'), Document(metadata={}, page_content='state is reached from which no better state can be achieved. See *meta-\\nheuristic, *local optimization, *hill climbing, *simulated annealing.\\nL-systems\\nA system that describes the way a set of artificial “trees” grow using a set of\\nsimple rules:\\nRule 1: a → ab\\nRule 2: b → a\\nM\\nMachine translation\\nThe use of computer software to translate text from one *human language\\nto another.\\nMamdani inference\\nA form of *fuzzy inference that allows a system to take in a set of *crisp'), Document(metadata={}, page_content='input values (from a set of sensors or inputs from a human operator, for\\nexample) and apply a set of *fuzzy rules to those values in order to derive a\\nsingle, crisp, output value or action recommendation.\\nMap coloring\\nThe problem of finding a way to color the countries in a map so that no\\ntwo adjoining countries are the same color. It can be shown that any nor-\\nmal two-dimensional map can be colored using at most 5 colors.\\nMaximum a posteriori hypothesis (MAP)'), Document(metadata={}, page_content='Maximum a posteriori hypothesis (MAP)\\nThe *hypothesis that has the greatest *posterior probability for explaining\\nan observed piece of evidence. The MAP classification for a piece of data is\\nthe classification that is the most likely one for the data.'), Document(metadata={}, page_content='Glossary 667\\nMeans–ends analysis\\nAn approach to *planning that involves identifying the differences between\\nthe goal state and the current state, and selecting actions that aim to lessen\\nthose differences.\\nMembership function\\nA function that defines the membership of a *fuzzy set. Membership of\\nfuzzy sets is defined as a value between 0 and 1, where 1 means complete\\nmembership of the fuzzy set, and 0 means nonmembership. For example,\\nthe following membership function:'), Document(metadata={}, page_content='the following membership function:\\ndefines a membership function for a fuzzy set, B. Input values above 2 are\\nnot members of B at all. The number 0 is completely a member ofB, with a\\nmembership value of 1.\\nMemory\\nA property of *recurrent neural networks that enables it to change its\\n*weights as new inputs are presented to it.\\nMental situation calculus\\nA form of *situation calculus that allows an agent to reason about the\\neffects that events have on an its beliefs about the world.'), Document(metadata={}, page_content='Messy genetic algorithm (MGA)\\nAn alternative to a standard *genetic algorithm. Each bit in a messy genetic\\nalgorithm *chromosome is represented by a pair of numbers: the first\\nnumber represents the position within the chromosome, and the second\\nnumber is the bit value (0 or 1). A chromosome in a messy genetic algo-\\nrithm can be *overspecified (a bit is defined more than once) or *under-\\nspecified (a bit is not defined at all). See *schema.\\nMetaheuristic'), Document(metadata={}, page_content='Metaheuristic\\nA *heuristic used by a *local search method. See *simulated annealing,\\n*tabu search, *local optimization.\\nMetaknowledge\\nKnowledge about knowledge. See *metarule.\\nMx\\nx for x\\nfor x\\nB( ) = −≤\\n>\\n\\uf8f1\\n\\uf8f2\\uf8f4\\n\\uf8f3\\uf8f4\\n1 2 2\\n02'), Document(metadata={}, page_content='668 Glossary\\nMetarule\\nRules that define how *conflict resolution and other aspects of an *expert\\nsystem work.\\nMetric\\nA measure that is used to quantify the performance of a system. See *fitness.\\nMinimax\\nAn algorithm that is used by game-playing software to determine the best\\nmove to make. The algorithm assumes that it is playing against a *rational\\nopponent and uses a *static evaluator at *leaf nodes. See *alpha–beta prun-\\ning, *expectiminimax.\\nMobile agent'), Document(metadata={}, page_content='ing, *expectiminimax.\\nMobile agent\\nAn *agent that is capable of moving, either in the physical world or across\\nnetworks such as the Internet.\\nModal logic\\nAn extended version of a *classical logic that allows reasoning about cer-\\ntainty and possibility.\\nModal operator, M\\nA logical operator that indicates that an expression is consistent with an\\nagent’s beliefs. See *modal logic, *nonmonotonic logic.\\nModus ponens\\nA logical rule that is used in *deduction, which states that if A implies B,'), Document(metadata={}, page_content='and we know that A is true, then we can deduce that B is true:\\nMonotonicity\\n1. A search method is described as monotone if it always reaches a\\ngiven node by the shortest possible path.\\n2. A function that increases monotonically is one that never decreases\\nif its argument increases.\\n3. A logical system is described as being monotonic if a valid proof in\\nthe system cannot be made invalid by adding additional premises\\nAA B\\nB\\n→'), Document(metadata={}, page_content='Glossary 669\\nor assumptions. In other words, if we find that we can prove a con-\\nclusion C by applying rules of deduction to a premise B with\\nassumptions A, then adding additional assumptions A/H11032and B/H11032will\\nnot stop us from being able to deduce C.\\nSee *nonmonotonic.\\nMorphologic analysis\\nAnalysis of the structure of individual words within an *utterance in a\\n*human language. See *natural language processing, *semantic analysis,\\n*syntactic analysis, *pragmatic analysis.'), Document(metadata={}, page_content='*syntactic analysis, *pragmatic analysis.\\nMost general hypothesis\\nThe most general hypothesis is defined as a vector such as <?, ?, ?, ?, ?, ?>,\\nwhich allows for any set of data values. See *most specific hypothesis,\\n*hypothesis.\\nMost general unifier (mgu)\\nA *unifier, u\\n1, is called a most general unifier if any other unifier, u2, can\\nbe expressed as the composition of u1 with some other substitution (i.e.,\\nu2 = u1 o u3).'), Document(metadata={}, page_content='u2 = u1 o u3).\\nA most general unifier is a unique unifier that provides the most general set\\nof *substitutions to unify a set of *clauses.\\nMost specific hypothesis\\nThe most specific hypothesis is defined as a vector such as <∅, ∅, ∅, ∅, ∅,\\n∅>, which does not allow for any set of data values. See *most general\\nhypothesis, *hypothesis.\\nMotion field\\nThe vectors that define the apparent motion in a still photograph. See\\n*optical flow.\\nMultiagent system'), Document(metadata={}, page_content='*optical flow.\\nMultiagent system\\nA system that uses a number of *agents to solve a single problem. See\\n*agent team, *collaboration, *intelligent agent.\\nMultilayer neural network\\nAn artificial *neural network that has more than one layer of *neurons. See\\n*perceptron, *Hebbian learning, *Kohonen map, *Hopfield network.'), Document(metadata={}, page_content='670 Glossary\\nMultiple inheritance\\nThe *inheritance of properties from more than one *frame or *class.\\nMultivalent logic\\nA logical system that has more than two logical values. See *bivalent logic,\\n*fuzzy logic.\\nMutation\\nA *unary operator that flips a single bit within a *chromosome from zero\\nto one or from one to zero. See *crossover, *genetic algorithm.\\nMutual exclusion (mutex)\\nIn a *planning graph, a mutex exists between two actions or effects that are'), Document(metadata={}, page_content='mutually exclusive—in other words, they cannot both exist at the same\\ntime. Hence, if a mutex exists between two actions and one action is taken,\\nthen the other action cannot be taken.\\nMYCIN\\nA medical *expert system that uses *certainty factors to diagnose symptoms.\\nN\\nN-gram\\nA grouping of n letters (Some examples of trigrams that occur commonly\\nin English are ing, the, ant, and ize). N-grams are used to identify a language\\nfrom a piece of text, using the *acquaintance algorithm.'), Document(metadata={}, page_content='Naïve Bayes classifier\\nA system that uses *Bayes’ theorem to learn to classify data.\\nNatural language\\nSee *human language.\\nNatural language processing (NLP)\\nThe analysis of the *syntax, *semantics, *morphology, *phonology, and\\n*pragmatics of *utterances in *human languages. Natural language pro-\\ncessing is used to enable a computer system to “hear” spoken human lan-\\nguage, interpret the words, and carry out some action (such as a database\\nquery) on the basis of the words.'), Document(metadata={}, page_content='Glossary 671\\nNearest neighbor algorithm\\nAn instance-based learning method. See *Shepard’s method.\\nNearest neighbor heuristic\\nA *heuristic used to solve problems such as the traveling salesman\\nproblem, which functions by extending the *path to the nearest unvis-\\nited city.\\nNegation by failure\\nSee *closed-world assumption, *PROLOG.\\nNeural network\\nA network of simple processing *nodes (*neurons), which is roughly mod-\\neled on the human brain. See also *backpropagation, *bidirectional asso-'), Document(metadata={}, page_content='ciative memory, *Hebbian learning, *activation function, *hidden layer,\\n*Hopfield network, *linear threshold function, *multilayer neural network.\\nNeuro-fuzzy system\\nA *neural network that learns to classify data using *fuzzy rules and *fuzzy sets.\\nNeuron\\nThe individual computation devices that make up the human brain. Neu-\\nrons are also the building blocks of artificial *neural networks. A neuron\\ngenerally takes in one or more inputs to which an *activation function is'), Document(metadata={}, page_content='applied. The result of this function is compared with the *activation level\\nto determine if the neuron should fire.\\nNode\\n1. A *neuron within an artificial *neural network.\\n2. The building block of *graphs, *nets, and *trees. A graph consists\\nof a set of nodes, which are connected by *edges. Each node repre-\\nsents a decision or a piece of data within the graph. See *leaf node,\\n*goal node, *and-node, *or-node.\\nNonchronological backtracking'), Document(metadata={}, page_content='Nonchronological backtracking\\nA form of *backtracking that involves using additional information about\\nthe search problem to backtrack to a more helpful *node than the last one\\nin the *tree. See *chronological backtracking.'), Document(metadata={}, page_content='672 Glossary\\nNoncontingent\\nA logical statement whose *truth value is fixed and does not vary with cir-\\ncumstances is noncontingent. For example, A ∧¬ A is always false, regard-\\nless of the value of A. See *contingent, *interpretation.\\nNondirected graph\\nA *graph in which an *edge between two *nodes goes in both directions.\\nSee *directed graph.\\nNonmonotonic\\nIn a nonmonotonic logical system, a valid proof can be made invalid by\\nadding additional premises or assumptions. See *monotonic, *abduction,'), Document(metadata={}, page_content='*circumscription, *classical logic, *default reasoning, *modal operator M,\\n*truth maintenance system.\\nNonterminal symbol\\nA symbol in a *grammar that is used to represent a number of *terminal\\nsymbols or nonterminal symbols.\\nNormal distribution\\nAlso known as the Gaussian distribution or bell curve, the normal distribu-\\ntion is defined as follows:\\nNormalization\\nNormalization is the process whereby the *posterior probabilities of a pair'), Document(metadata={}, page_content='of variables are divided by the normalizing constant to ensure that they\\nsum to 1. The normalizing constant is defined as follows:\\nNoun\\nA word in a *human language that is used to define a thing, a person, a\\nplace, or an abstract thing such as “happiness. ” See *noun phrase, *verb.\\nα =\\n( ) ⋅ ( ) +¬ ( ) ⋅¬( )\\n1\\nPA B PB PA B P B\\nPx e d t\\nt\\n−∞( ) =\\n−\\n−∞\\n∞\\n∫, 1\\n2\\n2\\n2\\nπ'), Document(metadata={}, page_content='Glossary 673\\nNoun phrase\\nA phrase that has the same properties within a *grammar as a noun and can\\nthus be used interchangeably with a noun, at least as far as the syntactic rules\\nof the grammar are concerned. For example, the following are all noun\\nphrases: America, a big green dog, the house that I lived in when I was younger.\\nNP-complete\\nA problem that is NP can be solved nondeterministically in polynomial time.\\nThis means that if a possible solution to the problem is presented to the com-'), Document(metadata={}, page_content='puter, it will be able to determine whether it is a solution or not in polynomial\\ntime. The hardest NP problems are termed NP-complete. All NP-complete\\nproblems can be mapped directly onto the satisfiability problem.\\nO\\nOccam’ s razor\\nThe assertion that the simplest possible solution to a problem should\\nalways be selected. See *inductive bias.\\nOccluding edge\\nA depth *discontinuity within a two-dimensional line drawing.\\nOpening book'), Document(metadata={}, page_content='Opening book\\nA database of opening moves for use in playing games such as chess and\\ncheckers.\\nOperator schema\\nA template that defines a number of operators within a *planning system.\\nOptical flow\\nThe apparent motion within a still photograph. See *motion field.\\nOptimality\\nAn optimal *search method is one that will find the solution that involves\\ntaking the least number of steps to a *goal node, if such a solution exists.\\nOptimal path'), Document(metadata={}, page_content='Optimal path\\nThe shortest *path from the *root node in a *search tree to a *goal node.'), Document(metadata={}, page_content='674 Glossary\\nOr-goal\\nA *goal that can be solved by solving any one of its *subgoals. Represented\\nin a *goal tree by an *or-node. See *problem reduction.\\nOr-node\\nA *node in a *goal tree that represents an *or-goal.\\nOverfitting\\nA problem that affects *learning systems, whereby the system performs well\\nat classifying the *training data but, due to fitting its model of the data too\\nclosely to inaccurate training data, does not perform well at classifying\\nunseen data.\\nOverriding'), Document(metadata={}, page_content='unseen data.\\nOverriding\\nThe act of assigning a new value to an inherited default value.\\nOverspecified chromosome\\nA *chromosome where one or more bits have more than one value assigned\\nto them. See *messy genetic algorithm, *underspecified chromosome.\\nP\\nParallel search\\n*Search methods that are designed to take advantage of multiple processor\\ncomputers by running parts of the search in parallel with each other.\\nParser\\nA tool that breaks down a *sentence in a *human language to its compo-'), Document(metadata={}, page_content='nent parts by matching the sentence with the structure imposed by the lan-\\nguage’s *grammar.\\nPartial order\\nA relation on a set that is reflexive, transitive, and antisymmetric. For\\nexample, ≤ defines a partial order on the set of integers. This can be proved\\nas follows:\\na ≤ a for all integers. Hence, ≤ is reflexive.\\na ≤ b\\n∧ b ≤ c → a ≤ c.H e n c e ,≤ is transitive.\\na ≤ b ∧ b ≤ a ⇔ a = b.H e n c e ,≤ is antisymmetric.'), Document(metadata={}, page_content='Glossary 675\\nPartial order planning\\nA *planning method in which the order of actions that are not dependent\\non each other is not necessarily defined.\\nPartial path\\nA *path in a *tree that leads from the *root node to a *leaf node that is not\\na *goal node.\\nPath\\nA route through a *tree or *graph. The shortest path consists of just one\\n*node. Normally, a path consists of more than one node, connected\\ntogether by one or more edges.\\nPattern matching'), Document(metadata={}, page_content='together by one or more edges.\\nPattern matching\\nThe identification of patterns in images, text, or other data by comparing\\nthe data with a set of templates or regular expressions.\\nPerceptron\\nA simple *neuron that is used to classify input data into one of two cate-\\ngories. See *linearly separable function.\\nPhenotype\\nThe physical characteristics of a creature, as determined by the creature’s\\n*genotype and its environment.\\nPixel'), Document(metadata={}, page_content='*genotype and its environment.\\nPixel\\nA picture element. A single unit of light and color as displayed on a com-\\nputer screen.\\nPlan\\nA sequence of actions that has been determined by the act of *planning to\\nbe a way to solve a particular problem.\\nPlanning\\nThe act of taking a starting state and a goal state and building a *plan that\\nconsists of a sequence of actions that when carried out should lead from\\nthe start state to the goal state. See *partial order planning, *case-based'), Document(metadata={}, page_content='planning, *atomic action, *STRIPS.'), Document(metadata={}, page_content='676 Glossary\\nPlanning Domain Definition Language (PDDL)\\nA planning language that can be used to represent problems expressed in\\n*STRIPS or *ADL.\\nPlanning graph\\nA *graph that contains a number of levels that represent states and actions\\nthat is used by algorithms such as *GraphPlan to devise plans for solving\\nproblems. See *GraphPlan, *mutual exclusion.\\nPlateau\\nA flat region in the *search space, where moving in any direction leads to'), Document(metadata={}, page_content='an area at the same height as the current height. See *hill climbing, *local\\nmaximum, *global maximum.\\nPly\\nOne level in a *game tree.\\nPopulation\\nThe complete collection of *chromosomes that a *genetic algorithm has\\ndeveloped in a given generation.\\nPossible world\\nA universe that could logically exist but is not necessarily the same as the\\none we live in now.\\nPosterior probability\\nThe *probability of a variable given that we know that another variable is'), Document(metadata={}, page_content='true. The posterior probability of B is written P(B | A). See *prior probabil-\\nity, *conditional probability.\\nPragmatic analysis\\nThe analysis of the real meaning of an *utterance in a human language, by\\ndisambiguating and contextualizing the utterance. See *semantic analysis,\\n*syntactic analysis, *morphologic analysis, *natural language processing.\\nPrecision\\nA measure of the success of an *information retrieval system. A system that'), Document(metadata={}, page_content='gives no *false positives has 100% precision. See *recall, *false negative.'), Document(metadata={}, page_content='Glossary 677\\nPrecondition\\nA requirement that must be met for a particular action to be carried out by\\na *planning system.\\nPredecessor\\nThe *node immediately above a given node in a *tree. Each node has\\nexactly one predecessor, except for the root node, which has no predeces-\\nsors. See *ancestor, *descendant, *successor.\\nPredicate calculus\\nSee *first-order predicate calculus.\\nPremises\\nA set of logical statements from which a conclusion is drawn using logical\\n*deduction.\\nPrenex normal form'), Document(metadata={}, page_content='*deduction.\\nPrenex normal form\\nAn expression that is in *conjunctive normal form and in which all *quan-\\ntifiers are at the beginning is in prenex normal form.\\nPrinciple component analysis\\nAnalysis of data by determining the features of the data that vary the most\\ngreatly from one item to another.\\nPrior probability\\nThe *probability of a variable, regardless of any other variables. The prior\\nprobability of B is written P(B). See *posterior probability, *conditional\\nprobability.'), Document(metadata={}, page_content='probability.\\nPrisoner’ s Dilemma\\nA two-player game based on the following scenario:\\nTwo prisoners have been arrested on suspicion of committing a crime.\\nThey are kept in separate cells, and each is told that if he betrays his friend\\nhe will receive a reward. If his friend does not betray him, then he will go\\nfree, and receive a reward, while his friend is tortured. If both betray each\\nother, they will both be tortured, and if neither betrays the other, they will\\nbe set free.'), Document(metadata={}, page_content='be set free.\\nProbabilistic planning\\n*Planning in nondeterministic environments, in which an action will cause\\nan effect with a given probability.'), Document(metadata={}, page_content='678 Glossary\\nProbabilistic reasoning\\nReasoning about the probabilities of events or attributes.\\nProbability\\nThe probability of A is a measure of how likely it is that A will occur\\nunder ordinary circumstances. This is written P(A). See *conditional\\nprobability, *joint probability distribution, *posterior probability, *prior\\nprobability.\\nProblem reduction\\nA method of solving problems by breaking each problem down into a\\nnumber of subproblems, each of which may in turn be further broken'), Document(metadata={}, page_content='down. By solving all of the subproblems and combining the results cor-\\nrectly, the original problem can be solved. See *goal reduction, *goal tree.\\nProcedural attachment\\nA *procedure that is associated with a *frame.\\nProcedure\\nA method that is associated with a *frame. Each procedure is a set of\\ninstructions that can be executed on request. Some procedures are executed\\nautomatically when particular events occur, such as upon creation. See\\n*demon, *procedural attachment.\\nProduct rule'), Document(metadata={}, page_content='*demon, *procedural attachment.\\nProduct rule\\nThe rule used in *Hebbian learning to determine the extent to which the\\nweights attached to each *node are increased or decreased during learning.\\nProduction rule\\n1. A rule used by an *expert system. Each rule has the form input ->\\naction or input -> diagnosis.\\n2. A rule used to define a part of a grammar. Each rule explains how\\none *nonterminal symbol is made up of one or more terminal or\\nnonterminal symbols. See *rewrite rule.\\nProduction system'), Document(metadata={}, page_content='Production system\\nSee *expert system.'), Document(metadata={}, page_content='Glossary 679\\nPROLOG\\n(PROgramming in LOGic). A language that is widely used in Artificial\\nIntelligence research. PROLOG programs are based around a database of\\nfacts and rules.\\nPropositional calculus\\nThe language that is used to express the concepts of *propositional logic.\\nPropositional logic\\nA *monotonic logical system based around logical operators (such as,\\n∧, ∨,\\nand ¬) and proposition terms. See *classical logic, *first-order predicate\\ncalculus, *propositional calculus.'), Document(metadata={}, page_content='calculus, *propositional calculus.\\nPropositional planning\\nA *planning system that models plans purely using *propositional calculus.\\nProtected link\\nA *causal link is said to be protected when it is needed to establish the *pre-\\nconditions of an operator below it in the plan that is being developed.\\nPruning\\nCutting off sections of a *search tree that are (probably) not worth examin-\\ning. See *alpha–beta pruning.\\nPure AND-OR tree'), Document(metadata={}, page_content='ing. See *alpha–beta pruning.\\nPure AND-OR tree\\nA *goal tree that has the following properties: the *tree has an *or-node at\\nthe top, each or-node has *and-nodes as its direct *successors, and each\\nand-node has or-nodes as its direct successors. Another condition of a pure\\nAND-OR tree is that it does not have any *constraints that affect which\\nchoices can be made. A *game tree is a pure AND-OR tree.\\nQ\\nQuantifier\\nSee *universal quantifier, *existential quantifier, *first-order predicate logic.\\nR'), Document(metadata={}, page_content='R\\nRamification problem\\nThe problem of identifying all consequences of an action including trivial\\nones or ones that are hard to foresee. See *frame problem.'), Document(metadata={}, page_content='680 Glossary\\nRationality\\nAn *agent or other computer program that behaves rationally is one that\\nacts to maximize some *utility function. In playing games, for example, a\\nrational opponent is one that is attempting to win. An irrational opponent\\nwould be one that wanted to win but did not always play its best move.\\nReactive agent\\nAn *agent that simply responds to inputs from its environment. A reactive\\nagent has a set of rules (like an *expert system) that instruct it how it'), Document(metadata={}, page_content='should behave based on any given input from the environment.\\nRecall\\nA measure of the success of an *information retrieval system. A system that\\ngives no *false negatives has 100% recall. See *precision, *false negative.\\nRecurrent network\\nA *multilayer neural network that is able to feed information back from its\\noutputs to its inputs, and thus is able to act as a *memory.\\nRecursively enumerable\\nA class of *grammars that has no restrictions on the *rewrite rules that can'), Document(metadata={}, page_content='be used to define the grammar. Recursively enumerable grammars are also\\nknown as unrestricted grammars. See *context-sensitive grammar, *con-\\ntext-free grammar.\\nReductio ad absurdum\\nA rule that states that if we assume that some expression E is false, and by a\\nprocess of logical deduction starting from this assumption can deduce fal-\\nsum, then E must in fact be true. See *refutation proof.\\nReflex agent\\nSee *reactive agent.\\nRefutation proof'), Document(metadata={}, page_content='See *reactive agent.\\nRefutation proof\\nA method that proves a logical *deduction is valid by first negating the con-\\nclusion and then using the resulting *clauses to deduce falsum. See *resolu-\\ntion, *reductio ad absurdum.\\nRegular expression\\nA *sentence defined by a *regular grammar.'), Document(metadata={}, page_content='Glossary 681\\nRegular grammar\\nA *grammar that defines the syntax of a *regular language.\\nRegular language\\nThe simplest *grammar from *Chomsky’s hierarchy of grammars. A simple\\nlanguage is one that can be described by a *finite state automaton.\\nReinforcement learning\\nA *learning system that uses positive reinforcement when it succeeds and\\nnegative reinforcement when it fails. See *bucket-brigade algorithm,\\n*credit assignment.\\nRelative likelihood\\nThe relative likelihood of two hypotheses, H'), Document(metadata={}, page_content='The relative likelihood of two hypotheses, H\\n1 and H2, given evidence E is\\ndefined as follows:\\nThe relative likelihood thus tells us how much more likely one explanation\\nis than another for a piece of observed evidence.\\nRelaxed problem\\nA version of a *combinatorial problem that has fewer *constraints.\\nReplanning\\nThe process of devising a new *plan during *execution when circumstances\\nhave changed such that the existing plan is no longer suitable—for exam-'), Document(metadata={}, page_content='ple, because one of the *preconditions of the next planned action is no\\nlonger satisfied.\\nRepresentation\\nA model used by a computer to represent a real-world situation or to store\\nsome data that are used in solving a problem.\\nRepresentational adequacy\\nThe ability of a representational system to represent different situations is\\nmeasured by representational adequacy. If representational system A can\\nmodel more situations than representational system B, then A has a higher'), Document(metadata={}, page_content='representational adequacy than B.\\nPH E\\nPH E\\n1\\n2\\n( )\\n( )'), Document(metadata={}, page_content='682 Glossary\\nResolution\\nA method used in *propositional logic and *first-order predicate logic to\\nprove theorems by *contradiction. See *unification, *skolemization,\\n*skolem normal form, *most general unifier, *substitution.\\nRete\\nA directed, acyclic *graph or *tree used by *expert systems to make the\\nprocess of modifying stored facts in the database efficient.\\nRewrite rule\\nSee *production rule.\\nRidge\\nA narrow high region in a *search space that can cause problems for search'), Document(metadata={}, page_content='methods such as *hill climbing.\\nRobot\\nA physical *agent. A robot can take many forms, such as an arm, an insect,\\nor a bucket on wheels.\\nRobotic agent\\nSee *robot, *software agent.\\nRoot goal\\nThe overall *goal of a problem that is being solved by *problem reduction.\\nThe root goal is represented by the *root node of the *goal tree.\\nRoot node\\nThe only *node in a *tree that has no predecessor. The top node in the tree.\\nRote learning'), Document(metadata={}, page_content='Rote learning\\n*Learning by simply storing each piece of *training data and its classification.\\nRoulette-wheel selection\\nA method that is used to make a random selection in which some items are\\nmore likely to be selected than others. Roulette-wheel selection is used, for\\nexample, to select *chromosomes to reproduce in *genetic algorithms.'), Document(metadata={}, page_content='Glossary 683\\nRule\\nA method of *representing the *knowledge used by a *rule-based system.\\nEach rule has an *antecedent and a *consequent. A rule can be written A →\\nB or IF A THEN B, which are equivalent.\\nRule-based system\\nA system whose behavior in a given situation is defined by a set of rules. See\\n*production system, *expert system.\\nS\\nSatisfiability\\nAn expression is satisfiable if it true under some *interpretation.\\nSAT planning'), Document(metadata={}, page_content='SAT planning\\nA method of determining whether a suitable *plan exists to solve a given\\nproblem. The problem is represented as a set of expressions, and if those\\nexpressions can be shown to be *satisfiable, then a plan can be devised to\\nsolve the problem.\\nScheduling\\nA method for allocating resources to machines (or other agents).\\nScheduling takes account of the time each task takes, which can be com-\\npared with *planning in which the time taken to carry out each task is\\nusually ignored.\\nSchema'), Document(metadata={}, page_content='usually ignored.\\nSchema\\nA template used to represent a set of *chromosomes, using the symbol * to\\nrepresent any value. See *messy genetic algorithm, *genetic algorithm.\\nSchema theorem\\nA theorem that states that short, low-order schemata that are fitter than the\\naverage fitness of the population will appear with exponentially increasing\\nregularity in subsequent generations. See *genetic algorithm, *schema.\\nScript\\nA structured *representation for a scenario that involves a sequence of'), Document(metadata={}, page_content='events, such as buying a house or going to a restaurant.'), Document(metadata={}, page_content='684 Glossary\\nSearch\\nThe process of locating a solution to a problem by systematically looking at\\nnodes in a *search tree or *search space until a *goal node is located. See\\n*heuristic, *blind search method, *informed search method.\\nSearch space\\nThe set of possible permutations that can be examined by a *search method in\\norder to find a solution. The search space represents every possible solution\\nand all the arrangements that do not satisfy the problem’s *constraints.\\nSearch tree'), Document(metadata={}, page_content='Search tree\\nA *tree that is used to represent a *search problem and is examined by a\\nsearch method to search for a solution.\\nSegmentation\\nThe process of breaking an image down into homogeneous areas. See *edge\\ndetection, *image recognition.\\nSemantic analysis\\nThe analysis of the meaning of words in an *utterance in a human lan-\\nguage. See *syntactic analysis, morphologic analysis, *pragmatic analysis,\\n*natural language processing.\\nSemantic net'), Document(metadata={}, page_content='*natural language processing.\\nSemantic net\\nA *graph in which the *nodes represent objects and the *edges between\\nnodes represent relationships between the objects. See *semantic tree.\\nSemantic tree\\nA *semantic net in which each *node has exactly one *predecessor, apart\\nfrom the *root node, which has none.\\nSentence\\nSee *well-formed formula.\\nS-expression\\nA symbolic expression used by *LISP as either data or as a program to\\nbe executed.'), Document(metadata={}, page_content='Glossary 685\\nShepard’ s method\\nA variant of the *nearest neighbor algorithm in which the contribution of\\neach neighbor is determined by its distance from the point that is being\\nclassified.\\nSigmoid function\\nA mathematical function that is defined as follows:\\nThe sigmoid function is often used as the *activation function in *back-\\npropagation *neural networks because it is easy to differentiate.\\nSign activation function\\nA mathematical function that is usually used as the *activation function in'), Document(metadata={}, page_content='*Hopfield networks:\\nSimulated annealing\\nA *local search method based on the way in which metal or glass can be\\nmade very strong by being heated and then cooled very slowly.\\nSituated action rule\\nA *rule used by an *augmented finite state automaton that takes the form\\ninput -> action.\\nSituation calculus\\nA form of *first-order predicate calculus that is able to represent change\\nand the way in which variables relate to each other over time.\\nSituation variable'), Document(metadata={}, page_content='Situation variable\\nA variable used in a *situation calculus expression that represents the situ-\\nation that that expression represents. For example, in the following expres-\\nsion, S\\n1 is the situation variable:\\n∃x(In(Robot, x, S1) ∧ In(cheese, x, S1))\\nSign X for X\\nfor X( ) = +>\\n−<\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n10\\n10\\nσ x\\ne x( ) =\\n+ −\\n1\\n1'), Document(metadata={}, page_content='686 Glossary\\nSkolem constant\\nA variable that is used to replace an *existentially quantified variable when\\n*skolemizing an expression.\\nFor example, the expression ∃x.x → b would be skolemized as x → c,w h e r e\\nc is the skolem constant.\\nSkolem function\\nA function that is used to replace an *existentially quantified variable that\\ncomes after a *universal quantification when *skolemizing an expression.\\nFor example, ∀x ∃y (x ∧ y) → b would be skolemized as ∀x (x ∧ f(x)) → b),'), Document(metadata={}, page_content='where f(x) is the skolem function.\\nSkolemization\\nThe process of replacing an *existentially quantified variable in an expres-\\nsion with a *skolem constant or *skolem function. Skolemization is a part\\nof the process of *resolution and results in an expression that is in *skolem\\nnormal form.\\nSkolem normal form\\nThe normal form produced by applying the process of *skolemization to\\nan expression.\\nSlipnet\\nA structure that represents the long-term memory of the *copycat architecture.\\nSlot'), Document(metadata={}, page_content='Slot\\nA named variable that is used in a *frame system to store an item of data.\\nSlot reader\\nA *procedure that returns the value that is stored in a *slot.\\nSlot value\\nThe item of data that is stored in a *slot in a *frame system.\\nSlot writer\\nA *procedure that inserts a value into a *slot.\\nSmart agent\\nA fully *autonomous, *intelligent, cooperative *agent. Smart agents are the\\nultimate goal of *agent research.'), Document(metadata={}, page_content='Glossary 687\\nSmoothing\\nThe process of removing noise from an image. See *convolution.\\nSoftware agent\\nAn *agent that exists only as a computer program, as compared with a\\n*robotic agent.\\nSoundness\\nA logical system is sound if every *theorem is a *tautology. See *completeness.\\nSpidering\\nThe process of retrieving documents from the Internet by following hyper-\\ntext links from one page to another. Spidering systems usually follow some\\nsearch strategy such as *depth-first or *breadth-first search.'), Document(metadata={}, page_content='Stack\\nA data structure that stores its data sequentially and has just two operators:\\n“push, ” which places a new item onto the top of the stack, and “pop, ” which\\nremoves the top item from the stack. A stack is a “LIFO” or last-in-first-out\\ndata structure.\\nState\\nThe set of variables that define the current situation in a *world model.\\nState space\\nSee *search space.\\nStatic evaluator\\nA function used to evaluate a single position in a game, such as chess.\\nStemming'), Document(metadata={}, page_content='Stemming\\nThe process of removing suffixes from words to render words with a com-\\nmon stem into the same form. For example, the following words would all\\nbe stemmed to swim: swimmer, swimming, swimmers, swims. Most stem-\\nmers would not successfully convert swam or swum into swim. Stemmers\\nare used in *information retrieval systems to increase *recall.\\nStop list\\nA list of words that an *information retrieval system is instructed to ignore'), Document(metadata={}, page_content='in queries and in the corpus it is searching against. The stop list usually\\ncontains extremely common words such as and, the, and of.'), Document(metadata={}, page_content='688 Glossary\\nSTRIPS\\nAn operator-based *planning approach and the corresponding planning lan-\\nguage that uses *well-formed formulae to represent the *state of the world.\\nSTRIPS assumption\\nThe assumption used by *STRIPS *planning systems that any statement\\nthat is true before applying an operator is also true after applying the oper-\\nator, unless it is included in the operator’s delete list.\\nStrong AI\\nThe belief that a computer system that is given sufficient processing power'), Document(metadata={}, page_content='and sufficiently powerful artificial intelligence would actually be capable of\\nhaving mental states in the way that humans are. See *weak AI.\\nStrong methods\\nArtificial Intelligence methods that rely on systems with a great deal of\\nknowledge built in. See *expert system, *weak methods.\\nSubclass\\nThe *class that *inherits the properties of a *superclass.\\nSubgoal\\nSee *subproblem.\\nSubproblem\\nOne of the smaller problems into which a large problem is broken down by'), Document(metadata={}, page_content='the process of *problem reduction. By achieving all of the subproblems of a\\nproblem, the complete problem can be solved.\\nSubset\\nSet A is a subset of set B if A is wholly contained by B. For example, the set\\nof all men is a subset of the set of all humans.\\nSubstitution\\nThe process of replacing a *free variable in an expression with another free\\nvariable in order to facilitate the process of *unification. See *resolution.\\nSubsumption architecture'), Document(metadata={}, page_content='Subsumption architecture\\nA layered architecture designed for the control of reactive *robotic agents.'), Document(metadata={}, page_content='Glossary 689\\nSuccess node\\nAn *and-node in a *goal tree that is a *leaf node.\\nSuccessor\\nThe *node immediately below a given node in a *tree. Each node has one or\\nmore successors, except for leaf nodes, which have no successors. See\\n*ancestor, *descendant, *predecessor.\\nSuperclass\\nThe *class from which a subclass *inherits certain properties.\\nSupervised learning\\nA form of *learning method in which the learning system is presented with'), Document(metadata={}, page_content='a set of preclassified data before being expected to classify unseen data. See\\n*backpropagation, *competitive learning, *unsupervised learning.\\nSyllogism\\nA logical argument that contains a set of statements (premises) from which\\nis logically derived a conclusion. An example of a syllogism is:\\nAll cats have two ears.\\nMavis is a cat.\\nTherefore, Mavis has two ears.\\nSynapse\\nA connection between two *neurons in the human brain.\\nSyntactic analysis'), Document(metadata={}, page_content='Syntactic analysis\\nThe analysis of the grammatical or syntactic structure of an *utterance in a\\n*human language. See *parser.\\nT\\nTabu search\\nA *metaheuristic that uses a list of states that have already been visited to\\nattempt to avoid repeating *paths.\\nTautology\\nAn expression that is true under all *interpretations.'), Document(metadata={}, page_content='690 Glossary\\nTemporal logic\\nA form of *modal logic that was designed to reason about change and the\\neffects of time.\\nTerm\\nA *constant, a variable, or a function of terms in *first-order predicate calculus.\\nTerm frequency - Inverse document frequency (TF-IDF)\\nA method used in *information retrieval to identify words that occur rarely\\nin a *corpus of text, but frequently in a particular document.\\nTerminal symbol\\nA symbol used in expressing a *grammar for a language that represents a'), Document(metadata={}, page_content='real word that appears in the language. See *nonterminal symbol.\\nTexel\\nA single *texture element that is repeated in an image to produce an area of\\na particular texture.\\nTexture\\nA perceived pattern on the surface of an object in an image. See *texel,\\n*image recognition.\\nTheorem\\nA theorem of a logical system is a statement that can be proved by applying\\nthe rules of *deduction to the axioms in the system.\\nThree-coloring problem\\nSee *map coloring.\\nTit for tat'), Document(metadata={}, page_content='See *map coloring.\\nTit for tat\\nA strategy employed when playing the *Prisoner’s Dilemma game. The\\nstrategy involves cooperating on the first iteration of the game, and then\\nfor each subsequent iteration, doing what the opponent did in the previous\\niteration.\\nTop down\\nAn approach to solving problems that involves recursively breaking a prob-\\nlem down into smaller *subproblems until trivial problems are obtained\\nand the whole problem can be solved. See *bottom up, *problem reduction.'), Document(metadata={}, page_content='Glossary 691\\nTowers of Hanoi\\nA problem that involves moving a number of discs from one peg to\\nanother. The *constraints that are applied are that no disc can ever be\\nplaced on top of a smaller disc and that a disc cannot be moved if it has\\nanother disc on top of it.\\nTraining\\nThe process of teaching a *learning system to classify data by showing it\\npreclassified *training data.\\nTraining data\\nThe preclassified data that is shown to a *learning system in the *train-\\ning phase.\\nTransition network'), Document(metadata={}, page_content='ing phase.\\nTransition network\\nA *finite state automaton used to represent a part of a *grammar.\\nTrihedral vertex\\nA vertex at which three faces meet.\\nTruth maintenance system (TMS)\\nA system that stores a set of beliefs along with information about how those\\nbeliefs were derived. The TMS is able to use this information to retract beliefs\\nif conflicting information later arrives. See *nonmonotonic reasoning.\\nTruth table\\nA table that represents the behavior of a logical operator by showing the'), Document(metadata={}, page_content='possible input *truth values for the operator and the corresponding output\\ntruth values.\\nTruth value\\nA representation of whether an expression is correct or not. In *classical\\nlogic, the truth values aretrue and false. In *multivalent logics, there are more\\ntruth values. *Fuzzy logic has an infinite range of truth values from 0 to 1.\\nTuring Test\\nA test devised by Alan Turing to determine whether an attempt to create a\\ntruly intelligent computer has been successful or not, by seeing whether the'), Document(metadata={}, page_content='computer can fool a human into thinking that it might actually be human\\ntoo. See *strong AI.'), Document(metadata={}, page_content='692 Glossary\\nU\\nUnary operator\\nA logical or mathematical operator that takes just one argument, such as ¬\\n(logical negation).\\nUncertainty\\nA lack of knowledge about the world. Most real-world problems are full of\\nuncertainty, but many Artificial Intelligence techniques deal very poorly\\nwith uncertainty.\\nUnderspecified chromosome\\nA *chromosome in which one or more bits do not have any value assigned\\nto them. See *messy genetic algorithm, *Overspecified chromosome.\\nUnification'), Document(metadata={}, page_content='Unification\\nThe process of applying a *substitution to a set of *clauses that enables\\nthose clauses to be *resolved.\\nUnifier\\nA *substitution that is applied to a set of clauses to enable those clauses to\\nbe resolved.\\nUniform crossover\\nA form of *crossover in which a probability, p, is used to determine\\nwhether a given bit from parent 1 will be used or from parent 2.\\nUniform tree\\nA *tree in which each non-leaf *node has the same *branching factor.\\nUninformed Search'), Document(metadata={}, page_content='Uninformed Search\\nA *search method that does not use *heuristics.\\nUniversal quantifier\\nThe quantifier \\n∀, which can be read “for all” and indicates that a property\\nholds for all possible values of the quantified variable. Hence, ∀x.P(x) can\\nbe read as “for all values ofx, P(x) is true. ” See *existential quantifier, *first-\\norder predicate calculus.\\nUniverse of discourse\\nThe range of possible values for a *linguistic variable.'), Document(metadata={}, page_content='Glossary 693\\nUnsupervised learning\\nA form of *learning method that does not require training or any other human\\nintervention. See *Hebbian learning, *Kohonen map, *supervised learning.\\nUtility-based agent\\nAn *agent that seeks to maximize some *utility function.\\nUtility function\\nA function that defines for any state how successful an *agent has been. A\\nhigh utility function is the goal of any *rational agent.\\nUtterance\\nA *sentence or partial sentence in a *human language that is spoken or'), Document(metadata={}, page_content='written by a human being or by an agent.\\nV\\nValidity\\nA logical *deduction is valid if its conclusions follow logically from its premises.\\nVanishing point\\nThe point to which perspective causes all parallel lines to appear to vanish.\\nVerb\\nA word in a *human language that is used to define an action. See *verb\\nphrase, *noun.\\nVerb phrase\\nA phrase that has the same properties within a *grammar as a *verb and can\\nthus be used interchangeably with a verb, at least as far as the syntactic rules of'), Document(metadata={}, page_content='the grammar are concerned. For example, the following are all verb phrases:\\njump, jump over the moon, jumping over the table which is next to the door.\\nVersion space\\nThe set of *hypotheses that correctly map each of a set of *training data to\\nits classification.\\nW\\nWeak AI\\nThe view that intelligent behavior can be modeled and used by computers\\nto solve complex problems. See *strong AI.'), Document(metadata={}, page_content='694 Glossary\\nWeak methods\\nArtificial Intelligence methods that use logic or other representational sys-\\ntems to solve problems but do not rely on any real-world knowledge. See\\n*strong methods.\\nWeight\\nA value associated with each connection between *neurons in an artificial\\n*neural network that indicates how important that connection is and how\\nmuch of a role in the learning process that connection plays.\\nWeighted linear function'), Document(metadata={}, page_content='Weighted linear function\\nA function that takes the form ax + by + cz +  ...,w h e r e  a  n u m b e r  o f v a r i -\\nables (in this case x, y, z...)  a r e  e a c h  m u l tiplied by a weight (in this case a,\\nb, c...)  and the results summed.\\nWeight vector\\nA vector that represents the *weights of all connections from a given *neu-\\nron in an artificial *neural network.\\nWell-formed formula (wff)\\nAn expression that is correctly constructed according to the syntactic rules'), Document(metadata={}, page_content='of *propositional calculus or *first-order predicate calculus.\\nWHEN-CHANGED procedure\\nA *procedure that is run automatically whenever the value of a *slot is changed.\\nWHEN-NEEDED procedure\\nA *procedure that is run automatically when the value of a given *slot\\nneeds to be determined.\\nWHEN-READ procedure\\nA *procedure that is run automatically whenever the value of a *slot is read.\\nWHEN-WRITTEN procedure\\nSee  *WHEN-CHANGED procedure.\\nWinner takes all algorithm'), Document(metadata={}, page_content='Winner takes all algorithm\\nThe algorithm used by *Kohonen maps to assign credit to nodes in the net-\\nwork. See *competitive learning, *credit assignment.'), Document(metadata={}, page_content='Glossary 695\\nWorkspace\\nA data structure similar to a *blackboard that is used by the *copycat\\narchitecture.\\nWorld model\\nA representation of the state of the world or environment as it affects an\\n*agent.\\nZ\\nZero sum game\\nA game in which if one player wins, the other player loses.'), Document(metadata={}, page_content='This page intentionally left blank'), Document(metadata={}, page_content='Bibliography\\nA\\nIntroduction to Artificial Life, by Christoph Adami (1997 – T elos)\\nNatural Language Understanding, by James Allen (1995 – Addison Wesley)\\nReasoning About Plans, by James F. Allen, Henry A. Kautz, Josh T enenberg,\\nand Richard Pelavin (1991 – Morgan Kaufmann)\\n2D Object Detection and Recognition: Models, Algorithms, and Networks ,b y\\nY ali Amit (2002 – MIT Press)\\nNonmonotonic Reasoning, by Grigoris Antoniou (1997 – MIT Press)'), Document(metadata={}, page_content='The Handbook of Brain Theory and Neural Networks: Second Edition, edited\\nby Michael A. Arbib (2002 – MIT Press)\\nBehavior-Based Robotics, by Ronald C. Arkin (1998 – MIT Press)\\nReal-Time and Multi-Agent Systems, by Ammar Attoui (2000 – Springer V erlag)\\nGenetic Algorithm for the Prisoner Dilemma Problem , by R. Axelrod (1987 -\\nin Genetic Algorithms and Simulated Annealing, edited by L. Davis – Hyper-\\nion Books)\\nB\\nHandbook of Evolutionary Computation, edited by T. Bäck, D. B. Fogel, and'), Document(metadata={}, page_content='Z. Michalewicz (1997- Institute of Physics Publishing)\\nFrancis Bacon: The New Organon , by Francis Bacon; edited by Lisa Jardine\\nand Michael Silverthorne (2002 – Cambridge University Press)'), Document(metadata={}, page_content='698 Bibliography\\nModern Information Retrieval , by Ricardo Baeza-Y ates and Berthier\\nRibeiro-Neto (1999 – Addison Wesley)\\nModeling the Internet and the Web: Probabilistic Methods and Algorithms,b y\\nPierre Baldi, Paolo Frasconi, and Padhraic Smyth (2003 – John Wiley &\\nSons)\\nGenetic Programming: An Introduction: On the Automatic Evolution of Com-\\nputer Programs and Its Applications , by Wolfgang Banzhaf, Peter Nordin,\\nRobert E. Keller, and Frank D. Francone (1997 – Morgan Kaufmann)'), Document(metadata={}, page_content='Knowledge Representation, Reasoning and Declarative Problem Solving ,b y\\nChitta Baral (2003 – Cambridge University Press)\\nThe Handbook of Artificial Intelligence, edited by A. Barr and E. Feigenbaum\\n(1989 – William Kaufman)\\nIntelligent Machine Vision: Techniques, Implementations and Applications ,\\nby Bruce G. Batchelor and Frederick M. Waltz (2001 – Springer V erlag)\\nDigital Biology, by Peter Bentley (2002 – Simon & Schuster)'), Document(metadata={}, page_content='Bayesian Theory, by José M. Bernardo and Adrian F. M. Smith (2001 – John\\nWiley & Sons)\\nNeural Networks for Pattern Recognition, by Christopher M. Bishop (1996 –\\nOxford University Press)\\nFundamentals of Expert System Technology: Principles and Concepts ,b y\\nSamuel J. Biondo (1990 – Intellect)\\nRecent Advances in AI Planning, by Susanne Biundo and Maria Fox (2000 –\\nSpringer V erlag)\\nFast Planning Through Planning Graph Analysis , by A. Blum and M. Furst'), Document(metadata={}, page_content='(1997 – in Artificial Intelligence, Vol. 90, pp. 281–300)\\nA Logical Theory of Nonmonotonic Inference and Belief Change , by Alexan-\\nder Bochman (2001 – Springer V erlag)\\nThe Philosophy of Artificial Life, by Margaret A. Boden (1996 – Oxford Uni-\\nversity Press)\\nSwarm Intelligence: From Natural to Artificial Systems , by Eric Bonabeau,\\nMarco Dorigo, and Guy Theraulaz (1999 – Oxford University Press)\\nUnderstanding 99% of Artificial Neural Networks: Introduction & Tricks ,b y'), Document(metadata={}, page_content='Marcelo Bosque (2002 – Writers Club Press)'), Document(metadata={}, page_content='Bibliography 699\\nAdvances in Image Understanding: A Festschrift for Azriel Rosenfeld , edited\\nby Kevin W. Bowyer and Narendra Ahuja (1996 – Wiley IEEE Press)\\nSoftware Agents, edited by Jeffrey M. Bradshaw (1997 – AAAI Press)\\nRobot Motion: Planning and Control, edited by Michael Brady, John Holler-\\nbach, Timothy Johnson, T omás Lozano-Pérez, and Matthew T. Mason\\n(1983 – MIT Press)\\nEmpirical Analysis of Predictive Algorithms for Collaborative Filtering ,b y'), Document(metadata={}, page_content='John S. Breese, David Heckerman, and Carl Kadie (1998 – in Proceedings of\\nthe Fourteenth Conference on Uncertainty in Artificial Intelligence ,M o r g a n\\nKaufmann)\\nNonmonotonic Reasoning: An Overview , by Gerhard Brewka, Jürgen Dix,\\nand Kurt Konolige (1995 – Cambridge University Press)\\nNonmonotonic Reasoning: From Theoretical Foundation to Efficient Compu-\\ntation, by G. Brewka (1991 – Cambridge University Press)\\nCambrian Intelligence: The Early History of the New AI , by Rodney A.'), Document(metadata={}, page_content='Brooks (1999 – MIT Press)\\nRule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuris-\\ntic Programming Project , by B. G. Buchanan and E. H. Shortliffe (1984 –\\nAddison Wesley)\\nPropositional Logic: Deduction and Algorithms, by Hans Kleine Büning and\\nTheodor Lettmann (1999 – Cambridge University Press)\\nA Resolution Principle for a Logic with Restricted Quantifiers, by H. J. Burck-\\nert (1992 – Springer V erlag)\\nC\\nThe Essence of Neural Networks, by Robert Callan (1999 – Prentice Hall)'), Document(metadata={}, page_content='Prolog Programming for Students: With Expert Systems and Artificial Intelli-\\ngence Topics, by David Callear (2001 – Continuum)\\nEfficient and Accurate Parallel Genetic Algorithms, by Erick Cantu-Paz (2002\\n– Kluwer Academic Publishers)\\nPlan Recognition in Natural Language Dialogue , by Sandra Carberry (1990\\n– MIT Press)\\nNeural Networks for Vision and Image Processing, edited by Gail A. Carpen-\\nter and Stephen Grossberg (1992 – MIT Press)'), Document(metadata={}, page_content='700 Bibliography\\nSymbolic Logic and Game of Logic, by Lewis Carroll (Published in one vol-\\nume – 1958 – Dover Books)\\nExpert Systems and Probabilistic Network Models , by Enrique Castillo, Jose\\nManuel Gutierrez, and Ali S. Hadi (1997 – Springer V erlag)\\nThe Essence of Artificial Intelligence , by Alison Cawsey (1998 – Prentice\\nHall)\\nArtificial Intelligence, by Jack Challoner (2002 – Dorling Kindersley, Essen-\\ntial Science)\\nPractical Handbook of Genetic Algorithms, by Lance Chambers (1995 – CRC'), Document(metadata={}, page_content='Press)\\nSymbolic Logic and Mechanical Theorem Proving, by Chin-Liang Chang and\\nRichard Char-Tung Lee (1973 – Academic Press)\\nIntroduction to Artificial Intelligence , by Eugene Charniak and Drew\\nMcDermott (1985 – Addison Wesley; out of print)\\nGenetic Algorithms and Genetic Programming in Computational Finance ,\\nedited by Shu-Heng Chen (2002 – Kluwer Academic Publishers)\\nLearning from Data: Concepts, Theory, and Methods , by Vladimir\\nCherkassky and Filip Mulier (1998 – Wiley Interscience)'), Document(metadata={}, page_content='The Computational Brain , by Patricia S. Churchland and T errence J.\\nSejnowski (1992 – The MIT Press)\\nAn Introduction to Genetic Algorithms for Scientists and Engineers, by David\\nA. Coley (1999 – World Scientific Publishing Company)\\nAdaptive Parallel Iterative Deepening Search, by Diane J. Cook and R. Craig\\nVarnell (1998 – in Journal of Artificial Intelligence Research, Vol. 9, pp.\\n139–166)\\nIntroduction to Algorithms , by Thomas H. Cormen, Charles E. Leiserson,'), Document(metadata={}, page_content='Ronald L. Rivest, and Clifford Stein (2001 – MIT Press)\\nProbabilistic Networks and Expert Systems , edited by Robert G. Cowell\\n(1999 – Springer V erlag)\\nThe Fuzzy Systems Handbook: A Practitioner’s Guide to Building, Using, &\\nMaintaining Fuzzy Systems, by Earl Cox (1999 – Morgan Kaufmann)\\nFuzzy Logic for Business and Industry , by Earl Cox (2000 – Charles River\\nMedia)'), Document(metadata={}, page_content='Bibliography 701\\nAI: The Tumultuous History of the Search for Artificial Intelligence, by Daniel\\nCrevier (1999 – Basic Books)\\nThe Turing Test and the Frame Problem: AI’s Mistaken Understanding of\\nIntelligence, by Larry J. Crockett (1994 – Intellect)\\nD\\nThe Origin of Species, by Charles Darwin (1859 – reprinted, by Penguin)\\nArtificial Immune Systems and Their Applications, edited by Dipankar Das-\\ngupta (1999 – Springer V erlag)'), Document(metadata={}, page_content='gupta (1999 – Springer V erlag)\\nMultiobjective Heuristic Search: An Introduction to Intelligent Search Meth-\\nods for Multicriteria Optimization , by Pallab Dasgupta, P . P . Chakrabarti,\\nand S. C. Desarkar (1999 - Friedrich Vieweg & Sohn)\\nSocially Intelligent Agents - Creating Relationships with Computers and\\nRobots, edited by Kerstin Dautenhahn, Alan H. Bond, Lola Canamero, and\\nBruce Edmonds (2002 – Kluwer Academic Publishers)'), Document(metadata={}, page_content='Bruce Edmonds (2002 – Kluwer Academic Publishers)\\nMachine Vision: Theory, Algorithms, Practicalities , by E. R. Davies (1996 –\\nAcademic Press)\\nAdaptive Learning ,b y  Genetic Algorithms: Analytical Results and Applica-\\ntions to Economic Models, by Herbert Dawid (1999 – Springer V erlag)\\nThe Blind Watchmaker, by Richard Dawkins (1996 – W. W. Norton & Com-\\npany)\\nArtificial Immune Systems: A New Computational Intelligence Paradigm ,b y'), Document(metadata={}, page_content='Leandro N. de Castro and Jonathan Timmis (2002 – Springer V erlag)\\nA Generalization of Bayesian Inference, by A. P . Dempster (1968 - in Journal\\nof the Royal Statistical Society Vol. 30 (pp. 205–217))\\nBayesian Methods for Nonlinear Classification and Regression , by David\\nG. T. Denison, Christopher C. Holmes, Bani K. Mallick, and Adrian F. M.\\nSmith (2002 – John Wiley & Sons)\\nBrainstorms: Philosophical Essays on Mind and Psychology , by Daniel Den-\\nnett (1978 – Bradford)'), Document(metadata={}, page_content='nett (1978 – Bradford)\\nConsciousness Explained, by Daniel Dennett (1992 – Little, Brown & Co.)\\nProbabilistic Theory of Pattern Recognition , by Luc Devroye, Laszlo Gyorfi,\\nand Gabor Lugosi (1998 – Springer V erlag)'), Document(metadata={}, page_content='702 Bibliography\\nUnderstanding Agent Systems, edited by Mark D’Inverno and Michael Luck\\n(2001 – Springer V erlag)\\nA Truth Maintenance System, by Jon Doyle (1979 – in Computation & Intel-\\nligence – Collected Readings, edited by George F. Luger, The MIT Press)\\nWhat Computers Still Can’t Do , by Hubert L. Dreyfus (1999 – The MIT\\nPress)\\nAn Introduction to Fuzzy Control, by Dimiter Driankov, Hans Hellendoorn\\nand M. Reinfrank (1996 – Springer V erlag)'), Document(metadata={}, page_content='and M. Reinfrank (1996 – Springer V erlag)\\nHow to Solve it, by Computer, by R. G. Dromey (1982 – out of print)\\nArtificial Intelligence: Strategies, Applications, and Models Through Search ,\\nby Benedict Du Boulay and Christopher James Thornton (1999 – AMA-\\nCOM)\\nE\\nIntelligent Agents for Mobile and Virtual Media , edited by Rae Earnshaw,\\nJohn Vince, and Margaret A. Arden (2002 – Springer V erlag)\\nNeural Darwinism: The Theory of Neuronal Group Selection , by Gerald M.'), Document(metadata={}, page_content='Edelman (1990 – Oxford University Press)\\nComputational Intelligence: An Introduction , by Andries P . Engelbrecht\\n(2003 – John Wiley & Sons)\\nPredicate Logic: The Semantic Foundations of Logic , by Richard L. Epstein\\n(2000 – Wadsworth Publishing)\\nPropositional Logics: The Semantic Foundations of Logic , by Richard L.\\nEpstein (2000 – Wadsworth Publishing)\\nF\\nFuzzy Control: Synthesis and Analysis, edited by Shehu S. Farinwata, Dimi-\\ntar P . Filev, and Reza Langari (2000 – John Wiley & Sons)'), Document(metadata={}, page_content='Three-Dimensional Computer Vision , by Olivier Faugeras (1993 – MIT\\nPress)\\nFundamentals of Neural Networks , by Laurene V . Fausett (1994 – Prentice\\nHall)\\nSTRIPS: A New Approach to the Application of Theorem Proving to Problem\\nSolving, by Richard E. Fikes and Nils J. Nilsson (1971 – in Computation &\\nIntelligence, edited by George F. Luger, 1995, MIT Press)'), Document(metadata={}, page_content='Bibliography 703\\nArtificial Intelligence: A Knowledge-Based Approach, by Morris W. Firebaugh\\n(1988 – Boyd & Fraser Publishing Company – out of print)\\nThe Anatomy of Programming Languages, by Alice E. Fischer and Frances S.\\nGrodzinsky (1993 – Prentice Hall)\\nBlondie 24: Playing at the Edge of AI , by David B. Fogel (2001 – Morgan\\nKaufmann)\\nEvolutionary Computation in Bioinformatics , edited by Gary B. Fogel and\\nDavid W. Corne (2002 – Morgan Kaufmann)'), Document(metadata={}, page_content='David W. Corne (2002 – Morgan Kaufmann)\\nThe Robots Dilemma Revisited: The Frame Problem in Artificial Intelligence ,\\nby Kenneth M. Ford and Zenon W. Pylyshyn (1996 – Ablex Publishing)\\nComputer Vision: A Modern Approach, by David A. Forsyth and Jean Ponce\\n(2002 – Prentice Hall)\\nParallel Computing Works, by G. C. Fox, R. D. Williams, and P . C. Messina\\n(1994 – Morgan Kaufmann)\\nUnderstanding Artificial Intelligence (Science Made Accessible), compiled by\\nSandy Fritz (2002 – Warner Books)\\nG'), Document(metadata={}, page_content='Sandy Fritz (2002 – Warner Books)\\nG\\nHandbook of Logic in Artificial Intelligence and Logic Programming: Nonmo-\\nnotonic Reasoning and Uncertain Reasoning, edited by Dov M. Gabbay, J. A.\\nRobinson, and Christopher John Hogger (1994 – Oxford University Press)\\nBayesian Data Analysis, by Andrew Gelman, Donald B. Rubin, and Hal S.\\nStern (2003 – CRC Press)\\nGenetic Algorithms and Engineering Optimization, by Mitsuo Gen and Run-\\nwei Cheng (1991 – Wiley Interscience)'), Document(metadata={}, page_content='wei Cheng (1991 – Wiley Interscience)\\nExpert Systems: Principles and Programming, by Joseph C. Giarratano (1998\\n– Brooks Cole)\\nT abu Search, by Fred W. Glover and Manuel Laguna (1998 – Kluwer Acade-\\nmic Publishers)\\nGenetic Algorithms in Search, Optimization and Machine Learning, by David\\nE. Goldberg (1989 – Addison Wesley)\\nMessy Genetic Algorithms: Motivation, Analysis and First Results , by D. E.\\nGoldberg (1989 - in Complex Systems, Vol. 3, pp. 493–530)'), Document(metadata={}, page_content='704 Bibliography\\nRapid, Accurate Optimization of Difficult Problems Using Fast Messy Genetic\\nAlgorithms, by David E. Goldberg, Kalyanmoy Deb, Hillol Kargupta, and\\nGeorges Harik (1993 – in Proceedings of the Fifth International Conference\\non Genetic Algorithms, pp. 56–64, Morgan Kaufmann)\\nDynamic Vision: From Images to Face Recognition , by Shaogang Gong,\\nStephen J. McKenna, and Stephen J. McKenna (2000 – Imperial College\\nPress)'), Document(metadata={}, page_content='Press)\\nCreation: Life and How to Make It, by Steve Grand (2001 – Harvard Univer-\\nsity Press)\\nCross-Language Information Retrieval , edited by Gregory Grefenstette\\n(1998 – Kluwer Academic Publishing)\\nManaging Uncertainty in Expert Systems, by Jerzy W. Grzymala-Busse (1991\\n– Kluwer Academic Publishers)\\nAn Introduction to Neural Networks, by Kevin Gurney (1997 – UCL Press)\\nH\\nFrom Animals to Animats 7: Proceedings of the Seventh International Confer-'), Document(metadata={}, page_content='ence on Simulation of Adaptive Behavior , edited by Bridget Hallam, Dario\\nFloreano, John Hallam, Gillian Hayes, and Jean-Arcady Meyer (2002 – MIT\\nPress; also available are the proceedings from the first to sixth conferences)\\nComputer and Robot Vision (Volume II), by Robert M. Haralick and Linda\\nG. Shapiro (2002 – Pearson Education)\\nAlgorithmics: The Spirit of Computing , by David Harel (1987 – Addison\\nWesley)\\nExpert Systems: Artificial Intelligence in Business , by Paul Harmon (1985 –'), Document(metadata={}, page_content='John Wiley & Sons – out of print)\\nArtificial Intelligence: The Very Idea , by J. Haugeland (1985 – The MIT\\nPress)\\nPractical Genetic Algorithms, by Randy L. Haupt and Sue Ellen Haupt (1998\\n– Wiley Interscience)\\nFoundations of Computational Linguistics: Human-Computer Communica-\\ntion in Natural Language, by Roland R. Hausser (2001 – Springer V erlag)\\nNeural Networks: A Comprehensive Foundation, by Simon S. Haykin (1998 –\\nPrentice Hall)'), Document(metadata={}, page_content='Bibliography 705\\nThe Organisation of Behavior: A Neuropsychological Theory ,b y  D .O .H e b b\\n(1949 – republished in 2002, by Lawrence Erlbaum Assoc.)\\nProbabilistic Interpretations for Mycin’s Certainty Factors, by O. Heckerman\\n(1986 – in Uncertainty in Artificial Intelligence , edited by L. N. Kanal and\\nJ. F. Lemmer, Elsevier Science, pp. 167–196)\\nSilicon Second Nature: Culturing Artificial Life in a Digital World , by Stefan\\nHelmreich (2000 – University of California Press)'), Document(metadata={}, page_content='Helmreich (2000 – University of California Press)\\nInformation Retrieval, by William R. Hersh (2002 – Springer V erlag)\\nFuzzy and Neural Approaches in Engineering , by J. Wesley Hines (1997 –\\nWiley Interscience)\\nVisual Intelligence: How We Create What We See , by Donald D. Hoffman\\n(1998 – W. W. Norton & Company)\\nBraitenberg Creatures, by David W. Hogg, Fred Martin, and Mitchel Resnick\\n(1991 - originally published as Epistemology and Learning Memo #13)'), Document(metadata={}, page_content='Adaptation in Natural and Artificial Systems: An Introductory Analysis with\\nApplications to Biology, Control, and Artificial Intelligence , by John H. Hol-\\nland (1992 – MIT Press)\\nRobot Vision, by Berthold K. Horn (1986 – McGraw Hill Higher Education)\\nBehind Deep Blue: Building the Computer That Defeated the World Chess\\nChampion, by Feng-Hsiung Hsu (2002 – Princeton University Press)\\nSpoken Language Processing: A Guide to Theory, Algorithm and System'), Document(metadata={}, page_content='Development, by Xuedong Huang, Alex Acero, Hsiao-Wuen Hon, and Raj\\nReddy (2001 – Prentice Hall)\\nI\\nNatural Language Processing and Knowledge Representation: Language for\\nKnowledge and Knowledge for Language , edited by Lucja M. Iwanska and\\nStuart C. Shapiro (2000 – AAAI Press)\\nJ\\nIntroduction to Expert Systems, by Peter Jackson (1999 – Addison Wesley)\\nIntroduction to Artificial Intelligence , by Philip C. Jackson (1985 – Dover\\nPublications)'), Document(metadata={}, page_content='706 Bibliography\\nNatural Language Processing for Online Applications: Text Retrieval, Extrac-\\ntion, and Categorization , by Peter Jackson and Isabelle Moulinier (2002 –\\nJohn Benjamins Publishing Company)\\nText-Based Intelligent Systems: Current Research and Practice in Information\\nExtraction and Retrieval , edited by Paul Schafran Jacobs (1992 – Lawrence\\nErlbaum Assoc.)\\nIncreased Rates of Convergence Through Learning Rate Adaptation , by R. A.'), Document(metadata={}, page_content='Jacobs (1987 - in Neural Networks, Vol. 1, pp. 295–307)\\nSpotting and Discovering Terms through Natural Language Processing ,b y\\nChristian Jacquemin (2001 – MIT Press)\\nMachine Vision, by Ramesh Jain, Rangachar Kasturi, and Brian G. Schunck\\n(1995 –McGraw Hill)\\nApplications of Fuzzy Logic: Towards High Machine Intelligence Quotient\\nSystems, edited by Mohammad Jamshidi, Andre Titli, Lotfi Zadeh, and\\nSerge Boverie (1997 – Prentice Hall)'), Document(metadata={}, page_content='Serge Boverie (1997 – Prentice Hall)\\nNeuro-Fuzzy and Soft Computing: A Computational Approach to Learning\\nand Machine Intelligence , by Jyh-Shing Roger Jang, Chuen-Tsai Sun, and\\nEiji Mizutani (1996 – Prentice Hall)\\nSimulated Annealing for Query Results ranking , by B. J. Jansen (1997 – in\\nACM Computer Science Education Conference, ACM Press)\\nAgent Technology: Foundations, Applications, and Markets , edited by\\nNicholas R. Jennings and Michael J. Wooldridge (1998 – Springer V erlag)'), Document(metadata={}, page_content='Emergence: The Connected Lives of Ants, Brains, Cities, and Software ,b y\\nSteven Johnson (2001 – Scribner)\\nAI Application Programming, by M. Tim Jones (2003 – Charles River Media)\\nSpeech and Language Processing: An Introduction to Natural Language Pro-\\ncessing, Computational Linguistics and Speech Recognition, by Dan Jurafsky,\\nJames H. Martin, Keith Vander Linden, and Nigel Ward (2000 – Prentice\\nHall)\\nK\\nMultistage Fuzzy Control: A Model-Based Approach to Fuzzy Control and'), Document(metadata={}, page_content='Decision Making, by Janusz Kacprzyk (1997 – John Wiley & Sons)\\nChoices, Values, and Frames, by Daniel Kahneman (Editor) and Amos Tver-\\nsky (2000 – Cambridge University Press)'), Document(metadata={}, page_content='Bibliography 707\\nAn Introduction to Computational Learning Theory , by Michael J. Kearns\\nand Umesh V . Vazirani (1994 – MIT Press)\\nLearning and Soft Computing: Support Vector Machines, Neural Networks,\\nand Fuzzy Logic Models (Complex Adaptive Systems) , by Vojislav Kecman\\n(2001 – MIT Press)\\nThe Essence of Logic, by John Kelly (1997 – Prentice Hall)\\nOut of Control: The New Biology of Machines, by Kevin Kelly (1994 – Fourth\\nEstate)'), Document(metadata={}, page_content='Estate)\\nSwarm Intelligence, by James Kennedy, Russell C. Eberhart, and Yuhui Shi\\n(2001 – Morgan Kaufmann)\\nKnowledge Acquisition for Expert Systems: A Practical Handbook , by Alison\\nL. Kidd (1987 – Plenum Publishing Corporation)\\nCommitment and Effectiveness of Situated Agents , by D. Kinny and M.\\nGeorgeff (1991 – in Proceedings of the Twelfth International Joint Conference\\non Artificial Intelligence, pp. 82–88, Morgan Kaufmann)'), Document(metadata={}, page_content='Intelligent Information Agents: The Agentlink Perspective (Lecture Notes in\\nComputer Science, 2586) , edited by Matthias Klusch, Sonia Bergamaschi,\\nand Pete Edwards (2003 – Springer V erlag)\\nAn Analysis of Alpha Beta Pruning , by Donald Knuth and R. W. Moore\\n(1975 - in Artificial Intelligence, Vol. 6 (4), pp. 293–326)\\nArt of Computer Programming: Sorting and Searching , by Donald Knuth\\n(1973 – Pearson Addison Wesley)\\nSelf-Organizing Maps, by T euvo Kohonen (2000 – Springer V erlag)'), Document(metadata={}, page_content='Case-Based Reasoning, by Janet Kolodner (1993 – Morgan Kaufmann)\\nLearning to Solve Problems, by Searching for Macro-Operators (Research\\nNotes in Artificial Intelligence, Vol. 5), by Richard E. Korf (1985 – Longman\\nGroup United Kingdom)\\nSearch, by Richard E. Korf (1987 – in Encyclopedia of Artificial Intelligence,\\nedited by E. Shapiro – Wiley)\\nBidirectional Associative Memories, by Bart Kosko (1988 - in IEEE Transac-\\ntions Systems, Man & Cybernetics, Vol. 18, pp. 49–60)'), Document(metadata={}, page_content='Fuzzy Thinking: The New Science of Fuzzy Logic , by Bart Kosko (1994 –\\nHyperion)'), Document(metadata={}, page_content='708 Bibliography\\nGenetic Programming: On the Programming of Computers,b y  Means of Nat-\\nural Selection, by John R. Koza (1992 – MIT Press)\\nGenetic Programming II: Automatic Discovery of Reusable Programs, by John\\nR. Koza (1994 – MIT Press)\\nA Learning Interface Agent for Scheduling Meetings , by R. Kozierok and P .\\nMaes (1993 – in Proceedings of the ACM-SIGCHI International Workshop on\\nIntelligent User Interfaces, ACM Press)'), Document(metadata={}, page_content='Intelligent User Interfaces, ACM Press)\\nStrategic Negotiation in Multiagent Environments , by Sarit Kraus (2001 –\\nMIT Press)\\nComputer Vision and Fuzzy Neural Systems , by Arun D. Kulkarni (2001 –\\nPrentice Hall)\\nThe Age of Spiritual Machines, by Ray Kurzweil (1999 – Viking Penguin)\\nL\\nArtificial Life: An Overview , edited by Christopher Langton (1995 – MIT\\nPress)\\nCase-Based Reasoning: Experiences, Lessons, and Future Directions, edited by\\nDavid B. Leake (1996 –AAAI Press)'), Document(metadata={}, page_content='David B. Leake (1996 –AAAI Press)\\nThe Resolution Calculus, by Alexander Leitsch (1997 – Springer V erlag)\\nBuilding Large Knowledge-Based Systems: Representation and Inference in\\nthe CYC Project, by Douglas B. Lenat and R. V . Guha (1990 – Addison Wes-\\nley)\\nThe Logic of Knowledge Bases , by Hector J. Levesque and Gerhard Lake-\\nmeyer (2001 – MIT Press)\\nFor the Sake of the Argument: Ramsey Test Conditionals, Inductive Inference\\nand Nonmonotonic Reasoning, by Isaac Levi (1996 – Cambridge University'), Document(metadata={}, page_content='Press)\\nArtificial Life: A Report from the Frontier Where Computers Meet Biology,b y\\nSteven Levy (1993 – Vintage Books)\\nMaking Decisions, by D. V . Lindley (1991 – John Wiley & Sons)\\nKnowledge Representation and Defeasible Reasoning (Studies in Cognitive\\nSystems, Vol. 5), edited by Ronald P . Loui and Greg N. Carlson (1990 –\\nKluwer Academic Publishers)'), Document(metadata={}, page_content='Bibliography 709\\nAutomated Theorem Proving: A Logical Basis, by Donald W. Loveland (1978\\n– Elsevier Science – Out of Print)\\nArtificial Intelligence & Manufacturing Research Planning Workshop , edited\\nby George F. Luger (1998 – AAAI)\\nArtificial Intelligence: Structures and Strategies for Complex Problem-Solving,\\nby George F. Luger (2002 – Addison Wesley)\\nComputation & Intelligence: Collected Readings , edited by George F. Luger\\n(1995 – The AAAI Press / The MIT Press)\\nM'), Document(metadata={}, page_content='(1995 – The AAAI Press / The MIT Press)\\nM\\nFoundations of Statistical Natural Language Processing , by Christopher D.\\nManning and Hinrich Schütze (1999 – MIT Press)\\nNonmonotonic Logic: Context-Dependent Reasoning, by V . W. Marek and M.\\nTruszczynski (1993 – Springer V erlag)\\nProbabilistic Situation Calculus , by Paulo Mateus, António Pacheco, Javier\\nPinto, Amílear Sernadas, and Cristina Sernadas (2001 – in Annals of Math-\\nematics and Artificial Intelligence)'), Document(metadata={}, page_content='ematics and Artificial Intelligence)\\nIntelligent Multimedia Information Retrieval , edited by Mark T. Maybury\\n(1997 – AAAI Press)\\nCircumscription: A Form of Non-Monotonic Reasoning , by John McCarthy\\n(1980 – in Computation & Intelligence – Collected Readings , edited by\\nGeorge F. Luger, The MIT Press)\\nSome Expert Systems Need Common Sense , by John McCarthy (1983 – in\\nComputer Culture: The Scientific, Intellectual and Social Impact of the Com-\\nputer, edited by Heinz Pagels, Vol. 426)'), Document(metadata={}, page_content='puter, edited by Heinz Pagels, Vol. 426)\\nA Production System Version of the Hearsay-II Speech Understanding System,\\nby Donald McCracken (1981 - UMI Research)\\nA Logical Calculus of the Ideas Immanent in Nervous Activity , by W. S.\\nMcCulloch and W. Pitts (1943 - in Bulletin of Mathematical Biophysics, Vol.\\n5, pp. 115–137)\\nComputational Linguistics, by T ony McEnery (1992 – Coronet Books – out\\nof print)\\nFuzzy Logic: The Revolutionary Computer Technology That Is Changing Our'), Document(metadata={}, page_content='World, by Daniel McNeill (1994 – Simon & Schuster)'), Document(metadata={}, page_content='710 Bibliography\\nText Information Retrieval Systems , by Charles T. Meadow, Bert R. Boyce,\\nand Donald H. Kraft (2000 – Academic Press)\\nUncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions,\\nby Jerry M. Mendel (2000 – Prentice Hall)\\nBuilding Expert Systems in Prolog, by Dennis Merritt (1995 – Springer V er-\\nlag)\\nGenetic Algorithms + Data Structures = Evolution Programs , by Zbigniew\\nMichalewicz (1999 - Springer)'), Document(metadata={}, page_content='Michalewicz (1999 - Springer)\\nHow to Solve It: Modern Heuristics, by Zbigniew Michalewicz and David B.\\nFogel (1999 – Springer V erlag)\\nA Framework for Representing Knowledge , by Marvin Minsky (1975 – in\\nComputation & Intelligence – Collected Readings, edited by George F. Luger,\\nThe MIT Press)\\nPerceptrons, by Marvin Minsky and Seymour A. Papert (1969 – now avail-\\nable in an extended edition: Perceptrons - Expanded Edition: An Introduc-\\ntion to Computational Geometry, 1987 – MIT Press)'), Document(metadata={}, page_content='tion to Computational Geometry, 1987 – MIT Press)\\nThe Society of Mind, by Marvin Minsky (1988 – Simon & Schuster)\\nSteps towards Artificial Intelligence, by Marvin Minsky (1961 – in Computa-\\ntion & Intelligence – Collected Readings , edited by George F. Luger, MIT\\nPress)\\nLearning Search Control Knowledge: An Explanation Based Approach ,b y\\nStephen Minton (1988 – Kluwer Academic Publishers)\\nMinimizing Conflicts: A Heuristic Repair Method for Constraint Satisfaction'), Document(metadata={}, page_content='and Scheduling Problems, by S.Minton, M. D. Johnson, A. B. Philips, and P .\\nLaird (1992 – Artificial Intelligence, Vol. 58)\\nAn Introduction to Genetic Algorithms , by Melanie Mitchell (1998 – MIT\\nPress)\\nThe Royal Road for Genetic Algorithms: Fitness Landscapes and GA Perfor-\\nmance, by Melanie Mitchell, Stephanie Forrest, and John H. Holland (1992\\n- In Towards a Practice of Autonomous Systems: Proceedings of the First Euro-'), Document(metadata={}, page_content='pean Conference on Artificial Life , edited by Francisco J. Varela and Paul\\nBourgine, pp. 245–254, MIT Press)\\nMachine Learning, by T om M. Mitchell (1997 – McGraw Hill)'), Document(metadata={}, page_content='Bibliography 711\\nThe Turing Test: The Elusive Standard of Artificial Intelligence , edited by\\nJames H. Moor (2003 – Kluwer Academic Publishers)\\nRobot: Mere Machine to Transcendent Mind , by Hans P . Moravec (2000 –\\nOxford University Press)\\nAn Introduction to AI Robotics, by Robin R. Murphy (2000 – MIT Press)\\nN\\nA Guided Tour of Computer Vision , by Vishvjit S. Nalwa (1993 – Addison\\nWesley)\\nLocal Search for Planning and Scheduling: Ecai 2000 Workshop, Berlin, Ger-'), Document(metadata={}, page_content='many, August 21, 2000: Revised Papers (Lecture Notes in Computer Science,\\n2148), edited by Alexander Nareyek (2001 – Springer V erlag)\\nMachine Learning: A Theoretical Approach , by Balas K. Natarajan (1991 –\\nMorgan Kaufmann)\\nLearning Bayesian Networks , by Richard E. Neapolitan (2003 – Prentice\\nHall)\\nArtificial Intelligence: A Guide to Intelligent Systems, by Michael Negnevitsky\\n(2002 – Addison Wesley)\\nAutomated Theorem Proving: Theory and Practice , by Monty Newborn\\n(2001 – Springer V erlag)'), Document(metadata={}, page_content='(2001 – Springer V erlag)\\nDeep Blue: An Artificial Intelligence Milestone , by Monty Newborn (2003 –\\nSpringer V erlag)\\nKasparov Versus Deep Blue: Computer Chess Comes of Age, by Monty New-\\nborn (1997 – Springer V erlag)\\nComputer Science as Empirical Enquiry: Symbols and Search , by Allen\\nNewell and Herbert A. Simon (1976 - in Computation & Intelligence – Col-\\nlected Readings, edited by George F. Luger, MIT Press)\\nGPS, A Program That Simulates Human Thought, by Alan Newell and Her-'), Document(metadata={}, page_content='bert A. Simon (1963 – in Computation & Intelligence , edited by George F.\\nLuger 1995 – MIT Press)\\nReport on a General Problem Solving Program , by Alan Newell, J. C. Shaw,\\nand Herbert A. Simon (1959 – in Proceedings of the International Conference\\non Information Processing, pp. 256–264, UNESCO)'), Document(metadata={}, page_content='712 Bibliography\\nBlackboard Systems: The Blackboard Model of Problem Solving and the Evo-\\nlution of Blackboard Architectures, by H. Penny Nii (1986 – in Computation\\n& Intelligence – Collected Readings , edited by George F. Luger, The MIT\\nPress)\\nArtificial Intelligence: A New Synthesis , by N. J. Nilsson (1998 – Morgan\\nKauffman)\\nReadings in Machine Translation , edited by Sergei Nirenburg, Harold L.\\nSomers, and Y orick A. Wilks (2002 – MIT Press)'), Document(metadata={}, page_content='Somers, and Y orick A. Wilks (2002 – MIT Press)\\nFeature Extraction in Computer Vision and Image Processing ,b y  M a r k  S .\\nNixon and Alberto Aguado (2002 – Butterworth-Heinemann)\\nEvolutionary Robotics: The Biology, Intelligence, and Technology of Self-Orga-\\nnizing Machines, by Stefano Nolfi and Dario Floreano (2000 – MIT Press)\\nO\\nComputational Explorations in Cognitive Neuroscience: Understanding the\\nMind, by Simulating the Brain , by Randall C. O’Reilly (Author) and Yuko'), Document(metadata={}, page_content='Munakata (2000 – MIT Press)\\nEvolutionary Algorithms for Single and Multicriteria Design Optimization ,\\nby Andrzej Osyczka (2001 – Physica V erlag)\\nP\\nSoft Computing in Case Based Reasoning, edited by Sankar K. Pal, Tharam S.\\nDillon, and Daniel S. Y eung (2000 – Springer V erlag)\\nKasparov and Deep Blue, by Bruce Pandolfini (1997 – Fireside)\\nCombinatorial Optimization: Algorithms and Complexity , by Christos H.\\nPapadimitriou and Kenneth Steiglitz (1998 – Dover Publications)'), Document(metadata={}, page_content='Evolving Hexapod Gaits Using a Cyclic Genetic Algorithm , by Gary Parker\\n(1997 – in Proceedings of the IASTED International Conference on Artificial\\nIntelligence and Soft Computing, pp. 141–144, IASTED/ACTA Press)\\nGenerating Arachnid Robot Gaits with Cyclic Genetic Algorithms , by Gary\\nParker (1998 - in Genetic Programming III, pp. 576–583)\\nMetachronal Wave Gait Generation for Hexapod Robots , by Gary Parker\\n(1998 – in Proceedings of the Seventh International Symposium on Robotics'), Document(metadata={}, page_content='with Applications, ISORA)'), Document(metadata={}, page_content='Bibliography 713\\nAlgorithms for Image Processing and Computer Vision, by J. R. Parker (1996\\n– John Wiley & Sons)\\nLearning-Based Robot Vision, edited by Josef Pauli (2001 – Springer V erlag)\\nHeuristics: Intelligent Search Strategies for Computer Problem Solving ,b y\\nJudea Pearl (1984 – Addison Wesley)\\nProbabilistic Reasoning in Intelligent Systems: Networks of Plausible Infer-\\nence, by Judea Pearl (1997 – Morgan Kaufmann)\\nAn Introduction to Fuzzy Sets: Analysis and Design , by Witold Pedrycz and'), Document(metadata={}, page_content='Fernando Gomide (1998 – MIT Press)\\nThe Emperor’s New Mind: Concerning Computers, Minds, and the Laws of\\nPhysics, by Roger Penrose (1989 – Oxford University Press)\\nNatural Language Processing , by Fernando C. N. Pereira and Barbara J.\\nGrosz (1994 – MIT Press)\\nUnderstanding Intelligence, by Rolf Pfeiffer and Christian Scheier (2000 –\\nISBN: The MIT Press)\\nAn Algorithm for Suffix Stripping , by M. F. Porter (1980 - in Spärck Jones\\nand Willett, 1997)'), Document(metadata={}, page_content='and Willett, 1997)\\nIntroduction to Logic: Propositional Logic , by Howard Pospesel (1999 –\\nPrentice Hall)\\nPrisoner’s Dilemma: John Von Neumann, Game Theory and the Puzzle of the\\nBomb, by William Poundstone (1994 – MIT Press)\\nViews into the Chinese Room: New Essays on Searle and Artificial Intelligence,\\nedited by John Preston and Mark Bishop (2002 – Oxford University Press)\\nThe Robots Dilemma: The Frame Problem in Artificial Intelligence, by Zenon\\nW. Pylyshyn (1987 – Ablex Publishing)\\nQ'), Document(metadata={}, page_content='W. Pylyshyn (1987 – Ablex Publishing)\\nQ\\nInduction of Decision Trees, by J. R. Quinlan (1986 – from Machine Learn-\\ning, Vol. 1 (1), pp. 81–106)\\nR\\nFundamentals of Speech Recognition , by Lawrence Rabiner and Biing-\\nHwang Juang (1993 – Pearson Education)\\nModern Heuristic Search Methods , edited by V . J. Rayward-Smith, I. H.\\nOsman, Colin R. Reeves, and G. D. Smith (1996 – John Wiley & Sons)'), Document(metadata={}, page_content='714 Bibliography\\nLogic for Computer Science , by Steve Reeves and Michael Clarke (1993 –\\nAddison Wesley)\\nAre We Spiritual Machines?: Ray Kurzweil vs. the Critics of Strong A.I., edited\\nby Jay W. Richards (2002 – Discovery Institute)\\nWord of Mouse: The Marketing Power of Collaborative Filtering , by John\\nRiedl and Joseph Konstan (2002 – Warner Books)\\nInside Case-Based Reasoning , by Christopher K. Riesbeck and Roger C.\\nSchank (1989 – Lawrence Erlbaum)'), Document(metadata={}, page_content='Schank (1989 – Lawrence Erlbaum)\\nThe Bayesian Choice: From Decision-Theoretic Foundations to Computa-\\ntional Implementation, by Christian P . Robert (2001 – Springer V erlag)\\nMonte Carlo Statistical Methods, by Christian P . Robert and George Casella\\n(1999 – Springer V erlag)\\nLogic, Form and Function: The Mechanization of Deductive Reasoning ,b y\\nJohn Alan Robinson (1980 – Elsevier Science)\\nThe Perceptron: A Probabilistic Model for Information Storage and Organiza-'), Document(metadata={}, page_content='tion in the Brain , by F. Rosenblatt (1958 - in Psychological Review, Vol. 65,\\npp. 386–408)\\nRepresentations for Genetic and Evolutionary Algorithms, by Franz Rothlauf\\nand David E. Goldberg (2002 – Springer V erlag)\\nChange, Choice and Inference: A Study of Belief Revision and Nonmonotonic\\nReasoning, by Hans Rott (2002 – Oxford University Press)\\nArtificial Intelligence: A Modern Approach , by Stuart Russell and Peter\\nNorvig (1995 – Prentice Hall)\\nS'), Document(metadata={}, page_content='Norvig (1995 – Prentice Hall)\\nS\\nLogical Forms: An Introduction to Philosophical Logic , by Mark Sainsbury\\n(1991 – Blackwell)\\nEvolutionary Language Understanding, by Geoffrey Sampson (1996 – Con-\\ntinuum)\\nSome Studies in Machine Learning Using the Game of Checkers , by Arthur\\nSamuel (1959 - in Computation & Intelligence – Collected Readings, edited\\nby George F. Luger - MIT Press)\\nUsing Sophisticated Models in Resolution Theorem Proving (Lecture Notes in'), Document(metadata={}, page_content='Computer Science, Vol. 90), by David M. Sandford (1981 – Springer V erlag)'), Document(metadata={}, page_content='Bibliography 715\\nThe Importance of Being Fuzzy , by Arturo Sangalli (1998 – Princeton Uni-\\nversity Press)\\nOne Jump Ahead: Challenging Human Supremacy in Checkers , by Jonathan\\nSchaeffer (1997 – Springer V erlag)\\nA Re-examination of Brute-force Search , by Jonathan Schaeffer, Paul Lu,\\nDuane Szafron, and Robert Lake (1993 – in Games: Planning and Learning,\\nAAAI 1993 Fall Symposium, Report FS9302, pp. 51–58)\\nA World Championship Caliber Checkers Program , by Jonathan Schaeffer,'), Document(metadata={}, page_content='Joseph Culberson, Norman Treloar, Brent Knight, Paul Lu, and Duane\\nSzafron (1992 – in Artificial Intelligence, Vol. 53 (2–3), pp. 273–290)\\nArtificial Intelligence: An Engineering Approach, by Robert J. Schalkoff (1990\\n– McGraw Hill)\\nThe Structure of Episodes in Memory , by Roger C. Schank (1975 – in Com-\\nputation & Intelligence – Collected Readings, edited by George F. Luger, The\\nMIT Press)\\nThe Evidential Foundations of Probabilistic Reasoning , by David A. Schum'), Document(metadata={}, page_content='(2001 – Northwestern University Press)\\nMinds, Brains, and Programs , by John R. Searle (1980 – in The Behavioral\\nand Brain Sciences, Vol. 3, Cambridge University Press)\\nMinds, Brains and Science , by John R. Searle (1986 – Harvard University\\nPress)\\nAlgorithms, by Robert Sedgewick (1988 – Addison Wesley)\\nA New Method for Solving Hard Satisfiability Problems , by B. Selman, H.\\nLevesque, and D. Mitchell (1992 – in Proceedings of the T enth National'), Document(metadata={}, page_content='Conference on Artificial Intelligence, pp. 440–446, AAAI)\\nNoise Strategies for Improving Local Search, by B. Selman, H.A. Kautz, and B.\\nCohen (1994 – Proceedings of the T enth National Conference on Artificial\\nIntelligence, pp. 337–343, AAAI)\\nComputer Vision, by Linda G. Shapiro and George C. Stockman (2001 –\\nPrentice Hall)\\nThe Encylopedia of Artificial Intelligence , edited by S. C. Shapiro (1992 –\\nWiley)'), Document(metadata={}, page_content='716 Bibliography\\nSocial Information Filtering: Algorithms for Automating “Word of Mouth,” by\\nU. Shardanand and P . Maes (1995 – in Proceedings of CHI’95 - Human Fac-\\ntors in Computing Systems, pp. 210–217)\\nA Two Dimensional Interpolation Function for Irregularly Spaced Data,b y  D .\\nShepard (1968 - Proceedings of the 23rd National Conference of the ACM,p p .\\n517–523, ACM Press)\\nComputer Based Medical Consultations: Mycin , by Edward Shortliffe (1976\\n– Elsevier Science, out of print)'), Document(metadata={}, page_content='– Elsevier Science, out of print)\\nArtificial Evolution for Computer Graphics , by Karl Sims (1991 – Siggraph\\n’91 - Annual Conference Proceedings, 1991, pp. 319–328, Eurographics Asso-\\nciation)\\nEvolving Virtual Creatures, by Karl Sims (1994 - Siggraph ’94 - Annual Con-\\nference Proceedings, 1994, pp. 43–50, Eurographics Association)\\nThe Algorithm Design Manual, by Steven S. Skiena (1997 – T elos)\\nData Analysis: A Bayesian Tutorial, by D. S. Sivia (1996 – Oxford University\\nPress)'), Document(metadata={}, page_content='Press)\\nImage Processing: Analysis and Machine Vision , by Milan Sonka, Vaclav\\nHlavac, and Roger Boyle (1998 – Brooks Cole)\\nKnowledge Representation: Logical, Philosophical, and Computational Foun-\\ndations, by John F. Sowa and David Dietz (1999 – Brooks Cole)\\nComputer Viruses as Artificial Life, by Eugene Spafford (1989 – in Artificial\\nLife An Overview, edited by Christopher G. Langton, 1995, MIT Press, pp.\\n249–265)\\nEvaluating Natural Language Processing Systems: An Analysis and Review,b y'), Document(metadata={}, page_content='Karen Spärck Jones and Julia R. Galliers (1996 – Springer V erlag)\\nReadings in Information Retrieval , edited by Karen Spärck Jones and Peter\\nWillett (1997 – Morgan Kaufmann)\\nLogic and Prolog, by Richard Spencer-Smith (1991 – Harvester Wheatsheaf)\\nResolution Proof Systems: An Algebraic Theory (Automated Reasoning Series,\\nVol. 4), by Zbigniew Stachniak (1996 – Kluwer Academic Publishers)\\nArtificial Life VIII: Proceedings of the Eighth International Conference on'), Document(metadata={}, page_content='Artificial Life, edited by Russell Standish, Mark A. Bedau, and Hussein A.\\nAbbass (2003 – MIT Press; also available are the proceedings from the first\\nthrough the seventh conferences)'), Document(metadata={}, page_content='Bibliography 717\\nLayered Learning in Multiagent Systems: A Winning Approach to Robotic\\nSoccer, by Peter Stone (2000 – MIT Press)\\nReinforcement Learning: An Introduction (Adaptive Computation and\\nMachine Learning), by Richard S. Sutton and Andrew G. Barto (1998 – MIT\\nPress)\\nBayes’s Theorem (Proceedings of the British Academy, Vol. 113) , edited by\\nRichard Swinburne (2002 – British Academy)\\nT\\nEvolutionary Art and Computers , by Stephen T odd and William Latham\\n(1992 – Academic Press)'), Document(metadata={}, page_content='(1992 – Academic Press)\\nIntroductory Techniques for 3-D Computer Vision, by Emanuele Trucco and\\nAlessandro V erri (1998 – Prentice Hall)\\nTranslation Engines: Techniques for Machine Translation, by Arturo Trujillo\\n(1999 – Springer V erlag)\\nManaging Expert Systems, edited by Efraim Turban and Jay Liebowitz (1992\\n– Idea Group Publishing)\\nU\\nHuman Face Recognition Using Third-Order Synthetic Neural Networks ,b y\\nOkechukwu A. Uwechue and Abhijit S. Pandya (1997 – Kluwer Academic\\nPublishers)\\nV'), Document(metadata={}, page_content='Publishers)\\nV\\nSimulated Annealing: Theory and Applications , by P . J. M. Van Laarhoven\\nand E. H. L. Aarts (1987 - D Reidel Publishing Company – out of Print)\\nStatistical Learning Theory , by Vladimir N. Vapnik (1998 – Wiley Inter-\\nscience)\\nPlanning and Learning ,b y  Analogical Reasoning , by Manuela M. V eloso\\n(1994 – Springer V erlag T elos)\\nLearning and Generalization: With Applications to Neural Networks ,b y\\nMathukumalli Vidyasagar (2002 – Springer V erlag)'), Document(metadata={}, page_content='Mathukumalli Vidyasagar (2002 – Springer V erlag)\\nThe Simple Genetic Algorithm: Foundations and Theory, by Michael D. Vose\\n(1999 – MIT Press)'), Document(metadata={}, page_content='718 Bibliography\\nW\\nVirtual Organisms: The Startling World of Artificial Life ,b y  M a r k  W a r d\\n(2000 – St Martin’s Press)\\nIn the Mind of the Machine: The Breakthrough in Artificial Intelligence ,b y\\nKevin Warwick (1998 – Random House)\\nFace Recognition: From Theory to Applications , by Harry Wechsler (1998 –\\nSpringer V erlag)\\nMultiagent Systems: A Modern Approach to Distributed Artificial Intelligence,\\nedited by Gerhard Weiss (1999 – MIT Press)'), Document(metadata={}, page_content='edited by Gerhard Weiss (1999 – MIT Press)\\nRecent Advances in AI Planning, by Daniel S. Weld in AI Magazine, Summer\\n1999\\nPractical Planning: Extending the Classical AI Planning Paradigm , by David\\nE. Wilkins (1989 – Morgan Kaufman)\\nArguing A. I.: The Battle for Twenty-First Century Science, by Sam Williams\\n(2002 – Random House)\\nArtificial Intelligence, by Patrick Henry Winston (1992 – Addison Wesley)\\nIntroduction to MultiAgent Systems , by Michael Wooldridge (2002 – John\\nWiley & Sons)\\nX\\nY'), Document(metadata={}, page_content='Wiley & Sons)\\nX\\nY\\nIntelligent Planning: A Decomposition and Abstraction Based Approach ,b y\\nQiang Y ang (1998 – Springer V erlag)\\nZ\\nIntelligent Scheduling, edited by Monte Zweben and Mark S. Fox (1998 –\\nMorgan Kaufmann)'), Document(metadata={}, page_content='Index\\nA\\nA* algorithms, 108–109\\nAbduction, 201–202, 633\\nand Bayesian theory, 338–339\\nAbductive reasoning, 467, 482–483\\nAbelard, Peter,Dialectica, 6–7\\nAccepting state, 366, 633\\nAccessibility, 560–561\\nAcquaintance algorithm, 633\\nAction, 242\\nAction description language (ADL), 633\\nAction potential, of neuron, 293\\nActivation function, 293, 633\\nActivation level, 294f, 633\\nActivity product rule, 634\\nAdaptation in Natural and Artificial Systems\\n(Holland), 387\\nAdd list, 436'), Document(metadata={}, page_content='(Holland), 387\\nAdd list, 436\\nADL (action description language), 434, 455–456, 633\\nand open world assumption, 480\\nAdmissibility, 93, 634\\nAdmissible heuristic, 96\\nAdventures of Sherlock Holmes (Conan Doyle), 605\\nAdversarial methods, of game playing, 146, 634\\nAgent architectures, 556–560\\nAgent team, 555, 634\\nAgents, 196–197, 634\\ndefinition, 543\\nintelligent, 135, 543–569\\naccessibility, 560–561\\narchitectures for, 556–560\\nbold, 558\\nBraitenberg vehicles, 562–565\\ncollaborative, 642'), Document(metadata={}, page_content='Braitenberg vehicles, 562–565\\ncollaborative, 642\\ngoal-based, 548–549, 657\\nhybrid, 660\\ninformation (internet), 553–554, 572, 662\\ninterface, 551–552, 663\\nlearning, 561–562\\nmobile, 552–553\\nmultiagent systems, 554–556\\nproperties of, 544–546\\nreactive (reflex), 547–548, 554\\nrobotic, 561–562\\nstatic, 552\\nutility-based, 549–551\\nmobile, 668\\nproperties of, 544–546\\nreactive (reflex), 547–548, 559, 680\\nsoftware, 543–544, 687\\nutility-based, 693\\nAggregation, 34\\nAI. see Artificial intelligence (AI)'), Document(metadata={}, page_content='AI. see Artificial intelligence (AI)\\nAI: Artificial Intelligence (film), 23\\nAIBO robotic dog, 23\\nAlan Turing the Enigma of Intelligence(Hodge), 291\\nAlfonso the Wise, 117\\nAlgorithms\\nacquaintance, 633\\nAD3, 268, 278, 281, 661\\nbucket brigade, 286, 377, 378, 379–380, 639\\ngenetic. See Genetic algorithms\\nGraphPlan, 434, 451, 454–455, 658\\nwinner-take-all, 316, 694'), Document(metadata={}, page_content='720 Index\\nAlice’s Adventures in Wonderland(Carroll), 241\\nAlignment method, and machine vision, 626\\nAlpha-beta pruning, 69, 133, 153–159, 634\\neffectiveness, 154–155\\nimplementation, 155–159\\nanalysis of search, 158t\\nAlphabet, 634\\nAmbiguity, 589–592, 634\\nand disambiguation, 591–592\\nAn Anatomy of the World(Donne), 503\\nAn Essay on Man (Pope), 433\\nAnalogy, and knowledge representation, 9.See also Copy-\\ncat architecture\\nAnalogy (Evans), 9\\nAnalogy of Religion, The (Butler), 327\\nAnalytic engine, 7'), Document(metadata={}, page_content='Analytic engine, 7\\nAnaphoric expressions, 590\\nAncestor, 45, 634\\nAnd-goals, 58–59, 634\\nAnd-elimination, 191\\nAnd-introduction, 191\\nAnd-nodes, 58–59, 635\\nAnd (operator), 178–179, 182\\nAnd-or trees, 635. See also Goal trees\\nAnnealing schedule, 130\\nAnnealing, simulated. See Simulated annealing\\nAnt colony optimization (ACO), 128\\nAntecedent, in logical statement, 183, 242\\nAPL2, 42\\nApplicable operators, action, 437\\nArchitectures, agent, 556–560\\nbelief desire intention (BDI), 558'), Document(metadata={}, page_content='belief desire intention (BDI), 558\\nhorizontal vs. vertical, 559–560\\nsubsumption, 556–557\\nT ouringMachines, 559, 560\\nAristotelian logic. See Logic, classical\\nAristotle, 6, 10, 364\\nPoetics, 327\\nArs Rhetorica (Dionysius), 3\\nART, 253\\nArt, and artificial evolution, 412–413\\nArtificial immune systems (AIS), 381–382\\nArtificial Intelligence: A Knowledge-Based Approach (Fire-\\nbaugh), 241\\nArtificial intelligence (AI)\\ndefinition, 4–5, 635\\nhistory of, 3–18\\nimportant areas of study, 10'), Document(metadata={}, page_content='history of, 3–18\\nimportant areas of study, 10\\nintroduction of term, 9\\nweak vs. strong, 4, 5, 23\\nArtificial life, 128, 266, 635\\ntechniques, 363–364\\nArtificial Life Roots of Artificial Intelligence(Steels), 363\\nArtificial neural network, 284, 635\\nAsexual reproduction, 373\\nAssociation, 34\\nAssociativity (associative property), 187, 635\\nAssumption-based truth maintenance system (ATMS),\\n479\\nAtilla (robot), 563\\nAtomic actions, 422, 635\\nAtomic formula, 199, 635\\nAttractor networks, 307, 635'), Document(metadata={}, page_content='Attractor networks, 307, 635\\nAugmented finite state machines (AFSMs), 556–557, 635\\nAugmented transition network (ATN), 585, 636\\nAutoassociative memory, 313, 636\\nAutomated reasoning, 5\\nAutomated translation, 572\\nAutonomy, of agents, 545, 636\\nAxelrod, R., 411\\nAxioms, 200\\nAxons, 292–293, 636\\nB\\nBabbage, Charles, 7\\nBackgammon, 167\\nBackpropagation algorithm, 291–292\\nin multilayer networks, 302–306, 636\\nimproving performance, 305–306\\nBacktracking, 76\\nnonchronological, 479\\nBacktracking search, 135'), Document(metadata={}, page_content='nonchronological, 479\\nBacktracking search, 135\\nBackus-Naur form (BNF), 575–579, 636\\nBackward chaining, 242, 243, 248–251, 469.See also\\nGoal-driven search\\ncompared to forward chaining, 249–251\\nin rule-based systems, 257–259\\nin STRIPS, 440–441\\nBaldrick (from Blackadder), 433\\nBates, Marston, 71\\nBayes’ optimal classifier, 637\\nBayes’ Theorem, 327, 330–337, 637\\napplications, 331–337\\ncomparing conditional probabilities, 334–335'), Document(metadata={}, page_content='Index 721\\nmedical diagnosis, 331–332\\nnormalization, 335–337\\nwitness reliability, 332–334\\nstated, 331\\nBayes, Thomas, 327\\nBayesian belief networks, 339–346, 347f, 636\\nexamples, 342–346\\nBayesian classifiers, 327, 349–356\\nnaive, 351–356\\nBayesian concept learning, 337–339\\nBayesian reasoning, 266, 504\\nand abduction and induction, 338–339\\nand collaborative filtering, 356–357\\nBeam search, 105–107\\nanalysis of search, 106t\\nBehavior, asynchronous, 557\\nBehavioral psychology, 12'), Document(metadata={}, page_content='Behavioral psychology, 12\\nBelief desire intention architecture (BDI), 558, 637\\nBelief networks, 339–346\\nBenevolence, 556\\nBest-first search, 69, 103–105\\nanalysis of search, 104t\\nBible, The, 19, 71, 267, 465, 543, 605\\nBidding system, of classifiers, 378, 379–380\\nBidirectional associative memories (BAMs), 292,\\n313–316, 637\\ncompared to Hopfield networks, 314, 315–316\\nBidirectional search, 136\\nBinary operators, 182, 637\\nBiology, and AI, 3, 4, 12, 128\\nBiomorphs, 372, 372–373, 412'), Document(metadata={}, page_content='Biomorphs, 372, 372–373, 412\\nBivalent logic, 504, 637\\nBlackboard architecture, 467, 469–472, 637\\nimplementing, 471–472\\nBletchley Park, 7\\nBlind search methods, 72, 91, 440, 637.See also Generate\\nand test\\nBlind Watchmaker, The(Dawkins), 363, 372, 372–373\\nBlocks world, 428–430, 433, 438–443, 638\\nBlondie, 24, 164\\nBlum, Avrim, 454\\nBoids system, 366, 367–368, 555\\nBoltzmann acceptance criterion, 129\\nBoole, George, 7\\nBoolean algebra, 7, 276–277, 296\\nBoolean operators, and perceptrons, 296, 299–300'), Document(metadata={}, page_content='Boolean operators, and perceptrons, 296, 299–300\\nBottom up or top down, 60–61, 278, 638\\nbuilding parse tree, 581–582\\nBound variables, 638\\nBounded lookahead, 151–153, 638\\nBraitenberg, Valentino, 563\\nBraitenberg vehicles, 464, 563–565, 638\\nBranch, 45, 638\\nBranch and bound search, 109–110\\nBranching factor, 45, 638\\nBranching time temporal logic, 488\\nBreadth-first search, 71, 76–78, 80, 638\\nimplementing, 83–90, 86–88\\nanalysis of search, 87t\\nBridge (game), 165, 167\\nBritish museum procedure, 107–108'), Document(metadata={}, page_content='British museum procedure, 107–108\\nBrooks, Rodney A., 556, 557\\nBrowne, Thomas, Religio Medici, 363\\nBrowning, Robert, Rabbi Ben Ezra, 421\\nBrute-force search (exhaustive search), 53, 639, 652.See\\nalso Breadth-first search; Depth-first search;\\nGenerate and test\\nBucket brigade algorithm, 286, 377, 639\\nand classifier systems, 378, 379–380\\nBuilding-block hypothesis, 403–404, 639\\nBurke, Edmund, Letter to a Member of the National\\nAssembly, 421\\nBurns, Robert, To a Mouse, 421\\nBuro, Michael, 166'), Document(metadata={}, page_content='Burns, Robert, To a Mouse, 421\\nBuro, Michael, 166\\nButler, Joseph,The Analogy of Religion, 327\\nByron, Manfred, 571\\nC\\nC++, 12–13, 32, 41–42, 173\\nCandidate elimination, 275, 639\\nCandide (Voltaire), 71\\nCanny edge detector, 611–612, 639\\nCarlyle, Thomas, Critical and Miscellaneous Essays ,3 ,\\n465\\nCarroll, Lewis\\nAlice’s Adventures in Wonderland, 241\\nThrough the Looking Glass, 19, 175\\nCarruth, William Herbert, Each in His Own Tongue,\\n387\\nCase-based planning systems, 457–458, 639'), Document(metadata={}, page_content='387\\nCase-based planning systems, 457–458, 639\\nCase-based reasoning, 467, 495–496, 639\\nCausal links, 446–447, 639\\nCells, in Conway’s Life, 368'), Document(metadata={}, page_content='722 Index\\nCellular automata, 364, 368–371, 639\\ndefinition, 369\\none-dimensional, 370–371\\nCenter of gravity. See Centroid\\nCentroid, 521–522, 640\\nCertainty, 328\\nCertainty factor algebra, 487\\nCertainty factors, 467, 485–487, 640\\nChange, and logical systems, 205, 210, 426, 487–494.\\nSee also Situation calculus\\nevent calculus, 490–492\\nmental situation calculus, 492–494\\ntemporal logic, 487–490\\nChart parser, 640\\nChart parsing, 585–588\\nCheckers, playing, 159–164, 165\\nWorld Checkers Championship, 161'), Document(metadata={}, page_content='World Checkers Championship, 161\\nCHEF, 457–458\\nChess, playing, 22, 164–165, 268–269\\nChinese room, 20–21, 640\\nChinook, 152\\nevaluation function, 162–163\\nforward pruning, 163\\nChomsky, Noam, 11, 579, 580\\nChomsky’s hierarchy, 640\\nChromosomes, 131, 388–389, 640\\noverspecified, 674\\nsize, 389\\nsplicing, 405–406\\ntemplate, 405\\nunder- and overspecified, 405–406\\nunderspecified, 692\\nChronological backtracking, 76, 640\\nCircumscription, 467, 480–482, 641\\nClass, 30, 641\\nClass frame, 32, 641\\nClassical logic, 641'), Document(metadata={}, page_content='Class frame, 32, 641\\nClassical logic, 641\\nClassification, of data, 268\\nClassifier systems, 286, 364, 377–381, 388, 389, 641\\nBayesian, 327, 349–356\\nnaive, 351–356\\noptimal, 349–351\\ncomponents of, 377\\noperation of, 377–381\\nreproduction in, 380–381\\nrules, 378–379\\nClause, 641\\nCLIPS (C language integrated production system), 253,\\n255–257, 641\\nCloning, 392, 641\\nClosed world assumption, 467, 480, 641\\nCluster, 316\\nCluster layer, 316\\nCNF-satisfiability problem, 414\\nCo-evolution, 413–414'), Document(metadata={}, page_content='Co-evolution, 413–414\\nCo-occurence matrix, 646\\nCodelets, 475\\nCoderack, 474–475\\nCoevolution, 642\\nCognitive psychology, 12, 642\\nCollaboration, 555\\nCollaborative agents, 555–556, 642\\nCollaborative filtering, 327–328, 356–357, 642\\nColony optimization, 126\\nColoring problems, 216–218\\nColossus, 161\\nCombinatorial explosion, 57, 125\\nCombinatorial problems, 24, 117, 173, 414, 642\\noptimization, 125–126\\nsearch, 216\\nCommunative property, 187, 331\\nCommunication, 555\\nCommutativity, 642\\nCompetitive agents, 555'), Document(metadata={}, page_content='Commutativity, 642\\nCompetitive agents, 555\\nCompetitive learning, 316, 642\\nComplete path, 45, 643\\nCompleteness, 173, 175, 200, 643\\nof search methods, 79\\nComplexity, of search methods, 78–79\\nComposition, 223–224, 643\\nComputation tree logic (CTL), 488, 643\\nComputing Machinery and Intelligence (Turing), 7\\nConan Doyle, Arthur\\nThe Adventures of Sherlock Holmes, 605\\nThe Sign of Four, 209\\nConcave edge, 613–614, 643\\nConcept learning, 270–271, 643\\nBayesian, 337–339\\nConclusion, 242'), Document(metadata={}, page_content='Bayesian, 337–339\\nConclusion, 242\\nConditional effects, 455–456\\nConditional planning, 457, 643\\nConditional probabilities, 329–330, 643\\ncalculating, 344–345\\ncomparing, 335–337'), Document(metadata={}, page_content='Index 723\\ntables, 340–341, 343–344, 644\\nConflict resolution, 242, 245–247, 644\\nConflicts, 37, 644\\nConjunction, 644\\nConjunctive normal forms (CNF), 210–211, 212, 644\\nConjunctive operators, 182\\nConnect-4 (game), 166\\nConsequent, 183, 242, 644\\nConsistent, 273–274\\nConstant, 644\\nConstraint satisfaction problems (CSPs), 118, 644\\nConstraint satisfaction search, 118–121\\nConstraints, 48, 62, 644\\nrelaxing, 95\\nConstructor, 41\\nContext-free grammar, 580, 645\\nContext-sensitive grammar, 580, 645'), Document(metadata={}, page_content='Context-sensitive grammar, 580, 645\\nContingent statements, 203–204, 645\\nContradiction, proof by, 192–193, 214–216, 645\\nContradictory assumptions, 201\\nContradictory expressions, 186–187, 192–193\\nConvex edge, 613–614, 645\\nConvolution, 611–612, 645\\nConway’s Life, 368–369, 646\\nCook, Stephen, 51, 134\\nCooperation\\nof agents, 545–546\\nin Prisoner’s Dilemma, 406\\nCopycat architecture, 9, 463, 467, 646\\nCorpus, 572–573, 646\\nCost, 107\\nCrabbe, George, Gretna Green, 143\\nCreatures, 388, 390'), Document(metadata={}, page_content='Creatures, 388, 390\\nCredit assignment, 286, 302, 378, 646\\nCrichton, Michael, Prey, 543\\nCrisp set, 646\\nCritical and Miscellaneous Essays (Carlyle), 3, 465\\nCrossover, 387, 389, 390–392, 646\\napplication, 390–391\\nand genetic programming, 375, 378\\nand schemata, 401–402\\nsingle-point, 391\\ntwo-point, 391\\nuniform, 391–392, 692\\nCrossover position, 380\\nCumulative selection, 372\\nCut and splice operators, 405–406\\nCYC, 259–260, 365, 646\\nCycles, 45, 49, 647\\nD\\nDartmouth College, 9\\nDarwin, Charles, 372, 413'), Document(metadata={}, page_content='D\\nDartmouth College, 9\\nDarwin, Charles, 372, 413\\nData\\ndirty, 554\\nnoisy, 282–283, 283–284\\nData-driven reasoning, 244\\nData-driven search, 73–74, 647\\nDatabase\\naccess, 572\\nof facts, 252f, 253\\nof rules, 243\\nDawkins, Richard, 390, 392, 393\\nbiomorphs, 412\\nThe Blind Watchmaker, 363, 372, 372–373\\nDeception, in genetic algorithms, 404, 647\\ncombatting, 405–406\\nDecidability, 173, 175, 200–201, 647\\nDecision tree, 56–57, 277f, 647\\nfor collaborative filtering, 357\\nDecision-tree induction, 276–278, 647'), Document(metadata={}, page_content='Decision-tree induction, 276–278, 647\\nDecision-tree learning, 268\\nDeclarative semantics, 38–39\\nDeduction, 243, 244–245. See also Forward chaining,\\nrules of, 189, 191–195, 647\\nDeduction theorem, rule of, 195–196\\nDeductive reasoning, 201–202\\nDeep Blue, 22, 133, 143, 165\\nDeep, definition, 133\\nDeep Fritz, 165\\nDeep Junior, 143, 165\\nDeep Thought, 133\\nDefault reasoning, 467, 477–478, 647\\nDefault value, 32, 648\\nDefection, in Prisoner’s Dilemma, 406\\nDefining length, 648'), Document(metadata={}, page_content='Defining length, 648\\nDefuzzification, 521–522, 531–533, 648\\nof neuro-fuzzy system, 538\\nDelete list, 436\\nDemons, 38, 648\\nDeMorgan’s laws, 188, 198, 442, 443, 648\\nDemoted operators, 447\\nDempster-Shafer theory, 467, 483–485, 648\\nDendrites, 292–293'), Document(metadata={}, page_content='724 Index\\nDennett, Daniel, 10, 11\\nDependence, 339-349\\nDependency-directed backtracking, 121, 479.\\nSee also Nonchronological backtracking\\nDepth-first iterative deepening (DFID), 88–90\\nDepth-first search, 71, 75–76, 107, 228, 648. See also\\nBacktracking search; Hill climbing, steepest\\nascent\\nand the eight queens problem, 118–121\\nexamples, 80–83\\nmaze, 81\\nsearching for gift, 81–83\\nimplementing, 83–90\\nanalysis of search, 85t\\nand minimax, 149–150\\nDepth threshold, 78, 648\\nDerivation tree, 582, 648'), Document(metadata={}, page_content='Derivation tree, 582, 648\\nDescartes, Rene, 10, 11\\nDescendent, 45, 648\\nDescribe and match, 649\\nDiagnosis, 649\\nDialectica (Abelard), 6–7\\nDionysius, Ars Rhetorica,3\\nDirected graphs, 45, 649\\nDirectives, 243\\nDirty data, 554\\nDisagreement set, 225\\nDisambiguation, 591–592\\nDiscontinuity, 649\\nDisjunction, 649\\nDisjunctive normal forms (DNF), 211, 649\\nDisjunctive operators, 182–183\\nDistributed computing architecture, 553\\nDistributed tree search (DTS), 134\\nDistributive property, 188\\nDiversity, 649'), Document(metadata={}, page_content='Distributive property, 188\\nDiversity, 649\\nDocument frequency, 595\\nDomain expert, 252, 649\\nDonne, John, An Anatomy of the World, 503\\nDouble negation, 193\\nDoyle, Jon, 478\\nDraughts, 159\\nDreyfus, Hubert, 10\\nDualism, 11, 649\\nDynamic world planning, 419, 456–457, 650\\nE\\nEach in His Own Tongue (Carruth), 387\\nEclipse, 253\\nEdelman, Gerald, M., 285\\nEdge detection, 609–612, 650\\ncanny edge detector, 611–612\\nEdges, 29, 31, 586, 650\\nconcave, 613–614\\nconvex, 613–614\\noccluding, 613–614'), Document(metadata={}, page_content='convex, 613–614\\noccluding, 613–614\\nEffect axioms, 422, 427, 450–451, 650\\nEffective branching factor, 650\\nEigenfaces, 627–628\\n8-puzzle, 92–95, 425\\nEight queens problem, 118–121, 425\\ndiagram, 119f\\nand heuristic repair, 123–125\\nrelaxed version, 125–126\\nsolution, 120f\\nEisenhower, Dwight D., 421\\nElimination, rule of, 191\\nexamples, 194\\nELIZA, 8\\nEmergent behavior, 266, 365–366, 650\\nEnd-user, of expert system, 251\\nEnergy of the system, 128\\nEntropy, 278–281, 650\\nEpoch, 297, 650'), Document(metadata={}, page_content='Entropy, 278–281, 650\\nEpoch, 297, 650\\nEquilibrium, of neural networks, 306–307\\nEquivalence, logical, 175, 187–189, 198, 442, 650\\nEquivalent sample size, 354\\nError gradient, 651\\nError value, 651\\nEspecially When the October Wind (Thomas), 291\\nEstablishes links, 446\\nEuclidean distance, calculating, 317\\nEvaluation functions, in game playing, 146–148\\nas weighted linear functions, 147\\nEvans, Thomas, Analogy,9\\nEvent calculus, 467, 490–492, 651\\nEvent, defined, 491\\nEvolution, 651'), Document(metadata={}, page_content='Event, defined, 491\\nEvolution, 651\\nEvolution, and artificial life, 365, 372–381, 396–397\\npredators, 413–414\\nstrategies, 373–374\\nin visual arts, 412–413\\nEvolution, theory of, 266\\nEvolutionary programming (EP), 364, 375–376, 651'), Document(metadata={}, page_content='Index 725\\nExchanging heuristic, 126–127\\nExcluded middle, law of, 203, 515, 651\\nExecution, 422, 651\\nmonitoring, 457, 652\\nExhaustive search (Brute-force search), 53, 639, 652.\\nSee also Breadth-first search; Depth-first search;\\nGenerate and test\\nExistential quantifiers, 197, 652\\neliminating, 220–222\\nExpected value, 167\\nExpectiminimax, 167, 652\\nExpert system shell, 252f, 254–255, 652\\nExpert systems, 23–24, 241–263, 652\\narchitecture of, 252–254\\nbuilding, 32–33\\nend-user, 251\\nand frames, 34'), Document(metadata={}, page_content='building, 32–33\\nend-user, 251\\nand frames, 34\\nfuzzy, 503, 522–533, 655\\nrule-based, 251–254\\npeople involved in, 251–252\\nExplanation system, 252, 252f\\nExponential growth, 57, 652\\nEye, human, diagram of, 607f\\nF\\nFables (Gay), 267\\nFace recognition, 464, 626–628, 652\\nFace space, 628\\nFact, 228, 653\\nFact database, 253\\nFailure nodes, 59, 653\\nFalse negatives, 598–599, 653\\nFalse positives, 598–599, 653\\nFalsum, 192, 193–194, 653\\nFeasible region, 125, 653\\nFeed-forward networks, 301, 306, 653'), Document(metadata={}, page_content='Feed-forward networks, 301, 306, 653\\nFikes, Richard E., 434\\nFilter, 612, 653\\nFinite state automaton (FSA), 366–368,\\n375–376, 579, 653–654. See also\\nTransition networks\\nFirebaugh, Morris W.,Artificial Intelligence: A Knowl-\\nedge-Based Approach, 241\\nFired rule, 244\\nFiring, of classifiers, 378, 379\\nFirst-order predicate calculus, 468.See alsoFirst-order\\npredicate logic (FOPL); Situation calculus\\nand blackboard architecture, 469, 471\\nFirst-order predicate logic (FOPL), 30, 199–200, 201,\\n558–559'), Document(metadata={}, page_content='558–559\\nmonotonicity, 201\\nrepresentational adequacy of, 40–41\\nresolution in, 218–219\\nFitness, 131, 387, 389, 393, 654\\ndetermining, 373–374, 378, 379, 390\\ncalculating fitness ratio, 393–396\\nmetrics for, 411–412\\nof offspring, 380\\nuser choice, 412\\nFluents, 490, 654\\nFocus of attention, 472\\nFogel, David, 164\\nFogel, Lawrence, 375\\nFoothills, 101–103, 654\\nFOPL (first-order predicate logic). See First-order predi-\\ncate logic (FOPL)\\nForgetting factor, 654\\nFormal language, 571–572, 654'), Document(metadata={}, page_content='Formal language, 571–572, 654\\nForward chaining, 242, 244–245, 469, 654.See also CLIPS\\n(C language integrated production system);\\nData-driven search; Deduction\\ncompared to backward chaining, 249–251\\nand STRIPS, 439\\nForward checking, 121\\nForward pruning, 654\\nFox, G. C., 133\\nFractional knapsack problem, 110–111\\nFrame axioms, 422, 427, 434–435, 450–451, 655\\nFrame-based representations\\nand FOPL, 40–41\\nof knowledge, 259–260\\nFrame problem, 419, 422, 427–428, 480, 655\\nFrame system, 32, 655'), Document(metadata={}, page_content='Frame system, 32, 655\\ngraphic representations, 33f\\nFrames, 32–41, 173, 469, 654.See Semantic nets\\ncombining, with rules, 40–41\\ninstance, 663\\nslots as, 35–36\\nFrankenstein (Shelley), 3\\nFree variable, 655\\nFunctions\\nand classification of data, 268\\nlinearly separable, 299–300, 301\\nin predicate calculus, 199\\nFundamental memories, 307, 655\\nFurst, Merrick, 454'), Document(metadata={}, page_content='726 Index\\nFuzzification, 517–519, 655\\nof neuro-fuzzy system, 536–537\\nFuzzy expert systems, 522–533, 655\\nbuilding, 522–533\\ndefining fuzzy rules, 527–528\\ndefining fuzzy sets, 523–527\\nusing, 528–533\\ndefuzzification, 531–533\\nFuzzy inference, 516–522, 655\\napplying fuzzy values, 519–520\\ndefuzzification, 521–522\\nfuzzification, 517–519\\nFuzzy logic, 23, 347, 463, 511–515, 655\\napplication, 515–516\\nand fuzzy variables, 511–512\\nand truth tables, 512–515\\nFuzzy operators, 503'), Document(metadata={}, page_content='and truth tables, 512–515\\nFuzzy operators, 503\\nFuzzy reasoning, 463, 503–541, 655\\nbivalent and multivalent logics, 504\\nfuzzy sets, 505–511\\nand linguistic variables, 504–505\\nsystems that learn, 534–538\\nFuzzy rules, 516, 656\\ndefining, 527–528\\nFuzzy sets, 503, 505–507\\ndefining, 523–527\\nhedges, 510–511\\nmembership functions, 507\\noperators, 508–510\\nFuzzy variables, 511–515\\nG\\nGame of life, 368–369, 664\\nGame playing, 143–171, 560–561\\nassumptions, 145–146\\nBackgammon, 167\\nBridge, 165'), Document(metadata={}, page_content='assumptions, 145–146\\nBackgammon, 167\\nBridge, 165\\nCheckers, 159–164, 165\\nChess, 22, 164–165, 268–269\\ndraughts, 159\\nevaluation functions in, 146–148\\ngames of chance, 166–167\\nGo, 165–166\\nGo-Moku, 166\\nOthello, 165–166\\nPrisoner’s Dilemma. See Prisoner’s Dilemma\\nReversi, 166\\nTic-tac-toe, 144–145, 166\\nzero-sum, 146, 695\\nGame trees, 63–64, 78, 144–145, 656\\nsearching, 148–149\\nGaussian function, 611\\nGay, John,Fables, 267\\nGelatt, C. D., 130\\nGeneral Magic, 552'), Document(metadata={}, page_content='Gelatt, C. D., 130\\nGeneral Magic, 552\\nGeneral Problem Solver (GPS), 6, 9, 422, 430, 656\\nGeneral-to-specific ordering, 272–273, 273–274\\nGeneralization, 33, 656\\nGeneralized delta rule, 305\\nGenerate and test, 74–75, 98, 656\\nand cyyptographic problems, 122–123\\nGeneration, in Conway’s Life, 368\\nGenerator, 74\\nGenes, 372, 388–389, 656\\nGenetic algorithms, 126, 131, 266, 321–322, 387–418,\\n555, 656\\ndeception, 404\\nmessy, 405–406\\nand optimization of mathematical function, 393–396'), Document(metadata={}, page_content='for Prisoner’s Dilemma, 410\\nproblems applied to, 414\\nrepresentations, 388–389\\nrunning, 389\\ntermination criteria, 392–393\\nwhy they work, 396–404\\nbuilding-block hypothesis, 403–404\\nschemata, 397–404\\nGenetic programming, 364, 374–375, 656\\nGenghis, 563\\nGenotype, 390, 412, 657\\nGeometric progression, 657\\nGeorgeff, M., 558\\nGlider gun, in Conway’s Life, 369\\nGlider, in Conway’s Life, 369\\nGlobal maximum, 102, 657\\nGo (game), 165–166\\nGo-Moku (game), 166\\nGoal, 228, 248, 657\\nGoal-based agents, 548–549, 657'), Document(metadata={}, page_content='Goal-based agents, 548–549, 657\\nGoal-driven reasoning, 248\\nGoal-driven search, 61, 73–74, 657\\nGoal nodes, 45, 657\\nGoal reduction, 57, 657\\nGoal state, 72\\ndiagram, 452f\\nin STRIPS, 437'), Document(metadata={}, page_content='Index 727\\nGoal trees, 58–64, 657\\nuses of, 61–64\\ngames, 63–64\\nmap coloring, 61–62\\nparsing sentences, 63\\nproving theorems, 62–63\\nGoals\\nand- and or-, 58–59\\nand planning, 428–430\\nvs. plans, 62\\nand PROLOG, 228, 229\\nroot, 59, 682\\nGödel implication, 515, 516, 657\\ndefinition, 513\\nGödel’s incompleteness theorem, 21\\nGoldberg, David E., 405, 406\\nGradient descent, 304, 658\\nGrammars, 575, 579–580, 658\\nGranularity, 181\\nGrapes of Wrath, The(Steinbeck), 291\\nGraphPlan algorithm, 434, 451, 454–455, 658'), Document(metadata={}, page_content='GraphPlan algorithm, 434, 451, 454–455, 658\\nGraphs, 29, 658\\ncoloring, 217–218\\ndirected vs. nondirected, 45\\nleveling off, 454\\nplanning, 451–455, 676\\nGreedy search, 110–112\\nGretna Green (Crabbe), 143\\nGround atoms, 230–231\\nGround instance, 230, 658\\nGround terms, 229, 658\\nGSAT system, 450\\nH\\nHAL, 19, 21–22, 22–23\\nHalting problem, 21, 658\\nHamlet (Shakespeare), 503\\nHamming distance, 313, 658\\nHeadless clause, 228\\nHEARSAY, 472\\nHEARSAY II, 469, 471, 472, 659\\nHebb, Donald O., 285, 320'), Document(metadata={}, page_content='Hebb, Donald O., 285, 320\\nHebbian learning, 285, 295, 304, 320–321\\nHebb’s law, 292, 320\\nHedges, 503, 510–511, 659\\nHeisenberg, Werner,Physics and Beyond, 241\\nHenry V (Shakespeare), 143\\nHerbrand universes, 229–233, 659\\ndefinition, 229\\nexample, 232–233\\nHerbrand base, 230–231, 659\\nHerbrand interpretations, 231–232, 659\\nHeteroassociative memory, 313, 659\\nHeuristic repair method, 123–125, 660\\nHeuristic search methods, 90–98\\nchoosing, 92–93\\nevaluation function, 91, 659\\nexamples, 92–98'), Document(metadata={}, page_content='evaluation function, 91, 659\\nexamples, 92–98\\nHeuristics, 53, 72, 91, 659\\nexchanging, 126–127\\nHidden layer, of multilayer network, 301, 660\\nHill climbing, 98–103, 126, 392, 396, 660\\nanalysis of search, 100t\\nand evolution strategies, 373\\nand foothills, plateaus and ridges, 101–103\\nsteepest ascent, 98–100\\nHillis, Danny, 413\\nHodge, A.,Alan Turing the Enigma of Intelligence, 291\\nHolland, John H., 377, 380, 388, 397, 403\\nAdaptation in Natural and Artificial Systems, 387\\nHopfield, John, 307'), Document(metadata={}, page_content='Hopfield, John, 307\\nHopfield networks, 292, 307–313, 660\\napplication, 310–313\\nstages of, 312–313\\ncompared to BAMs, 314, 315–316\\nHorizon problem, 152, 660\\nHorizontal layer architecture, 559.See also Subsumption\\narchitecture\\nHorn clauses, 173, 227–229, 660\\nHuman language, 571, 660\\nand ambiguity, 589–592\\nand  knowledge representation, 465–466\\nHybrid agent, 660\\nHyperbolic tangent function, 305\\nHypothesis, 248, 271, 661\\nmost general, 272\\nmost specific, 272\\nI\\nIBM corporation, 42, 165'), Document(metadata={}, page_content='most specific, 272\\nI\\nIBM corporation, 42, 165\\nID3 algorithm, 268, 278, 661\\ninductive bias of, 281\\nIF...THEN statements, 242, 243, 328. See also\\nImplication\\nIff (if and only if), 177–178, 184\\nImage capture, 661'), Document(metadata={}, page_content='728 Index\\nImage processing, 608–615\\nedge detection, 609–612\\nImage recognition, 661\\nImplementation, 38–39\\nImplication, 178, 180, 183–184, 661.See also IF...THEN\\nstatements\\nand fuzzy logic, 513–515\\nmaterial, 183\\nImplied introduction, 193\\nIncompleteness theorem, of Gödel, 21\\nIndependence, 661\\nInductive bias, 265, 276, 283, 661\\nof ID3 algorithm, 281\\nof nearest neighbor algorithm, 283–284\\nInductive-learning methods, 270–271\\nInductive reasoning, 201–202, 662\\nand Bayesian theory, 338–339'), Document(metadata={}, page_content='and Bayesian theory, 338–339\\nInference engine, 243, 252f, 253, 662\\nInference rules, 191–195, 662\\nInformation agents (Internet agents), 553–554, 572, 662\\nInformation gain, 278–281, 662\\nInformation retrieval (IR), 464, 572–573, 594–598, 662\\nInformed search methods, 72, 662\\nInge, Charles, On Monsieur Coue, 433\\nInheritance, 31–32, 662\\nand frames, 34–35, 469\\nmultiple, 36–37, 42\\nInitial state, 72, 662\\nInput layer, of multilayer network, 301\\nInstance-based learning, 283–284\\nInstance constructor, 662'), Document(metadata={}, page_content='Instance constructor, 662\\nInstance frame, 32, 663\\nInstances, 30, 41–42, 436–437, 662\\nInstantiated variables, 436–437\\nIntelligence\\nof agents, 544–545\\ndefining, 4, 10–11\\nIntelligent agents, 23, 434, 463, 543–569, 663\\nInterface agents, 551–552, 663\\nInternet agents (information agents), 553–554, 572, 663\\nInterpretation, 468, 663\\nInterpreter, 243\\nInteRRaP , 559–560\\nIntroduction, rule of, 191\\nexamples, 194, 194–195\\nInverse, 36\\nInversion, and deception, 404, 663\\nIR (information retrieval), 464'), Document(metadata={}, page_content='IR (information retrieval), 464\\nIrrevocability, of search methods, 80, 663\\nIsland, 137–138\\nIsland-driven search, 137\\nIterated local search, 127, 663\\nIterative approach, to segmenting, 613\\nIterative-deepening A* (IDA), 132, 160\\nIterative deepening search (IDS), 88–90\\nJ\\nJacobs, R. A., 305–306\\nJava, 11, 12–13, 32, 41–42, 173, 552\\nJESS, 253\\nJob shop scheduling, 458\\nJohnson, Samuel, 465\\nJoint. See Joint probability distribution\\nJoint probability distribution, 330, 344–345, 663–664\\ncomputing, 342'), Document(metadata={}, page_content='computing, 342\\nand dependence, 347–349\\nJustification-based truth maintenance system (JTMS),\\n478–479\\nK\\nK-exchange, 126–127\\nKahneman, Daniel, 550\\nKarp, Richard,Randomized Parallel Algorithms for Back-\\ntrack Search and Branch-and-Bound Computa-\\ntion, 135\\nKasparov, Garry, 22, 143, 165\\nKing, Ron, 161\\nKinny, D., 558\\nKirkpatrick, S., 130\\nKlee, Paul, Pedagogical Sketchbook, 543\\nKnapsack problem, 110–112, 414\\nKnight’s T our, 414\\nKnowledge\\nincomplete, 202\\nmeta, 247\\nKnowledge acquisition, 494'), Document(metadata={}, page_content='meta, 247\\nKnowledge acquisition, 494\\nKnowledge base, 243, 252f, 664\\nKnowledge base editor, 252f, 253\\nKnowledge base engineer, 664\\nKnowledge engineer, 252, 253\\nKnowledge engineering, 254, 467, 494–495\\nKnowledge, importance of, 6\\nKnowledge Level, The (Newell), 209\\nKnowledge representation, 11-12, 28–67, 465–501'), Document(metadata={}, page_content='Index 729\\nblackboard architecture, 469–472\\ncopycat architecture, 474–476\\nand human language, 465–466\\nimportance of, 28–29, 467\\nnonmonotonic reasoning, 476–487\\nrules for, 242–243\\nscripts, 472–474\\nstructured, 469\\nKnowledge sources, 469–470\\nKohonen maps, 265, 285, 292, 316–320, 664\\npurpose of, 316\\nKosko, Bart, 313\\nKozierok, R., 551\\nL\\nL-systems, 266, 376–377, 666\\nLa Pensee Sauvage (Levi-Strauss), 571\\nLa Vie d’Henri Brulard (Stendahl), 503\\nLafferty, Don, 161\\nLangton, Christopher G., 371'), Document(metadata={}, page_content='Lafferty, Don, 161\\nLangton, Christopher G., 371\\nLanguage. See also Linguistics, and AI\\nformal, 571–572\\nhuman, 468\\nand knowledge representation, 465–466\\nspoken, 472\\nnatural, 571–572\\nLanguage identification, 593–594\\nLanguages, of logic. See First-order predicate calculus;\\nPropositional calculus\\nLanguages, programming, 468\\nC++, 12–13, 32\\nJava, 11, 12–13, 32\\nLISP , 9, 11, 13, 388\\noverview, 14–15\\nobject-oriented. See C++; Java\\nPROLOG, 13, 13–14\\nLatham, William, 412'), Document(metadata={}, page_content='PROLOG, 13, 13–14\\nLatham, William, 412\\nLaw of the excluded middle, 203, 515, 651\\nLeaf nodes, 45, 664\\nLeak nodes, 348\\nLearning ability, of agents, 545\\nLearning agents, 561–562, 664\\nLearning algorithm, 273–274\\nLearning\\ncentralized vs. decentralized, 562\\ncompetitive, 316, 642 \\nmachine. See Machine learning\\nin multiagent systems, 267\\nLearning neural networks, 284–285\\nLearning rate, 297\\nLeast commitment, principle of, 447–448\\nLeast-constraining value, 122\\nLegal rules, 370\\nLeibniz, Gottfried, 7'), Document(metadata={}, page_content='Legal rules, 370\\nLeibniz, Gottfried, 7\\nLenat, Douglas B., Programming Artificial \\nIntelligence, 241\\nLevi-Strauss, Claude, La Pensee Sauvage, 571\\nLexical ambiguity, 589\\nLexicon, 664\\nLife, defining, 364–365\\nLife, game of, 664\\nLIFO, 84\\nLii, H. Penny, 469, 470\\nLikelihood, 338, 339, 504, 665\\nrelative, 681\\nLindenmayer, Aristid, 376\\nLinear threshold function, 665\\nof artificial neurons, 294\\nLinear time temporal logic, 488\\nLinearly separable functions, 299–300, 301, 665'), Document(metadata={}, page_content='Linearly separable functions, 299–300, 301, 665\\nLinguistic variables, 503, 504–505, 665\\nLinguistics, and AI, 3, 4, 11–12\\nLinks, protected, 679\\nLISP , 9, 11, 13, 255, 388, 665\\nLiteral, definition, 211\\nLiterals, 227, 665\\nLoad balancing, 134\\nLocal ambiguity, 590\\nLocal maxima, 101, 126, 392, 411, 665–666\\nLocal minima, 130\\nLocal optimization, 126, 666\\nLocal search methods, 117, 126–128, 666.See also\\nGenetic algorithms\\nLogic, 5, 6–7, 175–208\\nclassical, 177, 203, 504, 641\\nand change, 205'), Document(metadata={}, page_content='classical, 177, 203, 504, 641\\nand change, 205\\ndefinition, 176\\nfirst-order predicate (FOPL). See First-order predi-\\ncate logic (FOPL)\\nfuzzy. See Fuzzy logic\\nlogical operators, 177–181\\ntranslating, 178–181\\nmodal, 668\\npropositional, 175–196'), Document(metadata={}, page_content='730 Index\\npropositional calculus, 189–196\\nuse, in AI, 176–177\\nLogical systems, properties of, 175\\nLogics, nonclassical, 467\\nLogistello, 166\\nLongest-matching strategy, 246–247\\nLoops, as self-reproducing systems, 371\\nLove’s Labours Lost (Shakespeare), 28\\nM\\nM-estimate, 354\\nMachine learning, 265, 267–289\\nalgorithms, 273–274, 281, 283–284, 286\\nand artificial neural networks, 284–285\\ncandidate elimination, 275\\nconcept learning, 270–271\\nand decision-tree induction, 276–278'), Document(metadata={}, page_content='and decision-tree induction, 276–278\\ngeneral-to-specific ordering, 272–273, 273–274\\nand information gain, 278–281\\nand the problem of overfitting, 282–283\\nreinforcement, 286\\nrote learning, 270\\nand supervised learning, 285\\ntraining, 268–270\\nand unsupervised learning, 285\\nversion spaces, 274–275\\nMachine translation, 592, 666\\nMachine vision, 464, 605–628\\nand face recognition, 626–628\\nimage processing, 608–615\\nmotion in, 623–625\\nparts decomposition method, 626\\nusing, 625–626'), Document(metadata={}, page_content='parts decomposition method, 626\\nusing, 625–626\\nusing texture in, 615–623\\nMaes, P ., 551\\nMamdani, Ebrahim, 516\\nMamdani inference, 516, 666\\nManfred (Byron), 571\\nManhattan distances, 93–94\\nMap coloring, 666\\nMAP hypothesis, 351\\nMarkov decision processes (MDPs), 561\\nMassively parallel, 133\\nMaterial implication, 183\\nMateus, Paulo, 456\\nMathematical function, optimization of, 393–396\\nMatrix arithmetic, and Hopfield networks, 307–310\\nMax node, 149\\nMaximum a posteriori, 351, 666\\nMcCarthy, John, 9, 480'), Document(metadata={}, page_content='McCarthy, John, 9, 480\\nMcCulloch, W. S., 12, 291, 293\\nMeans-ends analysis, 419, 422, 428–430, 667\\nand STRIPS, 435, 440–441\\nMedical uses, for AI, 23, 73\\nmedical diagnosis, 331–332, 485–487\\nMembership function, 667\\nMemory, 306, 307, 667\\nautoassociative, 313\\nand Hopfield networks, 312–313\\nMental situation calculus, 467, 492–494, 667\\nMessy genetic algorithms (mGAs), 405–406, 667\\nMeta knowledge, 247, 667\\nMeta rules, 247–248, 668\\nMetaheuristics, 126, 667\\nMethods, weak vs. strong, 5–6\\nMetrics, 69, 668'), Document(metadata={}, page_content='Methods, weak vs. strong, 5–6\\nMetrics, 69, 668\\nand artificial evolution, 373–374, 390\\nfor determining fitness, 411–412\\nMetropolis Monte Carlo simulation, 128, 129\\nMin-conflicts heuristic, 123–125\\nMin node, 149–150\\nMinimax, 149–151\\nMinimax algorithm, 69, 149–153, 668\\nExpectiminimax, 167\\nlimitations of, 163–164\\nMinsky, Marvin, Steps Toward Artificial Intelligence,28\\nMIPS (millions of instructions per second), 160\\nMissionaries and cannibals, 47\\nMIT Mobot Lab, 563\\nMitchell, Melanie, 9, 474'), Document(metadata={}, page_content='MIT Mobot Lab, 563\\nMitchell, Melanie, 9, 474\\nMobile agents, 546, 552–553, 668\\nModal logics, 177, 203–204, 668\\nreasoning in, 204\\nModal operator, M, 668\\nModus ponens, rule of, 192, 201, 668\\nexamples, 193–194, 194–195\\nand fuzzy logic, 514, 515\\nMomentum, and backpropagation, 305\\nMonitor, 470\\nMonotonic heuristic, 96\\nMonotonic reasoning systems. See also Predicate logic;\\nPropositional logic\\ndefined, 476–477\\nMonotonicity, 41, 95–96, 175, 196, 201, 668–669'), Document(metadata={}, page_content='Index 731\\nMonte Carlo simulation, 128–130\\nMorphologic analysis, 573, 574–575, 669\\nMost-constrained variables, 121–122\\nMost general hypothesis, 272, 669\\nMost general unifier (MGU), 224, 669\\nMost specific hypothesis, 272, 669\\nMotion field, 669\\nMotion, interpreting, 623–625\\nMove notation, 426\\nMultiagent system, 669\\nMultiagent systems, 545, 554–556\\nlearning, 562\\nMultilayer neural networks, 291–292, 300–306, 669\\nMultiple inheritance, 36–37, 42, 670\\nMultivalent logic, 504, 670\\nMurakami, Takeshi, 166'), Document(metadata={}, page_content='Murakami, Takeshi, 166\\nMutation, artificial, 372–373, 373, 375, 378, 380–381,\\n387, 392, 405–406, 670\\nand schemata, 402–403\\nMutex conditions, 455\\nMutexes, 452, 453–454, 454–455\\nMutual exclusion information (mutex).See Mutexes\\nMYCIN, 255–256, 485–487, 670\\nN\\nN-gram, 670\\nNaive Bayes’ classifier, 351–356, 670\\nNatural language, 571–572\\nNatural language processing (NPL), 11, 12, 464, 472,\\n573–592, 670\\nand ambiguity, 589–592\\nBackus-Naur form (BNF), 575–579\\ndefinition, 572'), Document(metadata={}, page_content='Backus-Naur form (BNF), 575–579\\ndefinition, 572\\nmorphological analysis, 574–575\\nNatural selection, 372\\nNatural Selection, Incorporated, 164\\nNearest neighbor algorithm, 283–284, 671\\nNearest neighbor heuristic, 53, 671\\nNegation, 179–180, 181–182\\nNegation by failure, 480, 671\\nNegation, double, 193\\nNegative training example, 271\\nNegatives, false, 598–599, 653\\nNeural networks, 126, 265, 284–285, 291–326, 671\\nevolving, 321–322\\nmultilayer, 291–292, 300–306\\narchitecture of, 301–302'), Document(metadata={}, page_content='architecture of, 301–302\\nbackpropagation in, 302–306\\nBidirectional associative memories (BAMs),\\n313–316\\nfeed-forward, 301, 306\\nKohonen maps, 316–320\\nrecurrent, 292, 306–313\\nstability or equilibrium of, 306–307\\nunstable, 307\\nunsupervised learning networks, 316–321\\nNeuro-fuzzy systems, 503, 534–538, 671\\ndefuzzification layer, 538\\nfuzzification layer, 536–537\\nfuzzy rule layer, 537\\ninput layer, 536\\nlearning mechanism, 538\\noutput membership function layer, 537\\nNeurons, 284, 671\\nartificial, 293–295'), Document(metadata={}, page_content='Neurons, 284, 671\\nartificial, 293–295\\nbiological, 292–293\\nNewborn, Monty, 164\\nNewell, Alan, 6, 9, 430\\nThe Knowledge Level, 209\\nNewton, Isaac, 7\\nNight Thoughts (Y oung), 209\\nNilsson, Nils J., 434\\nNodes, 29, 31, 671\\nand- and or-, 58–59\\nand-nodes, 58–59\\ngoal, 45\\nleak, 348\\nmax, 149–150\\nmin, 149–150\\nroot, 45, 682\\nsuccess and failure, 59\\nNoise parameters, 348\\nNoisy data, 282–283, 283–284\\nNoisy logical relationships, 346, 347–349\\nNoisy-v function, 346, 347–349'), Document(metadata={}, page_content='Noisy-v function, 346, 347–349\\nNonchronological backtracking, 76, 121, 137–138, 671\\nNoncontingent statements, 203–204, 672\\nNondeterministic search, 136–137\\nNondirected graphs, 45, 672\\nNonmonotonic, 672\\nNonmonotonic logics, 467\\nNonmonotonic reasoning, 467, 476–487.See also Fuzzy\\nlogic\\nabductive, 482–483'), Document(metadata={}, page_content='732 Index\\ncircumscription, 480–482\\nclosed world assumption, 480\\ndefault reasoning, 477–478\\nDempster-Shafer theory, 483–485\\nMYCIN, 485–487\\nnonmonotonic logic, 477\\nramification problem, 480\\ntruth maintenance systems (TMS), 478–479\\nNonterminal symbol, 577, 672\\nNormal distribution, 373, 672\\nNormal forms, 210–212\\nfor predicate logic, 219–220\\nprenex, 219–220\\nNormalization, 335–337, 672\\nN o r v i g ,P e t e r ,1 6 7\\nNot (operator), 179–180, 181–182\\nNoun, 672\\nNoun phrases, 576, 581–582, 673'), Document(metadata={}, page_content='Noun, 672\\nNoun phrases, 576, 581–582, 673\\nNP-complete, 50–51, 673\\nNPL (natural language processing), 464\\nO\\nObject, 242\\nObject-oriented programming, 41–42\\nOccam’s razor, 276, 283, 673\\nOccluding edge, 613–614, 673\\nOffspring, in artificial evolution, 373\\ndetermining fitness of, 380\\nOpen world assumption, 480\\nOpening book, 161, 673\\nOperator-based planning, 434\\nOperator schema, 436–437, 673\\nOperators, action\\ndemoted vs. promoted, 447\\nand STRIPS, 435–437\\nOperators, logical, 177–184'), Document(metadata={}, page_content='and STRIPS, 435–437\\nOperators, logical, 177–184\\nand, 178, 242, 296, 299\\nbinary, 182, 637\\nimplication, 180–181, 183–184, 242\\nnot, 179–180, 181–182\\nor, 182–183, 242, 296, 299, 299–300\\nunary, 181, 392, 692\\nusing, in truth tables, 181–184\\nOpportunistic reasoning model, 469–470\\nOPS5, 253\\nOptical field, 623\\nOptical flow, 623, 673\\nOptimal classification, Bayes’ , 349–351\\nOptimal classifier, Bayes’ , 637\\nOptimal path, 91, 673\\nidentifying, 107–112\\nOptimality, of search methods, 79–80, 673'), Document(metadata={}, page_content='Optimality, of search methods, 79–80, 673\\nOr-goals, 58–59, 674\\nOr-introduction, rule of, 192\\nOr-nodes, 58–59, 674\\nOr (operator), 179, 182–183\\nOthello (game), 165, 166\\nOutput layer, of multilayer network, 301\\nOverfitting, problem of, 282–283, 674\\nOverridden, 32\\nOverriding, 674\\nOverspecified chromosome, 674\\nP\\nP class problems, 50–51\\nParadoxes, well-known, and fuzzy logic, 515–516\\nParallel search, 117–118, 132–133, 674\\nParallel window search (PWS), 134\\nParent, in artificial evolution, 373'), Document(metadata={}, page_content='Parent, in artificial evolution, 373\\nParse trees, 581–582, 590\\nParser, 63, 674\\nParsing, 575, 581–588\\nchart, 585–588\\nTransition networks, 582–585\\nPartial order, 674\\nPartial order planning, 434, 444–447, 675.See also\\nGraphPlan algorithm\\nPartial path, 45, 675\\nPartially observable Markov decision processes\\n(POMDPs), 561\\nParts decomposition method, and machine vision, 626\\nPath, 45, 675\\nPath-based evaluation function, 108\\nPattern-matching, 675\\nPattern-matching clauses, 40–41'), Document(metadata={}, page_content='Pattern-matching clauses, 40–41\\nPDDL. See planning domain definition language\\nPedagogical Sketchbook (Klee), 543\\nPerceptron training rule, 297\\nPerceptrons, 291, 295–300, 675\\nand Boolean operators, 296, 299–300\\nPersistence actions, 451, 453\\nPhenotype, 390, 412, 675\\nPhilosophische Untersuchunge (Wittgenstein), 571\\nPhilosophy, and AI, 3, 4, 10–11\\nPhonology, 573\\nPhysics and Beyond (Heisenberg), 241'), Document(metadata={}, page_content='Index 733\\nPictures, evolving, 412–413\\nPitts, W., 12, 291, 293\\nPixel, 675\\nPlan, 675\\nPlanner, 421–422\\nPlanning, 469, 675\\nAction description language (ADL), 455–456\\ncase-based systems, 457–458\\nconditional, 457, 643\\ndynamic world, 419, 456–457, 650\\nand goal-based agents, 548\\ngraphs, 451–455\\nmutexes, 452\\npartial order, 434, 444–447\\nprobabilistic, 419, 456, 677\\npropositional, 448–450\\nSAT, 450–451, 683\\nand scheduling, 458–459\\nas search, 423–425\\nvs. executing, 435–436'), Document(metadata={}, page_content='as search, 423–425\\nvs. executing, 435–436\\nPlanning domain definition language (PDDL), 456, 675\\nPlanning graph, 451–455, 676\\nPlanning methods, 433–462\\nPlans\\nvs. goals, 62\\npartial order. See also GraphPlan algorithm\\nPlateaus, 101–103\\nPlato, 6, 10\\nPly, 77, 676\\nPoetics (Aristotle), 327\\nPoint fluent, 494\\nPolaroid, 381\\nPolynomial time, 50–51\\nPope, Alexander,An Essay on Man, 433\\nPopulation, 131\\nin genetic algorithm, 388, 676\\nsize, 389\\nPorter’s stemmer, 597–598\\nPositives, false, 598–599, 653'), Document(metadata={}, page_content='Positives, false, 598–599, 653\\nPossible world, 479, 676\\nPosterior probability, 331, 676\\ncalculating, 345–346, 350-354\\nestimating, 354–355\\nhighest, 351\\nPraed, Winthrop Mackworth,The T alented Man, 175\\nPragmatic analysis, 676\\nand natural language processing, 573\\nPrecedence, 180\\nPrecision, 553, 598–599, 676\\nPrecondition, 677\\nPredators, and co-evolution, 413–414\\nPredecessor, 677\\nPredicate calculus, 173, 175, 196–199.See also First-order\\npredicate logic (FOPL)\\nand change, 210\\ninterpretation, 468'), Document(metadata={}, page_content='and change, 210\\ninterpretation, 468\\nsyntax of, 196–197\\nPredicate logic, 30, 179, 476\\nnormal forms for, 219–220\\nPremises, 176, 677\\nPrenex normal form, 219–220, 677\\nPrey (Crichton), 543\\nPrinciple component analysis, 627–628, 677\\nPrinciple of least commitment, 447–448\\nPrior probability, 331, 677\\nPrisoner’s Dilemma, 266, 388, 406–411, 677\\nchoice of opponents, 410–411\\ndiversity, 411–412\\nand predators, 413–414\\nstrategies, 407–410\\nevolution of, 410\\nrepresentation of, 407–408\\ntit-for-tat, 409'), Document(metadata={}, page_content='representation of, 407–408\\ntit-for-tat, 409\\nProbabilistic planning, 419, 456, 677\\nProbabilistic reasoning, 266, 327–361, 504, 678. See also\\nBayesian; Bayes’ Theoerem and propositional\\nlogic, 328-330\\nProbability, 504, 678\\nand ambiguity, 591\\ncalculating, 330-332, 350, 351\\nconditional, 329-330, 643\\ncomparing, 334-335\\ntables, 340-341\\nestimating, 354–355\\njoint probability distributions, 330\\nposterior. See Posterior probability\\nProbability theory, 201, 328–330\\nand dependence, 339-349'), Document(metadata={}, page_content='and dependence, 339-349\\nProblem reduction, 57–58, 678\\ntop down vs. bottom up, 60–61\\nProcedural attachments, 37, 678\\nProcedural semantics, 38–39\\nProcedures, 37–38, 678\\nProduct rule, 331, 678\\nProduction rule, 678\\nProduction systems, 469. See Expert systems'), Document(metadata={}, page_content='734 Index\\nProgramming\\nevolutionary, 375–376\\ngenetic, 374–375\\nProgramming Artificial Intelligence (Lenat), 241\\nPROLOG, 13, 173, 679\\nand closed world assumption, 480\\nand Horn clauses, 227–229\\noverview, 13–14\\nand resolution, 210\\nPromoted operators, 447\\nProof by contradiction, 192–193, 214–216\\nProof by refutation, 173, 214–216\\nProposition letters, 189\\nPropositional calculus, 173, 175, 189, 189–196, 679\\nrules of deduction, 190–196\\nsemantics, 190\\nsyntax, 189–190'), Document(metadata={}, page_content='semantics, 190\\nsyntax, 189–190\\nPropositional logic, 175–196, 347, 468, 476, 679\\nlogical operators, 177–178\\nmonotonicity, 201\\npropositional calculus, 189–196\\nresolution in, 210–216\\nrules of inference, 191–196\\nsemantics, 190\\nsyntax, 189–190\\ntranslating, 178–181\\nand truth tables, 181–184\\ncomplex, 184–189\\nPropositional planning, 448–450, 679\\nPropositional symbols, 189\\nProtected causal links, 446–447\\nProtected links, 679\\nPruning, 679\\nPsychology, and AI, 3, 4, 12\\nPure and-or tree, 64, 679\\nPush, 435\\nQ'), Document(metadata={}, page_content='Pure and-or tree, 64, 679\\nPush, 435\\nQ\\nQuantifiers, 679\\napplication, 199–200\\nexistential, 197\\neliminating, 220–222\\nmoving, 220–222\\nuniversal, 197\\nQuantum physics, 504\\nQuenching, 130\\nQueue, 83, 84\\nQuinlan, J. R., 278\\nR\\nRabbi Ben Ezra (Browning), 421\\nRamification problem, 480, 679\\nRamps, 413\\nRandomized Parallel Algorithms for Backtrack Search\\nand Branch-and Bound Computation (Karp and\\nZhang), 135\\nRationality, 550\\nand game playing, 146\\nRationality, and game playing, 149, 680'), Document(metadata={}, page_content='Rationality, and game playing, 149, 680\\nReactive agents, 547–548, 559, 680\\nReal-time A*, 131–132\\nReal-world systems, 573–574\\nRecall, 553, 598–599, 680\\nRechenberg, Ingo, 373\\nRecommendation rule, 243\\nRecurrent networks, 301, 306–313, 680\\nHopfield networks, 307–313\\nRecursive depth, 84\\nRecursively enumerable, 680\\nRecursively enumerable grammars, 580\\nReductio ad absurdum, 192–193, 680\\nexamples, 194–195\\nReferential ambiguity, 589–590\\nReflex agents, 547–548, 680.See also Reactive agents'), Document(metadata={}, page_content='Refutation proof, 192–193, 214–216, 680\\nRegular expression, 580, 680\\nRegular grammar, 681\\nRegular languages, 579–580, 681\\nReinforcement learning, 286, 681\\nRelative likelihood, 681\\nRelaxed problem, 95, 125–126, 681\\nRelaxing, 94\\nReligio Medici (Browne), 363\\nReplanning, 457, 681\\nRepresentational adequacy, 40–41, 467, 681\\nRepresentational frame problem, 427–428\\nRepresentational methods\\nblackboard architecture, 467\\ncopycat architecture, 467\\nscripts, 467\\nRepresentations, 681\\nof frame problem, 427–428'), Document(metadata={}, page_content='Representations, 681\\nof frame problem, 427–428\\nfor genetic algorithms, 388–389\\nof strategy, for Prisoner’s Dilemma, 407–408\\nReproduction, 389–390, 395–396.See also Crossover\\nin classifier system, 380–381\\nand messy genetic algorithms, 405–406'), Document(metadata={}, page_content='Index 735\\nand schemata, 399–403\\nsexual vs. asexual, 373\\nRepublic: The Revolution, 24\\nResolution, 209–239, 682\\nalgorithm, 226–227\\napplication, 216–218\\nexample, 233–236\\nin predicate logic, 218–219\\nin propositional logic, 210–216\\nnormal forms, 210–212\\nrule, 213–214\\nin PROLOG, 228\\nrules, 212–216\\nand STRIPS, 441–443\\nResolution, automation of, 173\\nResolution (Robinson), 209\\nResolvent, 213\\nResult function, 426\\nRete algorithm, 242, 253–254, 682\\nReversi (game), 166\\nRewrite rule, 682'), Document(metadata={}, page_content='Reversi (game), 166\\nRewrite rule, 682\\nRewrite rules, 577–579, 580\\nReynolds, Craig, 366, 555\\nRidges, 101–103, 682\\nRobinson, Alan, Resolution, 209\\nRobot navigation, 414\\nRobotic agents, 562–563, 682\\nRobots, 23, 423, 434, 438–443, 682\\ncontrol mechanisms, 557\\nmobile, 552\\nRomeo and Juliet (Shakespeare), 267\\nRoot goals, 59, 682\\nRoot nodes, 45, 682\\nRosenblatt, F., 295, 296\\nRote learning, 270, 682\\nRoulette-wheel selection, 394, 682\\nRule-based systems, 30, 34, 243–251, 683\\nand backward chaining, 248–251'), Document(metadata={}, page_content='and backward chaining, 248–251\\nbackward chaining in, 257–259\\nand conflict resolution, 245–247\\nand deduction, 244–245\\nand forward chaining, 244–245\\nand longest-matching strategy, 246–247\\nand meta rules, 247–248\\nRule relation, 227\\nRules, 173, 228, 683\\nas directives, 243\\nof inference, 191–195\\nlegal, 370\\npurpose of, 242\\nrecommendation, 243\\nresolution, 212–216\\ntotalistic, 370\\nRun, of genetic algorithm, 392–393\\nRuskin, John, Seven Lamps of Architecture, 28\\nRussell, Stuart, 167\\nRussell’s paradox, 515'), Document(metadata={}, page_content='Russell, Stuart, 167\\nRussell’s paradox, 515\\nS\\nS-expressions, 388, 684\\nSamuel, Arthur,Some Studies in Machine Learning Using\\nthe Game of Checkers, 160\\nSAT planning, 450–451, 683\\nSatisfiability, 175, 187, 231–233, 683\\nSatisfiability problem, 51\\nSatisfiability problem (SAT)\\nand propositional notation, 450–451\\nsystematic approach, 450\\nSchaeffer, Jonathan, 143, 160, 163, 164\\nSchank, 473\\nScheduling, 683\\nScheduling, and planning, 458–459\\nSchema, 472, 559, 683.See also Scripts\\nSchema theorem, 403, 683'), Document(metadata={}, page_content='Schema theorem, 403, 683\\nSchemata, 387, 397–403\\nbuilding blocks, 403–404\\nand crossover, 401–402\\nand mutation, 402–403\\nand reproduction, 399–403\\nSchwefel, Hans-Paul, 373\\nScrabble, 167\\nScripts, 20, 467, 683\\ndefinition, 472\\nSearch, 684\\ndata-driven, 73\\nand goal-based agents, 548\\ngoal-driven, 73, 657\\niterated, 663\\nproblem solving as, 72\\nSearch engines, 88, 135–136\\nSearch methods, 71–116\\nbreadth-first, 76–78\\nimplementing, 86–88\\ndata-driven or goal-driven, 73–74\\ndepth-first, 75–76, 77, 80–83'), Document(metadata={}, page_content='depth-first, 75–76, 77, 80–83\\nimplementing, 83–86'), Document(metadata={}, page_content='736 Index\\ndepth-first iterative deepening, 88–90\\ngenerate and test, 74–75\\nheuristics, 90–97\\nhill climbing, 98–103\\ninformed vs. uninformed, 91–92\\nlocal, 666\\nparallel, 674\\nand planning, 423–425\\nproblem solving as, 72\\nproperties of, 78–80\\ncompleteness, 79\\ncomplexity, 78–79\\nirrevocability, 80\\nmonotonicity, 95–96\\noptimality, 79–80\\nSearch spaces, 42–44, 72, 684\\nSearch trees, 684\\ndiagram, 46f\\nexamples\\ndescribe and match, 56–57\\nmissionaries and cannibals, 47–50\\ntowers of Hanoi, 54–56'), Document(metadata={}, page_content='towers of Hanoi, 54–56\\ntraveling salesman, 50–54\\nfor plan, 424–425\\nRete algorithm. See Rete algorithm\\nand STRIPS, 439–440\\nSearching, 49\\nSearle, John, 20\\nSegmentation, 612–613, 684\\nSelection, artificial, 372–373\\nSelection, evolutionary, 372\\nSelf-organizing feature map, 316. See also Kohonen maps\\nSelf-reproducing systems, 371–372\\nSelman, B., 450\\nSemantic ambiguity, 589\\nSemantic analysis, 588–589, 684\\nSemantic nets, 29–31, 588–589, 684\\ndiagram, 30f\\nframe system for, 32–33'), Document(metadata={}, page_content='diagram, 30f\\nframe system for, 32–33\\nSemantic trees, 44–57, 684\\ndiagram, 44f\\nsearch trees, 46–57\\nSemantics, 38–39\\nand natural language processing, 573\\nof propositional logic, 190\\nand representations, 468 \\nSen, Sandip, Learning in Multiagent Systems, 267\\nSentence, well-formed, 189–190, 199\\nSet notation, of propositional logic, 189\\nSeven Lamps of Architecture (Ruskin), 28\\nSexual reproduction, 373\\nShakespeare, William\\nHamlet, 503\\nHenry V ,143\\nLove’s Labours Lost, 28\\nRomeo and Juliet, 267'), Document(metadata={}, page_content='Love’s Labours Lost, 28\\nRomeo and Juliet, 267\\nShaw, George Bernard, 71\\nShaw, J. C., 430\\nShelley, Mary,Frankenstein, 3\\nShepard, D., 283\\nShepard’s method, 283, 685\\nSigmoid function, 294f, 302, 305, 685\\nSign activation function, 307, 685\\nSign of Four, The (Conan Doyle), 209\\nSimon, Herbert A., 6, 9, 430\\nSimple Monte Carlo simulation, 128–129\\nSimplification, of logical expressions, 188–189\\nSims, Karl, 390, 413\\nSimulated annealing, 69, 117, 126, 128–131, 396, 685\\nuses of, 130–131'), Document(metadata={}, page_content='uses of, 130–131\\nSingle-step selection, 372\\nSingular-extension heuristic, 153\\nSituated, 557\\nSituated action rules, 557, 685\\nSituation action rules. See Situated action rules\\nSituation calculus, 419, 422, 426–427, 467, 685\\nSituation variables, 426, 434–435, 685\\nSkolem constant, 221, 686\\nSkolem function, 686\\nSkolem normal form, 686\\nSkolemization, 173, 220–222, 686\\nexample, 221–222\\nSlipnet, 474–475, 686\\nSlippage, 476\\nSlot reader, 37, 686\\nSlot values, 32–33, 686\\nSlot writer, 38\\nSlots, 32–33, 686'), Document(metadata={}, page_content='Slot writer, 38\\nSlots, 32–33, 686\\nas frames, 35–36\\nSmart agent, 686\\nSmoothing, 611, 687\\nSocial interaction, of agents, 545–546\\nSocrates, 10\\nSoftware agents, 543–544, 687'), Document(metadata={}, page_content='Index 737\\nmobile, 552\\nSolved, 166\\nSoma, 292–293\\nSome Studies in Machine Learning Using the Game of\\nCheckers (Samuel), 160\\nSony corporation, 23\\nSoundness, 173, 175, 200, 687\\nSpace complexity, 78\\nSpärck Jones, Karen, 597\\nSpidering the web, 72, 88, 135–136, 687\\nSpielberg, Stephen, 23\\nSplice and cut operators, 405–406\\nSplitting and merging, 613\\nSQL, 572\\nStability, of neural networks, 306–307\\nStack, 84, 687\\nStagnant, 371\\nStagnate, 395, 411\\nStanford University, 255, 485–486\\nStart state, diagram, 452f'), Document(metadata={}, page_content='Start state, diagram, 452f\\nStart symbol, 577\\nState, 687\\nState spaces, 43–44, 72, 83, 84, 687\\ndiagram, 44f\\nStatic agents, 552\\nStatic evaluators, in game playing, 146–148, 687\\nSteels, Luc, The Artificial Life Roots of Artificial Intelli-\\ngence, 363\\nSteepest ascent hill climbing, 98–100\\nSteinbeck, John, The Grapes of Wrath, 291\\nStemming, 596–598, 687\\nStendahl, Henri Beyle, La Vie d’Henri Brulard, 503\\nStep function, of artificial neuron, 294, 302\\nSteps Toward Artificial Intelligence(Minsky), 28'), Document(metadata={}, page_content='Steps Toward Artificial Intelligence(Minsky), 28\\nStevenson, Adlai E., Jr., 117\\nStochastic, 560\\nStochastic methods, 450\\nStop list, 594, 687\\nSTRIPS (Stanford Research Institute Problem Solver),\\n419, 422, 430, 434–443, 688\\nassumption, 688\\nand closed world assumption, 480\\nand GraphPlan, 454–455\\nimplementing, 437–443\\nbackward chaining, 440–441\\nforward chaining, 439\\nmeans-ends analysis, 440–441\\nand resolution, 441–443\\nsearch trees, 439–440\\noperators, 435–437\\nand principle of least commitment, 447–448'), Document(metadata={}, page_content='and principle of least commitment, 447–448\\nand propositional planning, 448–450\\nSussman anomaly, 443–444\\nStrong AI, 688\\nStrong methods, 688\\nStructural texture analysis, 620\\nStructured knowledge representation, 469\\nSubclass, 31–32, 42, 688\\nSubgoal, 58, 688\\nSubproblems, 57, 688\\nSubset, 688\\nSubstitution, in logical expressions, 198, 222–223, 229,\\n688\\nSubsumption architecture, 556–557, 559, 688\\nSuccess nodes, 59, 689\\nSuccessor state axioms, 422, 427–428\\nSuccessors, 83, 689\\nSun corporation, 552'), Document(metadata={}, page_content='Successors, 83, 689\\nSun corporation, 552\\nSuperclass, 31–32, 689\\nSupervised learning, 285, 689\\nin multilayer networks, 292\\nSurvival of the fittest, 372\\nSussman anomaly, 443–444\\nSyllogism, 6, 689\\nSymbolic representation, 558–559\\nSynapses, 292–293, 689\\nSyntactic ambiguity, 589\\nSyntactic analysis, 581–582, 689\\nSyntactic structures, 11\\nSyntax\\nand natural language processing, 573\\nof predicate calculus, 196–197\\nof propositional logic, 189–190\\nSystematic approach, to SAT, 450'), Document(metadata={}, page_content='Systematic approach, to SAT, 450\\nSystems reply, to Chinese room, 21\\nT\\nTabu search, 69, 126, 127–128, 689\\nT alented Man, The(Praed), 175\\nTanh, 305\\nTask distribution, 134, 134–135\\nTautologies, 175, 186–187, 201, 689\\nT elescript, 552\\nT emperature, 129'), Document(metadata={}, page_content='738 Index\\nT emplate chromosomes, 405\\nT emporal logic, 467, 487–490, 690\\nlinear time, 488\\nusing, 489–490\\nT ennyson, Alfred, 465\\nT erm frequency-inverse document frequency (TF-IDF),\\n594–596, 690\\nT erminal symbols, 577, 580, 690\\nT ermination criteria, for genetic algorithm, 392–393\\nT erms, 199, 690\\nT exels, 620, 690\\nT exture, 690\\nin machine vision, 615–623\\ndetermining shape and orientation, 620–623\\nidentifying, 616–620\\nstructural texture analysis, 620'), Document(metadata={}, page_content='structural texture analysis, 620\\nTF-IDF (term frequency-inverse document frequency),\\n594–596, 690\\nTheorems, 200, 690\\nThomas, Dylan, Especially When the October Wind, 291\\nThreat tree, 60\\nThreatened causal links, 446–447\\nThree-coloring problem, 216–218, 690\\nThresholding, 612–613\\nThrough the Looking Glass (Carroll), 19, 175\\nTic-tac-toe, 144–145, 166\\nTime complexity, 78\\nTimetable problem, 414\\nTinsley, Marion, 160–161, 163\\nTit-for-tat strategy, 409, 690\\nTo a Mouse (Burns), 421\\nT odd, Stephen, 412'), Document(metadata={}, page_content='To a Mouse (Burns), 421\\nT odd, Stephen, 412\\nT op down or bottom up, 60–61, 278, 690\\nbuilding parse tree, 581–582\\nT otal order plans, 444\\nT otalistic rules, 370\\nT ouringMachines, 559, 560\\nT owers of Hanoi, 691\\nTractatus Logico-Philosophicus (Wittgenstein), 19\\nTraining, 691\\nand machine learning, 268–270\\nTraining data, 691\\npositive and negative, 282\\nTransition model, 560\\nTransition networks, 582–585, 691\\naugmented, 585\\nTranslating logical operators, 178–181\\nTranslation, 8–9\\nautomated, 572'), Document(metadata={}, page_content='Translation, 8–9\\nautomated, 572\\nmachine, 592\\nTraveling salesman problem, 50–54, 96–98, 414\\nTree ordering, 134, 135\\nTrihedral vertex, 614–615, 691\\nTruth maintenance system (TMS), 467, 478–479, 691\\nTruth tables, 173, 181–184, 691\\ncomplex, 184–186\\nand fuzzy logic, 512–515\\ntautologies, 186–187, 201\\nTruth values, 176, 691\\nTuring, Alan, 7–8, 21, 291\\nComputing Machinery and Intelligence,7\\nTuring test, 8, 22, 691\\nTversky, A., 550\\n2001: A Space Odyssey (film), 19, 21–22\\nU\\nUlam, Stanislaw, 369'), Document(metadata={}, page_content='U\\nUlam, Stanislaw, 369\\nUnary operators, 181, 392, 692\\nUncertainty, and logic, 177, 467, 564\\nUncertainty principle, 504\\nUnderspecified chromosomes, 692\\nUnification, 222–226, 228, 229\\nexample, 225–226\\nand STRIPS, 437, 442\\nUnification algorithm, 224–225\\nUnifiers, 692\\nUniform cost search, 109–110\\nUniform crossover, 391–392, 692\\nUniform tree, 692\\nUninformed search, 692\\nUniversal quantifiers, 197, 692\\nUniverse of discourse, 504–505, 692\\nUniversity of Alberta, Canada, 160\\nUnrestricted grammars, 580'), Document(metadata={}, page_content='Unrestricted grammars, 580\\nUnstable neural networks, 307\\nUnsupervised learning, 285, 292, 693\\nUnsupervised learning networks, 316–321\\nKohonen maps, 316–320\\nexample, 318–320\\nUser interface, 252f, 253\\nUtility-based agents, 549, 551, 693\\nUtility functions, 549–551, 693\\nUtterance, 693'), Document(metadata={}, page_content='Index 739\\nV\\nVagueness, 504, 590\\nValidity, 176, 693\\nVanishing point, 624, 693\\nVariables, 181\\nbound vs. free, 198, 638\\nV ecchi, M. P ., 130\\nV erb, 693\\nV erb phrase, 693\\nV erb phrases, 576, 581–582\\nV ersatility, of agents, 546\\nV ersion spaces, 274–275, 693\\nV ertical layer architecture, 559, 559–560\\nVision\\nhuman, 606–608\\nmachine, 605–628\\nVisual data, analyzing. See Machine vision\\nVLSI (very large-scale integration), 130\\nVoltaire,Candide,7 1\\nVon Neumann, John, 369–370, 372\\nW\\nWalksat, 450'), Document(metadata={}, page_content='Von Neumann, John, 369–370, 372\\nW\\nWalksat, 450\\nWatts, Isaac, 327\\nWave search, 136\\nWeak AI, 693\\nWeak methods, 694\\nWeb spidering, 88, 135–136\\nWeight, 694\\nWeight vector, 694\\nWeighted linear functions, 147, 694\\nWeiss, Gerhard,Learning in Multiagent Systems, 267\\nWeizenbaum, Joseph, 8\\nWell-formed formula (WFF ), 189–190, 694\\ndefined, 199–200\\nand STRIPS, 435, 436, 437\\nWHEN-CHANGED procedures, 39–40, 694\\nWHEN-NEEDED procedures, 38, 40, 694\\nWHEN-READ procedures, 694\\nWHEN-WRITTEN procedures, 694'), Document(metadata={}, page_content='WHEN-WRITTEN procedures, 694\\nWillett, Peter, 597\\nWilliam of Occam, 276\\nWilson, Stewart, 381\\nWinner-take-all algorithm, 316, 694\\nWinston, Patrick Henry, 412\\nWittgenstein, Ludwig\\nPhilosophische Untersuchungen, 571\\nTractatus Logico-Philosophicus,1 9\\nWordsworth, William, 605\\nWorkspace, 474–475, 695\\nWorld model, 436–437, 591–592, 695\\nY\\nY oung, Edward,Night Thoughts, 209\\nZ\\nZero-sum games, 146, 695\\nZhang, Y anjun,Randomized Parallel Algorithms for Back-\\ntrack Search and Branch-and-Bound Computa-'), Document(metadata={}, page_content='track Search and Branch-and-Bound Computa-\\ntions, 135'), Document(metadata={}, page_content='Computer Science Illuminated, Second Edition\\nNell Dale and John Lewis\\nISBN: 0-7637-0799-6\\n©2004\\nProgramming and Problem Solving with Java\\nNell Dale, Chip Weems, \\nand Mark R. Headington\\nISBN: 0-7637-0490-3\\n©2003\\nDatabases Illuminated\\nCatherine Ricardo\\nISBN: 0-7637-3314-8\\n©2004\\nFoundations of Algorithms Using Java\\nPseudocode\\nRichard Neapolitan and Kumarss Naimipour\\nISBN: 0-7637-2129-8\\n©2004\\nArtificial Intelligence Illuminated\\nBen Coppin\\nISBN: 0-7637-3230-3\\n©2004'), Document(metadata={}, page_content='Ben Coppin\\nISBN: 0-7637-3230-3\\n©2004\\nThe Essentials of Computer Organization and\\nArchitecture\\nLinda Null and Julia Lobur\\nISBN: 0-7637-0444-X\\n©2003\\nA Complete Guide to C#\\nDavid Bishop\\nISBN: 0-7637-2249-9\\n©2004\\nA First Course in Complex Analysis \\nwith Applications\\nDennis G. Zill and Patrick Shanahan\\nISBN: 0-7637-1437-2\\n©2003\\nProgramming and Problem Solving with C++,\\nFourth Edition\\nNell Dale and Chip Weems\\nISBN: 0-7637-0798-8\\n©2004\\nC++ Plus Data Structures, Third Edition\\nNell Dale'), Document(metadata={}, page_content='C++ Plus Data Structures, Third Edition\\nNell Dale\\nISBN: 0-7637-0481-4\\n©2003\\nApplied Data Structures with C++\\nPeter Smith\\nISBN: 0-7637-2562-5\\n©2004\\nFoundations of Algorithms Using C++\\nPseudocode, Third Edition\\nRichard Neapolitan and Kumarss Naimipour\\nISBN: 0-7637-2387-8\\n©2004\\nManaging Software Projects\\nFrank Tsui\\nISBN: 0-7637-2546-3\\n©2004\\nReadings in CyberEthics, Second Edition\\nRichard Spinello and Herman Tavani\\nISBN: 0-7637-2410-6\\n©2004\\nC#.NET Illuminated\\nArt Gittleman\\nISBN: 0-7637-2593-5\\n©2004'), Document(metadata={}, page_content='Art Gittleman\\nISBN: 0-7637-2593-5\\n©2004\\nDiscrete Mathematics, Second Edition\\nJames L. Hein\\nISBN: 0-7637-2210-3\\n©2003\\nOutstanding New Titles:\\nhttp://www.jbpub.com/ 1.800.832.0034'), Document(metadata={}, page_content='Take Your Courses to the Next Level\\nTurn the page to preview new and forthcoming titles\\nin Computer Science and Math from \\nJones and Bartlett…\\nProviding solutions for students and educators in the following \\ndisciplines:\\nPlease visit http://computerscience.jbpub.com/ and\\nhttp://math.jbpub.com/ to learn more about our exciting publishing \\nprograms in these disciplines.\\n• Introductory Computer Science\\n• Java \\n• C\\n++\\n• Databases \\n• C# \\n• Data Structures \\n• Algorithms\\n• Network Security'), Document(metadata={}, page_content='• Algorithms\\n• Network Security \\n• Software Engineering \\n• Discrete Mathematics \\n• Engineering Mathematics \\n• Complex Analysis\\nhttp://www.jbpub.com/ 1.800.832.0034')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "v26MiAKev7K0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(\n",
        "    documents = docs,\n",
        "    embedding = embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "e0fDveuZwZed"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = db.similarity_search_with_score(\"what is Ai?\", k=2)\n",
        "for res, score in results:\n",
        "    print(f\"{score:.3f} → {res.page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OwsRxcM0xB1c",
        "outputId": "1c5b7a11-6a1a-48ed-8463-39ecb29c96df"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.527 → AI. see Artificial intelligence (AI)\n",
            "AI: Artificial Intelligence (film), 23\n",
            "AIBO robotic dog, 23\n",
            "Alan Turing the Enigma of Intelligence(Hodge), 291\n",
            "Alfonso the Wise, 117\n",
            "Algorithms\n",
            "acquaintance, 633\n",
            "AD3, 268, 278, 281, 661\n",
            "bucket brigade, 286, 377, 378, 379–380, 639\n",
            "genetic. See Genetic algorithms\n",
            "GraphPlan, 434, 451, 454–455, 658\n",
            "winner-take-all, 316, 694\n",
            "0.671 → 1.11 Chapter Summary\n",
            "■ Intelligence is difficult to define, and as a result Artificial Intelli-\n",
            "gence is also hard to define.\n",
            "■ One definition of Artificial Intelligence is:\n",
            "Artificial intelligence is the study of systems that act in a way that to\n",
            "any observer would appear to be intelligent.\n",
            "■ Proponents of strong AI believe that a computer that behaves in an\n",
            "intelligent way is capable of possessing mental states and, there-\n",
            "fore, of being truly conscious and intelligent in the same way that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriver = db.as_retriever()"
      ],
      "metadata": {
        "id": "04D3HfjvyJ4Q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    'text2text-generation',\n",
        "    model = 'facebook/bart-large-cnn'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrYIFtqbymbq",
        "outputId": "2ba1a154-0d6d-4df7-c87a-5143672407cc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline = generator)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm = llm,\n",
        "    retriever = retriver\n",
        ")"
      ],
      "metadata": {
        "id": "K1REGeHSy0jm"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "    try:\n",
        "        return qa_chain.run(question)\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ],
      "metadata": {
        "id": "hGy-PSa_zoqn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Fields of Ai?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "noihCYvF2Cg8",
        "outputId": "4681d180-198a-4086-d735-13dca09d010a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The first few chapters of this book provide introductory material. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don’t know, don't try to make up an answer. In studying Artificial Intelligence, it is extremely useful to understand the background of philosophy, linguistics, biology, and psychology.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Answer: {answer_question('define Neural Networks')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT5JoAOF2IV1",
        "outputId": "80a2bf97-3c28-4da7-9e5c-f91d00abd181"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: An artificial neural network is a network of simple processing nodes, roughly modeled on the human brain. The word artificial is often used to describe neural networks, but in this book we shall simply refer to them as neural networks because it should be clear from the context which type of network we are referring to. Use the following pieces of context to answer the question at the end.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    question = input(\"Enter your question (or 'exit' to quit): \")\n",
        "    if question.lower() == 'exit':\n",
        "        break\n",
        "    answer = answer_question(question)\n",
        "    print(f\"Answer: {answer}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqKt3QrW9XIV",
        "outputId": "c5c12b67-707b-4f97-d620-6ffdd8eaa976"
      },
      "execution_count": 49,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question (or 'exit' to quit): Who coined the term Artificial Intelligence and when?\n",
            "Answer: In 1956, the term Artificial Intelligence was first used by John McCarthy at Dartmouth College, in Hanover, New Hampshire. In 1957, Newell and Simon invented the idea of the GPS, whose purpose was to solve almost any logical problem. In studying Artificial Intelligence, it is extremely useful to understand the background of philosophy, linguistics, biology, and psychology.\n",
            "\n",
            "\n",
            "Enter your question (or 'exit' to quit): What is the difference between strong AI and weak AI?\n",
            "Answer: In this chapter, we will look at the contributions made by philosophy, lin-                guistics, psychology, and biology to Artificial Intelligence. We will also look. at the difference between the claims made by proponents of weak AI (AI is. a commonly used abbreviation for Artificial Intelligence) compared with those who support strong AI.\n",
            "\n",
            "\n",
            "Enter your question (or 'exit' to quit): Which programming languages are most associated with early AI research?\n",
            "Answer: A number of programming languages exist that are used to build Artificial Intelligence systems. C++ andJava are often used because these are the languages with which most. scientists have experience. There also exist two programming lan-guages that have features that make them particularly useful for program-ming Artificial Intelligence projects—PROLOG and LISP.\n",
            "\n",
            "\n",
            "Enter your question (or 'exit' to quit): What is the Turing Test designed to measure?\n",
            "Answer: Turing test was designed by Alan Turing as a way to judge the success or not of an attempt to produce a thinking computer. The 1950s were a time of great optimism in Artificial Intelligence. The real way in which a computer proves its humanity is by giving complex answers that a human could not be expected to comprehend.\n",
            "\n",
            "\n",
            "Enter your question (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6XfYgQR60BY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Type your question here... 🤔\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"🌐 Web QA System\",\n",
        "    description=\"Ask any question about the selected websites and get answers from the extracted information! 📝\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "1mJtbk8C9ecS",
        "outputId": "987a05ba-a479-47d2-be28-1a94ebaaae79"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5940b0525d59ed6ed9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5940b0525d59ed6ed9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbtOr1CtB__d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}